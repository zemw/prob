             Introduction to Probability
                   Zeming WANG
                    2025-01-26
                    Table of contents
                    Overview                                                                                              3
                        Syllabus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    3
                        Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      4
                        Lecture notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     5
                        Homework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      5
                        Statistical software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    6
                        Students‚Äô evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      6
                        Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     6
                        Online playground . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .       7
                        Exam score lookup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .       7
                        Copyright ¬© . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     7
                    I    Probability Basics                                                                               8
                    1 What is probability?                                                                                9
                    2 Event and sample space                                                                            10
                             Use set language to describe events . . . . . . . . . . . . . . . . . . . . . . . . .       10
                        Simulating coin flipping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     11
                    3 Classical probability                                                                             13
                        Counting methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .       13
                        Binomial coeÔ¨Äicient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    14
                    4 Gambling problems                                                                                 16
                        Find probability by simulation      . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  18
                    5 Birthday paradox                                                                                  20
                        Rsimulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      21
                    6 Axiomatic probability                                                                             23
                    7 Conditional probability                                                                           26
                    8 Monty Hall problem                                                                                30
                        Rsimulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      31
                                                                       2
                    9 Simpson‚Äôs paradox                                                                                    33
                        Rdemostration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        34
                    10 Independence                                                                                        37
                        Common confusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .          38
                    11 Optimal mating problem*                                                                             39
                    12 Review of calculus*                                                                                 43
                              Differentiation    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   43
                              Integration    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   44
                    13 R tutorial*                                                                                         47
                    II   Random Variables                                                                                  51
                    14 What is a random variable?                                                                          52
                        Numeric encoding of events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .         52
                        Conceptualization of uncertain outcomes . . . . . . . . . . . . . . . . . . . . . . . . .          54
                    15 Data descriptives*                                                                                  56
                    16 Discrete RVs                                                                                        59
                    17 Continuous RVs                                                                                      61
                    18 Cumulative distribution                                                                             63
                    III  Discrete Distributions                                                                            65
                    19 Binomial distribution                                                                               66
                        Coin tossing problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .       67
                        Binomial functions in R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        68
                        Exam survival problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        69
                    20 Discrete expectation                                                                                72
                        Law of averages* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .       73
                    21 Hypergeometric dist                                                                                 74
                    22 Geometric distribution                                                                              76
                    23 Coin flip: HH vs HT*                                                                                79
                                                                        3
        24 Negative Binomial                     82
        25 Bivariate distribution                85
        26 Conditional distribution              88
        27 Poisson distribution                  90
        28 Birthday problem revisited            94
        29 Convolution                           96
        30 Dice rolling formula*                 98
        31 Application: seller ratings*         102
        IV Expectation and Variance             105
        32 Expectation revisited                106
        33 Life expectancy                      110
        34 Two envelope paradox*                112
        35 Linearity and indicators             113
        36 Median and mode                      117
        37 Variance                             120
        38 Covariance                           123
        39 Portfolio allocation*                128
        40 Conditional expectation              130
        41 Moments and MGF                      134
        42 Inequalities*                        137
        V Continuous Distributions              140
        43 Continuous vs Discrete               141
        44 Uniform distribution                 143
                             4
        45 Special integrals                    146
        46 Sum of random variables              148
        47 Normal distribution                  152
        48 Multivariate normal                  156
        49 Exponential distribution             159
        50 Gamma distribution                   164
        51 Beta distribution                    167
        52 Multivariate problems                170
        53 Transformation                       173
        VI Sampling distributions               176
        54 Law of large numbers                 177
        55 Central limit theorem                179
        56 Samples and statistics               182
        57 Estimator accuracy                   184
        58 Confidence intervals                 187
        59 Hypothesis testing                   190
            Hypothesis testing with Ì†µÌ±ß-statistics . . . . . . . . . . . . . . . . . . . . . . . . . 192
            Hypothesis testing with Ì†µÌ±°-statistics . . . . . . . . . . . . . . . . . . . . . . . . . 194
                             5
                Overview
                This book is adapted from MAT921: Probability at Southwest University of Finance and
                Economics (RIEM). It is an introductory probability course that aims to be not boring. In this
                course, we try to blend conventional teaching, interesting puzzles and data-oriented
                practical skills.
                                                                          ‚àö
                Course Instructor‚Äôs email: gamma12@126.com (b/c Œì(1/2) =   Ì†µÌºã is one of my favorite equa-
                tion!) Please indicate your class and student ID when you email me.
                Syllabus
                Topic 1: Classical probabilities
                How likely were some of your classmates born on the same day as you?
                Topic 2: Data and random variables
                Whyis your exam score in this class a random variable?
                                                          6
        Topic 3: Discrete distributions
        How many earthquakes are likely to happen in a random year?
        Topic 4: Expectation and variance
        How old are you expected to live?
        Topic 5: Continuous distributions
        How long are you expected to wait in the queue at a restaurant?
        Topic 6: Limiting theorems
        Whyalottery company never loses?
        Topic 7: Sampling distribution
        How do I know I am taller than an average person?
        Assessment
        Quiz(25%). Therewillbeanarbitrarynumberofin-classquizzes. Thedateforeachquizwill
        be announced in advance. Each quiz will consist of 1-2 questions based on material covered
        in previous weeks. Every quiz is mandatory; there will be no make-up quizzes under any
        circumstances.
        Project (25%). The goal of the projects is to encourage students to apply the knowledge
        learned in this course to solve practical problems. Projects are usually open-ended and may
        involve data analysis, simulations, or exploring real-world applications of probability. Essays
        that present innovative perspectives and use the data persuasively to support their conclusions
        will receive higher marks. Selective students may be invited to present their findings to the
        class.
        Final exam (50%). The final exam will be a closed-book, paper-and-pencil exam scheduled
        on Week 17. It will not simply repeat lecture material but will assess your ability to apply the
        knowledge you have gained to solve novel problems. To perform well, you must have a deep
        understanding of the concepts and acquire some degree of problem-solving skills. The average
        score of the past exam is 69 with a standard deviation 15. The pass rate (>=60) is about
        80%.
        Class participation (5%). Additional 5 marks for class participation on top of the above.
        Regular attendance and active participation in class discussions are encouraged (though not
        mandatory) and will be recognized.
                             7
          Lecture notes
          All lecture materials will be published through this online website. You are not required to read
          any textbook. For students who insist on a textbook, it would be DeGroot and Schervish‚Äôs
          Probability and Statistics (4th edition).
          It is recommended to use the textbook as a supplement not a replacement of the lecture
          note. For students who prefer to read the textbook. There are two key differences between
          this lecture note and the textbook. First, the sections are arranged differently. Second, the
          examples and exercises are entirely different despite the key definitions and theorems are the
          same.
          Homework
          Thereisnohomeworkassignmentinthiscourse. Wewilldoin-classexercisesinstead. However,
          problem solving is essential for learning math. You are encouraged to practice the exercises in
          DeGroot and Schervish‚Äôs textbook after class. But it is not mandatory.
                                    8
                  Statistical software
                  Statistical software is indispensable for modern statistics. For practical consideration, it is ben-
                  eficial to start learn it as early as possible. We will demonstrate how to do statistics in R, which
                  is a widely-used open-source statistical programming language. It is highly recommended that
                  you try it yourself while learning this course.
                  Students‚Äô evaluation
                  If you have not decided whether to enroll in this course or not. Here are some surveys from the
                  past students for your reference. In general, this is not an easy course especially for freshmen.
                  Wewill deal with serious maths, though I will try to convey the beauty of the subject as much
                  as possible.
                  Reference
                     1. Schervish, M. J., & DeGroot, M. H. (2014). Probability and statistics. Pearson Educa-
                        tion.
                     2. Blitzstein, J. K., & Hwang, J. (2019).      Introduction to probability.   Chapman and
                        Hall/CRC.
                                                                 9
          3. Grimmett, G., & Stirzaker, D. (2020). Probability and random processes. Oxford Uni-
           versity Press.
        Online playground
        Probability Playground: Interactive Probability Distributions
        StatKey: Statistics Unlocking the Power of Data
        MathIsFun: Standard Normal Distribution Table
        Central Limit Theorem Demo
        Exam score lookup
        Student number
        Find scores
        Loading exam scores‚Ä¶
        Copyright ¬©
        The content on this website is made available for online viewing by the public. Redistribution,
        reproduction, or any other use of the content, in whole or in part, is prohibited without prior
        written permission from the author.
                            10
                               Part I
                       Probability Basics
                                  11
                  1 What is probability?
                       Probability is the most important concept in modern science, especially as nobody
                       has the slightest notion of what it means. ‚Äî‚Äî B. Russell
                 What is probability? We all talk about probabilities in everyday life, but mostly in vague
                  languages.  This course is to introduce probability as a logical framework for quantifying
                  uncertainty and randomness.
                       Mathematics is the logic of certainty; probability is the logic of uncertainty. ‚Äî‚Äî
                       J. Blitzstein
                 The earliest development of probability is rooted in gambling.    The famous Monte Carlo
                  method in statistics, invented by Stanislaw Ulam in the late 1940s, takes its name from the
                 Monte Carlo Casino in Monaco, where Ulam‚Äôs uncle went to gamble.
                 Today, probability theory has been applied to almost every field of human knowledge. It is
                  the foundation of statistics, machine learning, and artificial intelligence. It also plays a crucial
                  role in everyday decision-making, from stock investments to effective strategies to combat an
                  infectious disease.
                 The first formal definition of probability is often attributed to Pierre-Simon Laplace in the
                 18th century. In his work Theorie analytique des probabilites, published in 1812,
                       The probability of an event is the ratio of the number of cases favorable to it, to
                       the number of all cases possible when nothing leads us to expect that any one of
                       these cases should occur more than any other, which renders them, for us, equally
                       possible.
                 We will soon discover that this definition is obsolete. We start the journey of modern proba-
                  bility theory by introducing the basic concepts of events and sample spaces.
                                                              12
            2 Event and sample space
            Definition 2.1. We use sets to build the foundational concepts in probability:
             ‚Ä¢ Experiment: a procedure with an uncertain outcome
             ‚Ä¢ Event Ì†µÌ∞¥: a set of possible outcomes
             ‚Ä¢ Sample space Ì†µÌ±Ü: the set of all possible outcomes
           Anything (a gamble, an exam, a financial year, ‚Ä¶) with uncertain outcomes can be an ex-
            periment. The sample space can be finite, countably infinite, or uncountably infinite. It is
            convenient to visualize events with Venn diagrams.
             ƒπ Don‚Äôs confuse events with outcomes
             Outcomes are individual results, while events are groups of outcomes that we are inter-
             ested in. Outcomes are elements of the sample space, and events are subsets of this
             space.
            Example 2.1. A coin is flipped twice. We write ‚ÄúH‚Äù if a coin lands Head, and ‚ÄúT‚Äù if a coin
            lands Tail.
             ‚Ä¢ The sample space (all possible outcomes): Ì†µÌ±Ü = {Ì†µÌ∞ªÌ†µÌ∞ª,Ì†µÌ∞ªÌ†µÌ±á,Ì†µÌ±áÌ†µÌ∞ª,Ì†µÌ±áÌ†µÌ±á}
             ‚Ä¢ Let Ì†µÌ∞¥ be the event that the first flip is Heads, Ì†µÌ∞¥ = {Ì†µÌ∞ªÌ†µÌ∞ª,Ì†µÌ∞ªÌ†µÌ±á}
                   1                           1
             ‚Ä¢ Let Ì†µÌ∞¥ be the event that the second flip is Heads, Ì†µÌ∞¥ = {Ì†µÌ∞ªÌ†µÌ∞ª,Ì†µÌ±áÌ†µÌ∞ª}
                   2                            2
             ‚Ä¢ Let Ì†µÌ∞µ be the event that at least one flip is Heads, Ì†µÌ∞µ = Ì†µÌ∞¥ ‚à™ Ì†µÌ∞¥
                                                   1   2
             ‚Ä¢ Let Ì†µÌ∞∂ be the event that all the flips are Heads, Ì†µÌ∞∂ = Ì†µÌ∞¥ ‚à© Ì†µÌ∞¥
                                                  1  2
                                              Ì†µÌ±ê
             ‚Ä¢ Let Ì†µÌ∞∑ be the event that no flip is Heads, Ì†µÌ∞∑ = Ì†µÌ∞µ
            Use set language to describe events
            English                                       Sets
            sample space                                  Ì†µÌ±Ü
            Ì†µÌ±† is a possible outcome                     Ì†µÌ±† ‚àà Ì†µÌ±Ü
            Ì†µÌ∞¥ is an event                               Ì†µÌ∞¥ ‚äÜ Ì†µÌ±Ü
            Ì†µÌ∞¥ or Ì†µÌ∞µ occurs                              Ì†µÌ∞¥ ‚à™ Ì†µÌ∞µ
                                        13
                  English                                                                   Sets
                  Both Ì†µÌ∞¥ and Ì†µÌ∞µ occur                                                     Ì†µÌ∞¥ ‚à© Ì†µÌ∞µ
                                                                                               Ì†µÌ±ê
                  Ì†µÌ∞¥ does not occur                                                          Ì†µÌ∞¥
                  at least one of Ì†µÌ∞¥ ,‚Ä¶,Ì†µÌ∞¥  occur                                       Ì†µÌ∞¥ ‚à™‚ãØ‚à™Ì†µÌ∞¥
                                   1      Ì†µÌ±õ                                              1        Ì†µÌ±õ
                  all of Ì†µÌ∞¥ ,‚Ä¶,Ì†µÌ∞¥  occur                                                Ì†µÌ∞¥ ‚à©‚ãØ‚à©Ì†µÌ∞¥
                          1      Ì†µÌ±õ                                                       1        Ì†µÌ±õ
                  Ì†µÌ∞¥ implies Ì†µÌ∞µ                                                            Ì†µÌ∞¥ ‚äÜ Ì†µÌ∞µ
                  Definition 2.2. Ì†µÌ∞¥ and Ì†µÌ∞µ are disjoint (mutually exclusive) if Ì†µÌ∞¥ ‚à© Ì†µÌ∞µ = Ì†µÌºô.
                  Definition 2.3. Ì†µÌ∞¥ ,‚Ä¶,Ì†µÌ∞¥ are a partition of Ì†µÌ±Ü if
                                      1      Ì†µÌ±õ
                     ‚Ä¢ Ì†µÌ∞¥ ‚à™‚ãØ‚à™Ì†µÌ∞¥ =Ì†µÌ±Ü, and
                          1        Ì†µÌ±õ
                     ‚Ä¢ Ì†µÌ∞¥ ‚à©Ì†µÌ∞¥ =Ì†µÌºô for Ì†µÌ±ñ ‚â† Ì†µÌ±ó.
                          Ì†µÌ±ñ   Ì†µÌ±ó
                  Simulating coin flipping
                  Randomly sampling from the set {Ì†µÌ∞ª,Ì†µÌ±á}:
                  # 10 draws with equal prob with replacement
                  sample(c('H', 'T'), 10, replace = T)
                   [1] "T" "T" "H" "H" "T" "T" "T" "T" "H" "T"
                  Compute the probability of Ì†µÌ∞ªÌ†µÌ∞ª when tossing two coins:
                  # simulate coin tossing 10000 times
                  toss <- sample(c('H','T'), 10000, replace =T)
                  # group them into pairs
                  toss.pair <- paste0(toss[-length(toss)], toss[-1])
                  # number of HH
                  n_HH <- sum(toss.pair == 'HH')
                  # total number of tosses
                  n_total <- length(toss.pair)
                  # compute the probability
                                                                14
               prob <- n_HH / n_total
               cat("Prob of HH: ", prob)
               Prob of HH:  0.2551255
                                                     15
                        3 Classical probability
                        Definition 3.1. Classical (naive) definition of probability:
                                                                |Ì†µÌ∞¥|     number of outcomes favorable to A
                                                     Ì†µÌ±É(Ì†µÌ∞¥) =        =
                                                                |Ì†µÌ±Ü|        total number of outcomes in A
                        assuming the outcomes are finite and equally likely.
                           ƒπ Don‚Äôt misuse the classical definition
                           The classical probability is very restrictive. It only applies to scenarios such as flipping
                           coins or rolling dice where the outcomes are equally likely. It has often been misapplied
                           by people who assume equally likely outcomes without justification. For example, if one
                           wants calculate the probability of a rainy day, it would be misleading to assume every
                                                                                   rainy days
                           day is equally likely to rain and compute                          .
                                                                                       365
                        Calculating the naive probability of an event Ì†µÌ∞¥ often involves counting the number of out-
                        comes in Ì†µÌ∞¥ and the number of outcomes in the sample space Ì†µÌ±Ü, which usually involve some
                        counting methods. We now review some of the counting methods (multiplications, factorials,
                        permutations, combinations) that was introduced in high schools.
                        Counting methods
                        Multiplications. Consider a compound experiment consisting of two sub-experiments, Ex-
                        periment A and Experiment B. Suppose that Experiment A has Ì†µÌ±é possible outcomes, and for
                        eachofthoseoutcomesExperimentBhasÌ†µÌ±èpossibleoutcomes. Thenthecompoundexperiment
                        has Ì†µÌ±é √ó Ì†µÌ±è possible outcomes.
                        Exponentiation. Consider Ì†µÌ±õ objects and making Ì†µÌ±ò choices from them, one at a time with
                                                                 Ì†µÌ±ò
                        replacement. Then there are Ì†µÌ±õ possible outcomes.
                        Factorials. Consider Ì†µÌ±õ objects 1,2,‚Ä¶,Ì†µÌ±õ. A permutation of 1,2,‚Ä¶,Ì†µÌ±õ is an arrangement of
                        them in some order, e.g., 3,5,1,2,4 is a permutation of 1,2,3,4,5. The are Ì†µÌ±õ! permutations
                        of 1,2,‚Ä¶,Ì†µÌ±õ.
                                                                                     16
                                     Permutations. Consider Ì†µÌ±õ objects and making Ì†µÌ±ò choices from them, one at a time without
                                                                                                                                                                  Ì†µÌ±õ!
                                                                                                    Ì†µÌ±ò
                                     replacement. Then there are Ì†µÌ±É                                     =Ì†µÌ±õ(Ì†µÌ±õ‚àí1)‚ãØ(Ì†µÌ±õ‚àíÌ†µÌ±ò+1) =                                              possible outcomes, for Ì†µÌ±ò ‚â§ Ì†µÌ±õ.
                                                                                                   Ì†µÌ±õ
                                                                                                                                                              (Ì†µÌ±õ‚àíÌ†µÌ±ò)!
                                    (Ordering matters in this case, e.g. 1,2,3 is considered different from 2,3,1)
                                     Combinations. Consider Ì†µÌ±õ objects and making Ì†µÌ±ò choices from them, one at a time without
                                     replacement, without distinguishing between the different orders in which they could be chosen
                                                                                                                                                                                       Ì†µÌ±õ(Ì†µÌ±õ‚àí1)‚ãØ(Ì†µÌ±õ‚àíÌ†µÌ±ò+1)
                                                                                                                                                                              Ì†µÌ±ò
                                    (e.g. 1,2,3 is considered no different from 2,3,1). Then there are Ì†µÌ∞∂ =                                                                                                          possible
                                                                                                                                                                              Ì†µÌ±õ
                                                                                                                                                                                                    Ì†µÌ±ò!
                                     outcomes. In modern math, we prefer the notation
                                                                                                                             Ì†µÌ±õ
                                                                                                                                             Ì†µÌ±ò
                                                                                                                         ( )‚â°Ì†µÌ∞∂ ,
                                                                                                                                             Ì†µÌ±õ
                                                                                                                             Ì†µÌ±ò
                                    which reads as ‚ÄúÌ†µÌ±õ choose Ì†µÌ±ò‚Äù.
                                    The following table summarizes the counting methods.
                                                                                                                   order matters                     order doesn‚Äôt matter
                                                                                                                                                                    Ì†µÌ±õ+Ì†µÌ±ò‚àí1
                                                                                                                                  Ì†µÌ±ò
                                                                           with replacement                                    Ì†µÌ±õ                                 (             )
                                                                                                                                                                         Ì†µÌ±ò
                                                                                                                                                                         Ì†µÌ±õ
                                                                                                                                Ì†µÌ±õ!
                                                                           non-replacement                                                                             ( )
                                                                                                                            (Ì†µÌ±õ‚àíÌ†µÌ±ò)!                                     Ì†µÌ±ò
                                    The upper-right corner case is equivalent to putting Ì†µÌ±ò indistinguishable balls into Ì†µÌ±õ distin-
                                     guishable baskets (e.g. two balls in Basket 3 means the 3rd object is chosen twice). Therefore,
                                                                                                                         Ì†µÌ±ò+Ì†µÌ±õ‚àí1
                                     the number of possible arrangements is (                                                        ).
                                                                                                                           Ì†µÌ±õ‚àí1
                                     Binomial coeÔ¨Äicient
                                                                                           Ì†µÌ±õ
                                    The Binomial coeÔ¨Äicient ( ) counts the number of subsets of size Ì†µÌ±ò for a set of size Ì†µÌ±õ. It is
                                                                                           Ì†µÌ±ò
                                                                                    Ì†µÌ±ò                                                                                    Ì†µÌ±õ
                                     also the coeÔ¨Äicient of Ì†µÌ±• when expanding the polynomial (Ì†µÌ±• +Ì†µÌ±¶) .
                                     Theorem 3.1 (Binomial theorem).
                                                                                                                                  Ì†µÌ±õ
                                                                                                                                           Ì†µÌ±õ
                                                                                                                       Ì†µÌ±õ                           Ì†µÌ±ò   Ì†µÌ±õ‚àíÌ†µÌ±ò
                                                                                                         (Ì†µÌ±• + Ì†µÌ±¶)         =‚àë( )Ì†µÌ±• Ì†µÌ±¶                           .
                                                                                                                                           Ì†µÌ±ò
                                                                                                                                Ì†µÌ±ò=0
                                                                                                                                   17
             ThecoeÔ¨Äicients form an infinite triangle called the Pascal triangle. By observing the patterns
             in the triangle, it is not hard to conclude the following recursive formula:
                                     Ì†µÌ±õ    Ì†µÌ±õ ‚àí 1  Ì†µÌ±õ ‚àí 1
                                    ( )=(      )+(     )
                                     Ì†µÌ±ò    Ì†µÌ±ò ‚àí 1   Ì†µÌ±ò
             The value of the binomial coeÔ¨Äicient is only defined for non-negative integers Ì†µÌ±õ and Ì†µÌ±ò with
             0 ‚â§ Ì†µÌ±ò ‚â§ Ì†µÌ±õ. But mathematics is about generalization. We can generalize the notion of ‚ÄúÌ†µÌ±õ
             choose Ì†µÌ±ò‚Äù for negative Ì†µÌ±õ:
                                       Ì†µÌ±ò‚àí1          Ì†µÌ±ò‚àí1
                                  ‚àíÌ†µÌ±õ     ‚àíÌ†µÌ±õ‚àíÌ†µÌ±ñ        Ì†µÌ±õ + Ì†µÌ±ñ
                                                    Ì†µÌ±ò
                                ( )=‚àè          =(‚àí1) ‚àè
                                  Ì†µÌ±ò      Ì†µÌ±ò ‚àí Ì†µÌ±ñ       Ì†µÌ±ò ‚àí Ì†µÌ±ñ
                                       Ì†µÌ±ñ=0          Ì†µÌ±ñ=0
                                           Ì†µÌ±õ(Ì†µÌ±õ + 1)‚Ä¶(Ì†µÌ±õ + Ì†µÌ±ò ‚àí 1)
                                          Ì†µÌ±ò
                                     =(‚àí1)
                                                   Ì†µÌ±ò!
                                           (Ì†µÌ±õ + Ì†µÌ±ò ‚àí 1)!
                                          Ì†µÌ±ò
                                     =(‚àí1)
                                            Ì†µÌ±ò!(Ì†µÌ±õ ‚àí 1)!
                                            Ì†µÌ±õ + Ì†µÌ±ò ‚àí 1
                                          Ì†µÌ±ò
                                     =(‚àí1) (       )
                                               Ì†µÌ±ò
             Wecan also extend the formula to real numbers and even complex numbers:
                                     Ì†µÌ±•       Œì(Ì†µÌ±• + 1)
                                    ( )=
                                     Ì†µÌ±¶  Œì(Ì†µÌ±¶ + 1)Œì(Ì†µÌ±• ‚àí Ì†µÌ±¶ + 1)
             where Œì(Ì†µÌ±•+1) = Ì†µÌ±•! is a generalization of factorials. We will come back to the Gamma function
             when we discuss Gamma distributions.
                                             18
               4 Gambling problems
               Example 4.1. Texas hold‚Äôem is one of the most popular variant of the card game of poker.
               Essentially, the players in the game bet on the rankings of their hand of five cards (illustrated in
               the figure below). For the game to be fair, a hand of higher values must have lower probability
               than a hand of lower values. Compute the probability for each type of hand.
               Solution. To apply Definition 3.1, we first determine the total number of possible five-card
               hands from a standard 52-card deck. The total number of combinations is:
                                    52    52√ó51√ó50√ó49√ó48
                                  ( )=                       =2,598,960
                                    5       5√ó4√ó3√ó2√ó1
               We then find the number of combinations for each hand type. As an illustration, we only
               compute the case of Full House.
               Afull house consists of three cards of one rank and two cards of another rank. The number of
                                                 13
               ways to choose the rank for the triplet is ( ) = 13 , and the number of ways to choose 3 cards
                                                 1
                            4                                                  12
               of that rank is ( ) = 4 . The number of ways to choose the rank for the pair is ( ) = 12 , and
                            3                                                  1
                                                    19
                                                                               4
                    the number of ways to choose 2 cards of that rank is ( ) = 6 . Therefore, the total number of
                                                                               2
                                     13  4    12  4
                    full houses is: (  )( ) ‚ãÖ ( )( ) = 3,744.
                                     1   3    1   2
                    Thus, the probability is:
                                                                        3,744
                                                  Ì†µÌ±É(Full House) =               ‚âà0.144%
                                                                     2,598,960
                    Example 4.2. (Newton-Pepys problem) The Newton-Pepys problem is a classical probabil-
                    ity problem involving dice rolls, posed in correspondence between Samuel Pepys, a famous
                    diarist and government oÔ¨Äicial, and Isaac Newton in 1693. The problem concerns which of the
                    following three events is most likely when rolling fair dice:
                       1. At least one 6 appears when 6 fair dice are rolled.
                       2. At least two 6‚Äôs appear when 12 fair dice are rolled.
                       3. At least three 6‚Äôs appear when 18 fair dice are rolled.
                    Solution. We compute the probabilities for each scenario:
                       1. Probability of at least one 6 in six rolls of a single die:
                          The probability of not rolling a 6 in six rolls is:
                                                                                      6
                                                                                     5
                                                             Ì†µÌ±É(no 6 in six rolls) =
                                                                                      6
                                                                                     6
                          Thus, the probability of at least one 6 in six rolls is:
                                                                                   6
                                                                                  5
                                                        Ì†µÌ±É(at least one 6) = 1 ‚àí      ‚âà0.67
                                                                                   6
                                                                                  6
                       2. Probability of at least two 6s in twelve rolls of a single die:
                          We use the complement, finding the probabilities of getting 0 or 1 six. The probability
                          of getting exactly 0 sixes in twelve rolls is similar as above. The probability of getting
                          exactly 1 six is:
                                                                                   11
                                                                             12 5
                                                                                )
                                                               Ì†µÌ±É(1 six) = (
                                                                                   12
                                                                              1   6
                          Thus, the probability of at least two 6s is:
                                              Ì†µÌ±É(at least two 6s) = 1 ‚àí Ì†µÌ±É(0 six) ‚àí Ì†µÌ±É(1 six) ‚âà 0.62
                                                                      20
                 3. Probability of at least three 6s in eighteen rolls of a single die:
                    Similarly, we calculate the complement, finding the probabilities of getting fewer than 3
                    sixes.
                                Ì†µÌ±É(at least three 6s) = 1 ‚àí Ì†µÌ±É(0 six) ‚àí Ì†µÌ±É(1 six) ‚àí Ì†µÌ±É(2 sixes)
                                                            18      18
                                                       18      17      16
                                                      5  +( )5 +( )5
                                                            1       2
                                                 =1‚àí                      ‚âà0.60
                                                               18
                                                              6
               Thus, Ì†µÌ±É(one 6 in 6 rolls) > Ì†µÌ±É(two 6s in 12 rolls) > Ì†µÌ±É(three 6s in 18 rolls).
               Intuitively, this is because as the number of dice increases, the likelihood of matching higher
               thresholds does not keep pace with the probability of rolling individual sixes. This is perhaps
               contrary to most people‚Äôs common sense: the more dice rolled, the more likely to see the sixes.
               Find probability by simulation
               Let‚Äôs redo Example 4.1 by simulation.
               # generate a deck of cards
               deck.grid <- expand.grid(c(1:10,'J','Q','K','A'), c('Ôøø','Ôøø','Ôøø','Ôøø'))
               # convert the grid to a vector
               deck <- do.call(paste0, deck.grid)
               # total number of simulations
               N <- 100000
               # number of target hand
               K <- 0
               # for random generator
               set.seed(1000)
               for (j in 1:N) {
                 # a random five-cards hand
                 hand <- sample(deck, 5)
                 # drop the color
                 num <- substr(hand, 1, nchar(hand)-1)
                                                     21
         # Full House have only two distinguished numbers
         if ( length(unique(num)) == 2 ) {
          K <- K + 1
         }
        }
        # compute the probability
        P <- K / N
        sprintf("Prob of Full House: %.3f %%", P*100)
        [1] "Prob of Full House: 0.146 %"
                            22
              5 Birthday paradox
              The birthday problem is a classic probability puzzle that demonstrates how likely it is for at
              least two people in a group to share the same birthday. While it might seem intuitive that the
              probability is low in small groups, the results are surprising.
              Example 5.1. In a group of Ì†µÌ±ò people, what is the probability that at least two people share
              the same birthday? Assume (1) there are 365 possible birthdays; (2) birthdays are evenly
              distributed across the year; (3) people are equally likely to be born on any given day.
              Solution. If Ì†µÌ±ò > 365, the probability is 1. Assume Ì†µÌ±ò ‚â§ 365 for the rest. Instead of directly
              calculating the probability of at least two people sharing a birthday, we first compute the
              complementary probability, Ì†µÌ±É(no match) , where no two people in the group have the same
              birthday.
              For the first person, there are 365 choices for their birthday. For the second person, to avoid a
              shared birthday, there are 364 remaining choices. For the third person, there are 363 choices,
              and so on. For Ì†µÌ±ò people, the total number of arrangements (no shared birthday) is:
                                   365√ó364√ó363√ó‚ãØ√ó(365‚àíÌ†µÌ±ò+1)
                                                                             Ì†µÌ±ò
              The total number of possible arrangements (with or without shared birthdays) is 365 . Thus,
              the probability of no shared birthday is:
                                             365‚ãÖ364‚ãØ(365‚àíÌ†µÌ±ò+1)
                                 Ì†µÌ±É(no match) =
                                                       Ì†µÌ±ò
                                                     365
              Thus, the probability of at least one matched birthday is:
                                                      ‚éß
                                                        50.7% Ì†µÌ±ò = 23
                                                      {
                                                      {
                                                        70.6% Ì†µÌ±ò = 30
                              Ì†µÌ±É(match) = 1‚àíÌ†µÌ±É(no match) =
                                                      ‚é®
                                                        97.0% Ì†µÌ±ò = 50
                                                      {
                                                      {
                                                        99.9% Ì†µÌ±ò = 70
                                                      ‚é©
                                                23
        R simulation
        # a class of k people
        k <- 30
        # number of experiments
        N <- 1000
        # number of matches
        m <- 0
        for (i in 1:N) {
         # draw k random numbers from 1 to 365 with replacement
         birthdays <- sample(1:365, k, replace = T)
         # number of duplicated birthdays
         dups <- duplicated(birthdays)
         # if duplicated birthdays found
         if (any(dups)) m <- m + 1
        }
        cat("Prob of at least one match: ", m / N)
        Prob of at least one match: 0.696
        There is even a built-in function pbirthday that computes the probability of birthday coinci-
        dence. We may utilize this function to plot the probability as the number of people increases.
        # compute the probability of birthday match for 30 people
        prob <- pbirthday(30)
        # compute a vector of probabilities for 1,2...100 people
        probs <- sapply(1:100, pbirthday)
        # make a plot
        plot(1:100, probs, type="l",main = "Probability of >1 people with the same birthday")
                            24
             Probability of >1 people with the same birthday
            0.8
         probs0.4
            0.0
              0    20   40    60   80   100
                          1:100
                            25
                   6 Axiomatic probability
                   Definition 6.1. A probability space consists of Ì†µÌ±Ü and Ì†µÌ±É, where Ì†µÌ±Ü is a sample space, and Ì†µÌ±É is
                   a function which takes an event Ì†µÌ∞¥ ‚äÜ Ì†µÌ±Ü as input and returns Ì†µÌ±É(Ì†µÌ∞¥) ‚àà [0,1] such that
                      ‚Ä¢ Ì†µÌ±É(Ì†µÌºô) = 0,
                      ‚Ä¢ Ì†µÌ±É(Ì†µÌ±Ü) = 1,
                                           ‚àû
                             ‚àû
                      ‚Ä¢ Ì†µÌ±É(‚à™     Ì†µÌ∞¥ ) = ‚àë      Ì†µÌ±É(Ì†µÌ∞¥ ) if Ì†µÌ∞¥ ,Ì†µÌ∞¥ ,‚Ä¶,Ì†µÌ∞¥  are disjoint.
                                   Ì†µÌ±õ              Ì†µÌ±õ     1   2       Ì†µÌ±õ
                             Ì†µÌ±õ=1
                                           Ì†µÌ±õ=1
                   Note that this Definition does not imply any particular interpretation of probability. In fact,
                   any function Ì†µÌ±É that satisfies the axioms are valid ‚Äúprobabilities‚Äù. Thus, the theories of proba-
                   bility do not depend on any particular interpretation. It is purely axiomatic. From the three
                   axioms, we can derive any property of probabilities. The interpretation also matters, but it is
                   more of a philosophical debate.
                     ƒπ Two interpretations of probability
                        ‚Ä¢ The frequentist view of probability is that it represents a long-run frequency over a
                           large number of repetitions of an experiment: if we say a coin has probability 1/2
                           of Heads, that means the coin would land Heads 50% of the time if we tossed it
                           over and over and over.
                        ‚Ä¢ The Bayesian view of probability is that it represents a degree of belief about the
                           event in question, so we can assign probabilities to hypotheses like ‚Äúcandidate A
                           will win the election‚Äù or ‚Äúthe defendant is guilty‚Äù even if it isn‚Äôt possible to repeat
                           the same election or the same crime over and over again.
                   Proposition 6.1. For any events Ì†µÌ∞¥ and Ì†µÌ∞µ, we have
                              Ì†µÌ±ê
                      ‚Ä¢ Ì†µÌ±É(Ì†µÌ∞¥ ) = 1‚àíÌ†µÌ±É(Ì†µÌ∞¥)
                      ‚Ä¢ If Ì†µÌ∞¥ ‚äÜ Ì†µÌ∞µ, then Ì†µÌ±É(Ì†µÌ∞¥) ‚â§ Ì†µÌ±É(Ì†µÌ∞µ).
                      ‚Ä¢ Ì†µÌ±É(Ì†µÌ∞¥‚à™Ì†µÌ∞µ) = Ì†µÌ±É(Ì†µÌ∞¥)+Ì†µÌ±É(Ì†µÌ∞µ)‚àíÌ†µÌ±É(Ì†µÌ∞¥‚à©Ì†µÌ∞µ).
                   Proof. We prove the three properties by just using the Axioms.
                                        Ì†µÌ±ê
                      1) Since Ì†µÌ∞¥ and Ì†µÌ∞¥ are disjoint and their union is Ì†µÌ±Ü, apply the third axiom:
                                                                     Ì†µÌ±ê                 Ì†µÌ±ê
                                                    Ì†µÌ±É(Ì†µÌ±Ü) = Ì†µÌ±É(Ì†µÌ∞¥ ‚à™ Ì†µÌ∞¥ ) = Ì†µÌ±É(Ì†µÌ∞¥) + Ì†µÌ±É(Ì†µÌ∞¥ );
                                                                            Ì†µÌ±ê
                         By the second axiom, Ì†µÌ±É(Ì†µÌ±Ü) = 1. So Ì†µÌ±É(Ì†µÌ∞¥) + Ì†µÌ±É(Ì†µÌ∞¥ ) = 1.
                                                                   26
                                                                                                                                               Ì†µÌ±ê
                            2) The key is to break up the set into disjoint sets. If Ì†µÌ∞¥ ‚äÜ Ì†µÌ∞µ, then Ì†µÌ∞µ = Ì†µÌ∞¥‚à™(Ì†µÌ∞µ‚à©Ì†µÌ∞¥ ) where
                                                   Ì†µÌ±ê
                                 Ì†µÌ∞¥ and Ì†µÌ∞µ ‚à© Ì†µÌ∞¥ are disjoint (draw a Venn diagram for intuition). By the third axiom, we
                                 have
                                                                                      Ì†µÌ±ê                             Ì†µÌ±ê
                                                       Ì†µÌ±É(Ì†µÌ∞µ) = Ì†µÌ±É(Ì†µÌ∞¥ ‚à™ (Ì†µÌ∞µ ‚à© Ì†µÌ∞¥ )) = Ì†µÌ±É(Ì†µÌ∞¥) + Ì†µÌ±É(Ì†µÌ∞µ ‚à© Ì†µÌ∞¥ ) ‚â• Ì†µÌ±É(Ì†µÌ∞¥).
                                                                                                                             Ì†µÌ±ê
                            3) We can write Ì†µÌ∞¥ ‚à™ Ì†µÌ∞µ as the union of the disjoint set Ì†µÌ∞¥ and Ì†µÌ∞µ ‚à© Ì†µÌ∞¥ . Then by the third
                                 axiom,
                                                                                               Ì†µÌ±ê                             Ì†µÌ±ê
                                                          Ì†µÌ±É(Ì†µÌ∞¥ ‚à™ Ì†µÌ∞µ) = Ì†µÌ±É(Ì†µÌ∞¥ ‚à™ (Ì†µÌ∞µ ‚à© Ì†µÌ∞¥ )) = Ì†µÌ±É(Ì†µÌ∞¥) + Ì†µÌ±É(Ì†µÌ∞µ ‚à© Ì†µÌ∞¥ ).
                                                                               Ì†µÌ±ê                                                                  Ì†µÌ±ê
                                 It suÔ¨Äices to show that Ì†µÌ±É(Ì†µÌ∞µ ‚à© Ì†µÌ∞¥ ) = Ì†µÌ±É(Ì†µÌ∞µ) ‚àí Ì†µÌ±É(Ì†µÌ∞¥ ‚à© Ì†µÌ∞µ). Since Ì†µÌ∞µ ‚à© Ì†µÌ∞¥ and Ì†µÌ∞µ ‚à© Ì†µÌ∞¥ are
                                 disjoint, we have
                                                                                                                Ì†µÌ±ê
                                                                       Ì†µÌ±É(Ì†µÌ∞µ) = Ì†µÌ±É(Ì†µÌ∞µ ‚à© Ì†µÌ∞¥) + Ì†µÌ±É(Ì†µÌ∞µ ‚à© Ì†µÌ∞¥ ).
                                                 Ì†µÌ±ê
                                 So Ì†µÌ±É(Ì†µÌ∞µ ‚à© Ì†µÌ∞¥ ) = Ì†µÌ±É(Ì†µÌ∞µ) ‚àí Ì†µÌ±É(Ì†µÌ∞¥ ‚à© Ì†µÌ∞µ) as desired.
                         The last property is a very useful formula for finding the probability of a union of events when
                         the events are not necessarily disjoint. We can generalize it to Ì†µÌ±õ events.
                         Theorem 6.1. For any events Ì†µÌ∞¥ ,Ì†µÌ∞¥ ,‚Ä¶,Ì†µÌ∞¥ , it holds that
                                                                       1     2        Ì†µÌ±õ
                                                                   Ì†µÌ±õ
                                   Ì†µÌ±É(Ì†µÌ∞¥ ‚à™Ì†µÌ∞¥ ‚ãØ‚à™Ì†µÌ∞¥ ) = ‚àëÌ†µÌ±É(Ì†µÌ∞¥ )‚àí‚àëÌ†µÌ±É(Ì†µÌ∞¥ ‚à©Ì†µÌ∞¥ )+ ‚àë Ì†µÌ±É(Ì†µÌ∞¥ ‚à©Ì†µÌ∞¥ ‚à©Ì†µÌ∞¥ )‚àí‚ãØ
                                         1      2          Ì†µÌ±õ                Ì†µÌ±ó               Ì†µÌ±ñ     Ì†µÌ±ó                  Ì†µÌ±ñ     Ì†µÌ±ó     Ì†µÌ±ò
                                                                                    Ì†µÌ±ñ<Ì†µÌ±ó
                                                                  Ì†µÌ±ó=1
                                                                                                            Ì†µÌ±ñ<Ì†µÌ±ó<Ì†µÌ±ò
                                                                       Ì†µÌ±õ+1
                                                                 (‚àí1)       Ì†µÌ±É(Ì†µÌ∞¥ ‚à©‚ãØ‚à©Ì†µÌ∞¥ ).
                                                                                  1            Ì†µÌ±õ
                         This formula can be proved by induction using the axioms. Below is a famous application
                        (known as de Montmort‚Äôs problem, named after French mathematician Pierre Remond de
                         Montmort) of the inclusion-exclusion theorem.
                         Example 6.1 (Matching problem). Suppose there are Ì†µÌ±õ people who each check in a hat at a
                         party. The hats are randomly returned to them without any concern for whose hat is whose.
                         What is the probability that at least one person gets their own hat back?
                         Solution. Let Ì†µÌ∞¥ be the event: the Ì†µÌ±ó-th person gets his own hat. The problem is equivalent to
                                              Ì†µÌ±ó
                         find Ì†µÌ±É(Ì†µÌ∞¥ ‚à™ Ì†µÌ∞¥ ‚à™‚ãØ‚à™Ì†µÌ∞¥ ).
                                     1       2            Ì†µÌ±õ
                                                                                         1
                         Since all position are equally likely, Ì†µÌ±É(Ì†µÌ∞¥ ) =                  . The probability of there being two matches
                                                                                  Ì†µÌ±ó
                                                                                         Ì†µÌ±õ
                                                    (Ì†µÌ±õ‚àí2)!
                                                                     1
                         is:  Ì†µÌ±É(Ì†µÌ∞¥ ‚à© Ì†µÌ∞¥ ) =                 =            .  Similarly, the probability of there being Ì†µÌ±ò matches is:
                                    1       2
                                                       Ì†µÌ±õ!        Ì†µÌ±õ(Ì†µÌ±õ‚àí1)
                                                                                        27
                                              (Ì†µÌ±õ‚àíÌ†µÌ±ò)!
                                                                1
                      Ì†µÌ±É(Ì†µÌ∞¥ ‚à©‚ãØ‚à©Ì†µÌ∞¥ ) =                =                    . Using the property of the union of events,
                           1           Ì†µÌ±ò
                                                Ì†µÌ±õ!     Ì†µÌ±õ(Ì†µÌ±õ‚àí1)‚ãØ(Ì†µÌ±õ‚àíÌ†µÌ±ò+1)
                                                                    1      Ì†µÌ±õ       1           Ì†µÌ±õ          1
                                  Ì†µÌ±É(Ì†µÌ∞¥ ‚à™Ì†µÌ∞¥ ‚à™‚ãØ‚à™Ì†µÌ∞¥ ) =Ì†µÌ±õ‚ãÖ              ‚àí( )                 +( )                         ‚àí‚ãØ
                                        1     2           Ì†µÌ±õ
                                                                    Ì†µÌ±õ     2 Ì†µÌ±õ(Ì†µÌ±õ ‚àí 1)         3 Ì†µÌ±õ(Ì†µÌ±õ ‚àí 1)(Ì†µÌ±õ ‚àí 2)
                                                                     1     1      1                     1
                                                                                                   Ì†µÌ±õ+1
                                                             =1‚àí + ‚àí +‚ãØ+(‚àí1)
                                                                     2!    3!    4!                    Ì†µÌ±õ!
                                                                     1
                                                             ‚âà1‚àí .
                                                                     Ì†µÌ±í
                         ƒé Pattern matching with Taylor series
                         Pattern matching is a very useful technique. In the last step, we recognize that the Taylor
                                     Ì†µÌ±•
                         series of Ì†µÌ±í  is
                                                                                 2      3
                                                                               Ì†µÌ±•     Ì†µÌ±•
                                                                Ì†µÌ±•
                                                               Ì†µÌ±í =1+Ì†µÌ±•+           +      +‚ãØ
                                                                                2!     3!
                                                        1     1
                                        ‚àí1
                         Therefore, Ì†µÌ±í     =1‚àí1+ ‚àí +‚ãØ
                                                        2!    3!
                      Example6.2(Infinitemonkeytheorem). Amonkeyhittingkeysindependentlyandatrandom
                      on a typewriter keyboard for an infinite amount of time will almost surely type any given text
                      (e.g. the complete works of William Shakespeare). In other words, an infinite random sequence
                      of letters contain every finite string infinitely often with probability 1.
                      Proof. Let‚Äôs compute the probability of the monkey typing ‚Äúbanana‚Äù correctly with random
                      strokes. Suppose there are 50 keys on the keyboard. The monkey typed correctly ‚Äúbanana‚Äù is
                       1
                          . Suppose the monkey tried Ì†µÌ±õ times, the probability that he did not typed the correct text
                         6
                      50
                      is
                                                                                         Ì†µÌ±õ
                                                                                    1
                                                                                       )
                                                                    Ì†µÌ±ã =(1‚àí
                                                                       Ì†µÌ±õ
                                                                                      6
                                                                                   50
                                                                                                                                       6
                      For finite Ì†µÌ±õ (even Ì†µÌ±õ is very large), Ì†µÌ±ã       would be very close to 1. For example, when Ì†µÌ±õ = 10 ,
                                                                    Ì†µÌ±õ
                      Ì†µÌ±ã   ‚âà0.9999. But as Ì†µÌ±õ ‚Üí ‚àû, Ì†µÌ±ã ‚Üí 0. That means, if Ì†µÌ±õ is infinitely large, the probability
                        Ì†µÌ±õ                                     Ì†µÌ±õ
                      that the monkey produced the correct text goes to 1.
                      The theorem reminds us that infinite limits behave very differently from large finite numbers.
                      It is also used as a metaphor: given enough time, randomness can generate order, structure,
                      or meaning.
                                                                              28
                 7 Conditional probability
                 The probability of A conditioned on B is the updated probability of event A after we learn
                 that event B has occurred. Since events contain information, the occurring of a certain event
                 may change our believes on probabilities of other relevant events. The updated probability of
                 event A after we learn that event B has occurred is the conditional probability of A given B.
                 Definition 7.1. If Ì†µÌ∞¥ and Ì†µÌ∞µ are events with Ì†µÌ±É(Ì†µÌ∞µ) > 0, then the conditional probability of Ì†µÌ∞¥
                 given Ì†µÌ∞µ is defined as
                                                             Ì†µÌ±É(Ì†µÌ∞¥ ‚à© Ì†µÌ∞µ)
                                                  Ì†µÌ±É(Ì†µÌ∞¥|Ì†µÌ∞µ) =         .
                                                               Ì†µÌ±É(Ì†µÌ∞µ)
                   ƒπ Don‚Äôt confuse P(A|B) with P(A,B)
                   Ì†µÌ±É(Ì†µÌ∞¥|Ì†µÌ∞µ) is the probability of Ì†µÌ∞¥ occurring given that Ì†µÌ∞µ has already occurred. While
                   Ì†µÌ±É(Ì†µÌ∞¥,Ì†µÌ∞µ) = Ì†µÌ±É(Ì†µÌ∞¥ ‚à© Ì†µÌ∞µ) is the probability that Ì†µÌ∞¥ and Ì†µÌ∞µ occur simultaneously.
                 Proposition 7.1. Properties of conditional probability:
                   ‚Ä¢ Ì†µÌ±É(Ì†µÌ∞¥‚à©Ì†µÌ∞µ) = Ì†µÌ±É(Ì†µÌ∞µ)Ì†µÌ±É(Ì†µÌ∞¥|Ì†µÌ∞µ) = Ì†µÌ±É(Ì†µÌ∞¥)Ì†µÌ±É(Ì†µÌ∞µ|Ì†µÌ∞¥)
                   ‚Ä¢ Ì†µÌ±É(Ì†µÌ∞¥ ‚à©‚ãØ‚à©Ì†µÌ∞¥ )=Ì†µÌ±É(Ì†µÌ∞¥ )Ì†µÌ±É(Ì†µÌ∞¥ |Ì†µÌ∞¥ )Ì†µÌ±É(Ì†µÌ∞¥ |Ì†µÌ∞¥ ,Ì†µÌ∞¥ )‚ãØÌ†µÌ±É(Ì†µÌ∞¥ |Ì†µÌ∞¥ ‚Ä¶Ì†µÌ∞¥    )
                          1        Ì†µÌ±õ       1     2  1     3  1   2       Ì†µÌ±õ 1    Ì†µÌ±õ‚àí1
                 Theorem 7.1 (Bayes‚Äô rule). Assume Ì†µÌ±É(Ì†µÌ∞µ) > 0, we have
                                                           Ì†µÌ±É(Ì†µÌ∞µ|Ì†µÌ∞¥)Ì†µÌ±É(Ì†µÌ∞¥)
                                                 Ì†µÌ±É(Ì†µÌ∞¥|Ì†µÌ∞µ) =
                                                               Ì†µÌ±É(Ì†µÌ∞µ)
                   ƒπ Thomas Bayes and ‚Äúcauses and effects‚Äù
                   The Bayes‚Äô rule is named after Thomas Bayes (18th century) who wanted to know how
                   to infer causes from effects. Human intelligence wants to know the cause given its effects.
                   However, we are only able to observe the effects given the cause. Here is Bayes‚Äô reasoning.
                   Suppose we have a prior belief about the cause of something we want to learn. We may
                   not be able to learn the true cause directly, but after we observe its effects (the Data),
                   we would update our belief based on the new information we have learned from the data.
                                                           29
                The updated belief (the posterior) is therefore somewhat closer to the ‚Äútruth‚Äù.
                                                     Likelihood
                                                                 Prior
                                                 ‚èû‚èû‚èû‚èû‚èû‚èû‚èû
                                                               ‚èû‚èû‚èû‚èû‚èû
                                                 Ì†µÌ±É(Data | Belief) Ì†µÌ±É(Belief)
                                 Ì†µÌ±É(Belief | Data) =
                                 ‚èü‚èü‚èü‚èü‚èü‚èü‚èü
                                                         Ì†µÌ±É(Data)
                                     Posterier
               Theorem 7.2 (Law of total probability (LOTP)). Let Ì†µÌ∞µ ,...,Ì†µÌ∞µ be a partition of the sample
                                                             1     Ì†µÌ±õ
               space Ì†µÌ±Ü (i.e., the Ì†µÌ∞µ are disjoint events and their union is Ì†µÌ±Ü), with Ì†µÌ±É(Ì†µÌ∞µ ) > 0 for all Ì†µÌ±ñ. Then
                              Ì†µÌ±ñ                                         Ì†µÌ±ñ
                                                 Ì†µÌ±õ
                                          Ì†µÌ±É(Ì†µÌ∞¥) = ‚àëÌ†µÌ±É(Ì†µÌ∞¥|Ì†µÌ∞µ )Ì†µÌ±É(Ì†µÌ∞µ ).
                                                         Ì†µÌ±ñ   Ì†µÌ±ñ
                                                Ì†µÌ±ñ=1
                            (Conditional version of LOTP)
               Theorem 7.3                           . The law of total probability has an analog
               conditional on another event Ì†µÌ∞∂, namely,
                                               Ì†µÌ±õ
                                      Ì†µÌ±É(Ì†µÌ∞¥|Ì†µÌ∞∂) = ‚àëÌ†µÌ±É(Ì†µÌ∞¥|Ì†µÌ∞µ ‚à©Ì†µÌ∞∂)Ì†µÌ±É(Ì†µÌ∞µ |Ì†µÌ∞∂).
                                                       Ì†µÌ±ñ      Ì†µÌ±ñ
                                               Ì†µÌ±ñ=1
               Example 7.1. Get a random 2-card hand from a standard deck. Find the probability of (a)
               Both cards are aces given that at least one of them (not necessarily the first one) is an ace;
              (b) Getting another ace given the first draw is an ace of spade.
               Solution. The example shows the subtleness of conditional probabilities. The seemingly indif-
               ferent probabilities are in fact different:
                                                     Ì†µÌ±É(both aces)
                                 Ì†µÌ±É(two aces | one ace) =
                                                      Ì†µÌ±É(one ace)
                                                        4  52
                                                       ( )/( )
                                                        2   2
                                                   =
                                                         48  52
                                                     1‚àí( )/( )
                                                          2   2
                                                      1
                                                   = ;
                                                     33
                                                     Ì†µÌ±É(ace of spade & another ace)
                           Ì†µÌ±É(another ace | ace of spade) =
                                                           Ì†µÌ±É(ace of spade)
                                                      3   52
                                                      ( )/( )
                                                      1   2
                                                   =
                                                      51  52
                                                     ( )/( )
                                                      1   2
                                                      1
                                                   = .
                                                     17
               Note that, in the first case, the denominator is interpreted as ‚Äúat least one ace‚Äù; whereas in
               the second case, it is ‚Äúace of space + another card‚Äù.
                                                    30
                  Example 7.2. A disease has a prevalence rate of 10% (i.e., the probability of being infected is
                  10%). A diagnostic test for the disease has an accuracy of 98%, meaning it correctly identifies
                  infected individuals as positive 98% of the time. Calculate the probability that an individual
                  is infected given that the test result is positive.
                  Solution. Let Ì†µÌ∞∑ denote being actually infected by the disease; and Ì†µÌ±á denote a positive test.
                                                                                 Ì†µÌ∞∂
                  The test accuracy means: Ì†µÌ±É(Ì†µÌ±á|Ì†µÌ∞∑) = 98%. It also means Ì†µÌ±É(Ì†µÌ±á|Ì†µÌ∞∑ ) = 2%. We also know that
                  Ì†µÌ±É(Ì†µÌ∞∑) = 0.1. We want to find Ì†µÌ±É(Ì†µÌ∞∑|Ì†µÌ±á). Note they are two different conditional probabilities,
                  though we mostly confuse the two in everyday life. The two conditional probabilities are
                  associated with Bayes‚Äô rule:
                                                     Ì†µÌ±É(Ì†µÌ±á|Ì†µÌ∞∑)Ì†µÌ±É(Ì†µÌ∞∑)
                                          Ì†µÌ±É(Ì†µÌ∞∑|Ì†µÌ±á) =
                                                         Ì†µÌ±É(Ì†µÌ±á)
                                                              Ì†µÌ±É(Ì†µÌ±á|Ì†µÌ∞∑)Ì†µÌ±É(Ì†µÌ∞∑)
                                                   =
                                                                           Ì†µÌ∞∂     Ì†µÌ∞∂
                                                     Ì†µÌ±É(Ì†µÌ±á|Ì†µÌ∞∑)Ì†µÌ±É(Ì†µÌ∞∑) + Ì†µÌ±É(Ì†µÌ±á|Ì†µÌ∞∑ )Ì†µÌ±É(Ì†µÌ∞∑ )
                                                           0.98√ó0.1
                                                   =                       ‚âà84%.
                                                     0.98√ó0.1+0.02√ó0.9
                  Note that how Ì†µÌ±É(Ì†µÌ±á|Ì†µÌ∞∑) is far away from Ì†µÌ±É(Ì†µÌ∞∑|Ì†µÌ±á)!
                    ƒπ Thinking conditionally
                    Abraham Wald, the renowned statistician, was hired by the Statistical Research Group
                    (SRG) at Columbia University to figure out how to minimize the damage to bomber
                    aircraft. The data they had comprised aircraft returning from missions with bullet holes
                    on their bodies.  If asked which parts of the aircraft should be armored to enhance
                    survivability, the obvious answer seemed to be to armor the damaged parts. However,
                    Wald suggested the exact opposite‚Äîto armor the parts that were not damaged. Why?
                    Becausetheobserveddamagewasconditionedontheaircraftreturning. Ifanaircrafthad
                    been damaged on other parts, it likely would not have returned. Thinking conditionally
                    completely changes the answer!
                                                              31
         See The Soul of Statistics by Professor Joseph Blitzstein.
                            32
                   8 Monty Hall problem
                   Example 8.1 (Monty Hall problem). Suppose you are on Monty Hall‚Äôs TV show. There are
                   three doors. One of them has a car behind it. The other two doors have goats. Monty knows
                   which one has the car. Monty now asks you to pick one door. You will win whatever is behind
                   the door. After you pick one door. Monty opens another door that shows a goat. Monty then
                   asks you if you want to switch. Is it optimal to switch?
                   We present two solutions to the problem. The first one is using the law of total probability.
                   Let Ì†µÌ±Ü: succeed assuming switch; Ì†µÌ∞∑ : door Ì†µÌ±ó has the car, Ì†µÌ±ó ‚àà 1,2,3. Without loss of generality,
                                                        Ì†µÌ±ó
                   assume the initial pick is Door 1. Monty will always open the door with a goat. By the law of
                   total probability,
                          Ì†µÌ±É(Ì†µÌ±Ü) =     Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ∞∑ )    Ì†µÌ±É(Ì†µÌ∞∑ ) +    Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ∞∑ )   Ì†µÌ±É(Ì†µÌ∞∑ ) +    Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ∞∑ )   Ì†µÌ±É(Ì†µÌ∞∑ )
                                              1           1              2          2               3         3
                                       ‚èü                          ‚èü
                                                                                             ‚èü
                                                               Monty opens door 3
                                  switch from initial pick
                                                                                         Monty opens door 2
                                          1        1    2
                                =0+1√ó +1√ó = .
                                          3        3    3
                   The problem can also be solved using the Bayes‚Äô rule. Let Ì†µÌ∞∑ : door Ì†µÌ±ó has the car; Ì†µÌ±Ä : Monty
                                                                                    Ì†µÌ±ó                       Ì†µÌ±ó
                   opens door Ì†µÌ±ó, Ì†µÌ±ó ‚àà 1,2,3. Assume the initial pick is Door 1. If Monty opens door 3, the
                   probability of winning the car assuming switching is
                                              Ì†µÌ±É(Ì†µÌ±Ä |Ì†µÌ∞∑ )Ì†µÌ±É(Ì†µÌ∞∑ )
                                                   3   2      2
                                Ì†µÌ±É(Ì†µÌ∞∑ |Ì†µÌ±Ä ) =
                                     2   3
                                                   Ì†µÌ±É(Ì†µÌ±Ä )
                                                        3
                                                                   Ì†µÌ±É(Ì†µÌ±Ä |Ì†µÌ∞∑ )Ì†µÌ±É(Ì†µÌ∞∑ )
                                                                        3   2      2
                                            =
                                              Ì†µÌ±É(Ì†µÌ±Ä |Ì†µÌ∞∑ )Ì†µÌ±É(Ì†µÌ∞∑ ) + Ì†µÌ±É(Ì†µÌ±Ä |Ì†µÌ∞∑ )Ì†µÌ±É(Ì†µÌ∞∑ ) + Ì†µÌ±É(Ì†µÌ±Ä |Ì†µÌ∞∑ )Ì†µÌ±É(Ì†µÌ∞∑ )
                                                   3   1      1         3   2      2         3   3      3
                                                         1
                                                    1√ó
                                                                     2
                                                         3
                                            =                     = .
                                              1    1       1
                                                                     3
                                                √ó +1√ó +0
                                              2    3       3
                                                                    33
                  Note that, if door 1 has the car, Monty will open door 2 and 3 with equal probability, thus
                                1
                  Ì†µÌ±É(Ì†µÌ±Ä |Ì†µÌ∞∑ ) =  . And Monty will never open the door with the car, therefore Ì†µÌ±É(Ì†µÌ±Ä |Ì†µÌ∞∑ ) = 0.
                       3   1                                                                           3  3
                                2
                                                                             2
                  Similarly, if Monty opens door 2, we have Ì†µÌ±É(Ì†µÌ∞∑ |Ì†µÌ±Ä ) =     . Therefore, the optimal choice is
                                                                   3   2
                                                                             3
                  always to switch. Intuitively, because Monty knows which door has the car, the fact that he
                  always opens the door without the car gives additional information regarding the choice of the
                  door.
                  R simulation
                  # Number of simulations
                  N <- 1000
                  # Number of total wins
                  W <- 0
                  # Prizes offered
                  prizes <- c('Car', 'Goat 1', 'Goat 2')
                  for (i in 1:N) {
                     # Prizes are in random order behind the doors
                     doors <- sample(prizes, 3)
                     # Guest picks a random door
                     pick <- sample(doors, 1)
                     # Monty opens the door that is not picked by the guest
                     # nor does it has the Car behind it
                     open <- sample(setdiff(doors, c(pick, 'Car')), 1)
                     # If the Guest swithes, he chooses the door that is not
                     # his initial pick nor the one opened by Monty
                     switch <- setdiff(doors, c(pick, open))
                     # The guest wins if his final choice has the Car
                     win <- switch == 'Car'
                     # Increase the winning counter
                     W <- W + win
                  }
                                                                34
        # Frequency of winning the game if always switching
        W/N
        [1] 0.665
                            35
                     9 Simpson‚Äôs paradox
                     Example 9.1. (Simpson‚Äôs paradox). There are two doctors, Dr. Lee and Dr. Wong, perform-
                     ing two types of surgeries ‚Äî heart surgery (hard) and band-aid removal (easy). Dr. Lee has
                     higher overall surgery success rate. Is Dr. Lee necessarily a better doctor than Dr. Wong?
                     No. Consider the following example:
                                          Dr. Lee                                          Dr. Wong
                                           Heart       Band-Aid       Total                  Heart       Band-Aid        Total
                     Success                  2        81             83                       70        10              80
                     Failure                  8        9              17                       20        0               20
                     Success rate           20%        90%            83%                     78%        100%            80%
                     The truth is Dr. Lee has overall higher success rate because he only does easy surgeries (band-
                     aid removal). Dr. Wong does mostly hard surgeries and thus has a lower overall success rate.
                     Yet, he is better at each single type of surgery. To formalize the argument, let Ì†µÌ±Ü: successful
                                                              Ì†µÌ±ê                                                    Ì†µÌ±ê
                     surgery; Ì†µÌ∞∑: treated by Dr. Lee, Ì†µÌ∞∑ : treated by Dr. Wong; Ì†µÌ∞∏: heart surgery, Ì†µÌ∞∏ : band-aid
                     removal. Dr. Wong is better at each type of surgery,
                                                                                    Ì†µÌ±ê
                                                            Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ∞∑,Ì†µÌ∞∏) < Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ∞∑ ,Ì†µÌ∞∏)
                                                                      Ì†µÌ±ê            Ì†µÌ±ê  Ì†µÌ±ê
                                                           Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ∞∑,Ì†µÌ∞∏ ) < Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ∞∑ ,Ì†µÌ∞∏ );
                     But, Dr. Lee has a higher overall successful rate,
                                                                                    Ì†µÌ±ê
                                                               Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ∞∑) > Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ∞∑ ).
                     This is because there is a ‚Äúconfounder‚Äù Ì†µÌ∞∏:
                                                                                            Ì†µÌ±ê      Ì†µÌ±ê
                                            Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ∞∑) = Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ∞∑,Ì†µÌ∞∏)Ì†µÌ±É(Ì†µÌ∞∏|Ì†µÌ∞∑)+Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ∞∑,Ì†µÌ∞∏ )Ì†µÌ±É(Ì†µÌ∞∏ |Ì†µÌ∞∑).
                                                        ‚èü‚èü‚èü‚èü‚èü‚èü ‚èü‚èü‚èü‚èü‚èü‚èü
                                                                Ì†µÌ±ê                       Ì†µÌ±ê Ì†µÌ±ê
                                                                       weight
                                                                                                 weight
                                                         <Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ∞∑ ,Ì†µÌ∞∏)          <Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ∞∑ ,Ì†µÌ∞∏ )
                     Aconfounder is a variable that influences with both explanatory variable and the outcome
                     variable, which therefore ‚Äúconfounds‚Äù the correlation between the two. In our example, the
                     type of surgery (Ì†µÌ∞∏) is associated with both the doctor and the outcome. Without the con-
                     founder being controlled, it is impossible to draw valid conclusions from the statistics.
                                                                          36
                 In general terms, Simpson‚Äôs paradox refers to the paradox in which a trend that appears across
                 different groups of aggregate data is the reverse of the trend that appears when the aggregate
                 data is broken up into its components. It is one of the most common sources of statistical
                 misuse. Here is another example.
                 Example 9.2. (UC Berkeley gender bias). One of the best-known examples of Simpson‚Äôs
                 paradox comes from a study of gender bias among graduate school admissions to University of
                 California, Berkeley. The admission figures for the fall of 1973 showed that men applying were
                 more likely than women to be admitted, and the difference was so large that it was unlikely
                 to be due to chance.
                                            Male                     Female
                                         Applicants  Admitted      Applicants  Admitted
                                  Total    8,442     44%              4,321    35%
                 However, when taking into account the information about departments being applied to, the
                 conclusion turns to the opposite: in most departments, the admission rate for women is higher
                 than men. The lower overall admission rate is caused by the fact that women tended to apply
                 to more competitive departments with lower rates of admission, whereas men tended to apply
                 to less competitive departments with higher rates of admission.
                                Department      Male                   Female
                                             Applicants   Admitted   Applicants  Admitted
                                    A            825      62%           108      82%
                                    B            560      63%            25      68%
                                    C            325      37%           593      34%
                                    D            417      33%           375      35%
                                    E            191      28%           393      24%
                                     F           373      6%            341      7%
                                   Total        2691      45%           1835     30%
                 See https://setosa.io/simpsons for a really good illustration of the Simpson‚Äôs paradox.
                 R demostration
                 # R has a built-in dataset `UCBAdmissions`
                 # we convert it to data frame for analysis
                 data <- as.data.frame(UCBAdmissions)
                                                            37
               # browse the first a few rows
               head(data)
                    Admit Gender Dept Freq
               1 Admitted   Male    A  512
               2 Rejected   Male    A  313
               3 Admitted Female    A   89
               4 Rejected Female    A   19
               5 Admitted   Male    B  353
               6 Rejected   Male    B  207
               # subset of the data with only admissions
               data <- subset(data, Admit == 'Admitted')
               # number of admissions by Gender
               aggregate(Freq ~ Gender, data = data, FUN = sum)
                 Gender Freq
               1   Male 1198
               2 Female  557
               # number of admissions by Gender and Department
               aggregate(Freq ~ Gender + Dept, data = data, FUN = sum)
                  Gender Dept Freq
               1    Male    A  512
               2 Female     A   89
               3    Male    B  353
               4 Female     B   17
               5    Male    C  120
               6 Female     C  202
               7    Male    D  138
               8 Female     D  131
               9    Male    E   53
               10 Female    E   94
               11   Male    F   22
               12 Female    F   24
                                                     38
         ƒπ The importance of conditional thinking
         Whenever we talk about probability or statistics, always remind ourselves what we are
         the conditioning on. Any statistical reasoning without specifying the conditions can be
         misleading. We are prone to such fallacies everyday everywhere.
           ‚Ä¢‚Äú10 millions new jobs were added during the term of President X.‚Äù But it doesn‚Äôt
            tell you this was achieved conditioned on that the economy had just had the worst
            recession.
           ‚Ä¢‚ÄúPrivate schools‚Äô graduates earned 50% more than those graduated from public
            schools.‚Äù But it doesn‚Äôt tell you the background of those students who enrolled in
            private schools.
         Be vigilant to these claims when you see them next time.
                            39
                 10 Independence
                 Definition 10.1 (Independence for two events). If event Ì†µÌ∞µ‚Äôs occurrence does not change the
                 probability of Ì†µÌ∞¥, then we say Ì†µÌ∞¥ and Ì†µÌ∞µ are independent. That is to say Ì†µÌ∞¥ and Ì†µÌ∞µ are independent
                 if
                                                   Ì†µÌ±É(Ì†µÌ∞¥ ‚à© Ì†µÌ∞µ) = Ì†µÌ±É(Ì†µÌ∞¥)Ì†µÌ±É(Ì†µÌ∞µ).
                 Assuming Ì†µÌ±É(Ì†µÌ∞µ) > 0, this is equivalent to
                                                       Ì†µÌ±É(Ì†µÌ∞¥|Ì†µÌ∞µ) = Ì†µÌ±É(Ì†µÌ∞¥)
                 Theorem 10.1. If events Ì†µÌ∞¥ and Ì†µÌ∞µ are independent, then
                               Ì†µÌ±ê
                    ‚Ä¢ Ì†µÌ∞¥ and Ì†µÌ∞µ are independent;
                         Ì†µÌ±ê     Ì†µÌ±ê
                    ‚Ä¢ Ì†µÌ∞¥ and Ì†µÌ∞µ are independent.
                 Ì†µÌ∞¥ and Ì†µÌ∞µ are independent means they do not provide information to each other in the sense
                 that conditional probability is not different from the unconditional probability. It is not an
                 intuitive idea as it seems. It will become clearer when we discuss random variables in later
                 chapters.
                 Definition 10.2 (Independenceforthreeevents). EventsÌ†µÌ∞¥, Ì†µÌ∞µ, andÌ†µÌ∞∂ aresaidtobe(mutually)
                 independent if all of the following equations hold:
                                                   Ì†µÌ±É(Ì†µÌ∞¥ ‚à© Ì†µÌ∞µ) =Ì†µÌ±É(Ì†µÌ∞¥)Ì†µÌ±É(Ì†µÌ∞µ),
                                                   Ì†µÌ±É(Ì†µÌ∞¥ ‚à© Ì†µÌ∞∂) =Ì†µÌ±É(Ì†µÌ∞¥)Ì†µÌ±É(Ì†µÌ∞∂),
                                                   Ì†µÌ±É(Ì†µÌ∞µ ‚à© Ì†µÌ∞∂) =Ì†µÌ±É(Ì†µÌ∞µ)Ì†µÌ±É(Ì†µÌ∞∂),
                                               Ì†µÌ±É(Ì†µÌ∞¥ ‚à© Ì†µÌ∞µ ‚à© Ì†µÌ∞∂) =Ì†µÌ±É(Ì†µÌ∞¥)Ì†µÌ±É(Ì†µÌ∞µ)Ì†µÌ±É(Ì†µÌ∞∂).
                 Definition 10.3 (Independence for Ì†µÌ±õ events). For Ì†µÌ±õ events Ì†µÌ∞¥ ,Ì†µÌ∞¥ ,‚Ä¶,Ì†µÌ∞¥   to be (mutually)
                                                                               1   2     Ì†µÌ±õ
                 independent, we require any pair to satisfy Ì†µÌ±É(Ì†µÌ∞¥ ‚à© Ì†µÌ∞¥ ) = Ì†µÌ±É(Ì†µÌ∞¥ )Ì†µÌ±É(Ì†µÌ∞¥ ) (for Ì†µÌ±ñ ‚â† Ì†µÌ±ó), any triplet
                                                                Ì†µÌ±ñ   Ì†µÌ±ó       Ì†µÌ±ñ    Ì†µÌ±ó
                 to satisfy Ì†µÌ±É(Ì†µÌ∞¥ ‚à© Ì†µÌ∞¥ ‚à© Ì†µÌ∞¥ ) = Ì†µÌ±É(Ì†µÌ∞¥ )Ì†µÌ±É(Ì†µÌ∞¥ )Ì†µÌ±É(Ì†µÌ∞¥ ) (for Ì†µÌ±ñ, Ì†µÌ±ó, Ì†µÌ±ò distinct), and similarly for all
                                Ì†µÌ±ñ   Ì†µÌ±ó   Ì†µÌ±ò        Ì†µÌ±ñ    Ì†µÌ±ó    Ì†µÌ±ò
                 quadruplets, quintuplets, and so on.
                                                             40
                    ƒπ Don‚Äôt assume independence without justification
                    Independence provides lots of nice properties, which are not necessarily true without
                    independence. A common mistake is to assume independence without justification. Be
                    careful when you apply properties that assume independence.
                  Common confusions
                    ¬æ Independence is not the same as disjointness.
                    Ì†µÌ∞¥ and Ì†µÌ∞µ are disjoint means if Ì†µÌ∞¥ occurs, Ì†µÌ∞µ cannot occur. But independence means Ì†µÌ∞¥
                    occurs has nothing to do with Ì†µÌ∞µ.
                    ¬æ Pairwise independence does not imply independence.
                    In Definition 10.2, If the first three conditions hold, we say that Ì†µÌ∞¥, Ì†µÌ∞µ, and Ì†µÌ∞∂ are pairwise
                    independent. Pairwise independence does not imply independence. Convince yourself
                    with the following example.
                  Example 10.1. Consider two fair, independent coin tosses, and let Ì†µÌ∞¥ be the event that the
                  first is Heads, Ì†µÌ∞µ the event that the second is Heads, and Ì†µÌ∞∂ the event that both tosses have
                  the same result. Show that Ì†µÌ∞¥, Ì†µÌ∞µ, and Ì†µÌ∞∂ are pairwise independent but not independent.
                                                     1           1
                  Solution. For each event, Ì†µÌ±É(Ì†µÌ∞¥) =  , Ì†µÌ±É(Ì†µÌ∞µ) =  . Consider the two events together, there are
                                                     2           2
                                                                                           1
                  four possible outcomes: HH, HT, TH, TT. Ì†µÌ±É(Ì†µÌ∞∂) = Ì†µÌ±É(Ì†µÌ∞ªÌ†µÌ∞ª)+Ì†µÌ±É(Ì†µÌ±áÌ†µÌ±á) = . Thus,
                                                                                           2
                                                                     1
                                              Ì†µÌ±É(Ì†µÌ∞¥ ‚à© Ì†µÌ∞µ) = Ì†µÌ±É(Ì†µÌ∞ªÌ†µÌ∞ª) = =Ì†µÌ±É(Ì†µÌ∞¥)Ì†µÌ±É(Ì†µÌ∞µ)
                                                                     4
                                                                     1
                                             Ì†µÌ±É(Ì†µÌ∞¥ ‚à© Ì†µÌ∞∂) = Ì†µÌ±É(Ì†µÌ∞ªÌ†µÌ∞ª) =  =Ì†µÌ±É(Ì†µÌ∞¥)Ì†µÌ±É(Ì†µÌ∞∂)
                                                                     4
                                                                     1
                                             Ì†µÌ±É(Ì†µÌ∞µ ‚à© Ì†µÌ∞∂) = Ì†µÌ±É(Ì†µÌ∞ªÌ†µÌ∞ª) =  =Ì†µÌ±É(Ì†µÌ∞µ)Ì†µÌ±É(Ì†µÌ∞∂)
                                                                     4
                  But Ì†µÌ∞¥,Ì†µÌ∞µ,Ì†µÌ∞∂ are not independent, because
                                                                    1
                                        Ì†µÌ±É(Ì†µÌ∞¥ ‚à© Ì†µÌ∞µ ‚à© Ì†µÌ∞∂) = Ì†µÌ±É(Ì†µÌ∞ªÌ†µÌ∞ª) =  ‚â†Ì†µÌ±É(Ì†µÌ∞¥)Ì†µÌ±É(Ì†µÌ∞µ)Ì†µÌ±É(Ì†µÌ∞∂).
                                                                    4
                                                                41
                   11 Optimal mating problem*
                   The question
                   How many people should you date before you settle down with someone for marriage? The
                   answer is you should date 37% of your potential options and choose the next one who is better.
                   1
                     ƒπ The 37% rule
                     The 37% Rule, also known as the Optimal Stopping Theory, provides a strategy to
                     maximize the chances of making the best choice when faced with a sequence of options
                     where decisions are irreversible. It suggests that you should review and reject the first
                     37%ofthe total options without selecting any, then choose the next option that is better
                     than all those previously considered.
                   Mathematical framework
                   Let‚Äôs assume there‚Äôs a pool of Ì†µÌ±Å people out there from which you are choosing. We‚Äôll also
                   assume that you have a clear-cut way of rating people. You know who is the best to be your
                   partner. We will call that person Mr/Ms Ì†µÌ±ã. The people that you will meet show up one by
                   one in random order. Ì†µÌ±ã may show up anywhere in the sequence. Sadly, a person you have
                   dated and then rejected isn‚Äôt available to you any longer later on. So you cannot date all of
                   them and pick the best one.
                   Your dating strategy is to date Ì†µÌ±Ä of the Ì†µÌ±Å people and then settle with the next person who is
                   better. Our task is to find the optimal Ì†µÌ±Ä. If Ì†µÌ±Ä is too small, you will likely land with someone
                   before Ì†µÌ±ã shows up. If Ì†µÌ±Ä is too large, Ì†µÌ±ã will likely pass Ì†µÌ±ã and pick someone less optimal. Of
                   course, there is no perfect solution. We want to find the Ì†µÌ±Ä that maximizes the probability of
                   landing Ì†µÌ±ã.
                   Let Ì†µÌ±É(Ì†µÌ±Ä,Ì†µÌ±Å) be the probability of successfully picking Ì†µÌ±ã if you date Ì†µÌ±Ä people out of Ì†µÌ±Å and
                   then go for the next person who is better than the previous ones. Let Ì†µÌ±Ü be the event of
                   successfully picking Ì†µÌ±ã, and Ì†µÌ±ã means Ì†µÌ±ã is in the Ì†µÌ±óth position in the sequence. The overall
                                                   Ì†µÌ±ó
                   probability is:
                                Ì†µÌ±É(Ì†µÌ±Ä,Ì†µÌ±Å) = Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ±ã )Ì†µÌ±É(Ì†µÌ±ã ) + Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ±ã )Ì†µÌ±É(Ì†µÌ±ã ) + ‚ãØ + Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ±ã )Ì†µÌ±É(Ì†µÌ±ã )
                                                    1     1           2      2                Ì†µÌ±õ     Ì†µÌ±õ
                    1
                     See Kissing the frog: A mathematician‚Äôs guide to mating and Strategic dating: The 37% rule for reference.
                                                                  42
                     For a given value of Ì†µÌ±Ä, if Ì†µÌ±ã is among the first Ì†µÌ±Ä people you date, then you have missed your
                     chance. The probability of settling with Ì†µÌ±ã is zero. Therefore, the first Ì†µÌ±Ä terms are all zero.
                     If Ì†µÌ±ã is in Ì†µÌ±Ä + 1, you‚Äôre in luck: since Ì†µÌ±ã is better than all others so far, you will pick Ì†µÌ±ã for
                     sure. Therefore,
                                                                                                    1
                                                    Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ±ã     )Ì†µÌ±É(Ì†µÌ±ã     ) = 1‚ãÖÌ†µÌ±É(Ì†µÌ±ã        ) =
                                                            Ì†µÌ±Ä+1       Ì†µÌ±Ä+1               Ì†µÌ±Ä+1
                                                                                                    Ì†µÌ±Å
                     Since Ì†µÌ±ã is equally likely to be in any position, the probability of Ì†µÌ±ã being in Ì†µÌ±Ä + 1 out of Ì†µÌ±Å
                     people is 1/Ì†µÌ±Å.
                     If Ì†µÌ±ã is in Ì†µÌ±Ä + 2, you‚Äôll pick him/her up as long as the (Ì†µÌ±Ä +1)st person didn‚Äôt have a higher
                     rating than all the previous Ì†µÌ±Ä people. In other words, you would pass the (Ì†µÌ±Ä + 1)st person
                     and pick Ì†µÌ±ã if the best one out of the (Ì†µÌ±Ä +1) people has shown up among the first Ì†µÌ±Ä people.
                     The change is Ì†µÌ±Ä/(Ì†µÌ±Ä +1). Thus,
                                                                                         Ì†µÌ±Ä    1
                                                         Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ±ã     )Ì†µÌ±É(Ì†µÌ±ã     ) =
                                                                 Ì†µÌ±Ä+2       Ì†µÌ±Ä+2
                                                                                      Ì†µÌ±Ä +1Ì†µÌ±Å
                     Similarly, if Ì†µÌ±ã shows up in Ì†µÌ±Ä +3, you‚Äôll pick him/her up to as long as neither the (Ì†µÌ±Ä +1)st
                     nor the (Ì†µÌ±Ä +2)nd person have a higher rating than all the previous Ì†µÌ±Ä people. In other words,
                     the best one out of the first (Ì†µÌ±Ä + 2) people has to show up among the first Ì†µÌ±Ä people. The
                     chance is Ì†µÌ±Ä/(Ì†µÌ±Ä +2). Thus,
                                                                                         Ì†µÌ±Ä    1
                                                         Ì†µÌ±É(Ì†µÌ±Ü|Ì†µÌ±ã     )Ì†µÌ±É(Ì†µÌ±ã     ) =
                                                                 Ì†µÌ±Ä+3       Ì†µÌ±Ä+3
                                                                                      Ì†µÌ±Ä +2Ì†µÌ±Å
                     Putting them all together, we have
                                                         1         Ì†µÌ±Ä              Ì†µÌ±Ä                    Ì†µÌ±Ä
                                         Ì†µÌ±É(Ì†µÌ±Ä,Ì†µÌ±Å) =        +               +               +‚ãØ+
                                                        Ì†µÌ±Å     Ì†µÌ±Å(Ì†µÌ±Ä +1)       Ì†µÌ±Å(Ì†µÌ±Ä +2)            Ì†µÌ±Å(Ì†µÌ±Å ‚àí1)
                                                        Ì†µÌ±Ä     1        1           1               1
                                                     =      (     +          +           +‚ãØ+             )
                                                         Ì†µÌ±Å   Ì†µÌ±Ä     Ì†µÌ±Ä +1      Ì†µÌ±Ä +2            Ì†µÌ±Å ‚àí1
                     Maximizing your chance of success
                     Assuming Ì†µÌ±É(Ì†µÌ±Ä,Ì†µÌ±Å) is strictly concave (fortunately this is the case), the Ì†µÌ±Ä the maximizes the
                     chance satisfies
                                            Ì†µÌ±É(Ì†µÌ±Ä ‚àí1,Ì†µÌ±Å) < Ì†µÌ±É(Ì†µÌ±Ä,Ì†µÌ±Å) and Ì†µÌ±É(Ì†µÌ±Ä +1,Ì†µÌ±Å) < Ì†µÌ±É(Ì†µÌ±Ä,Ì†µÌ±Å)
                     Wecan ask the computer to find the solution:
                     N <- 100
                     p <- sapply(1:(N-1), function(m) m/N*sum(1/seq(m, N-1)))
                     plot(1:(N-1), p, type="l")
                                                                            43
                          0.3
                    p     0.2
                          0.1
                          0.0
                              0           20          40          60           80         100
                                                         1:(N ‚àí 1)
                  For Ì†µÌ±Å = 100, the highest probability if achieved at Ì†µÌ±Ä = 37.
                  The limiting solution
                  We can find the solution analytically if Ì†µÌ±Ä,Ì†µÌ±Å are large. For large Ì†µÌ±õ, the harmonic sequence
                  can be approximated by the logarithm function:
                                                         1   1        1
                                              Ì†µÌ∞ª =1+ + +‚ãØ+ ‚âàln(Ì†µÌ±õ)+Ì†µÌªæ
                                                Ì†µÌ±õ
                                                         2   3        Ì†µÌ±õ
                  where Ì†µÌªæ is a constant.
                  Werewrite the function Ì†µÌ±É(Ì†µÌ±Ä,Ì†µÌ±Å) as
                                                Ì†µÌ±Å‚àí1             Ì†µÌ±Å‚àí1
                                                    Ì†µÌ±Ä   1    Ì†µÌ±Ä      1   Ì†µÌ±Ä
                                   Ì†µÌ±É(Ì†µÌ±Ä,Ì†µÌ±Å) = ‚àë        ‚ãÖ  =     ‚àë = (Ì†µÌ∞ª             ‚àíÌ†µÌ∞ª     )
                                                                                Ì†µÌ±Å‚àí1     Ì†µÌ±Ä‚àí1
                                                     Ì†µÌ±Å  Ì†µÌ±ò   Ì†µÌ±Å      Ì†µÌ±ò   Ì†µÌ±Å
                                                Ì†µÌ±ò=Ì†µÌ±Ä            Ì†µÌ±ò=Ì†µÌ±Ä
                  For large Ì†µÌ±Ä and Ì†µÌ±Å, it is approximated by
                                       Ì†µÌ±Ä                     Ì†µÌ±Ä                            Ì†µÌ±Ä     Ì†µÌ±Å
                           Ì†µÌ±É(Ì†µÌ±Ä,Ì†µÌ±Å) =    (Ì†µÌ∞ª    ‚àíÌ†µÌ∞ª     ) ‚âà     [ln(Ì†µÌ±Å ‚àí 1) ‚àí ln(Ì†µÌ±Ä ‚àí 1)] ‚âà   ln(    )
                                             Ì†µÌ±Å‚àí1    Ì†µÌ±Ä‚àí1
                                       Ì†µÌ±Å                     Ì†µÌ±Å                            Ì†µÌ±Å     Ì†µÌ±Ä
                           Ì†µÌ±Ä
                  Let Ì†µÌ±• =    ‚àà(0,1). We want to maximize
                           Ì†µÌ±Å
                                                                 1
                                                    Ì†µÌ±ì(Ì†µÌ±•) = Ì†µÌ±•ln( ) = ‚àíÌ†µÌ±•lnÌ†µÌ±•
                                                                 Ì†µÌ±•
                  Differentiate:
                                                             1
                                                  ‚Ä≤
                                                 Ì†µÌ±ì (Ì†µÌ±•) = ln( )‚àí1 = ‚àílnÌ†µÌ±•‚àí1
                                                             Ì†µÌ±•
                       ‚Ä≤                                               ‚àí1                      ‚Ä≥
                  Set Ì†µÌ±ì (Ì†µÌ±•) = 0 ‚áí ‚àílnÌ†µÌ±• ‚àí 1 = 0 ‚áí lnÌ†µÌ±• = ‚àí1 ‚áí Ì†µÌ±• = Ì†µÌ±í   . Second derivative Ì†µÌ±ì (Ì†µÌ±•) = ‚àí1/Ì†µÌ±• < 0,
                  so it‚Äôs a maximum.
                                                                44
                    Therefore, the maximizing fraction satisfies
                                                              ‚ãÜ
                                                           Ì†µÌ±Ä         1
                                                                 ‚ü∂ asÌ†µÌ±Å‚Üí‚àû,
                                                            Ì†µÌ±Å        Ì†µÌ±í
                    and the maximal success probability tends to
                                                                             1
                                                                  Ì†µÌ±ì(1/Ì†µÌ±í) =   .
                                                                             Ì†µÌ±í
                                                                        45
                    12 Review of calculus*
                    Calculus is a prerequisite to work with continuous distributions. The following chapters assume
                    readers are proficient in calculus. We nonetheless review some basic concepts here as a warm-
                    up. This review is not exhaustive, so please refer to a specific textbook if needed for a more
                    comprehensive understanding.
                    Differentiation
                    Wedefine the derivative of a function Ì†µÌ±ì(Ì†µÌ±•) to be
                                                                       Ì†µÌ±ì(Ì†µÌ±• + ‚Ñé) ‚àí Ì†µÌ±ì(Ì†µÌ±•)
                                                          ‚Ä≤
                                                         Ì†µÌ±ì (Ì†µÌ±•) = lim
                                                                  ‚Ñé‚Üí0
                                                                               ‚Ñé
                    Loosely speaking, a function is continuous if there is no jump in the graph, differentiable if
                    the curve is smooth. Some commonly used derivatives:
                                                          Ì†µÌ±ë
                                                               Ì†µÌ±õ           Ì†µÌ±õ‚àí1
                                                            (Ì†µÌ±• )      =Ì†µÌ±õÌ†µÌ±•
                                                         Ì†µÌ±ëÌ†µÌ±•
                                                          Ì†µÌ±ë
                                                              Ì†µÌ±•           Ì†µÌ±•
                                                            (Ì†µÌ±í )      =Ì†µÌ±í
                                                         Ì†µÌ±ëÌ†µÌ±•
                                                          Ì†µÌ±ë              1
                                                            (ln(Ì†µÌ±•))   =
                                                         Ì†µÌ±ëÌ†µÌ±•             Ì†µÌ±•
                                                          Ì†µÌ±ë
                                                            (sin(Ì†µÌ±•))  =cos(Ì†µÌ±•)
                                                         Ì†µÌ±ëÌ†µÌ±•
                                                          Ì†µÌ±ë
                                                            (cos(Ì†µÌ±•))  =‚àísin(Ì†µÌ±•)
                                                         Ì†µÌ±ëÌ†µÌ±•
                                                              ‚Ä≤            ‚Ä≤       ‚Ä≤
                                                         (Ì†µÌ±ìÌ†µÌ±î)        =Ì†µÌ±ì Ì†µÌ±î + Ì†µÌ±ìÌ†µÌ±î
                                                               ‚Ä≤
                                                                           ‚Ä≤       ‚Ä≤
                                                           Ì†µÌ±ì             Ì†µÌ±ì Ì†µÌ±î ‚àí Ì†µÌ±ìÌ†µÌ±î
                                                         ( )           =
                                                                               2
                                                           Ì†µÌ±î                 Ì†µÌ±î
                                                                   ‚Ä≤       ‚Ä≤        ‚Ä≤
                                                         [Ì†µÌ±ì(Ì†µÌ±î(Ì†µÌ±•))]  =Ì†µÌ±ì (Ì†µÌ±î(Ì†µÌ±•))Ì†µÌ±î (Ì†µÌ±•)
                                                                0       ‚àû
                    When dealing with limits of the form ‚Äú ‚Äù or ‚Äú          ‚Äù, the L‚ÄôHospital rule is very handy.
                                                                0       ‚àû
                                                                                ‚Ä≤
                                                                 Ì†µÌ±ì(Ì†µÌ±•)       Ì†µÌ±ì (Ì†µÌ±•)
                                                            lim        =lim         .
                                                                                ‚Ä≤
                                                            Ì†µÌ±•‚ÜíÌ†µÌ±é        Ì†µÌ±•‚ÜíÌ†µÌ±é
                                                                 Ì†µÌ±î(Ì†µÌ±•)       Ì†µÌ±î (Ì†µÌ±•)
                                                                       46
                                One important application of derivatives is the Taylor‚Äôs theorem, which gives the approxima-
                                tion of a function around a given point by polynomials. Assume function Ì†µÌ±ì is at least Ì†µÌ±ò times
                                differentiable, then
                                                                                                           ‚Ä≥                                      (Ì†µÌ±ò)
                                                                                                         Ì†µÌ±ì  (Ì†µÌ±é)                              Ì†µÌ±ì     (Ì†µÌ±é)
                                                                                ‚Ä≤                                               2                                       Ì†µÌ±ò
                                                    Ì†µÌ±ì(Ì†µÌ±•) = Ì†µÌ±ì(Ì†µÌ±é) + Ì†µÌ±ì (Ì†µÌ±é)(Ì†µÌ±• ‚àí Ì†µÌ±é) +                           (Ì†µÌ±• ‚àí Ì†µÌ±é) + ‚ãØ +                         (Ì†µÌ±• ‚àí Ì†µÌ±é)       +‚ãØ
                                                                                                            2!                                     Ì†µÌ±ò!
                                whichmeanswecanapproximateafunctionarbitrarilywellbyhigherorderpolynomials. Some
                                commonly used Taylor series (expanding around Ì†µÌ±é = 0):
                                                                           1
                                                                                                                2        3
                                                                                            =1+Ì†µÌ±•+Ì†µÌ±• +Ì†µÌ±• +‚ãØ for |Ì†µÌ±•|<1
                                                                       1‚àíÌ†µÌ±•
                                                                                                                 2         3
                                                                                                              Ì†µÌ±•        Ì†µÌ±•
                                                                         Ì†µÌ±•
                                                                       Ì†µÌ±í                   =1+Ì†µÌ±•+                  +         +‚ãØ
                                                                                                               2!        3!
                                                                                                         3          5         7
                                                                                                       Ì†µÌ±•        Ì†µÌ±•        Ì†µÌ±•
                                                                        sinÌ†µÌ±•               =Ì†µÌ±•‚àí             +         ‚àí         +‚ãØ
                                                                                                        3!        5!        7!
                                                                                                         2         4         6
                                                                                                       Ì†µÌ±•        Ì†µÌ±•        Ì†µÌ±•
                                                                        cosÌ†µÌ±•               =1‚àí             +         ‚àí         +‚ãØ
                                                                                                       2!        4!        6!
                                                                                                         2          3         4
                                                                                                       Ì†µÌ±•        Ì†µÌ±•        Ì†µÌ±•
                                                                        ln(1 +Ì†µÌ±•)           =Ì†µÌ±•‚àí             +         ‚àí         +‚ãØ for|Ì†µÌ±•|<1
                                                                                                        2         3         4
                                                                                                         3          5         7
                                                                                                       Ì†µÌ±•        Ì†µÌ±•        Ì†µÌ±•
                                                                        arctan(Ì†µÌ±•)          =Ì†µÌ±•‚àí             +         ‚àí         +‚ãØ for|Ì†µÌ±•|‚â§1
                                                                                                        3         5         7
                                    ƒπ Approximating Ì†µÌºã with Taylor series
                                    Taylor series are one of the most amazing results in calculus. For example, in the last
                                    formula, if we let Ì†µÌ±• = 1:
                                                                                  Ì†µÌºã                                    1       1       1
                                                                                      =arctan(1) = 1‚àí                       + ‚àí +‚ãØ
                                                                                  4                                     3       5       7
                                    Therefore, we can approximate Ì†µÌºã by summing up a sequence of fractions:
                                                                                                             1       1       1
                                                                                        Ì†µÌºã = 4(1‚àí                + ‚àí +‚ãØ).
                                                                                                             3       5       7
                                Integration
                                Integration is the inverse operation of differentiation. Integral has the geometric interpretation
                                as the area under the curve. Let Ì†µÌ∞¥(Ì†µÌ±•) be the area under the curve of Ì†µÌ±¶ = Ì†µÌ±ì(Ì†µÌ±•). Thus
                                                Ì†µÌ±•
                                              ‚à´
                                Ì†µÌ∞¥(Ì†µÌ±•) =           Ì†µÌ±ì(Ì†µÌ±°)Ì†µÌ±ëÌ†µÌ±°. The change of the area resulted from a tiny little change of Ì†µÌ±• is approximated
                                               0
                                                                                                                 47
                                                        Ì†µÌ∞¥(Ì†µÌ±•+‚Ñé)‚àíÌ†µÌ∞¥(Ì†µÌ±•)
                  by Ì†µÌ∞¥(Ì†µÌ±•+‚Ñé)‚àíÌ†µÌ∞¥(Ì†µÌ±•) ‚âà Ì†µÌ±ì(Ì†µÌ±•)‚Ñé. That is             =Ì†µÌ±ì(Ì†µÌ±•). If the change is infinitesimal, ‚Ñé ‚Üí 0,
                                                             ‚Ñé
                             ‚Ä≤
                  we have Ì†µÌ∞¥ (Ì†µÌ±•) = Ì†µÌ±ì(Ì†µÌ±•).
                  The Fundamental Theorem of Calculus: if Ì†µÌ∞π is the anti-derivative of Ì†µÌ±ì, then
                                                                   Ì†µÌ±•
                                                         Ì†µÌ∞π(Ì†µÌ±•) = ‚à´ Ì†µÌ±ì(Ì†µÌ±°)Ì†µÌ±ëÌ†µÌ±°
                                                                 Ì†µÌ±é
                                                       Ì†µÌ±è
                                                    ‚à´ Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±• = Ì†µÌ∞π(Ì†µÌ±è) ‚àí Ì†µÌ∞π(Ì†µÌ±é)
                                                     Ì†µÌ±é
                  One interpretation of the integral is ‚Äî the integral of a rate of change of a quantity gives the
                                                                                   Ì†µÌ±è
                                                                                  ‚à´
                  net change in that quantity. Think about speed and distance:       Ì†µÌ±£(Ì†µÌ±°)Ì†µÌ±ëÌ†µÌ±° = Ì†µÌ±†(Ì†µÌ±è) ‚àí Ì†µÌ±†(Ì†µÌ±é).
                                                                                   Ì†µÌ±é
                                                                                                      Ì†µÌ±è
                                                                                                    ‚à´
                  Because the integral is just a sum over infinitely many approximating rectangles,    Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±• =
                                                                                                     Ì†µÌ±é
                                                                                                Ì†µÌ±è
                             Ì†µÌ±õ
                                                                                            1
                                                                                               ‚à´
                  lim      ‚àë Ì†µÌ±ì(Ì†µÌ±• )ŒîÌ†µÌ±•. Integrals behave just like sums. For example,           Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±• has the
                     Ì†µÌ±õ‚Üí‚àû           Ì†µÌ±ñ
                                                                                           Ì†µÌ±è‚àíÌ†µÌ±é
                             Ì†µÌ±ñ=1
                                                                                               Ì†µÌ±é
                  interpretation of the average of Ì†µÌ±ì(Ì†µÌ±•) from Ì†µÌ±é to Ì†µÌ±è.
                  Indefinite integrals are the general antiderivatives without specifying the interval of the inte-
                  gration. It always comes with a constant Ì†µÌ∞∂. Some commonly used integrals:
                                                                48
                                                                              ‚à´Ì†µÌ±ëÌ†µÌ±•
                                                                                                    =Ì†µÌ±•+Ì†µÌ∞∂
                                                                                                          Ì†µÌ±õ+1
                                                                                                        Ì†µÌ±•
                                                                                    Ì†µÌ±õ
                                                                              ‚à´Ì†µÌ±• Ì†µÌ±ëÌ†µÌ±•              =            +Ì†µÌ∞∂
                                                                                                       Ì†µÌ±õ + 1
                                                                                    Ì†µÌ±•                   Ì†µÌ±•
                                                                              ‚à´Ì†µÌ±í Ì†µÌ±ëÌ†µÌ±•              =Ì†µÌ±í +Ì†µÌ∞∂
                                                                                   1
                                                                              ‚à´ Ì†µÌ±ëÌ†µÌ±•                =ln|Ì†µÌ±•| +Ì†µÌ∞∂
                                                                                   Ì†µÌ±•
                                                                              ‚à´cos(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•          =sin(Ì†µÌ±•) +Ì†µÌ∞∂
                                                                              ‚à´sin(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•          =‚àícos(Ì†µÌ±•)+Ì†µÌ∞∂
                                                                                       1
                                                                              ‚à´              Ì†µÌ±ëÌ†µÌ±•   =arctan(Ì†µÌ±•)+Ì†µÌ∞∂
                                                                                           2
                                                                                   1+Ì†µÌ±•
                            Two common integration techniques are substitution and integration by parts.
                                                                                                                 ‚àö
                                                                                                              ‚à´
                            Example 12.1 (Integration by substitution). Find                                        3Ì†µÌ±• + 2Ì†µÌ±ëÌ†µÌ±•.
                            Solution. Let Ì†µÌ±¢ = 3Ì†µÌ±• + 2, then Ì†µÌ±ëÌ†µÌ±¢ = 3Ì†µÌ±ëÌ†µÌ±•. Then
                                                          ‚àö
                                                                                       ‚àö
                                                                                1                    2                    2
                                                                                                         3/2                             3/2
                                                     ‚à´ 3Ì†µÌ±•+2Ì†µÌ±ëÌ†µÌ±• =                 ‚à´ Ì†µÌ±¢Ì†µÌ±ëÌ†µÌ±¢ =          Ì†µÌ±¢     +Ì†µÌ∞∂ = (3Ì†µÌ±•+2)                   +Ì†µÌ∞∂.
                                                                                3                    9                    9
                                                                                                    ‚à´
                            Example 12.2 (Integration by parts). Find                                  Ì†µÌ±• sin Ì†µÌ±•Ì†µÌ±ëÌ†µÌ±•.
                            Solution. Integration by parts follows the formula:
                                                                                ‚Ä≤                                      ‚Ä≤
                                                                   ‚à´Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±î (Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±• = Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±î(Ì†µÌ±•) ‚àí ‚à´Ì†µÌ±ì (Ì†µÌ±•)Ì†µÌ±î(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•
                                                    ‚Ä≤
                            Let Ì†µÌ±ì(Ì†µÌ±•) = Ì†µÌ±•, Ì†µÌ±î (Ì†µÌ±•) = sinÌ†µÌ±•. Then Ì†µÌ±î(Ì†µÌ±•) = ‚àícosÌ†µÌ±•. Then,
                                                    ‚à´Ì†µÌ±•sinÌ†µÌ±•Ì†µÌ±ëÌ†µÌ±• = ‚àíÌ†µÌ±•cosÌ†µÌ±• ‚àí ‚à´(‚àícosÌ†µÌ±•)Ì†µÌ±ëÌ†µÌ±• = ‚àíÌ†µÌ±•cosÌ†µÌ±• + sinÌ†µÌ±• + Ì†µÌ∞∂.
                                                                                                    49
               13 R tutorial*
               Variables
               a = 5
               b <- 5
               c <- "Hello"
               Assignment
               a <- a + 1 # assignment
               a == a + 1 # math equal
               Vectors
               u <- c(1,2,3,4,5)
               v <- 6:10
               b <- c('good', 'night', 'Ôøø')
               Matrices
               A <- matrix(c(1,2,3,4), nrow = 2, ncol = 2)
               B <- matrix(c(5,6,7,8), nrow = 2, ncol = 2)
               Linear algebra
               u * v   # element-wise
               u %*% v # dot product
               A * B   # element-wise
               A %*% B # matrix multiplication
               t(A)    # transpose
               det(A) # determinant
               Random numbers
                                                     50
        # a random number from 0 to 100
        runif(1, 0, 100)
        # generate 10 random numbers
        runif(10, 0, 100)
        # random sampling
        sample(1:100, 10)
        Conditional statement
        # draw a random integer
        x <- sample(1:100, 1)
        # if the remainder divided by 2 is 0
        if (x %% 2 == 0) {
         # display it is an even number
         print("even number")
         } else {
          # otherwise, it is an odd number
          cat("odd number")
        }
        Loop
        # for-loop
        # loop for a given number of times
        for (k in 1:10) {
         # the code to be repeated
         print("Hello!")
        }
        # while-loop
        # loop on condition
        k <- 0
        while (k < 10) {
                            51
         # the code to be repeated
         print("Hello!")
         # keep track of the
         k <- k + 1
        }
        # 1+2+...+100=?
        s = 0;
        k = 1;
        while (k <= 100) {
         s <- s + k;
         k <- k + 1;
        }
        Functions
        # built-in functions
        sin(pi/2)
        log(100)
        exp(2.3)
        # custom functions
        square <- function(a) {
         a * a
        }
        # call the function
        square(10)
        # function as a reusable code block
        seqsum <- function(begin, end) {
         s = 0;
         k = begin;
         while (k <= end) {
          s <- s + k;
          k <- k + 1;
         }
         return(s)
        }
                            52
        # function call
        seqsum(1, 100)
                            53
                                Part II
                        Random Variables
                                   54
         14 What is a random variable?
         In the previous chapter, we have been working with events, which is a conceptualization of
         real world outcomes occurred with probabilities. In this chapter, we introduce a much more
         powerful conceptualization that deals with uncertain outcomes ‚Äî random variables, which is
         the foundation of all probability and statistical studies.
         Informally, a random variable differs from a normal variable as it is ‚Äúrandom‚Äù. A random
         variable, say Ì†µÌ±ã, is never associated with a certain value. It could different values probabilis-
         tically. For example, Ì†µÌ±ã may take the value 1 with probability 0.4, and take the value 2 with
         probability 0.3. The formal definition of a random variable is as follows.
         Numeric encoding of events
         Definition 14.1 (Random variable). Given an experiment with sample space Ì†µÌ±Ü, a random
         variable is a function from the sample space Ì†µÌ±Ü to the real numbers ‚Ñù.
         As an example, flipping a coin twice, let Ì†µÌ±ã be the number of heads. Then Ì†µÌ±ã(‚ãÖ) is a functions
         that maps events in {Ì†µÌ∞ªÌ†µÌ∞ª,Ì†µÌ∞ªÌ†µÌ±á,Ì†µÌ±áÌ†µÌ∞ª,Ì†µÌ±áÌ†µÌ±á} into real numbers. In our case, the mapping goes
         like
                  Ì†µÌ±ã(Ì†µÌ∞ªÌ†µÌ∞ª) = 2,Ì†µÌ±ã(Ì†µÌ∞ªÌ†µÌ±á) = 1,Ì†µÌ±ã(Ì†µÌ±áÌ†µÌ∞ª) = 1,Ì†µÌ±ã(Ì†µÌ±áÌ†µÌ±á) = 0.
         Ì†µÌ±ã is therefore an encoding of events in the sample space into real numbers. We could, of
         course, have different encoding. Consider the random variable Ì†µÌ±å as the number of tails. Then
         we have Ì†µÌ±å = 2‚àíÌ†µÌ±ã.
                   Ì†µÌ±å (Ì†µÌ∞ªÌ†µÌ∞ª) = 0,Ì†µÌ±å (Ì†µÌ∞ªÌ†µÌ±á) = 1,Ì†µÌ±å (Ì†µÌ±áÌ†µÌ∞ª) = 2,Ì†µÌ±å (Ì†µÌ±áÌ†µÌ±á) = 2.
         Wecould also define Ì†µÌ±ç as the number heads in the 1st toss only. The encoding goes like
                   Ì†µÌ±ç(Ì†µÌ∞ªÌ†µÌ∞ª) = 1,Ì†µÌ±ç(Ì†µÌ∞ªÌ†µÌ±á) = 1,Ì†µÌ±ç(Ì†µÌ±áÌ†µÌ∞ª) = 0,Ì†µÌ±ç(Ì†µÌ±áÌ†µÌ±á) = 0.
         We have listed three ways of ‚Äúencoding‚Äù the same experiment as random variables. All of
         them are valid random variables, but they map the outcomes into different numbers. We can
         say that, a random variable is a numeric ‚Äúsummary‚Äù of an aspect of an experiment.
                              55
                            ¬æ Notation for random variables
                            Weusually use capital letters, such as Ì†µÌ±ã,Ì†µÌ±å,Ì†µÌ±ç, to denote random variables. We use small
                            letters, such as Ì†µÌ±•,Ì†µÌ±¶,Ì†µÌ±ß, to denote specific values. Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•) means the probability of Ì†µÌ±ã
                            taking the value Ì†µÌ±•. Don‚Äôt confuse the random variable Ì†µÌ±ã with the number Ì†µÌ±•.
                            ¬æ Don‚Äôt confuse random variables, numbers, and events
                                                                                                                                                    2
                            Random variables are never fixed numbers. Functions of random variables, such as Ì†µÌ±ã ,
                                    Ì†µÌ±ã
                            |Ì†µÌ±ã|, Ì†µÌ±í  , are also random variables. Random variables are not events. It does not make
                            sense to write Ì†µÌ±É(Ì†µÌ±ã), because Ì†µÌ±ã is not an event. But Ì†µÌ±ã = Ì†µÌ±é is an event, it makes sense
                            to write Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±é).
                         Definition 14.2 (Distribution). Let Ì†µÌ±ã be a random variable. The distribution of Ì†µÌ±ã is the
                         collection of all probabilities of the form Ì†µÌ±É(Ì†µÌ±ã ‚àà Ì†µÌ∞∂) for all sets Ì†µÌ∞∂ of real numbers such that
                         {Ì†µÌ±ã ‚àà Ì†µÌ∞∂} is an event.
                         A distribution specifies the probabilities associated with all values of a random variable. In
                         the above example, the distribution of Ì†µÌ±ã is given by
                                                                            1                    1                     1
                                                          Ì†µÌ±É(Ì†µÌ±ã = 0) =        , Ì†µÌ±É (Ì†µÌ±ã = 1) =      , Ì†µÌ±É (Ì†µÌ±ã = 2) =       .
                                                                            4                    2                     4
                         The distribution of Ì†µÌ±å is given by
                                                                            1                    1                    1
                                                          Ì†µÌ±É(Ì†µÌ±å = 0) =        , Ì†µÌ±É (Ì†µÌ±å = 1) =      , Ì†µÌ±É (Ì†µÌ±å = 2) =      .
                                                                            4                    2                    4
                         The distribution of Ì†µÌ±ç is given by
                                                                                      1                    1
                                                                     Ì†µÌ±É(Ì†µÌ±ç = 0) =       , Ì†µÌ±É (Ì†µÌ±ç = 1) =      .
                                                                                      2                    2
                         Youmayhavenotedthattheprobabilities in a distribution always sums up to 1, as all possible
                         events constitute the entire sample space.
                            ƒé Specifying the distribution
                            Listing all the values is not a smart way to specify a distribution. We like to use a
                                                                             ?
                                                                                 ‚àíÌ†µÌ±•
                            function (if possible), such as Ì†µÌ±ì(Ì†µÌ±•) = Ì†µÌ±í             , to specify the probability of a random variable
                            Ì†µÌ±ã taking the value Ì†µÌ±•. This is convenient, because once we know the function, we know
                            all the probabilities. But how to specify this function depends on whether a random
                            variable is discrete or continuous.
                                                                                        56
                    Conceptualization of uncertain outcomes
                    Many real-world processes have uncertain outcomes. For example, the outcome of tossing a
                    coin or the temperature of tomorrow. In many applications like this, we simply do not have
                    perfect information to predict the future with certainty. In such cases, we model the uncertain
                    outcome as an RV, which takes uncertain values with probabilities. The exact distribution of
                    many applications may be unknown, but we can approximate it with frequencies observed in
                    samples.
                    Experiment:                        Tossing a coin
                                                       Conceptualization                  Observations
                    Random variable                    Ì†µÌ±ã with support {0,1}              {0,1,1,0,0,1,...}
                    Distribution                       Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±ñ) = 0.5,Ì†µÌ±ñ ‚àà {0,1}       Proportion of 1s = 0.45
                    Experiment:                        Taking an exam
                                                       Conceptualization                  Observations
                    Random variable                    Ì†µÌ±ç with support                    {80,69,75,60,92,...}
                                                       {0,1,2,...,100}
                    Distribution                       Ì†µÌ±ç ‚àº Ì†µÌ±Å(80,10) (assumed)           Proportion of 80+ = 0.14
                      ƒπ Deterministic vs probabilistic models
                      In high school, mathematical models are typically presented as if they operate with
                      certainty. For example, the time it takes an object to fall from a height ‚Ñé to the ground
                                           2‚Ñé
                      is given by Ì†µÌ±° =        , where Ì†µÌ±î denotes the gravitational constant. The outcome here
                                         ‚àö
                                            Ì†µÌ±î
                      is deterministic: once the values of the variables are specified, the result follows with
                      certainty. While the variables may or may not be known in practice, they are not random
                      in the sense that the outcome is fully determined once inputs are given. Errors can only
                      arise from frictions or measurement inaccuracies.
                      By contrast, many real-world processes are inherently uncertain. Consider tomorrow‚Äôs
                      temperature or stock market returns: such outcomes can only be predicted probabilisti-
                      cally. This uncertainty does not reflect randomness in the nature of the universe itself,
                      but rather the limits of human knowledge. In principle, with perfect information about
                      the climate system, tomorrow‚Äôs temperature could be predicted exactly. However, given
                      informational constraints, the only feasible approach is to incorporate uncertainty into
                      mathematical models. Probabilistic models thus arise from the deliberate or unavoidable
                      abstraction from complete information. The concept of the random variable provides the
                      mathematical foundation for formalizing such uncertainty.
                                                                                                      1
                    Exercise 14.1. Let Œ© = {Ì†µÌºî ,Ì†µÌºî ,Ì†µÌºî }, with Ì†µÌ±É(Ì†µÌºî ) = Ì†µÌ±É(Ì†µÌºî ) = Ì†µÌ±É(Ì†µÌºî ) =           .  Define random
                                                    1   2   3             1          2          3
                                                                                                      3
                                                                      57
                    variables Ì†µÌ±ã,Ì†µÌ±å ,Ì†µÌ±ç ‚à∂ Œ© ‚Üí ‚Ñù by
                                                  Ì†µÌ±ã(Ì†µÌºî ) = 1,   Ì†µÌ±ã(Ì†µÌºî ) = 2,  Ì†µÌ±ã(Ì†µÌºî ) = 3
                                                       1              2             3
                                                   Ì†µÌ±å (Ì†µÌºî ) = 2, Ì†µÌ±å (Ì†µÌºî ) = 3, Ì†µÌ±å (Ì†µÌºî ) = 1
                                                       1             2              3
                                                   Ì†µÌ±ç(Ì†µÌºî ) = 2,  Ì†µÌ±ç(Ì†µÌºî ) = 2,  Ì†µÌ±ç(Ì†µÌºî ) = 1
                                                       1             2              3
                       1. Show that Ì†µÌ±ã and Ì†µÌ±å have the same distribution.
                       2. Find the distribution of Ì†µÌ±ã + Ì†µÌ±å, Ì†µÌ±ãÌ†µÌ±å , and Ì†µÌ±ã/Ì†µÌ±å .
                                                                     58
               15 Data descriptives*
               Arandom variable is a mathematical abstraction that provides a bridge between theoretical
               probability and real-world data. Every dataset can be viewed as observations from random
               variables.
               Despite the outcome of any one event being uncertain, we can use patterns from past observa-
               tions to predict the general behavior of these variables. By collecting data, we can figure out
               how often certain outcomes occur and connect them to theoretical distributions.
                            Question with ‚Üí Data ‚Üí      Patterns
                             uncertainty       ‚Üì           ‚Üì
                                             RVs   ‚Üí Distributions ‚Üí Predictions
               Columns as random variables
               In a dataset, we view every column as a random variable.
               # Load a dataset from a CSV file
               exam <- read.csv("../dataset/exam.csv")
               # View the data: each column is a random variable
               head(exam)
                 id gender     major hw mid final overall
               1 1 Female Economics 85 89      74      81
               2 2 Female    Finance 90  84    79      83
               3 3 Female Economics 90 71      51      65
               4 4 Female    Finance 86  84    68      76
               5 5    Male   Finance 80  84    67      75
               6 6 Female    Finance 96 100    99      99
               Summary statistics
               We can describe the distribution of a variable with summary statistics: such as quartiles,
               deciles and percentiles.
                                                     59
                   summary(exam$overall)
                       Min. 1st Qu.      Median      Mean 3rd Qu.         Max.
                      45.00     72.00     77.50     76.56     84.00     99.00
                   Histograms
                   One way to visualize the distribution of a variable is to plot a histogram. A histogram groups
                   data points into intervals, showing how often data values fall within each range. The horizontal
                   axis represents the intervals (or bins), and the vertical axis shows the frequency or count of
                   data points in each bin.
                   A histogram gives an approximation of the true (unknown) distribution. It is not itself the
                   distribution.  A distribution refers to the theoretical assignment of probabilities across all
                   possible outcomes, whereas a histogram represents the empirical frequencies observed in a
                   finite sample.
                   hist(exam$final, prob = T, ann=F)
                     0.030
                     0.015
                     0.000
                          30       40       50       60       70       80       90      100
                   Boxplots
                   Aboxplot, also known as a box-and-whisker plot, displays the median, quartiles, and range of
                   the data. The box represents the interquartile range (IQR), which contains the middle 50% of
                   the data, with the lower and upper edges corresponding to the first (Q1) and third quartiles
                   (Q3). Whiskers extend from the box to indicate the range of values within 1.5 times the IQR
                   from Q1 and Q3, while points beyond this range are considered outliers.
                   boxplot(final ~ major, exam)
                                                                   60
            90
            70
         final
            50
            30
                  Economics     Finance
                          major
        Scatter plots
        To observe the relationship between variables, it is straightforward to make a scatter plot of
        Ì†µÌ±å against Ì†µÌ±ã. An upward-sloping pattern in the scatter plot indicates that the variables tend
        to move together, whereas a downward-sloping pattern suggests that they move in opposite
        directions. A flat slope implies the absence of correlation between the two variables.
        plot(final ~ mid, exam)
            90
            70
         final
            50
            30
              40  50   60  70  80   90  100
                           mid
                            61
                  16 Discrete RVs
                  Definition 16.1 (Discrete random variable). We say Ì†µÌ±ã is a discrete random variable if Ì†µÌ±ã can
                  take a finite or countable number of values Ì†µÌ±• ,Ì†µÌ±• ,‚Ä¶,Ì†µÌ±• .
                                                               1  2      Ì†µÌ±õ
                  Definition 16.2 (Support). The finite or countably infinite set of values Ì†µÌ±• such that Ì†µÌ±É(Ì†µÌ±ã =
                  Ì†µÌ±•) > 0 is called the support of Ì†µÌ±ã.
                  Definition 16.3 (Probability mass function). If a random variable Ì†µÌ±ã has a discrete distribu-
                  tion, the probability mass function (PMF) of Ì†µÌ±ã is defined as the function Ì†µÌ±ì ‚à∂ ‚Ñù ‚Üí [0,1] such
                  that
                                                        Ì†µÌ±ì(Ì†µÌ±•) ‚â° Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•).
                  Note that the PMF Ì†µÌ±ì(Ì†µÌ±•) is a discrete function which can only take values in the support
                  {Ì†µÌ±• ,Ì†µÌ±• ,‚Ä¶,Ì†µÌ±• }.
                    1   2     Ì†µÌ±õ
                    ƒπ Notation for PMF
                    Throughout this course, we use PMF to refer to the probability function for a discrete
                    random variable. Some textbooks may call it the probability function (p.f.), while others
                    may use the term mass function. All these terms describe the same concept.
                    Note that how Ì†µÌ±ì(Ì†µÌ±•) differs from the probability function Ì†µÌ±É(‚ãÖ). Ì†µÌ±ì(Ì†µÌ±•) is a real-valued
                    function, whereas Ì†µÌ±É(‚ãÖ) is the probability operator. The two should not be confused even
                    when the notation Ì†µÌ±ù(Ì†µÌ±•) is used to represent a PMF.
                    Wemaywanttouse a subscript to distinguish PMFs for different RVs. For example, Ì†µÌ±ì
                                                                                                          Ì†µÌ±ã
                    is the PMF for random variable Ì†µÌ±ã, Ì†µÌ±ì   is the PMF for random variable Ì†µÌ±å.
                                                         Ì†µÌ±å
                  Proposition 16.1. A probability mass function Ì†µÌ±ì ‚à∂ ‚Ñù ‚Üí [0,1] satisfies
                     1. Ì†µÌ±ì(Ì†µÌ±•) ‚â• 0 for all Ì†µÌ±• and Ì†µÌ±ì(Ì†µÌ±•) ‚â† 0 if and only if Ì†µÌ±• is in the support.
                        ‚àë
                     2.    Ì†µÌ±ì(Ì†µÌ±• ) = 1 where Ì†µÌ±ñ indexes every value in the support.
                               Ì†µÌ±ñ
                          Ì†µÌ±ñ
                  There are different ways to represent a PMF. We can (1) list all the possible values and their
                  associated probabilities; (2) write a formula for the PMF; or (3) visualize it in a graph.
                                                               62
                      Example 16.1 (Bernoulli distribution). A random variable Ì†µÌ±ã is said to have the Bernoulli
                      distribution if Ì†µÌ±ã has only two possible values, 0 and 1, and Ì†µÌ±É(Ì†µÌ±ã = 1) = Ì†µÌ±ù, Ì†µÌ±É(Ì†µÌ±ã = 0) = 1‚àíÌ†µÌ±ù.
                      The PMF of a Bernoulli random variable Ì†µÌ±ã is given by
                                                                           Ì†µÌ±ù       if Ì†µÌ±ò = 1,
                                                               Ì†µÌ±ì(Ì†µÌ±ò) = {
                                                                           1‚àíÌ†µÌ±ù if Ì†µÌ±ò = 0.
                      This can also be expressed as
                                                                    Ì†µÌ±ò        1‚àíÌ†µÌ±ò
                                                          Ì†µÌ±ì(Ì†µÌ±ò) = Ì†µÌ±ù (1 ‚àí Ì†µÌ±ù)    ,   Ì†µÌ±ò ‚àà {0,1}.
                      Example 16.2. A student is trying to connect to the campus Wi-Fi network. Each attempt
                      is independent, and:
                         ‚Ä¢ With probability Ì†µÌ±ù the attempt is successful.
                         ‚Ä¢ With probability 1‚àíÌ†µÌ±ù the attempt fails, and the student tries again.
                      The student will keep trying until the first success.
                         1. Define Ì†µÌ∞¥ = ‚Äúthe first successful connection occurs on the Ì†µÌ±ò-th attempt.‚Äù Find Ì†µÌ±É(Ì†µÌ∞¥ ).
                                       Ì†µÌ±ò                                                                                          Ì†µÌ±ò
                         2. Define a random variable Ì†µÌ±ã = ‚Äúthe number of attempts needed until the first success.‚Äù
                             What is the support of Ì†µÌ±ã?
                         3. Derive the probability mass function (PMF) of Ì†µÌ±ã.
                         4. Show that this is a valid PMF (Proposition 16.1).
                                                                             63
                    17 Continuous RVs
                    Definition 17.1 (Continuous random variable). We say a random variable Ì†µÌ±ã has a continuous
                    random variable if the possible values of Ì†µÌ±ã takes the form of a continuum.
                    Definition 17.2 (Probability density function). For a continuous random variable Ì†µÌ±ã, the
                    probability density function (PDF) of Ì†µÌ±ã is a real-valued function Ì†µÌ±ì ‚à∂ ‚Ñù ‚Üí [0,‚àû) such that
                                                                            Ì†µÌ±è
                                                       Ì†µÌ±É(Ì†µÌ±é ‚â§ Ì†µÌ±ã ‚â§ Ì†µÌ±è) = ‚à´ Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•.
                                                                           Ì†µÌ±é
                    Continuous random variables are usually measurements. Examples include height, weight,
                    temperature, the amount of money and so on.
                      ƒπ Density is not probability
                      PDFdiffers from the discrete PMF in important ways:
                         ‚Ä¢ For a continuous random variable, Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•) = 0 for all Ì†µÌ±•;
                         ‚Ä¢ The quantity Ì†µÌ±ì(Ì†µÌ±•) is not a probability. To get the probability, we integrate the
                            PDF(probability is the area under the PDF):
                                                                                       Ì†µÌ±è
                                                  Ì†µÌ±É(Ì†µÌ±é < Ì†µÌ±ã ‚â§ Ì†µÌ±è) = Ì†µÌ∞π(Ì†µÌ±è) ‚àí Ì†µÌ∞π(Ì†µÌ±é) = ‚à´ Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•.
                                                                                      Ì†µÌ±é
                         ‚Ä¢ Since any single value has probability 0, including or excluding endpoints does not
                            matter.
                                     Ì†µÌ±É(Ì†µÌ±é < Ì†µÌ±ã < Ì†µÌ±è) = Ì†µÌ±É(Ì†µÌ±é < Ì†µÌ±ã ‚â§ Ì†µÌ±è) = Ì†µÌ±É(Ì†µÌ±é ‚â§ Ì†µÌ±ã < Ì†µÌ±è) = Ì†µÌ±É(Ì†µÌ±é ‚â§ Ì†µÌ±ã ‚â§ Ì†µÌ±è).
                    Proposition 17.1. If Ì†µÌ±ã has density function Ì†µÌ±ì then
                       1. Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•) = 0 for all Ì†µÌ±• ‚àà ‚Ñù
                                              Ì†µÌ±è
                                            ‚à´
                       2. Ì†µÌ±É(Ì†µÌ±é ‚â§ Ì†µÌ±ã ‚â§ Ì†µÌ±è) =   Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•
                                             Ì†µÌ±é
                           ‚àû
                          ‚à´
                       3.      Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±• = 1
                           ‚àí‚àû
                                                                     64
                    Example 17.1 (Uniform distribution). A uniform distribution is a probability distribution
                    where all values within a specified interval [Ì†µÌ±é,Ì†µÌ±è] are equally likely to occur, and its probability
                    density function (PDF) is given by:
                                                                 1
                                                       Ì†µÌ±ì(Ì†µÌ±•) =        for   Ì†µÌ±é ‚â§ Ì†µÌ±• ‚â§ Ì†µÌ±è
                                                               Ì†µÌ±è ‚àí Ì†µÌ±é
                    and Ì†µÌ±ì(Ì†µÌ±•) = 0 otherwise.
                      ¬æ Don‚Äôt confuse a random variable with its distribution
                                                                                               2         2
                      If random variable Ì†µÌ±ã has distribution Ì†µÌ±ì(Ì†µÌ±•), the distribution of Ì†µÌ±ã      is not Ì†µÌ±ì (Ì†µÌ±•). To get
                      the distribution of Ì†µÌ±ã + Ì†µÌ±å , you can‚Äôt just add up Ì†µÌ±ì (Ì†µÌ±•) + Ì†µÌ±ì (Ì†µÌ±¶). The right way to do it
                                                                              Ì†µÌ±ã       Ì†µÌ±å
                      will be discussed in later chapters (transformation and convolution).
                    Example 17.2. Every morning, a student waits for the elevator in their dormitory. The
                    waiting time (in minutes) is equally likely to be anywhere between 0 and 3 minutes, depending
                    on when the elevator arrives.
                       1. Define Ì†µÌ±ã = ‚Äúthe student‚Äôs elevator waiting time.‚Äù What is the support of Ì†µÌ±ã?
                       2. Derive the probability density function (PDF) of Ì†µÌ±ã.
                       3. Compute Ì†µÌ±É(Ì†µÌ±ã ‚â§ 1), i.e. the probability that the waiting time is at most 1 minute.
                       4. If the student must wait more than 2 minutes, they decide to take the stairs instead.
                          Define a new indicator random variable Ì†µÌ±å, which equals 1 if Ì†µÌ±ã > 2 and 0 otherwise.
                          Compute Ì†µÌ±É(Ì†µÌ±å = 1).
                                                                      65
                       18 Cumulative distribution
                       Definition 18.1 (Cumulative distribution function). The cumulative distribution function
                      (CDF) of a random variable Ì†µÌ±ã is the function Ì†µÌ∞π given by Ì†µÌ∞π(Ì†µÌ±•) = Ì†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±•).
                       For discrete random variables, Ì†µÌ∞π(Ì†µÌ±•) = ‚àë                Ì†µÌ±ù(Ì†µÌ±ò).
                                                                           Ì†µÌ±ò‚â§Ì†µÌ±•
                                                                              Ì†µÌ±•
                                                                                                             Ì†µÌ±ëÌ†µÌ∞π(Ì†µÌ±•)
                                                                            ‚à´
                       For continuous random variables, Ì†µÌ∞π(Ì†µÌ±•) =                 Ì†µÌ±ì(Ì†µÌ±°)Ì†µÌ±ëÌ†µÌ±°. We thus have           =Ì†µÌ±ì(Ì†µÌ±•).
                                                                                                               Ì†µÌ±ëÌ†µÌ±•
                                                                             ‚àí‚àû
                       Unlike PMF or PDF, a cumulative distribution function can be defined for both discrete and
                       continuous random variables. CDF gives the full distribution of a random variable. Given the
                       CDF, we can figure out any probability distribution of the random variable:
                                                            Ì†µÌ±É(Ì†µÌ±•  <Ì†µÌ±• ‚â§ Ì†µÌ±• ) = Ì†µÌ∞π(Ì†µÌ±• ) ‚àí Ì†µÌ∞π(Ì†µÌ±• ).
                                                                 1           2           2          1
                       Proposition 18.1. Any CDF has the following properties:
                          ‚Ä¢ Ì†µÌ±É(Ì†µÌ±ã > Ì†µÌ±•) = 1 ‚àíÌ†µÌ∞π(Ì†µÌ±•)
                          ‚Ä¢ Ì†µÌ±É(Ì†µÌ±• < Ì†µÌ±• ‚â§ Ì†µÌ±• ) = Ì†µÌ∞π(Ì†µÌ±• ) ‚àí Ì†µÌ∞π(Ì†µÌ±• )
                                   1           2          2          1
                          ‚Ä¢ Increasing: if Ì†µÌ±• ‚â§ Ì†µÌ±• , then Ì†µÌ∞π(Ì†µÌ±• ) ‚â§ Ì†µÌ∞π(Ì†µÌ±• ).
                                                 1      2              1          2
                          ‚Ä¢ Right-continuous: for any Ì†µÌ±é, Ì†µÌ∞π(Ì†µÌ±é) = lim                 Ì†µÌ∞π(Ì†µÌ±•).
                                                                                 Ì†µÌ±•‚ÜíÌ†µÌ±é+
                          ‚Ä¢ Ì†µÌ∞π(Ì†µÌ±•) ‚Üí 0 as Ì†µÌ±• ‚Üí ‚àí‚àû; Ì†µÌ∞π(Ì†µÌ±•) ‚Üí 1 as Ì†µÌ±• ‚Üí +‚àû.
                      The CDF for a continuous random variable is differentiable, while the CDF for a discrete
                       random variable consists of jumps and flat regions.
                                                                                66
                            67
                                Part III
                      Discrete Distributions
                                   68
                   19 Binomial distribution
                   Definition 19.1 (Binomial distribution). Suppose Ì†µÌ±ã ,Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã are independent and identi-
                                                                           1   2       Ì†µÌ±õ
                   cal Bern(Ì†µÌ±ù) distributions. Let Ì†µÌ±ã be the total number of successes of the Ì†µÌ±õ independent trials.
                   That is, Ì†µÌ±ã = Ì†µÌ±ã +Ì†µÌ±ã +‚ãØ+Ì†µÌ±ã . Then Ì†µÌ±ã has the Binomial distribution, Ì†µÌ±ã ‚àº Bin(Ì†µÌ±õ,Ì†µÌ±ù).
                                   1     2          Ì†µÌ±õ
                   The PMF of Ì†µÌ±ã directly follows from the combination theory:
                                                                   Ì†µÌ±õ
                                                                        Ì†µÌ±ò      Ì†µÌ±õ‚àíÌ†µÌ±ò
                                                    Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±ò) = ( )Ì†µÌ±ù (1 ‚àí Ì†µÌ±ù)    .
                                                                   Ì†µÌ±ò
                   This is a valid PMF because, by the Binomial theorem, we have
                                               Ì†µÌ±õ
                                                   Ì†µÌ±õ
                                                        Ì†µÌ±ò      Ì†µÌ±õ‚àíÌ†µÌ±ò               Ì†µÌ±õ
                                             ‚àë( )Ì†µÌ±ù (1‚àíÌ†µÌ±ù)          =(Ì†µÌ±ù+(1‚àíÌ†µÌ±ù)) =1.
                                                   Ì†µÌ±ò
                                              Ì†µÌ±ò=0
                     ƒπ Binomial distribution and Binomial theorem
                     You may have noticed the connection between Binomial distribution and Binomial theo-
                     rem. Consider using the polynomial Ì†µÌ±ùÌ†µÌ±•+Ì†µÌ±û to represent the outcome of a single Bernoulli
                                                                                      Ì†µÌ±õ
                     trial, where Ì†µÌ±• is the indicator for a success. Then (Ì†µÌ±ùÌ†µÌ±• + Ì†µÌ±û)   is the outcome for Ì†µÌ±õ in-
                                                              Ì†µÌ±ò
                     dependent trials. The coeÔ¨Äicient of Ì†µÌ±•      gives the probability of there being exactly Ì†µÌ±ò
                     successes.
                   Theorem 19.1. Let Ì†µÌ±ã ‚àº Bin(Ì†µÌ±õ,Ì†µÌ±ù) and Ì†µÌ±å ‚àº Bin(Ì†µÌ±ö,Ì†µÌ±ù) be two independent Binomial random
                   variables. Then Ì†µÌ±ã +Ì†µÌ±å ‚àº Bin(Ì†µÌ±õ+Ì†µÌ±ö,Ì†µÌ±ù).
                                                                                       Ì†µÌ±õ
                   Proof. By the definition of the Binomial distribution, Ì†µÌ±ã = ‚àë          Ì†µÌ±ã where Ì†µÌ±ã ‚àº Bern(Ì†µÌ±ù);
                                                                                            Ì†µÌ±ñ          Ì†µÌ±ñ
                                                                                       Ì†µÌ±ñ=1
                           Ì†µÌ±ö
                   Ì†µÌ±å = ‚àë     Ì†µÌ±å where Ì†µÌ±å ‚àº Bern(Ì†µÌ±ù). Therefore,
                                Ì†µÌ±ó        Ì†µÌ±ó
                           Ì†µÌ±ó=1
                                                                               Ì†µÌ±õ+Ì†µÌ±ö
                                                             Ì†µÌ±õ       Ì†µÌ±ö
                                                  Ì†µÌ±ã +Ì†µÌ±å = ‚àëÌ†µÌ±ã +‚àëÌ†µÌ±å = ‚àëÌ†µÌ±ç
                                                                  Ì†µÌ±ñ       Ì†µÌ±ó        Ì†µÌ±ò
                                                            Ì†µÌ±ñ=1      Ì†µÌ±ó=1
                                                                               Ì†µÌ±ò=1
                   where Ì†µÌ±ç ‚àº Bern(Ì†µÌ±ù). Since Ì†µÌ±ã and Ì†µÌ±å are identical Bernoulli random variables,
                           Ì†µÌ±ò                     Ì†µÌ±ñ      Ì†µÌ±ó
                                                       Ì†µÌ±ã ,     for Ì†µÌ±ò = 1,‚Ä¶,Ì†µÌ±õ
                                                         Ì†µÌ±ò
                                               Ì†µÌ±ç ={
                                                 Ì†µÌ±ò
                                                       Ì†µÌ±å   ,   for Ì†µÌ±ò = Ì†µÌ±õ + 1,‚Ä¶,Ì†µÌ±õ + Ì†µÌ±ö
                                                        Ì†µÌ±ò‚àíÌ†µÌ±õ
                                               Ì†µÌ±õ+Ì†µÌ±ö
                   By definition, Ì†µÌ±ã + Ì†µÌ±å = ‚àë      Ì†µÌ±ç  ‚àºBin(Ì†µÌ±õ+Ì†µÌ±ö,Ì†µÌ±ù).
                                                     Ì†µÌ±ò
                                               Ì†µÌ±ò=1
                                                                   69
               Coin tossing problem
               Example19.1. Inthepreviousexampleoftossingtwocoins,wecomputethedistributionofÌ†µÌ±ã
               by counting the equally likely outcomes in an event. We can get the same result by realizing
               it is a Binomial distribution. Ì†µÌ±ã ‚àº Bin(2,1/2). Since each coin tossing is an independent
               Bernoulli trial. The probabilities come directly from the PMF.
                                                          0    2
                                                    2   1    1     1
                                       Ì†µÌ±É(Ì†µÌ±ã = 0) = ( )( ) ( ) = ;
                                                    0   2    2     4
                                                          1    1
                                                    2   1    1     1
                                       Ì†µÌ±É(Ì†µÌ±ã = 1) = ( )( ) ( ) = ;
                                                    1   2    2     2
                                                          2    0
                                                    2   1    1     1
                                       Ì†µÌ±É(Ì†µÌ±ã = 2) = ( )( ) ( ) = .
                                                    2   2    2     4
               Utilizing the Binomial distribution also allows us to generalize the problem. Suppose we are
               tossing Ì†µÌ±õ coins, we want to find the probability of getting Ì†µÌ±ò heads. It is almost impossible to
               count all the possible outcomes, but the answer immediately follows from the Binomial PMF.
               The following code simulates the number of heads if tossing Ì†µÌ±Å coins:
               # Number of simulations
               k <- 1000
               # Number of coins
               n <- 20
               # Store the results
               n_heads <- numeric(k)
               # Initialize random generator
               set.seed(100)
               # Run simulations
               for (i in 1:k) {
                 toss <- sample(c('H','T'), n, replace = T)
                 n_heads[i] <- sum(toss == 'H')
               }
               # Plot distribution
               hist(n_heads, probability=TRUE)
                                                     70
        # Overlay the Binomial PMF
        curve(choose(n,x)* 0.5^x * 0.5^(n-x), from=1, to=n, n=n, col=2, add=T)
                     Histogram of n_heads
            0.10
         Density
            0.00
                4   6   8   10  12  14  16
                          n_heads
        Binomial functions in R
        There are built-in functions in R to work with Binomial distributions.
        # computes P(X=5) for Bin(10,0.5)
        p <- dbinom(5, 10, 0.5)
        par(mfrow=c(1,2))
        # plot the PMF for Bin(10,0.5)
        curve(dbinom(x, 10, 0.5), from=0, to=10, n=11, type="b", ann=F)
        # `pbinom` computes the CDF
        curve(pbinom(x, 10, 0.5), from=0, to=10, n=11, type="b",ann=F)
                            71
              0.15                          0.6
              0.00                          0.0
                 0  2  4  6  8 10              0  2  4  6  8 10
             # draw a random value from a given Binomial distribution
             # this allows us to simulate a random experiment
             # e.g. the number of heads when flipping 10 fair coins
             outcome <- rbinom(1, 10, 0.5)
             # Repeat the experiment 1000 times
             heads <- rbinom(1000, 10, 0.5)
             # the histogram will converge to the ideal Binomial distribution
             # if the experiment is repeated a large number of times
             hist(heads)
                                   Histogram of heads
                   250
                   150
              Frequency
                   50
                   0
                           2         4        6         8        10
                                          heads
             Exam survival problem
             Example 19.2. An exam consists of 20 multiple-choice questions, each with four choices and
             exactly one correct answer. Suppose a student answers every question by guessing at random.
                                              72
                What is the probability that the student passes the exam, defined as answering more than
                60% of the questions correctly?
                Solution. The probability of correctly answering one question is Ì†µÌ±ù = 1/4. Let Ì†µÌ±Å be the total
                number of questions, Ì†µÌ±Å = 20. Let Ì†µÌ±ã be the number of correctly answered questions, Ì†µÌ±ã ‚â§ Ì†µÌ±Å.
                Then Ì†µÌ±ã follows the Binomial distribution Ì†µÌ±ã ‚àº Ì†µÌ∞µ(Ì†µÌ±Å,1/4). The probability of passing the
                exam is therefore
                                                                     Bin
                                Ì†µÌ±É(Ì†µÌ±ã ‚â• 12) = 1 ‚àíÌ†µÌ±É(Ì†µÌ±ã ‚â§ 11) = 1‚àíCDF   (11) ‚âà 0.001.
                Now we compare the survival probability for different choice of Ì†µÌ±Å and Ì†µÌ±ù :
                # Percentage of correct answers
                x <- seq(0, 1, .1)
                # Survival probabilities for different N
                y1 <- 1 - pbinom(10*x, 10, .25)
                y2 <- 1 - pbinom(20*x, 20, .25)
                y3 <- 1 - pbinom(30*x, 30, .25)
                # Compare the curves for different N
                plot(x, y1, type="b", col=1, ann=F)
                lines(x, y2, type="b", col=2)
                lines(x, y3, type="b", col=3)
                # Indicating passing the exam
                abline(v=0.6, lty=2)
                # Add a legend at the top right corner of the plot
                legend("topright", c("N=10", "N=20", "N=30"), lty=1, col=1:3)
                  0.8                                                   N=10
                                                                        N=20
                                                                        N=30
                  0.4
                  0.0
                     0.0        0.2        0.4       0.6        0.8       1.0
                                                        73
        # Percentage of correct answers
        x <- seq(0, 1, .1)
        # Survival probabilities for different p (number of choices)
        y1 <- 1 - pbinom(10*x, 10, .25)
        y2 <- 1 - pbinom(10*x, 10, .33)
        y3 <- 1 - pbinom(10*x, 10, .5)
        # Compare the curves for different p
        plot(x, y1, type="b", col=1, ann=F)
        lines(x, y2, type="b", col=2)
        lines(x, y3, type="b", col=3)
        # Indicating passing the exam
        abline(v=0.6, lty=2)
        # Add a legend at the top right corner of the plot
        legend("topright", c("p=1/4", "p=1/3", "p=1/2"), lty=1,col=1:3)
         0.8                        p=1/4
                                    p=1/3
                                    p=1/2
         0.4
         0.0
           0.0  0.2   0.4  0.6  0.8  1.0
                            74
                                     20 Discrete expectation
                                     Definition 20.1 (Expectation of a discrete random variable). Let Ì†µÌ±ã be a discrete random
                                     variable. The expectation of Ì†µÌ±ã (or the mean of Ì†µÌ±ã) is defined as:
                                                                                                            Ì†µÌ∞∏(Ì†µÌ±ã) = ‚àëÌ†µÌ±•Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•).
                                                                                                                              all Ì†µÌ±•
                                     In other words, the expected value of Ì†µÌ±ã is a weighted average of the possible values that Ì†µÌ±ã
                                     can take on, weighted by their probabilities.
                                         ƒπ Note
                                         The expected value of Ì†µÌ±ã is a fixed number, Ì†µÌ∞∏(Ì†µÌ±ã) ‚àà ‚Ñù. It is not a random variable such
                                         as Ì†µÌ±î(Ì†µÌ±ã).
                                     Sometimes, we would like to omit the parentheses for simplicity and write Ì†µÌ∞∏Ì†µÌ±ã ‚à∂= Ì†µÌ∞∏(Ì†µÌ±ã). We
                                     also like to denote expectation by the Greek letter Ì†µÌºá ‚à∂= Ì†µÌ∞∏(Ì†µÌ±ã).
                                     Example 20.1. The expectation of a Bernoulli random variable Ì†µÌ±ã ‚àº Bern(Ì†µÌ±ù):
                                                                                        Ì†µÌ∞∏(Ì†µÌ±ã) = 1 √ó Ì†µÌ±É(Ì†µÌ±ã = 1) + 0 √ó Ì†µÌ±É(Ì†µÌ±ã = 0) = Ì†µÌ±ù.
                                     Example 20.2. The expectation of a Binomial random variable Ì†µÌ±ã ‚àº Bin(Ì†µÌ±õ,Ì†µÌ±ù):
                                                                                                                       Ì†µÌ±õ
                                                                                                    Ì†µÌ∞∏(Ì†µÌ±ã) =‚àëÌ†µÌ±òÌ†µÌ±ù(Ì†µÌ±ò)
                                                                                                                     Ì†µÌ±ò=0
                                                                                                                       Ì†µÌ±õ
                                                                                                                                      Ì†µÌ±õ
                                                                                                                                               Ì†µÌ±ò  Ì†µÌ±õ‚àíÌ†µÌ±ò
                                                                                                                 =‚àëÌ†µÌ±ò‚ãÖ( )Ì†µÌ±ù Ì†µÌ±û
                                                                                                                                      Ì†µÌ±ò
                                                                                                                     Ì†µÌ±ò=0
                                                                                                                       Ì†µÌ±õ
                                                                                                                                      Ì†µÌ±õ ‚àí 1
                                                                                                                                                       Ì†µÌ±ò   Ì†µÌ±õ‚àíÌ†µÌ±ò
                                                                                                                 =‚àëÌ†µÌ±õ‚ãÖ(                          )Ì†µÌ±ù Ì†µÌ±û
                                                                                                                                      Ì†µÌ±ò ‚àí 1
                                                                                                                     Ì†µÌ±ò=1
                                                                                                                             Ì†µÌ±õ
                                                                                                                                     Ì†µÌ±õ ‚àí 1
                                                                                                                                                      Ì†µÌ±ò‚àí1 Ì†µÌ±õ‚àíÌ†µÌ±ò
                                                                                                                          ‚àë( )Ì†µÌ±ù
                                                                                                                 =Ì†µÌ±õÌ†µÌ±ù                                       Ì†µÌ±û
                                                                                                                                      Ì†µÌ±ò ‚àí 1
                                                                                                                          Ì†µÌ±ò=1
                                                                                                                           Ì†µÌ±õ‚àí1
                                                                                                                                      Ì†µÌ±õ ‚àí 1
                                                                                                                                                       Ì†µÌ±ó  Ì†µÌ±õ‚àí1‚àíÌ†µÌ±ó
                                                                                                                 =Ì†µÌ±õÌ†µÌ±ù ‚àë(                        )Ì†µÌ±ù Ì†µÌ±û
                                                                                                                                           Ì†µÌ±ó
                                                                                                                           Ì†µÌ±ó=0
                                                                                                                          ‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü
                                                                                                                              another Binomial PMF
                                                                                                                 =Ì†µÌ±õÌ†µÌ±ù.
                                                                                                                                   75
                  Proposition 20.1. Expectation has the following properties:
                     ‚Ä¢ Ì†µÌ∞∏(Ì†µÌ±ã +Ì†µÌ±å) = Ì†µÌ∞∏(Ì†µÌ±ã)+Ì†µÌ∞∏(Ì†µÌ±å)
                     ‚Ä¢ Ì†µÌ∞∏(Ì†µÌ±éÌ†µÌ±ã + Ì†µÌ±è) = Ì†µÌ±éÌ†µÌ∞∏(Ì†µÌ±ã) + Ì†µÌ±è
                  Example 20.3. Redo the expectation of Ì†µÌ±ã ‚àº Bin(Ì†µÌ±õ,Ì†µÌ±ù) with properties of expectation:
                                            Ì†µÌ∞∏(Ì†µÌ±ã) = Ì†µÌ∞∏(Ì†µÌ±ã +‚ãØ+Ì†µÌ±ã ) = Ì†µÌ±õÌ†µÌ∞∏(Ì†µÌ±ã ) = Ì†µÌ±õÌ†µÌ±ù
                                                          1         Ì†µÌ±õ         Ì†µÌ±ñ
                  where Ì†µÌ±ã ‚àº Bern(Ì†µÌ±ù).
                           Ì†µÌ±ñ
                  Law of averages*
                  You may wonder what is the difference between Ì†µÌ∞∏(Ì†µÌ±ã) defined in Definition 20.1 and the
                                                    1
                                                 ÃÑ
                  average of values defined as Ì†µÌ±ã =   (Ì†µÌ±ã +Ì†µÌ±ã +‚ãØ+Ì†µÌ±ã ).
                                                        1     2          Ì†µÌ±õ
                                                    Ì†µÌ±õ
                                                                                  ÃÑ
                  The short answer is this: Ì†µÌ∞∏(Ì†µÌ±ã) is a theoretical value, while Ì†µÌ±ã is an approximation to Ì†µÌ∞∏(Ì†µÌ±ã)
                  with finite observations. They are associated by the following theorem.
                    ƒπ Law of averages
                    The law of averages (or the law of large numbers) states that if you repeat a random
                    experiment, such as tossing a coin or rolling a die, a very large number of times, your
                    individual outcomes, when averaged, should be very close to the theoretical mean (a
                    constant parameter). In mathematical language,
                                                           Ì†µÌ±ù
                                                       ÃÑ
                                                     Ì†µÌ±ã ‚Üí Ì†µÌºá when Ì†µÌ±õ ‚Üí ‚àû.
                                                       Ì†µÌ±õ
                             Ì†µÌ±ù
                    where ‚Üí reads as ‚Äúconverge in probability‚Äù.
                  There is another fundamental difference. In probability theory, we treat Ì†µÌ∞∏(Ì†µÌ±ã) as a fixed
                                   ÃÑ
                  number; whileÌ†µÌ±ã is another randomvariable! Because the sample {Ì†µÌ±ã ,Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã } is generated
                                                                                       1   2      Ì†µÌ±õ
                  randomly. Consider the coin flipping example, while Ì†µÌ∞∏(Ì†µÌ±ã) = 0.5 is a constant, each time you
                  compute the average of, say, 10 flips, you get a different number. We will come back to this
                  point later.
                                                                76
                               21 Hypergeometric dist
                               Example 21.1. Let‚Äôs explore an example that appears to be Binomial but is, in fact, not a
                               Binomial distribution. Given a 5-card hand. Find the distribution of the number of aces.
                               Let Ì†µÌ±ã be the number of aces. It is tempting to say Ì†µÌ±ã ‚àº Bin(5,Ì†µÌ±ù). But this not correct.
                               Because having one ace is NOT independent from having another ace. We need to use the
                               classical approach:
                                                                                                                      4     48
                                                                                                                    ( )(        )
                                                                                                                      Ì†µÌ±ò   5‚àíÌ†µÌ±ò
                                                                                             Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±ò) =                         .
                                                                                                                          52
                                                                                                                        ( )
                                                                                                                          5
                               This is a Hypergeometric distribution.
                               Suppose we have a box filled with Ì†µÌ±§ white and Ì†µÌ±è black balls. We draw Ì†µÌ±õ balls out of the box
                               with replacement. Let Ì†µÌ±ã be the number of white balls. Then Ì†µÌ±ã ‚àº Bin(Ì†µÌ±õ,Ì†µÌ±§/(Ì†µÌ±§+Ì†µÌ±è)). Since the
                               drawsareindependentBernoullitrials, each with probability Ì†µÌ±§/(Ì†µÌ±§+Ì†µÌ±è) of success. If we instead
                               sample without replacement, then the number of white balls follows a Hypergeometric
                               distribution. We denote this by Ì†µÌ±ã ‚àº HGeom(Ì†µÌ±§,Ì†µÌ±è,Ì†µÌ±õ).
                               Theorem 21.1. If Ì†µÌ±ã ‚àº HGeom(Ì†µÌ±§,Ì†µÌ±è,Ì†µÌ±õ), then the PMF of Ì†µÌ±ã is
                                                                                                                  Ì†µÌ±§      Ì†µÌ±è
                                                                                                                ( )(         )
                                                                                                                  Ì†µÌ±ò    Ì†µÌ±õ‚àíÌ†µÌ±ò
                                                                                                Ì†µÌ±ù   (Ì†µÌ±ò) =                     ,
                                                                                                  Ì†µÌ±ã
                                                                                                                     Ì†µÌ±§+Ì†µÌ±è
                                                                                                                   (       )
                                                                                                                       Ì†µÌ±õ
                               for integers Ì†µÌ±ò satisfying 0 ‚â§ Ì†µÌ±ò ‚â§ Ì†µÌ±§ and 0 ‚â§ Ì†µÌ±õ ‚àí Ì†µÌ±ò ‚â§ Ì†µÌ±è, and Ì†µÌ±ù (Ì†µÌ±ò) = 0 otherwise.
                                                                                                                                         Ì†µÌ±ã
                               In Example 21.1, the number of aces in the hand has the HGeom(4,48,5) distribution, which
                               can be seen by thinking of the aces as white balls and the non-aces as black balls. The
                               probability of having exactly three aces is 0.0017%.
                               Example 21.2. Let Ì†µÌ±ã ‚àº HGeom(Ì†µÌ±§,Ì†µÌ±è,Ì†µÌ±õ). Find Ì†µÌ∞∏(Ì†µÌ±ã) the expected number of white balls.
                               Similarly, we can decompose Ì†µÌ±ã:
                                                                                                  Ì†µÌ±ã = Ì†µÌ∞º +‚ãØ+Ì†µÌ∞º
                                                                                                             1               Ì†µÌ±õ
                               where Ì†µÌ∞º equals 1 if the Ì†µÌ±óth ball is white and 0 otherwise. We have said that {Ì†µÌ∞º } are not
                                             Ì†µÌ±ó                                                                                                                                 Ì†µÌ±ó
                               independent, but the linearity of expectation still holds:
                                                                        Ì†µÌ∞∏(Ì†µÌ±ã) = Ì†µÌ∞∏(Ì†µÌ∞º + ‚ãØ + Ì†µÌ∞º ) = Ì†µÌ∞∏(Ì†µÌ∞º ) + ‚ãØ + Ì†µÌ∞∏(Ì†µÌ∞º ).
                                                                                              1               Ì†µÌ±õ             1                      Ì†µÌ±õ
                                                                                                               77
                   Meanwhile we have
                                                                                    Ì†µÌ±§
                                                Ì†µÌ∞∏(Ì†µÌ∞º ) = Ì†µÌ±É(Ì†µÌ±ó-th ball is white) =
                                                    Ì†µÌ±ó
                                                                                  Ì†µÌ±§ + Ì†µÌ±è
                                                                                                                 Ì†µÌ±õÌ†µÌ±§
                   since unconditionally the Ì†µÌ±óth ball is equally likely to be any of the balls. Thus, Ì†µÌ∞∏(Ì†µÌ±ã) =     .
                                                                                                                Ì†µÌ±§+Ì†µÌ±è
                   The Binomial and Hypergeometric distributions are often confused. Both are discrete distri-
                   butions taking on integer values between 0 and Ì†µÌ±õ for some Ì†µÌ±õ, and both can be interpreted as
                   the number of successes in Ì†µÌ±õ Bernoulli trials. However, a crucial part of the Binomial story is
                   that the Bernoulli trials involved are independent. The Bernoulli trials in the Hypergeometric
                   story are dependent, since the sampling is done without replacement.
                                                                   78
                 22 Geometric distribution
                 Definition 22.1 (Geometric distribution). Consider a sequence of independent Bernoulli tri-
                 als, each with the same success probability Ì†µÌ±ù. Let Ì†µÌ±ã be the number of failures before the first
                 successful trial. Then Ì†µÌ±ã has a Geometric distribution: Ì†µÌ±ã ‚àº Geom(Ì†µÌ±ù).
                 Let‚Äôs derive the PMF for the Geometric distribution. By definition,
                                                                    Ì†µÌ±ò
                                                      Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±ò) = Ì†µÌ±û Ì†µÌ±ù
                 where Ì†µÌ±û = 1 ‚àí Ì†µÌ±ù. This is a valid PMF because
                                                ‚àû          ‚àû
                                                                     Ì†µÌ±ù
                                                    Ì†µÌ±ò         Ì†µÌ±ò
                                               ‚àëÌ†µÌ±û Ì†µÌ±ù = Ì†µÌ±ù‚àëÌ†µÌ±û =          =1.
                                                                   1‚àíÌ†µÌ±û
                                               Ì†µÌ±ò=0       Ì†µÌ±ò=0
                 The expectation of Ì†µÌ±ã is given by
                                                   ‚àû             ‚àû
                                                                           Ì†µÌ±û   Ì†µÌ±û
                                                         Ì†µÌ±ò           Ì†µÌ±ò
                                          Ì†µÌ∞∏(Ì†µÌ±ã) = ‚àëÌ†µÌ±ò‚ãÖÌ†µÌ±û Ì†µÌ±ù = Ì†µÌ±ù‚àëÌ†µÌ±òÌ†µÌ±û = Ì†µÌ±ù   = .
                                                                            2
                                                                           Ì†µÌ±ù   Ì†µÌ±ù
                                                  Ì†µÌ±ò=0          Ì†µÌ±ò=0
                                                                                              ‚àû
                                                                                                         1
                                                                                                   Ì†µÌ±ò
                 To see why this holds, taking derivative with respect to Ì†µÌ±û on both sides of ‚àë   Ì†µÌ±û =
                                                                                                        1‚àíÌ†µÌ±û
                                                                                              Ì†µÌ±ò=0
                 yields
                                                     ‚àû
                                                                   1
                                                          Ì†µÌ±ò‚àí1
                                                    ‚àëÌ†µÌ±òÌ†µÌ±û    =          ;
                                                                      2
                                                                (1‚àíÌ†µÌ±û)
                                                    Ì†µÌ±ò=1
                 Then multiply both sides by Ì†µÌ±û:
                                                   ‚àû
                                                               Ì†µÌ±û      Ì†µÌ±û
                                                        Ì†µÌ±ò
                                                  ‚àëÌ†µÌ±òÌ†µÌ±û =           = .
                                                                   2    2
                                                            (1‚àíÌ†µÌ±û)     Ì†µÌ±ù
                                                  Ì†µÌ±ò=1
                 Plot the PMF and CDF
                                                             79
             par(mfrow=c(1,2))
             # PMF for Geom(0.5)
             curve(dgeom(x, 0.5), from=0, to=10, n=11, type="b", ann=F)
             # CDF for Geom(0.5)
             curve(pgeom(x, 0.5), from=0, to=10, n=11, type="b", ann=F)
              0.4                           0.9
              0.2                           0.7
              0.0                           0.5
                 0  2  4  6  8 10              0  2  4  6  8 10
             Example 22.1 (Coin flip until Head). Flipping a fair coin, what is the expected number of
             flips before the first Head?
             Let Ì†µÌ±ã be the number of flips until the first head. We know Ì†µÌ±ã ‚àí 1 ‚àº Geom(0.5) as geometric
             distribution models the number of failures excluding the success. Thus, Ì†µÌ∞∏(Ì†µÌ±ã‚àí1) = 0.5/0.5 = 1,
             Ì†µÌ∞∏(Ì†µÌ±ã) = 2. Let‚Äôs compare the theoretical value with results from simulations.
             # number of simulations
             N <- 1000
             # X: number of flips until first head
             # stores value of X in each simulation
             X <- numeric(N)
             set.seed(100)
             # run simulations
             for (i in 1:N) {
               x <- 0
               # repeat until first head
               while(TRUE) {
                x <- x + 1
                t <- sample(c('H','T'), 1, F)
                if (t == 'H') break
               }
                                              80
         # record the number
         X[i] <- x
        }
        # plot distribution of X
        hist(X, probability=T)
        #overlay with geometric distribution
        curve(dgeom(x-1,.5), from=1, to=10,n=10,add=T,col=2)
                       Histogram of X
            0.6
            0.4
         Density
            0.2
            0.0
                 2     4     6     8    10
                           X
        cat("Average number of flips until Head:", mean(X))
        Average number of flips until Head: 2.021
                            81
             23 Coin flip: HH vs HT*
             Flip a coin indefinite times. Let Ì†µÌ±ã denote the number of flips until you see HH. Let Ì†µÌ±å denote
             the number of flips until you see HT. Find Ì†µÌ∞∏(Ì†µÌ±ã) and Ì†µÌ∞∏(Ì†µÌ±å).
             It is tempting to think they are the same, since either H or T happens with probability 1/2.
             But the answer is extremely counter-intuitive: Ì†µÌ∞∏(Ì†µÌ±ã) > Ì†µÌ∞∏(Ì†µÌ±å)!
             HHcase. Let Ì†µÌ∞∏ = E(X|No H observed), and Ì†µÌ∞∏ = E(X|One H observed). Then
                         0                      1
                                             1    1
                                      Ì†µÌ∞∏ =1+ Ì†µÌ∞∏ + Ì†µÌ∞∏
                                       0       1    0
                                             2    2
             The first term is we need to flip once. If the first flip is H, the additional expected number of
             flips is Ì†µÌ∞∏ . If the first flip is T, we have to start over again (Ì†µÌ∞∏ ).
                   1                                    0
                                             1    1
                                      Ì†µÌ∞∏ =1+ (0)+ Ì†µÌ∞∏
                                       1            0
                                             2    2
             Once we have observed an H, we do another flip. If it is another H, we are done. If it is a T,
             we have to start over again (Ì†µÌ∞∏ ).
                                  0
             Solve the two equations, we have Ì†µÌ∞∏ = 6, Ì†µÌ∞∏ = 4. Thus, Ì†µÌ∞∏(Ì†µÌ±ã) = 6.
                                      0    1
             HTcase. Let Ì†µÌ∞∏ = E(Y|No H observed), and Ì†µÌ∞∏ = E(Y|One H observed). Then
                         0                     1
                                             1    1
                                      Ì†µÌ∞∏ =1+ Ì†µÌ∞∏ + Ì†µÌ∞∏
                                       0       1    0
                                             2    2
             If the first flip is H, we need Ì†µÌ∞∏ . If the first flip is T, we have wasted the flip, so it is Ì†µÌ∞∏
                                   1                                          0
             again.
                                             1    1
                                      Ì†µÌ∞∏ =1+ (0)+ Ì†µÌ∞∏
                                       1            1
                                             2    2
             If we have a T by 1/2 chance, we are done (the first term). If it is an H, we get another Ì†µÌ∞∏ .
                                                                            1
             In this case, we have Ì†µÌ∞∏ = 4, Ì†µÌ∞∏ = 2. Thus, Ì†µÌ∞∏(Ì†µÌ±å) = 4.
                             0     1
                                             82
               # number of simulations
               N <- 1000
               # X: number of flips until HH
               X <- numeric(N)
               set.seed(100)
               # run simulations
               for (i in 1:N) {
                 x <- 0
                 # repeat until first head
                 while(TRUE) {
                   x <- x + 1
                   t <- sample(c('H','T'), 1, F)
                   if (x >=2 && t == 'H' && tt == 'H') break
                   else tt <- t  # store last toss
                 }
                 # record the number
                 X[i] <- x
               }
               cat("Average number of flips until HH:", mean(X))
               Average number of flips until HH: 5.789
               # number of simulations
               N <- 1000
               # X: number of flips until HT
               Y <- numeric(N)
               set.seed(100)
               # run simulations
               for (i in 1:N) {
                 y <- 0
                 # repeat until first head
                 while(TRUE) {
                   y <- y + 1
                   t <- sample(c('H','T'), 1, F)
                                                     83
                   if (y >= 2 && t == 'T' && tt == 'H') break
                   else tt <- t  # store last toss
                 }
                 # record the number
                 Y[i] <- y
               }
               cat("Average number of flips until HT:", mean(Y))
               Average number of flips until HT: 4.047
                                                     84
                                     24 Negative Binomial
                                     Definition 24.1 (Negative Binomial distribution). In a sequence of independent Bernoulli
                                     trials with success probability Ì†µÌ±ù, if Ì†µÌ±ã is the number of failures before the Ì†µÌ±ü-th success, then Ì†µÌ±ã
                                     is said to have a Negative Binomial distribution, denoted Ì†µÌ±ã ‚àº NBin(Ì†µÌ±ü,Ì†µÌ±ù).
                                    The PMF for Negative Binomial distribution is given by
                                                                                                                                    Ì†µÌ±ò + Ì†µÌ±ü ‚àí 1
                                                                                                                                                            Ì†µÌ±ò   Ì†µÌ±ü
                                                                                                      Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±ò) = (                                  )Ì†µÌ±û Ì†µÌ±ù .
                                                                                                                                        Ì†µÌ±ü ‚àí 1
                                    To compute the expectation, let Ì†µÌ±ã = Ì†µÌ±ã +‚ãØ+Ì†µÌ±ã where Ì†µÌ±ã is the number of failures between
                                                                                                                      1                   Ì†µÌ±ü                   Ì†µÌ±ñ
                                     the (Ì†µÌ±ñ ‚àí 1)-th success and the Ì†µÌ±ñ-th success, 1 ‚â§ Ì†µÌ±ñ ‚â§ Ì†µÌ±ü. Then Ì†µÌ±ã ‚àº Geom(Ì†µÌ±ù). By linearity of
                                                                                                                                                                        Ì†µÌ±ñ
                                     expectations,
                                                                                                                                                                 1‚àíÌ†µÌ±ù
                                                                                            Ì†µÌ∞∏(Ì†µÌ±ã) = Ì†µÌ∞∏(Ì†µÌ±ã ) + ‚ãØ + Ì†µÌ∞∏(Ì†µÌ±ã ) = Ì†µÌ±ü                                             .
                                                                                                                        1                           Ì†µÌ±ü
                                                                                                                                                                     Ì†µÌ±ù
                                     Theorem 24.1. Let Ì†µÌ±ã ,Ì†µÌ±ã ,...,Ì†µÌ±ã be independent geometric distributions with the same pa-
                                                                                     1       2              Ì†µÌ±õ
                                    rameter Ì†µÌ±ù. Then
                                                                                                            Ì†µÌ±ã = Ì†µÌ±ã +Ì†µÌ±ã +‚ãØ+Ì†µÌ±ã
                                                                                                                          1           2                    Ì†µÌ±õ
                                    is a negative binomial distribution, namely Ì†µÌ±ã ‚àº NBin(Ì†µÌ±õ,Ì†µÌ±ù).
                                    The theorem is straightforward if we interpret Ì†µÌ±ã as the number of failures before the 1st
                                                                                                                                             1
                                     success, Ì†µÌ±ã               as the number of failures between the 1st and 2nd successes, and Ì†µÌ±ã                                                                                      be the
                                                           2                                                                                                                                                       Ì†µÌ±õ
                                     number of failures between the (Ì†µÌ±õ ‚àí 1)-th and Ì†µÌ±õ-th successes.
                                     Example 24.1 (Toy collection). There are Ì†µÌ±õ types of toys. Assume each time you buy a toy,
                                     it is equally likely to be any of the Ì†µÌ±õ types. What is the expected number of toys you need to
                                     buy until you have a complete set?
                                                                                                                                   85
                           Solution. Define the following random variables:
                                                            Ì†µÌ±á =Ì†µÌ±á +Ì†µÌ±á +‚ãØ+Ì†µÌ±á
                                                                    1       2              Ì†µÌ±õ
                                                           Ì†µÌ±á   =number of toys until 1st new type
                                                             1
                                                           Ì†µÌ±á   =additional number of toys until 2nd new type
                                                             2
                                                           Ì†µÌ±á   =additional number of toys until 3rd new type
                                                             3
                                                                  ‚ãÆ
                                                                                                                     Ì†µÌ±õ‚àí(Ì†µÌ±ó‚àí1)
                                                                                Ì†µÌ±õ‚àí1
                           Weknow, Ì†µÌ±á =1, Ì†µÌ±á ‚àí1 ‚àº Geom(                              ),‚Ä¶, Ì†µÌ±á ‚àí 1 ‚àº Geom(                        ). Thus,
                                             1          2                                     Ì†µÌ±ó
                                                                                 Ì†µÌ±õ                                       Ì†µÌ±õ
                                                                      Ì†µÌ∞∏(Ì†µÌ±á) =Ì†µÌ∞∏(Ì†µÌ±á ) + Ì†µÌ∞∏(Ì†µÌ±á ) + ‚ãØ + Ì†µÌ∞∏(Ì†µÌ±á )
                                                                                        1            2                   Ì†µÌ±õ
                                                                                           Ì†µÌ±õ           Ì†µÌ±õ               1
                                                                               =1+               +            +‚ãØ+
                                                                                        Ì†µÌ±õ ‚àí 1       Ì†µÌ±õ ‚àí 2              Ì†µÌ±õ
                                                                                           1      1             1
                                                                               =Ì†µÌ±õ(1+         + +‚ãØ+ )
                                                                                           2      3             Ì†µÌ±õ
                                                                              ‚ÜíÌ†µÌ±õ(logÌ†µÌ±õ+0.577).
                           If Ì†µÌ±õ = 5, Ì†µÌ∞∏(Ì†µÌ±á) ‚âà 11; if Ì†µÌ±õ = 10, Ì†µÌ∞∏(Ì†µÌ±á) ‚âà 29.
                           # number of simulations
                           N <- 1000
                           # number of toys bought in each simulation
                           X <- numeric(N)
                           # the set of toys
                           Toys <- c('Ôøø','Ôøø','Ôøø','Ôøø','Ôøø','Ôøø','Ôøø','Ôøø')
                           set.seed(100)
                           # run simulation
                           for (i in 1:N) {
                              C <- c() # the collection of toys bought
                              # repeat until a full set is collected
                              while(TRUE) {
                                  t <- sample(Toys, 1, replace=T)
                                  C <- c(C, t)
                                  if(setequal(C, Toys)) break
                              }
                              X[i] <- length(C)
                           }
                                                                                               86
        hist(X, probability = T)
        abline(v = mean(X), col = 2)
                       Histogram of X
            0.04
         Density0.02
            0.00
                10  20  30  40  50  60  70
                           X
                            87
          25 Bivariate distribution
          We need a tool to study collections of variables. Knowledge of each individual PMF is of
          little help. Because variables can be dependent on each each other (they are not necessarily
          independent). Weneedtoknowtheirinter-relationship. Jointdistributiongivestheprobability
          that two or more random variables simultaneously takes particular values.
          Definition 25.1 (Joint distribution). The joint PMF of random variables (Ì†µÌ±ã,Ì†µÌ±å) is given by
                           Ì†µÌ±ì(Ì†µÌ±•, Ì†µÌ±¶) = Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•,Ì†µÌ±å = Ì†µÌ±¶).
          The joint CDF of random variables (Ì†µÌ±ã,Ì†µÌ±å) is given by
                           Ì†µÌ∞π(Ì†µÌ±•,Ì†µÌ±¶) = Ì†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±•,Ì†µÌ±å ‚â§ Ì†µÌ±¶).
          Theorem 25.1. The discrete random variables Ì†µÌ±ã and Ì†µÌ±å are independent if and only if
                        Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•,Ì†µÌ±å = Ì†µÌ±¶) = Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•)Ì†µÌ±É(Ì†µÌ±å = Ì†µÌ±¶)
          for all possible values of Ì†µÌ±•,Ì†µÌ±¶.
          Equivalently, the condition can be stated with CDF: the random variables Ì†µÌ±ã and Ì†µÌ±å are inde-
          pendent if and only if
                        Ì†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±•,Ì†µÌ±å ‚â§ Ì†µÌ±¶) = Ì†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±•)Ì†µÌ±É(Ì†µÌ±å ‚â§ Ì†µÌ±¶)
          for all possible values of Ì†µÌ±•,Ì†µÌ±¶.
          Proof. Ì†µÌ±ã and Ì†µÌ±å are independent implies the event {Ì†µÌ±ã = Ì†µÌ±•} and {Ì†µÌ±å = Ì†µÌ±¶} are independent for
          any Ì†µÌ±•,Ì†µÌ±¶. By Definition 10.1, we have
                 Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•,Ì†µÌ±å = Ì†µÌ±¶) = Ì†µÌ±É({Ì†µÌ±ã = Ì†µÌ±•} ‚à© {Ì†µÌ±å = Ì†µÌ±¶}) = Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•)Ì†µÌ±É(Ì†µÌ±å = Ì†µÌ±¶).
           ƒπ Note
           If Ì†µÌ±ã and Ì†µÌ±å are independent, then any function of Ì†µÌ±ã is independent of any function of Ì†µÌ±å.
                                 88
                                Definition 25.2 (Marginal distribution). The marginal distribution gives the distribution of a
                                subset of variables in a joint distribution without reference to the values of the other variables.
                                The marginal PMF of Ì†µÌ±ã given the joint PMF of (Ì†µÌ±ã,Ì†µÌ±å) is given by
                                                                         Ì†µÌ±ì   (Ì†µÌ±•) = ‚àëÌ†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•,Ì†µÌ±å = Ì†µÌ±¶) = ‚àëÌ†µÌ±ì                                   (Ì†µÌ±•, Ì†µÌ±¶).
                                                                           Ì†µÌ±ã                                                               Ì†µÌ±ã,Ì†µÌ±å
                                                                                           Ì†µÌ±¶                                        Ì†µÌ±¶
                                    ƒπ Note
                                    It is easy to compute the marginal distribution given the joint distribution. However,
                                    in general, we cannot deduce the joint distribution from the marginal distribution. Un-
                                    less the random variables are independent, the joint distribution is not the product of
                                    marginal distributions.
                                Example 25.1. Let Ì†µÌ±ã be an indicator of an individual being a current smoker. Let Ì†µÌ±å be the
                                indicator of his developing lung cancer at some point in his life. The joint PMF of Ì†µÌ±ã and Ì†µÌ±å
                                is as specified in the table below.
                                                                                                      Ì†µÌ±å = 1         Ì†µÌ±å = 0          Total
                                                                                       Ì†µÌ±ã = 1           0.05           0.20           0.25
                                                                                       Ì†µÌ±ã = 0           0.03           0.72           0.75
                                                                                                                 89
                                                              Total       0.08       0.92         1
                       The marginal PMF for having lung cancer is
                                               Ì†µÌ±É(Ì†µÌ±å = 1) =Ì†µÌ±É(Ì†µÌ±å = 1,Ì†µÌ±ã = 0) + Ì†µÌ±É(Ì†µÌ±å = 1,Ì†µÌ±ã = 1) = 0.08,
                                               Ì†µÌ±É(Ì†µÌ±å = 0) =Ì†µÌ±É(Ì†µÌ±å = 0,Ì†µÌ±ã = 0) + Ì†µÌ±É(Ì†µÌ±å = 0,Ì†µÌ±ã = 1) = 0.92.
                       In this example, Ì†µÌ±ã,Ì†µÌ±å are not independent, because
                                                         Ì†µÌ±É(Ì†µÌ±ã = 1,Ì†µÌ±å = 1) ‚â† Ì†µÌ±É(Ì†µÌ±ã = 1)Ì†µÌ±É(Ì†µÌ±å = 1).
                       Definition 25.3. If a given number of random variables are independent and have the same
                       distribution, we call them independent and identically distributed, or i.i.d for short.
                          ‚Ä¢ Independent and identically distributed (Ì†µÌ±ã,Ì†µÌ±å independent die rolls)
                          ‚Ä¢ Independent and not identically distributed (Ì†µÌ±ã: die roll; Ì†µÌ±å: coin flip)
                          ‚Ä¢ Dependent and identically distributed (Ì†µÌ±ã: number of Heads; Ì†µÌ±å: number of Tails)
                          ‚Ä¢ Dependent and not identically distributed (Ì†µÌ±ã: economic growth; Ì†µÌ±å: presidential elec-
                              tion)
                          ƒπ Note
                          Weviewrandomsampleasacollectionofi.i.drandomvariablesfromthesamepopulation
                                                                                                                                       Ì†µÌ±ñÌ†µÌ±ñÌ†µÌ±ë
                          distribution. For example, let Ì†µÌ±ã be the test score of student Ì†µÌ±ñ. We say Ì†µÌ±ã ,Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã                        ‚àº
                                                                   Ì†µÌ±ñ                                                  1    2        Ì†µÌ±õ
                          Ì†µÌ∞∫ where Ì†µÌ∞∫ is the (unknown) population distribution for test scores.
                          The independent assumption means that one observation does not influence another,
                          while the identically distributed assumption ensures all observations follow the same
                          probability law. This perspective simplifies statistical analysis and is foundational for
                          many statistical inference.
                       Exercise 25.1 (Benford‚Äôs law). The distribution of first two digits in many real-life data
                       (e.g. annual accounts
                       of a corporation) can be approximated by the joint mass function:
                                                                              1
                                                Ì†µÌ±ì(Ì†µÌ±•, Ì†µÌ±¶) = log   (1+              ),     1 ‚â§ Ì†µÌ±• ‚â§ 9,0 ‚â§ Ì†µÌ±¶ ‚â§ 9.
                                                                10
                                                                          10Ì†µÌ±• + Ì†µÌ±¶
                          1. Verify this is valid joint PMF.
                          2. Find the marginal PMF of Ì†µÌ±ã.
                          3. Give an approximation to Ì†µÌ∞∏(Ì†µÌ±ã).
                                                                                 90
                                26 Conditional distribution
                                Definition 26.1 (Conditional distribution). The conditional PMF of Ì†µÌ±å given Ì†µÌ±ã = Ì†µÌ±• is defined
                                as
                                                                                                                                 Ì†µÌ±É(Ì†µÌ±å = Ì†µÌ±¶,Ì†µÌ±ã = Ì†µÌ±•)
                                                                       Ì†µÌ±ì      (Ì†µÌ±¶|Ì†µÌ±•) = Ì†µÌ±É(Ì†µÌ±å = Ì†µÌ±¶|Ì†µÌ±ã = Ì†µÌ±•) =
                                                                         Ì†µÌ±å |Ì†µÌ±ã
                                                                                                                                       Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•)
                                for any Ì†µÌ±• such that Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•) > 0.
                                Plot conditional distribution
                                library(lattice) # easy to make conditional plots
                                # conditional distribution of exam scores
                                exam <- read.csv("../dataset/exam.csv")
                                # distribution of exam scores conditioned on major
                                histogram(~ final | major, data = exam)
                                                                                                                     40           60            80          100
                                                                  Economics                                                     Finance
                                        40
                                   otal 30
                                        20
                                   ercent of T10
                                   P
                                          0
                                                          40            60           80           100
                                                                                                      final
                                                                                                                 91
                    Definition 26.2 (Conditional expectation). The conditional expectation of Ì†µÌ±å given Ì†µÌ±ã = Ì†µÌ±• is
                    defined as
                                                  Ì†µÌºì(Ì†µÌ±•) = Ì†µÌ∞∏(Ì†µÌ±å |Ì†µÌ±ã = Ì†µÌ±•) = ‚àëÌ†µÌ±¶ Ì†µÌ±ì    (Ì†µÌ±¶|Ì†µÌ±•).
                                                                                   Ì†µÌ±å |Ì†µÌ±ã
                                                                            all Ì†µÌ±¶
                    Ì†µÌºì(Ì†µÌ±•) depends on the value of Ì†µÌ±• taken by Ì†µÌ±ã, so it can be thought of as a function Ì†µÌºì(Ì†µÌ±ã) of Ì†µÌ±ã
                    itself.
                                                              Ì†µÌºì(Ì†µÌ±ã) = Ì†µÌ∞∏(Ì†µÌ±å |Ì†µÌ±ã)
                    is called the conditional expectation of Ì†µÌ±å given Ì†µÌ±ã.
                      ƒπ Note
                      Although Ì†µÌ∞∏(Ì†µÌ±ã) is a number, Ì†µÌ∞∏(Ì†µÌ±å|Ì†µÌ±ã) is a random variable. It is a function of random
                      variable Ì†µÌ±ã, and therefore it is a random variable itself.
                    Conditional distribution is a key concept in probability, describing how the distribution of
                    one random variable depends on the values of other variables‚Äîan idea central to many prac-
                    tical applications. For instance, we might be interested in how income distributions vary by
                    education level or how the probability of a disease changes with age.
                    Conditional expectation gives the expected value of one variable given the value of another.
                    It is frequently used for making predictions, such as predicting your earnings given that you
                    graduate from a this college.
                    Figure 26.1: Given each value of X, there is a distribution of Y|X. E(Y|X) is a function of X.
                                                                      92
                        27 Poisson distribution
                        Nowweintroducearguablythemostpopulardiscrete distribution‚ÄîPoisson distribution. Pois-
                        son distribution is used to model independent events occurring at a constant mean rate. It
                        is like the Binomial distribution in the sense that they both model the number of occurrence
                        of events, but it is parametrized on the ‚Äúrate‚Äù of the event (how many times an event occurs
                        in a unit of time on average) rather than the total number of events and the probability of
                        each event. It is therefore more practical in real-world modeling since we mostly observe the
                        rate rather than the totality. We introduce the Poisson distribution by showing that it is a
                        limiting case of the Binomial distribution.
                        Question: Suppose we are studying the distribution of the number of visitors to a certain
                        website. Every day, a million people independently decide whether to visit the site, with
                                                     ‚àí6
                        probability Ì†µÌ±ù = 2√ó10            of visiting. What is the probability of getting Ì†µÌ±ò visitors on a particular
                        day?
                        Wecanmodel the problem with a Binomial distribution. Let Ì†µÌ±ã ‚àº Bin(Ì†µÌ±õ,Ì†µÌ±ù) be the number of
                                                      6                     ‚àí6
                        visitors, where Ì†µÌ±õ = 10 and Ì†µÌ±ù = 2√ó10                  . But it is easy to run into computational diÔ¨Äiculties
                        with such a large Ì†µÌ±õ and small Ì†µÌ±ù. This is not uncommon, if we want to model the number of
                        emails one receives per day, or the number of phone calls in a service center. In such cases, we
                        could reasonably assume Ì†µÌ±õ ‚Üí ‚àû and Ì†µÌ±ù ‚Üí 0 while Ì†µÌ±õÌ†µÌ±ù = Ì†µÌºÜ is a constant. We may call Ì†µÌºÜ ‚Äî the
                        ‚Äúrate‚Äù, as it can be interpreted as the average visitors per day.
                        Take limit of the Binomial distribution:
                                                                            Ì†µÌ±õ
                                                                                  Ì†µÌ±ò        Ì†µÌ±õ‚àíÌ†µÌ±ò
                                                  Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±ò) = lim ( )Ì†µÌ±ù (1‚àíÌ†µÌ±ù)
                                                                   Ì†µÌ±õ‚Üí‚àû
                                                                            Ì†µÌ±ò
                                                                                       Ì†µÌ±ò            Ì†µÌ±õ‚àíÌ†µÌ±ò
                                                                            Ì†µÌ±õ     Ì†µÌºÜ            Ì†µÌºÜ
                                                                = lim ( )( ) (1‚àí )
                                                                   Ì†µÌ±õ‚Üí‚àû
                                                                            Ì†µÌ±ò     Ì†µÌ±õ            Ì†µÌ±õ
                                                                                                          Ì†µÌ±õ             ‚àíÌ†µÌ±ò
                                                                                            Ì†µÌ±ò
                                                                               Ì†µÌ±õ!        Ì†µÌºÜ          Ì†µÌºÜ             Ì†µÌºÜ
                                                                = lim                   ‚ãÖ     (1‚àí ) (1‚àí )
                                                                                            Ì†µÌ±ò
                                                                   Ì†µÌ±õ‚Üí‚àû
                                                                          (Ì†µÌ±õ ‚àí Ì†µÌ±ò)!Ì†µÌ±ò!   Ì†µÌ±õ          Ì†µÌ±õ             Ì†µÌ±õ
                                                                                              ‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü
                                                                                                     ‚àíÌ†µÌºÜ
                                                                                                                   ‚Üí1
                                                                                                  ‚ÜíÌ†µÌ±í
                                                                                           Ì†µÌ±ò
                                                                               Ì†µÌ±õ!       Ì†µÌºÜ
                                                                                              ‚àíÌ†µÌºÜ
                                                                = lim                       Ì†µÌ±í
                                                                            Ì†µÌ±ò
                                                                   Ì†µÌ±õ‚Üí‚àû
                                                                          Ì†µÌ±õ (Ì†µÌ±õ ‚àí Ì†µÌ±ò)! Ì†µÌ±ò!
                                                                          ‚èü‚èü‚èü‚èü‚èü
                                                                               ‚Üí1
                                                                     Ì†µÌ±ò
                                                                   Ì†µÌºÜ
                                                                        ‚àíÌ†µÌºÜ
                                                                = Ì†µÌ±í .
                                                                   Ì†µÌ±ò!
                                                                                     93
                                 This is the PMF of the Poisson distribution.
                                      ƒπ The limiting definition of exponential function
                                                                                                                                         Ì†µÌ±õ
                                                                                                                                   Ì†µÌ±•
                                                                                                       Ì†µÌ±•
                                                                                                     Ì†µÌ±í    = lim (1+ )
                                                                                                               Ì†µÌ±õ‚Üí‚àû
                                                                                                                                   Ì†µÌ±õ
                                 Definition 27.1 (Poisson distribution). A random variable Ì†µÌ±ã has the Poisson distribution
                                 with parameter Ì†µÌºÜ if the PMF of Ì†µÌ±ã is
                                                                                                                   ‚àíÌ†µÌºÜ Ì†µÌ±ò
                                                                                                                 Ì†µÌ±í    Ì†µÌºÜ
                                                                                        Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±ò) =                       ,     Ì†µÌ±ò = 0,1,2,‚Ä¶
                                                                                                                    Ì†µÌ±ò!
                                 Wedenote this as Ì†µÌ±ã ‚àº Pois(Ì†µÌºÜ).
                                                                                                                                            Ì†µÌ±ò
                                                                                                                                  ‚àû
                                                                                                                                          Ì†µÌºÜ
                                                                                                                                                      Ì†µÌºÜ
                                 Wecan easily verify this is a valid PMF because ‚àë                                                             =Ì†µÌ±í .
                                                                                                                                          Ì†µÌ±ò!
                                                                                                                                  Ì†µÌ±ò=0
                                 Theorem 27.1. If Ì†µÌ±ã ‚àº Bin(Ì†µÌ±õ,Ì†µÌ±ù) and we let Ì†µÌ±õ ‚Üí ‚àû and Ì†µÌ±ù ‚Üí 0 such that Ì†µÌºÜ = Ì†µÌ±õÌ†µÌ±ù remains
                                 fixed, then the PMF of Ì†µÌ±ã converges to the PMF of Pois(Ì†µÌºÜ).
                                 The expectation of the Poisson distribution is
                                                                                                                   ‚àû
                                                                                                                                ‚àíÌ†µÌºÜ Ì†µÌ±ò
                                                                                                                              Ì†µÌ±í     Ì†µÌºÜ
                                                                                                  Ì†µÌ∞∏(Ì†µÌ±ã) =‚àëÌ†µÌ±ò‚ãÖ
                                                                                                                                  Ì†µÌ±ò!
                                                                                                                 Ì†µÌ±ò=0
                                                                                                                         ‚àû
                                                                                                                                       Ì†µÌ±ò
                                                                                                                                    Ì†µÌºÜ
                                                                                                                   ‚àíÌ†µÌºÜ
                                                                                                             =Ì†µÌ±í        ‚àë
                                                                                                                               (Ì†µÌ±ò ‚àí 1)!
                                                                                                                        Ì†µÌ±ò=1
                                                                                                                            ‚àû
                                                                                                                                       Ì†µÌ±ò‚àí1
                                                                                                                                     Ì†µÌºÜ
                                                                                                                     ‚àíÌ†µÌºÜ
                                                                                                             =Ì†µÌºÜÌ†µÌ±í        ‚àë
                                                                                                                                  (Ì†µÌ±ò ‚àí 1)!
                                                                                                                           Ì†µÌ±ò=1
                                                                                                                     ‚àíÌ†µÌºÜ Ì†µÌºÜ
                                                                                                             =Ì†µÌºÜÌ†µÌ±í        Ì†µÌ±í   =Ì†µÌºÜ.
                                                                                                                       94
                                     PMF of Poisson distribution
                    0.3
                    0.2
                    0.1
                    0.0
                          0              5             10              15             20
                  Example 27.1. Continued with the website visiting example, there are one million people
                                                                               ‚àí6
                  visiting the site every day, each with probability Ì†µÌ±ù = 2√ó10   . Give an approximation for the
                  probability of getting at least three visitors on a particular day.
                  Let Ì†µÌ±ã be the number of visitors. Since Ì†µÌ±õ is large, Ì†µÌ±ù is small, Ì†µÌ±õÌ†µÌ±ù = 2 is fixed, Ì†µÌ±ã is well
                  approximated by Pois(2). Therefore,
                               Ì†µÌ±É(Ì†µÌ±ã ‚â• 3) = 1 ‚àí Ì†µÌ±É(Ì†µÌ±ã < 3) = 1 ‚àí Ì†µÌ±É(Ì†µÌ±ã = 0) ‚àí Ì†µÌ±É(Ì†µÌ±ã = 1) ‚àí Ì†µÌ±É(Ì†µÌ±ã = 2)
                                                                                2
                                                                               2
                                                                  ‚àí2      ‚àí2       ‚àí2
                                                           =1‚àíÌ†µÌ±í ‚àí2Ì†µÌ±í ‚àí Ì†µÌ±í
                                                                               2!
                                                                    ‚àí2
                                                           =1‚àí5Ì†µÌ±í      ‚âà0.32.
                  The Poisson distribution is often used in situations where we are counting the number of
                  successes in a particular region or interval of time, where there are a large number of trials,
                  each with a small probability of success. The Poisson paradigm says in situations like this, we
                  can approximate the number of successes by a Poisson distribution. It is more general than
                  Theorem 27.1, as we relax the assumption of independence and identical events.
                  Proposition 27.1 (Poisson paradigm). Let Ì†µÌ∞¥ ,‚Ä¶,Ì†µÌ∞¥ be events with Ì†µÌ±ù = Ì†µÌ±É(Ì†µÌ∞¥ ), where Ì†µÌ±õ is
                                                                  1      Ì†µÌ±õ                Ì†µÌ±ó       Ì†µÌ±ó
                                                                                                        Ì†µÌ±õ
                  large, the Ì†µÌ±ù are small, and the Ì†µÌ∞¥ are independent or weakly dependent. Then Ì†µÌ±ã = ‚àë      Ì†µÌ∞º(Ì†µÌ∞¥ ),
                             Ì†µÌ±ó                    Ì†µÌ±ó                                                          Ì†µÌ±ó
                                                                                                        Ì†µÌ±ó=1
                                                                                                          Ì†µÌ±õ
                  that is how many of the Ì†µÌ∞¥ occur, is approximately distributed as Pois(Ì†µÌºÜ) with Ì†µÌºÜ = ‚àë     Ì†µÌ±ù .
                                             Ì†µÌ±ó                                                                Ì†µÌ±ó
                                                                                                          Ì†µÌ±ó=1
                  The Poisson paradigm is also called the law of rare events. The interpretation of ‚Äúrare‚Äù is
                  that the Ì†µÌ±ù are small, but Ì†µÌºÜ is relatively stable. The number of events that occur may not be
                             Ì†µÌ±ó
                  exactly Poisson, but the Poisson distribution often gives good approximations. Note that the
                  conditions for the Poisson paradigm to hold are fairly flexible: the Ì†µÌ±õ trials can have different
                  success probabilities, and the trials don‚Äôt have to be independent, though they should not be
                                                                 95
        very dependent. So there are a wide variety of situations that can be cast in terms of the
        Poisson paradigm. This makes the Poisson a very popular model.
        Poisson distribution is also used to model the number of events occurring randomly over
        time with constant rate, such as the number of customers visiting a store, the number of
        phone calls to a call center, and so on.
        Whytherandom occurrence of events has anything to do with the Poisson distribution? Con-
        sider in this way: one can divide the time line into infinitely small intervals (e.g. milliseconds).
        In each interval, an event either happens or not. The chance that an event occurs in a mil-
        lisecond is very small. While there are infinitely many trials. So counting events occurring
        randomlyatafixedaveragerateovertimeismathematicallyequivalent to counting rare events
        in many trials.
        Definition 27.2 (Poisson process). A sequence of arrivals in continuous time is a Poisson
        process with rate Ì†µÌºÜ per unit of time if
          ‚Ä¢ The number of arrivals in an interval of length Ì†µÌ±° is distributed Pois(Ì†µÌºÜÌ†µÌ±°);
          ‚Ä¢ The numbers of arrivals in disjoint time intervals are independent.
                            96
                    28 Birthday problem revisited
                    The beauty if approximating discrete problems by continuous function is that it makes calcu-
                    lation easier. Now we revisit the birthday problem with Poisson distribution.
                                                                      Ì†µÌ±ö
                    Example 28.1. If we have Ì†µÌ±ö people and ( ) pairs. Each pair of people has probability
                                                                      2
                    Ì†µÌ±ù = 1/365 of having the same birthday. Find the probability of at least one match.
                    Solution. The probability of match is small, and the number of pairs is large. We consider
                    using the Poisson paradigm to approximate the number Ì†µÌ±ã of birthday matches. Ì†µÌ±ã ‚âà Ì†µÌ±ÉÌ†µÌ±úÌ†µÌ±ñÌ†µÌ±†(Ì†µÌºÜ)
                                 Ì†µÌ±ö
                                     1
                    where Ì†µÌºÜ = ( )     . Then the probability of at least one match is
                                 2 365
                                                                                        ‚àíÌ†µÌºÜ
                                                  Ì†µÌ±É(Ì†µÌ±ã ‚â• 1) = 1 ‚àí Ì†µÌ±É(Ì†µÌ±ã = 0) ‚âà 1 ‚àí Ì†µÌ±í     .
                                                           ‚àíÌ†µÌºÜ
                    For Ì†µÌ±ö = 23, Ì†µÌºÜ = 253/365 and 1 ‚àí Ì†µÌ±í       ‚âà0.5, which agrees with our previous finding that we
                    need 23 people to have 50% chance of a birthday match.
                    Example 28.2. Continued with the assumption above. What‚Äôs the probability of two people
                    who were born not only on the same day, but also at the same hour and the same minute?
                    Solution. This is the birthday problem with Ì†µÌ±ê = 365‚ãÖ24‚ãÖ60 = 525600 categories rather than 365
                    categories. By Poisson approximation, the probability of at least one match is approximately
                                             Ì†µÌ±ö
                                                  1
                         ‚àíÌ†µÌºÜ
                            1
                    1 ‚àíÌ†µÌ±í     where Ì†µÌºÜ = ( )          . This would require Ì†µÌ±ö = 854 to reach the break even point,
                                       1
                                             2 525600
                    50% chance of getting a match.
                    You may wonder how good the Poisson approximation is. We can compare it with the true
                    values.
                    # compute the probability of coincidences for 1,2...100 people
                    n <- 1:100
                    p <- sapply(n, pbirthday)
                    # approximate the probability by Poisson paradigm
                    lambda <- choose(n, 2)/365
                    q <- 1 - exp(-lambda)
                    # black line is the true probability
                                                                      97
        # red line is the Poisson approximation
        plot(n, p, type = "s")
        lines(n, q, col = 2, type="s")
            0.8
         p
            0.4
            0.0
              0    20   40   60    80   100
                           n
                            98
                                29 Convolution
                                Aconvolution is a sum of independent random variables. The main task in this section is to
                                determine the distribution of Ì†µÌ±á = Ì†µÌ±ã + Ì†µÌ±å, where Ì†µÌ±ã and Ì†µÌ±å are independent random variables
                                whose distributions are known.
                                Theorem 29.1 (Convolution). If Ì†µÌ±ã and Ì†µÌ±å are independent discrete random variables, then
                               the PMF of their sum Ì†µÌ±á = Ì†µÌ±ã +Ì†µÌ±å is
                                                                                Ì†µÌ±É(Ì†µÌ±á = Ì†µÌ±°) = ‚àëÌ†µÌ±É(Ì†µÌ±å = Ì†µÌ±° ‚àí Ì†µÌ±•)Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•)
                                                                                                       Ì†µÌ±•
                                                                                                                                                     .
                                                                                                 =‚àëÌ†µÌ±É(Ì†µÌ±ã=Ì†µÌ±°‚àíÌ†µÌ±¶)Ì†µÌ±É(Ì†µÌ±å = Ì†µÌ±¶)
                                                                                                       Ì†µÌ±¶
                                If Ì†µÌ±ã and Ì†µÌ±å are independent continuous random variables, then the PDF of their sum Ì†µÌ±á = Ì†µÌ±ã+Ì†µÌ±å
                               is
                                                                                                          ‚àû
                                                                                        Ì†µÌ±ì  (Ì†µÌ±°) = ‚à´          Ì†µÌ±ì   (Ì†µÌ±° ‚àí Ì†µÌ±•)Ì†µÌ±ì      (Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•
                                                                                          Ì†µÌ±á                    Ì†µÌ±å               Ì†µÌ±ã
                                                                                                        ‚àí‚àû
                                                                                                          ‚àû
                                                                                                  =‚à´ Ì†µÌ±ì (Ì†µÌ±°‚àíÌ†µÌ±¶)Ì†µÌ±ì (Ì†µÌ±¶)Ì†µÌ±ëÌ†µÌ±¶.
                                                                                                                Ì†µÌ±ã               Ì†µÌ±å
                                                                                                        ‚àí‚àû
                                Theorem 29.2 (Sum of Binomial random variables). Let Ì†µÌ±ã ‚àº Bin(Ì†µÌ±õ,Ì†µÌ±ù) and Ì†µÌ±å ‚àº Bin(Ì†µÌ±ö,Ì†µÌ±ù)
                               be two independent Binomial random variables. Then Ì†µÌ±ã +Ì†µÌ±å ‚àº Bin(Ì†µÌ±õ+Ì†µÌ±ö,Ì†µÌ±ù).
                                Proof. We have proved the theorem in Theorem 19.1. Here is another way to prove it using
                                convolution.
                                                                                                                 99
                                                                                                                                        Ì†µÌ±ò
                                                                                                                                     ‚àë
                                                                                       Ì†µÌ±É(Ì†µÌ±ã + Ì†µÌ±å = Ì†µÌ±ò) =                                      Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±ñ)Ì†µÌ±É(Ì†µÌ±å = Ì†µÌ±ò ‚àí Ì†µÌ±ñ)
                                                                                                                                      Ì†µÌ±ñ=0
                                                                                                                                        Ì†µÌ±ò
                                                                                                                                                   Ì†µÌ±õ                                              Ì†µÌ±ö
                                                                                                                                                              Ì†µÌ±ñ                   Ì†µÌ±õ‚àíÌ†µÌ±ñ                            Ì†µÌ±ò‚àíÌ†µÌ±ñ                     Ì†µÌ±ö‚àíÌ†µÌ±ò+Ì†µÌ±ñ
                                                                                                                               =‚àë( )Ì†µÌ±ù(1‚àíÌ†µÌ±ù)                                               (                )Ì†µÌ±ù             (1‚àíÌ†µÌ±ù)
                                                                                                                                                    Ì†µÌ±ñ                                         Ì†µÌ±ò ‚àí Ì†µÌ±ñ
                                                                                                                                      Ì†µÌ±ñ=0
                                                                                                                                        Ì†µÌ±ò
                                                                                                                                                   Ì†µÌ±õ              Ì†µÌ±ö
                                                                                                                                                                                    Ì†µÌ±ò                   Ì†µÌ±ö+Ì†µÌ±õ‚àíÌ†µÌ±ò
                                                                                                                               =‚àë( )(                                       )Ì†µÌ±ù (1 ‚àíÌ†µÌ±ù)
                                                                                                                                                    Ì†µÌ±ñ          Ì†µÌ±ò ‚àí Ì†µÌ±ñ
                                                                                                                                      Ì†µÌ±ñ=0
                                                                                                                                                                                  Ì†µÌ±ò
                                                                                                                                                                                             Ì†µÌ±õ              Ì†µÌ±ö
                                                                                                                                        Ì†µÌ±ò                   Ì†µÌ±ö+Ì†µÌ±õ‚àíÌ†µÌ±ò
                                                                                                                               =Ì†µÌ±ù (1‚àíÌ†µÌ±ù)                                      ‚àë( )(                                  )
                                                                                                                                                                                              Ì†µÌ±ñ          Ì†µÌ±ò ‚àí Ì†µÌ±ñ
                                                                                                                                                                                Ì†µÌ±ñ=0
                                                                                                                                                                                  Ì†µÌ±õ + Ì†µÌ±ö
                                                                                                                                        Ì†µÌ±ò                   Ì†µÌ±ö+Ì†µÌ±õ‚àíÌ†µÌ±ò
                                                                                                                                                                              (                    ).
                                                                                                                               =Ì†µÌ±ù (1‚àíÌ†µÌ±ù)
                                                                                                                                                                                         Ì†µÌ±ò
                                                                                                                    Ì†µÌ±ò
                                                                                        Ì†µÌ±õ+Ì†µÌ±ö                                  Ì†µÌ±õ        Ì†µÌ±ö
                                               The last step: (                                    ) = ‚àë                    ( )(               )
                                                                                            Ì†µÌ±ò                                 Ì†µÌ±ñ      Ì†µÌ±ò‚àíÌ†µÌ±ñ
                                                                                                                    Ì†µÌ±ñ=0
                                               is known as the Vandermonde‚Äôs identity.
                                               Example 29.1 (Sum of Poisson random variables). If Ì†µÌ±ã ‚àº Pois(Ì†µÌºÜ ), Ì†µÌ±å ‚àº Pois(Ì†µÌºÜ ), and Ì†µÌ±ã,Ì†µÌ±å
                                                                                                                                                                                                                              1                                     2
                                               are independent, then Ì†µÌ±ã +Ì†µÌ±å ‚àº Pois(Ì†µÌºÜ +Ì†µÌºÜ ).
                                                                                                                                                     1             2
                                               Proof. Intuitively, Ì†µÌ±ã is the number of events occurring at rate Ì†µÌºÜ ; Ì†µÌ±å is the number of events
                                                                                                                                                                                                                          1
                                               occurring at rate Ì†µÌºÜ . Therefore, Ì†µÌ±ã + Ì†µÌ±å should be events occurring at rate Ì†µÌºÜ + Ì†µÌºÜ .
                                                                                                 2                                                                                                                                                      1             2
                                               To get the PMF of Ì†µÌ±ã +Ì†µÌ±å, condition on Ì†µÌ±ã and use the law of total probability:
                                                                                                                                                                 Ì†µÌ±ò
                                                                                                                Ì†µÌ±É(Ì†µÌ±ã + Ì†µÌ±å = Ì†µÌ±ò) = ‚àëÌ†µÌ±É(Ì†µÌ±å = Ì†µÌ±ò ‚àí Ì†µÌ±ó)Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±ó)
                                                                                                                                                              Ì†µÌ±ó=0
                                                                                                                                                                                       Ì†µÌ±ò‚àíÌ†µÌ±ó                        Ì†µÌ±ó
                                                                                                                                                                 Ì†µÌ±ò
                                                                                                                                                                           ‚àíÌ†µÌºÜ                          ‚àíÌ†µÌºÜ
                                                                                                                                                                                 2                            1
                                                                                                                                                                        Ì†µÌ±í          Ì†µÌºÜ               Ì†µÌ±í          Ì†µÌºÜ
                                                                                                                                                                                       2                            1
                                                                                                                                                              ‚àë
                                                                                                                                                       =                                         ‚ãÖ
                                                                                                                                                                          (Ì†µÌ±ò ‚àí Ì†µÌ±ó)!                        Ì†µÌ±ó!
                                                                                                                                                              Ì†µÌ±ó=0
                                                                                                                                                                                           Ì†µÌ±ò
                                                                                                                                                                 ‚àí(Ì†µÌºÜ +Ì†µÌºÜ )
                                                                                                                                                                         1        2
                                                                                                                                                               Ì†µÌ±í                                     Ì†µÌ±ò
                                                                                                                                                                                                                 Ì†µÌ±ó    Ì†µÌ±ò‚àíÌ†µÌ±ó
                                                                                                                                                       =                                ‚àë()Ì†µÌºÜ Ì†µÌºÜ
                                                                                                                                                                                                                 1 2
                                                                                                                                                                        Ì†µÌ±ò!                           Ì†µÌ±ó
                                                                                                                                                                                        Ì†µÌ±ó=0
                                                                                                                                                                 ‚àí(Ì†µÌºÜ +Ì†µÌºÜ )
                                                                                                                                                                         1        2
                                                                                                                                                               Ì†µÌ±í
                                                                                                                                                                                                               Ì†µÌ±ò
                                                                                                                                                       =                               (Ì†µÌºÜ      +Ì†µÌºÜ ) .
                                                                                                                                                                                            1             2
                                                                                                                                                                        Ì†µÌ±ò!
                                              We thus arrive at the PMF for Pois(Ì†µÌºÜ + Ì†µÌºÜ ). Intuitively, if there are two different types of
                                                                                                                                                     1             2
                                               events occurring at rates Ì†µÌºÜ and Ì†µÌºÜ , independently, then the overall event rate is Ì†µÌºÜ +Ì†µÌºÜ .
                                                                                                                     1                   2                                                                                                                               1            2
                                                                                                                                                                     100
                                         30 Dice rolling formula*
                                         The Binomial distribution gives the formula for the probability of observing Ì†µÌ±ò heads when
                                         flipping Ì†µÌ±õ coins. Can we find a formula for the probability of getting a total of Ì†µÌ±ù points when
                                         rolling Ì†µÌ±õ dice?
                                         The probability of obtaining Ì†µÌ±ù points on Ì†µÌ±õ Ì†µÌ±†-sided dice can be computed as the coeÔ¨Äicient of
                                            Ì†µÌ±ù
                                         Ì†µÌ±•     in
                                                                                                                                                      2                      Ì†µÌ±†   Ì†µÌ±õ
                                                                                                                       Ì†µÌ±ì(Ì†µÌ±•) = (Ì†µÌ±• + Ì†µÌ±• + ... + Ì†µÌ±• )
                                         since each possible arrangement contributes one term.
                                                                                                                                                                                                         Ì†µÌ±õ
                                                                                                                                                                                                   Ì†µÌ±†
                                                                                                                                                                                       1‚àíÌ†µÌ±•
                                                                                                                   Ì†µÌ±õ                                    Ì†µÌ±†‚àí1 Ì†µÌ±õ               Ì†µÌ±õ
                                                                                               Ì†µÌ±ì(Ì†µÌ±•) = Ì†µÌ±• (1 + Ì†µÌ±• + ‚ãØ + Ì†µÌ±•                                     )     =Ì†µÌ±• (                           )
                                                                                                                                                                                        1‚àíÌ†µÌ±•
                                                                                                            Ì†µÌ±ù
                                         To obtain the coeÔ¨Äicient of Ì†µÌ±• , expand the binomial power:
                                                                                                                                               Ì†µÌ±õ                                     ‚àû
                                                                                                                                                                     Ì†µÌ±õ                          Ì†µÌ±õ + Ì†µÌ±ô ‚àí 1
                                                                                Ì†µÌ±õ               Ì†µÌ±†  Ì†µÌ±õ                  ‚àíÌ†µÌ±õ            Ì†µÌ±õ                     Ì†µÌ±ò              Ì†µÌ±†Ì†µÌ±ò                                          Ì†µÌ±ô
                                                                             Ì†µÌ±• (1 ‚àí Ì†µÌ±• ) (1 ‚àí Ì†µÌ±•)                              =Ì†µÌ±• ‚àë(‚àí1) ( )Ì†µÌ±• ‚àë(                                                                    )Ì†µÌ±•
                                                                                                                                                                     Ì†µÌ±ò                                    Ì†µÌ±ô
                                                                                                                                            Ì†µÌ±ò=0                                     Ì†µÌ±ô=0
                                                                                      Ì†µÌ±ù
                                         The coeÔ¨Äicient of Ì†µÌ±• include all terms with Ì†µÌ±ù = Ì†µÌ±õ + Ì†µÌ±†Ì†µÌ±ò + Ì†µÌ±ô. Therefore,
                                                                                                                              Ì†µÌ±õ
                                                                                                                                                    Ì†µÌ±õ          Ì†µÌ±ù ‚àí Ì†µÌ±†Ì†µÌ±ò ‚àí 1
                                                                                                                                              Ì†µÌ±ò
                                                                                                                Ì†µÌ±ê    =‚àë(‚àí1) ( )(                                                       )
                                                                                                                  Ì†µÌ±ù
                                                                                                                                                    Ì†µÌ±ò         Ì†µÌ±ù ‚àí Ì†µÌ±†Ì†µÌ±ò ‚àí Ì†µÌ±õ
                                                                                                                            Ì†µÌ±ò=0
                                         ButÌ†µÌ±ù‚àíÌ†µÌ±†Ì†µÌ±ò‚àíÌ†µÌ±õ > 0 only when Ì†µÌ±ò < (Ì†µÌ±ù‚àíÌ†µÌ±õ)/Ì†µÌ±†, so the other terms do not contribute. Furthermore,
                                         applying the symmetric property of the binomial formula, we have
                                                                                                                      Ì†µÌ±ù ‚àí Ì†µÌ±†Ì†µÌ±ò ‚àí 1                          Ì†µÌ±ù ‚àí Ì†µÌ±†Ì†µÌ±ò ‚àí 1
                                                                                                                  (                           )=(                                    )
                                                                                                                      Ì†µÌ±ù ‚àí Ì†µÌ±†Ì†µÌ±ò ‚àí Ì†µÌ±õ                               Ì†µÌ±õ ‚àí 1
                                         Therefore, the probability of getting Ì†µÌ±ù points when rolling Ì†µÌ±õ Ì†µÌ±†-sided dice is given by
                                                                                                                                                  101
                                                                               ‚åä(Ì†µÌ±ù‚àíÌ†µÌ±õ)/Ì†µÌ±†‚åã
                                                                                                     Ì†µÌ±õ     Ì†µÌ±ù ‚àí Ì†µÌ±†Ì†µÌ±ò ‚àí 1
                                                                                                 Ì†µÌ±ò
                                                              Ì†µÌ±ì(Ì†µÌ±ù, Ì†µÌ±õ, Ì†µÌ±†) =    ‚àë (‚àí1) ( )(                              ).
                                                                                                     Ì†µÌ±ò         Ì†µÌ±õ ‚àí 1
                                                                                  Ì†µÌ±ò=0
                             ƒπ Binomial formula for negative Ì†µÌ±õ
                                                                               Ì†µÌ±ò‚àí1                         Ì†µÌ±ò‚àí1
                                                                    ‚àíÌ†µÌ±õ              ‚àíÌ†µÌ±õ‚àíÌ†µÌ±ñ                       Ì†µÌ±õ + Ì†µÌ±ñ
                                                                                                          Ì†µÌ±ò
                                                                 ( )=‚àè                          =(‚àí1) ‚àè
                                                                     Ì†µÌ±ò               Ì†µÌ±ò ‚àí Ì†µÌ±ñ                     Ì†µÌ±ò ‚àí Ì†µÌ±ñ
                                                                                Ì†µÌ±ñ=0                         Ì†µÌ±ñ=0
                                                                                        Ì†µÌ±õ(Ì†µÌ±õ + 1)‚Ä¶(Ì†µÌ±õ + Ì†µÌ±ò ‚àí 1)
                                                                                      Ì†µÌ±ò
                                                                            =(‚àí1)
                                                                                                       Ì†µÌ±ò!
                                                                                        (Ì†µÌ±õ + Ì†µÌ±ò ‚àí 1)!
                                                                                      Ì†µÌ±ò
                                                                            =(‚àí1)
                                                                                          Ì†µÌ±ò!(Ì†µÌ±õ ‚àí 1)!
                                                                                          Ì†µÌ±õ + Ì†µÌ±ò ‚àí 1
                                                                                      Ì†µÌ±ò
                                                                            =(‚àí1) (                     )
                                                                                                Ì†µÌ±ò
                          Wecan verify our formula by simulating the dice rolling game.
                          set.seed(0)
                          # simulates rolling n dice and returns the sum
                          roll_dice <- function(n, s=6) {
                              sum(sample(seq(1,s), n, replace = T))
                          }
                          # rolling 10 dice 1000 times and collect the results
                          points <- replicate(1000, roll_dice(10))
                          # distribution of the sum of points
                          hist(points, freq = F)
                                                                                           102
                      Histogram of points
            0.06
         Density0.03
            0.00
                 20     30    40     50
                          points
        # formula for computing probability of dice points
        dice_formula <- function(p, n, s=6) {
         prob <- 1/s^n*sum(
          sapply(seq(0, floor((p-n)/s)),
              function(k) (-1)^k*choose(n,k)*choose(p-s*k-1,n-1)))
        }
        # computing the probability of getting 20-50 when rolling 10 dice
        x <- 20:50;
        y <- sapply(x, function(p) dice_formula(p,n=10))
        # overlay the formula on the histogram
        # it turns out the formula does a nice job!
        hist(points, ylim = c(0, 0.07), freq = F)
        lines(x, y, col = 2, lwd=2)
                            103
                      Histogram of points
            0.06
         Density0.03
            0.00
                 20     30    40     50
                          points
                            104
                                             31 Application: seller ratings*
                                             This example involves multiple types of discrete distributions. The technique used to solve this
                                             problem aligns with Bayesian inference, which is beyond the scope of this course. However,
                                             it remains an interesting case. The procedure illustrates the process of statistical modeling:
                                             we begin with an assumption and a proposed statistical model, then update it with new data.
                                             Finally, we draw inferences based on the model, typically addressing the question we aim to
                                             answer. You are not required to understand everything in this example. Nonetheless, it helps
                                             to develop a mindset of statistical inference early in the study.
                                             Supposeyouareshoppingaproductonline. Therearethreesellerswiththefollowingratings:
                                                    ‚Ä¢ Seller 1: 100% positive out of 10 reviews
                                                    ‚Ä¢ Seller 2: 96% positive out of 50 reviews
                                                    ‚Ä¢ Seller 3: 93% positive out of 200 reviews
                                             Which seller is likely to give the best service?
                                             The problem is intriguing because it is obvious that higher ratings do not necessarily means
                                             higher satisfaction. We have to weight in the number of reviews. The more reviews, the more
                                                                                                                                     (Ì†µÌ±ñ)
                                             trustworthy the ratings are. Let Ì†µÌ±ã                                                            be a random variable that means consumer Ì†µÌ±ó is satisfied
                                                                                                                                     Ì†µÌ±ó
                                                                                                                                                           (Ì†µÌ±ñ)
                                             with seller Ì†µÌ±ñ, where Ì†µÌ±ñ ‚àà {1,2,3}. Assume Ì†µÌ±ã                                                                        follows a Bernoulli distribution:
                                                                                                                                                           Ì†µÌ±ó
                                                                                                                                       1 satisfied with probability Ì†µÌºÉ
                                                                                                                     (Ì†µÌ±ñ)
                                                                                                                                                                                                                    Ì†µÌ±ñ
                                                                                                                Ì†µÌ±ã          ={
                                                                                                                     Ì†µÌ±ó
                                                                                                                                       0 otherwise
                                             where Ì†µÌºÉ is an unknown parameter of seller Ì†µÌ±ñ that captures their ‚Äúgenuine‚Äù satisfaction rate.
                                                                 Ì†µÌ±ñ
                                             Weassumethe consumers independently write their ratings. The overall positive rate of seller
                                                                                                                    (Ì†µÌ±ñ)
                                                                                                1
                                             Ì†µÌ±ñ  is therefore Ì†µÌ±Ö =                                   ‚àë Ì†µÌ±ã                  where Ì†µÌ±õ is the total number of reviews. We want to infer the
                                                                                    Ì†µÌ±ñ                                                         Ì†µÌ±ñ
                                                                                                                   Ì†µÌ±ó
                                                                                               Ì†µÌ±õ
                                                                                                           Ì†µÌ±ó
                                                                                                  Ì†µÌ±ñ
                                             value of Ì†µÌºÉ from their observed positive rate Ì†µÌ±Ö . From now on we drop the seller index Ì†µÌ±ñ to
                                                                      Ì†µÌ±ñ                                                                                             Ì†µÌ±ñ
                                             simply the notation since it is symmetric for all sellers.
                                             Because we have no prior knowledge about Ì†µÌºÉ. We assume that Ì†µÌºÉ takes any value from [0,1]
                                             equally likely, i.e. Ì†µÌºÉ ‚àº Unif(0,1). Assuming each Ì†µÌ±ã is independent and identical, then
                                                                                                                                                                               Ì†µÌ±ó
                                                                                                                                     Ì†µÌ±Ü = Ì†µÌ±ã +Ì†µÌ±ã +‚ãØ+Ì†µÌ±ã
                                                                                                                                                      1             2                         Ì†µÌ±õ
                                                                                                                                                               105
                           follows the Binomial distribution with PMF:
                                                                                             Ì†µÌ±õ
                                                                                                    Ì†µÌ±ò         Ì†µÌ±õ‚àíÌ†µÌ±ò
                                                                             Ì†µÌ±ù(Ì†µÌ±ò|Ì†µÌºÉ) = ( )Ì†µÌºÉ (1 ‚àí Ì†µÌºÉ)
                                                                                             Ì†µÌ±ò
                           Our goal is to find: Ì†µÌ±ù(Ì†µÌºÉ|Ì†µÌ±ò). Recall that the Bayes‚Äô rule allows us to invert the conditional
                           probability:
                                                                                 Ì†µÌ±ù(Ì†µÌ±ò|Ì†µÌºÉ)Ì†µÌ±ù(Ì†µÌºÉ)          Ì†µÌ±ù(Ì†µÌ±ò|Ì†µÌºÉ)Ì†µÌ±ù(Ì†µÌºÉ)
                                                                   Ì†µÌ±ù(Ì†µÌºÉ|Ì†µÌ±ò) =                   =
                                                                                                        ‚àû
                                                                                     Ì†µÌ±ù(Ì†µÌ±ò)
                                                                                                     ‚à´
                                                                                                            Ì†µÌ±ù(Ì†µÌ±ò|Ì†µÌºÉ)Ì†µÌ±ù(Ì†µÌºÉ)Ì†µÌ±ëÌ†µÌºÉ
                                                                                                       ‚àí‚àû
                           Since Ì†µÌºÉ ‚àº Unif(0,1), we have
                                                                                             1 if Ì†µÌºÉ ‚àà [0,1]
                                                                               Ì†µÌ±ù(Ì†µÌºÉ) = {
                                                                                             0 otherwise
                           Wenowfocus on Ì†µÌºÉ ‚àà [0,1], since the probability is 0 otherwise. Substitute in the PMF of the
                           Binomial distribution,
                                                                                              Ì†µÌ±õ
                                                                                                  Ì†µÌ±ò          Ì†µÌ±õ‚àíÌ†µÌ±ò
                                                                                            ( )Ì†µÌºÉ (1 ‚àí Ì†µÌºÉ)
                                                                                              Ì†µÌ±ò
                                                                          Ì†µÌ±ù(Ì†µÌºÉ|Ì†µÌ±ò) =
                                                                                          1
                                                                                              Ì†µÌ±õ
                                                                                                   Ì†µÌ±ò          Ì†µÌ±õ‚àíÌ†µÌ±ò
                                                                                        ‚à´
                                                                                             ( )Ì†µÌºÉ (1 ‚àí Ì†µÌºÉ)         Ì†µÌ±ëÌ†µÌºÉ
                                                                                              Ì†µÌ±ò
                                                                                         0
                           The hard part is to evaluate the integral. We state without proof (this is known as the Beta
                           function, which we will prove in later chapters):
                                                                              1
                                                                                                        Ì†µÌ±ò!(Ì†µÌ±õ ‚àí Ì†µÌ±ò)!
                                                                                  Ì†µÌ±ò          Ì†µÌ±õ‚àíÌ†µÌ±ò
                                                                           ‚à´ Ì†µÌºÉ (1‚àíÌ†µÌºÉ)              =
                                                                                                          (Ì†µÌ±õ + 1)!
                                                                            0
                           Therefore,
                                                                                         (Ì†µÌ±õ + 1)!
                                                                                                        Ì†µÌ±ò          Ì†µÌ±õ‚àíÌ†µÌ±ò
                                                                         Ì†µÌ±ù(Ì†µÌºÉ|Ì†µÌ±ò) =                  Ì†µÌºÉ (1‚àíÌ†µÌºÉ)
                                                                                       Ì†µÌ±ò!(Ì†µÌ±õ ‚àí Ì†µÌ±ò)!
                           Now suppose you are the next customer. The probability that you would be satisfied is
                                                                                              106
                                                                                                              1
                                                                Ì†µÌ±É(Ì†µÌ±ã            =1|Ì†µÌ±Ü = Ì†µÌ±ò) =‚à´ Ì†µÌ±É(Ì†µÌ±•                           =1|Ì†µÌºÉ)Ì†µÌ±ù(Ì†µÌºÉ|Ì†µÌ±ò)Ì†µÌ±ëÌ†µÌºÉ
                                                                         Ì†µÌ±õ+1                                            Ì†µÌ±õ+1
                                                                                                            0
                                                                                                              1
                                                                                                                           (Ì†µÌ±õ + 1)!
                                                                                                                                             Ì†µÌ±ò             Ì†µÌ±õ‚àíÌ†µÌ±ò
                                                                                                      =‚à´ Ì†µÌºÉ√ó                               Ì†µÌºÉ  (1‚àíÌ†µÌºÉ)              Ì†µÌ±ëÌ†µÌºÉ
                                                                                                                         Ì†µÌ±ò!(Ì†µÌ±õ ‚àí Ì†µÌ±ò)!
                                                                                                            0
                                                                                                                                 1
                                                                                                            (Ì†µÌ±õ + 1)!
                                                                                                                                      Ì†µÌ±ò+1               (Ì†µÌ±õ+1)‚àí(Ì†µÌ±ò+1)
                                                                                                      =                      ‚à´ Ì†µÌºÉ           (1‚àíÌ†µÌºÉ)                          Ì†µÌ±ëÌ†µÌºÉ
                                                                                                          Ì†µÌ±ò!(Ì†µÌ±õ ‚àí Ì†µÌ±ò)!
                                                                                                                               0
                                                                                                            (Ì†µÌ±õ + 1)!             (Ì†µÌ±ò + 1)!(Ì†µÌ±õ ‚àí Ì†µÌ±ò)!
                                                                                                      =                      √ó
                                                                                                          Ì†µÌ±ò!(Ì†µÌ±õ ‚àí Ì†µÌ±ò)!                  (Ì†µÌ±õ + 2)!
                                                                                                          Ì†µÌ±ò + 1
                                                                                                      =             .
                                                                                                          Ì†µÌ±õ + 2
                                 Now we substitute the ratings for the three sellers:
                                      ‚Ä¢ Seller 1: Ì†µÌ±õ = 10,Ì†µÌ±ò = 10
                                      ‚Ä¢ Seller 2: Ì†µÌ±õ = 50,Ì†µÌ±ò = 48
                                      ‚Ä¢ Seller 3: Ì†µÌ±õ = 200,Ì†µÌ±ò = 186
                                 The probabilities that you would be satisfied with each seller are: 92%, 94%, 93%. The result
                                 is known as the Laplace‚Äôs rule of succession. The rule of thumb is, pretending we have too
                                                                                                                                                                                                  Ì†µÌ±ò+1
                                 more reviews: one is positive, the other is negative. Compute the satisfaction rate as                                                                                  .
                                                                                                                                                                                                  Ì†µÌ±õ+2
                                                                                                                     107
                            Part IV
                 Expectation and Variance
                               108
                 32 Expectation revisited
                 Definition 32.1. For discrete random variable Ì†µÌ±ã, the expectation of Ì†µÌ±ã is defined as
                                                Ì†µÌ∞∏(Ì†µÌ±ã) = ‚àëÌ†µÌ±•Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•);
                                                        all Ì†µÌ±•
                 For continuous random variable Ì†µÌ±ã with density function Ì†µÌ±ì(Ì†µÌ±•), the expectation is defined as
                                                           ‚àû
                                                 Ì†µÌ∞∏(Ì†µÌ±ã) = ‚à´  Ì†µÌ±• Ì†µÌ±ì(Ì†µÌ±•) Ì†µÌ±ëÌ†µÌ±•.
                                                         ‚àí‚àû
                 Proposition 32.1 (Linearity). For random variables Ì†µÌ±ã ,Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã , regardless of their de-
                                                                     1   2      Ì†µÌ±õ
                 pendencies, it holds that
                                         Ì†µÌ∞∏(Ì†µÌ±ã +‚ãØ+Ì†µÌ±ã )=Ì†µÌ∞∏(Ì†µÌ±ã )+‚ãØ+Ì†µÌ∞∏(Ì†µÌ±ã ).
                                             1        Ì†µÌ±õ       1            Ì†µÌ±õ
                 Proof. We prove the simplest case Ì†µÌ∞∏(Ì†µÌ±ã +Ì†µÌ±å) = Ì†µÌ∞∏(Ì†µÌ±ã) +Ì†µÌ∞∏(Ì†µÌ±å).
                        Ì†µÌ∞∏(Ì†µÌ±ã + Ì†µÌ±å ) = ‚àë Ì†µÌ±ßÌ†µÌ±É(Ì†µÌ±ã +Ì†µÌ±å = Ì†µÌ±ß)
                                    Ì†µÌ±ß=Ì†µÌ±•+Ì†µÌ±¶
                                        ‚àë
                                  =‚àë (Ì†µÌ±•+Ì†µÌ±¶)Ì†µÌ±É(Ì†µÌ±ã =Ì†µÌ±•,Ì†µÌ±å =Ì†µÌ±¶)
                                     Ì†µÌ±•  Ì†µÌ±¶
                                  =‚àë‚àëÌ†µÌ±•Ì†µÌ±É(Ì†µÌ±ã=Ì†µÌ±•,Ì†µÌ±å =Ì†µÌ±¶)+‚àë‚àëÌ†µÌ±¶Ì†µÌ±É(Ì†µÌ±ã =Ì†µÌ±•,Ì†µÌ±å =Ì†µÌ±¶)
                                     Ì†µÌ±•  Ì†µÌ±¶                     Ì†µÌ±• Ì†µÌ±¶
                                  =‚àëÌ†µÌ±•‚àëÌ†µÌ±É(Ì†µÌ±ã=Ì†µÌ±•,Ì†µÌ±å =Ì†µÌ±¶)+‚àëÌ†µÌ±¶‚àëÌ†µÌ±É(Ì†µÌ±ã =Ì†µÌ±•,Ì†µÌ±å =Ì†µÌ±¶)
                                     Ì†µÌ±•   Ì†µÌ±¶                    Ì†µÌ±¶   Ì†µÌ±•
                                                      ‚ãÉ                   ‚ãÉ
                                  =‚àëÌ†µÌ±•Ì†µÌ±É((Ì†µÌ±ã =Ì†µÌ±•)‚à©      (Ì†µÌ±å = Ì†µÌ±¶)) + ‚àëÌ†µÌ±¶Ì†µÌ±É(  (Ì†µÌ±ã = Ì†µÌ±•) ‚à© (Ì†µÌ±å = Ì†µÌ±¶))
                                     Ì†µÌ±•                             Ì†µÌ±¶
                                                     all Ì†µÌ±¶               all Ì†µÌ±•
                                  =‚àëÌ†µÌ±•Ì†µÌ±É(Ì†µÌ±ã =Ì†µÌ±•)+‚àëÌ†µÌ±¶Ì†µÌ±É(Ì†µÌ±å =Ì†µÌ±¶)
                                     Ì†µÌ±•               Ì†µÌ±¶
                                  =Ì†µÌ∞∏(Ì†µÌ±ã)+Ì†µÌ∞∏(Ì†µÌ±å).
                 Proposition 32.2. Further properties on the linearity of expectations:
                                                          109
                       ‚Ä¢ If Ì†µÌ±å = Ì†µÌ±éÌ†µÌ±ã + Ì†µÌ±è, then Ì†µÌ∞∏(Ì†µÌ±å ) = Ì†µÌ±éÌ†µÌ∞∏(Ì†µÌ±ã) + Ì†µÌ±è.
                       ‚Ä¢ Ì†µÌ∞∏(Ì†µÌ±é Ì†µÌ±ã +‚ãØ+Ì†µÌ±é Ì†µÌ±ã +Ì†µÌ±è) = Ì†µÌ±é Ì†µÌ∞∏(Ì†µÌ±ã )+‚ãØ+Ì†µÌ±é Ì†µÌ∞∏(Ì†µÌ±ã )+Ì†µÌ±è
                              1   1         Ì†µÌ±õ Ì†µÌ±õ          1     1          Ì†µÌ±õ     Ì†µÌ±õ
                    Proposition 32.3 (Multiplication). If Ì†µÌ±ã and Ì†µÌ±å are independent, we have
                                                           Ì†µÌ∞∏(Ì†µÌ±ãÌ†µÌ±å ) = Ì†µÌ∞∏(Ì†µÌ±ã)Ì†µÌ∞∏(Ì†µÌ±å ).
                    In general, if Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã    are independent, we have
                                     1       Ì†µÌ±õ
                                                Ì†µÌ∞∏(Ì†µÌ±ã Ì†µÌ±ã ‚ãØÌ†µÌ±ã ) = Ì†µÌ∞∏(Ì†µÌ±ã )Ì†µÌ∞∏(Ì†µÌ±ã )‚ãØÌ†µÌ∞∏(Ì†µÌ±ã ).
                                                      1  2     Ì†µÌ±õ        1       2         Ì†µÌ±õ
                    Proof. For discrete and independent Ì†µÌ±ã,Ì†µÌ±å,
                                        Ì†µÌ∞∏(Ì†µÌ±ãÌ†µÌ±å ) = ‚àë‚àëÌ†µÌ±•Ì†µÌ±¶Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•,Ì†µÌ±å = Ì†µÌ±¶)
                                                     Ì†µÌ±•  Ì†µÌ±¶
                                                 =‚àë‚àëÌ†µÌ±•Ì†µÌ±¶Ì†µÌ±É(Ì†µÌ±ã =Ì†µÌ±•)Ì†µÌ±É(Ì†µÌ±å = Ì†µÌ±¶)         if independent
                                                     Ì†µÌ±•  Ì†µÌ±¶
                                                 =‚àëÌ†µÌ±•Ì†µÌ±É(Ì†µÌ±ã =Ì†µÌ±•)‚àëÌ†µÌ±¶Ì†µÌ±É(Ì†µÌ±å = Ì†µÌ±¶)
                                                     Ì†µÌ±•               Ì†µÌ±¶
                                                 =Ì†µÌ∞∏(Ì†µÌ±ã)Ì†µÌ∞∏(Ì†µÌ±å).
                      ¬æ Multiplication does not hold without independence
                      It is misleadingly natural to extend the generality of the addition rule to multiplication.
                      But the multiplication rule of expectation is very restrictive. Always remember to check
                      independence before applying the multiplication rule.
                      ƒπ SuÔ¨Äicient but not necessary condition
                      If Ì†µÌ±ã,Ì†µÌ±å are independent, it follows that Ì†µÌ∞∏(Ì†µÌ±ãÌ†µÌ±å ) = Ì†µÌ∞∏(Ì†µÌ±ã)Ì†µÌ∞∏(Ì†µÌ±å ). However, the latter does
                      not imply independence. Consider a counter-example,
                                               1 with prob. 1/2                1    with prob. 1/2
                                       Ì†µÌ±ã = {                       ,  Ì†µÌ±ç = {                        ;
                                               0 with prob. 1/2                ‚àí1 with prob. 1/2
                      Then
                                                                ‚éß
                                                                  ‚àí1 with prob. 1/4
                                                                {
                                                   Ì†µÌ±å = Ì†µÌ±ãÌ†µÌ±ç =                           .
                                                                  0     with prob. 1/2
                                                                ‚é®
                                                                {
                                                                  1     with prob. 1/4
                                                                ‚é©
                      We have Ì†µÌ∞∏(Ì†µÌ±ã) = 1/2, Ì†µÌ∞∏(Ì†µÌ±å) = 0, Ì†µÌ∞∏(Ì†µÌ±ãÌ†µÌ±å) = 0. So Ì†µÌ∞∏(Ì†µÌ±ãÌ†µÌ±å) = Ì†µÌ∞∏(Ì†µÌ±ã)Ì†µÌ∞∏(Ì†µÌ±å). But clearly
                      Ì†µÌ±ã,Ì†µÌ±å are not independent.
                                                                     110
                  Proposition 32.4 (Law of total expectation). Let {Ì†µÌ∞¥ } be a finite (or countable) partition of
                                                                        Ì†µÌ±ñ
                  the sample space, then
                                                   Ì†µÌ∞∏(Ì†µÌ±ã) = ‚àëÌ†µÌ∞∏(Ì†µÌ±ã|Ì†µÌ∞¥ )Ì†µÌ±É(Ì†µÌ∞¥ ).
                                                                      Ì†µÌ±ñ    Ì†µÌ±ñ
                                                             Ì†µÌ±ñ
                  Theorem 32.1 (Law of the unconscious statistician (LOTUS)). Let Ì†µÌ±ã be a random variable,
                  and Ì†µÌ±î be a real-valued function of a real variable. If Ì†µÌ±ã has a discrete distribution, then
                                                  Ì†µÌ∞∏[Ì†µÌ±î(Ì†µÌ±ã)] = ‚àëÌ†µÌ±î(Ì†µÌ±•)Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•).
                                                             all Ì†µÌ±•
                  LOTUSsays we can compute the expectation of Ì†µÌ±î(Ì†µÌ±ã) without knowing the PMF of Ì†µÌ±î(Ì†µÌ±ã).
                                                             2
                  Example 32.1. Compute Ì†µÌ∞∏(Ì†µÌ±ã) and Ì†µÌ∞∏(Ì†µÌ±ã ) given the following distribution:
                                                               1
                                                             ‚éß
                                                                ,   Ì†µÌ±• = 0
                                                               4
                                                             {
                                                               1
                                                     Ì†µÌ±ì(Ì†µÌ±•) =
                                                                ,   Ì†µÌ±• = 1
                                                               2
                                                             ‚é®
                                                             {
                                                               1
                                                                ,   Ì†µÌ±• = 2
                                                             ‚é©
                                                               4
                  Solution. Compute the expectations of Ì†µÌ±ã by definition:
                                                            1       1       1
                                               Ì†µÌ∞∏(Ì†µÌ±ã) = 0 √ó   +1√ó +2√ó =1
                                                            4       2       4
                                                  2
                  Compute the expectations of Ì†µÌ±ã by LOTUS:
                                                           1         1        1    3
                                                 2     2         2        2
                                            Ì†µÌ∞∏(Ì†µÌ±ã ) = 0 √ó    +1 √ó +2 √ó = .
                                                           4         2        4    2
                                 2           2
                  Note that Ì†µÌ∞∏(Ì†µÌ±ã ) ‚â† [Ì†µÌ∞∏(Ì†µÌ±ã)] .
                    ¬æ Don‚Äôt pull non-linear functions out of expectation
                    In general, Ì†µÌ∞∏[Ì†µÌ±î(Ì†µÌ±ã)] ‚â† Ì†µÌ±î(Ì†µÌ∞∏(Ì†µÌ±ã)). Linearity implies Ì†µÌ∞∏[Ì†µÌ±î(Ì†µÌ±ã)] = Ì†µÌ±î(Ì†µÌ∞∏(Ì†µÌ±ã)) if Ì†µÌ±î is a linear
                    function. For a nonlinear function Ì†µÌ±î, you can‚Äôt pull function Ì†µÌ±î out of expectation Ì†µÌ∞∏. The
                    right way to find Ì†µÌ∞∏[Ì†µÌ±î(Ì†µÌ±ã)] is with LOTUS.
                  Example 32.2 (St. Petersburg Paradox). Flip a fair coin over and over again until the head
                                                      Ì†µÌ±ò
                  lands the first time. You will win 2 dollars if the head lands in the Ì†µÌ±ò-th trial (including the
                  successful trial). What is the expected payoff of this game?
                                                               111
                                          Ì†µÌ±ò
                    Solution. Let Ì†µÌ±ã = 2 . We want to find Ì†µÌ∞∏(Ì†µÌ±ã). The probability of the first head showing up in
                                      1
                    the Ì†µÌ±ò-th trial is  . Therefore,
                                       Ì†µÌ±ò
                                      2
                                                                ‚àû             ‚àû
                                                                        1
                                                                    Ì†µÌ±ò
                                                     Ì†µÌ∞∏(Ì†µÌ±ã) = ‚àë2 ‚ãÖ         =‚àë1=‚àû
                                                                         Ì†µÌ±ò
                                                                       2
                                                               Ì†µÌ±ò=1           Ì†µÌ±ò=1
                    Theexpected payoff is infinitely high! This is against most people‚Äôs intuition, which is likely to
                                                                                                                     Ì†µÌ±ò
                    be a small number. This is because we mistakenly go through the calculation Ì†µÌ∞∏(Ì†µÌ±ã) = Ì†µÌ∞∏(2 ) =
                     Ì†µÌ∞∏(Ì†µÌ±ò)                                                                                  Ì†µÌ∞∏(Ì†µÌ±ò)
                    2     in our mind. Ì†µÌ∞∏(Ì†µÌ±ò) the expected number of flips before a head is 2. Thus, 2           =4.
                    Another way to resolve the paradox is that we don‚Äôt typically reason about infinity. No one
                    would play this game infinitely many times. For finite number of plays, the probability of
                                                      100
                    getting very large payoff, say 2     , is none. We can demonstrate this with a simulation.
                    # number of simulations
                    N <- 1000
                    # store simulated results
                    X <- numeric(N)
                    set.seed(0)
                    # run simulation
                    for (i in 1:N) {
                      # start with the initial reward
                      x <- 2
                      # flip a coin until it lands tails
                      while (runif(1) < 0.5) {
                         x <- x * 2
                      }
                      # store the reward for this simulation
                      X[i] <- x
                    }
                    cat("Expected Reward:", mean(X))
                    Expected Reward: 12.938
                                                                     112
                   33 Life expectancy
                   Life expectancy is the average number of years a person is expected to live. It is a crucial
                   indicator of the quality of living and one of the three components of the Human Development
                   Index (HDI) (the other two components are education and per capita GDP). Here is a toy
                                                                                  1
                   example to compute life expectancy with hypothetical data.
                   To simplify our analysis, we will assume there are only five possible ages: 0, 20, 40, 60, and 80.
                   Ababy is born at age 0, and can either die at that age or survive to age 20. We intentionally
                   exclude intermediate ages such as 5 and 10 for the sake of computational simplicity.
                   It‚Äôs important to note that life expectancy is not the same as the average age of the population.
                   For instance, based on the hypothetical data presented, the average age can be calculated as:
                             Age=(0√ó200+20√ó300+40√ó250+60√ó150+80√ó100)/1000=33.
                   However, the expected age, denoted as Ì†µÌ∞∏(Age), is defined as:
                                                    Ì†µÌ∞∏(Age) = ‚àëAge√óÌ†µÌ±É(Age).
                   To compute this expected value, we need to determine Ì†µÌ±É(Age), the probability of living to a
                   specific age or dying at that age. This requires consideration of the mortality rate at each age,
                   which is given in Column 3.
                    1
                     This is an overly simplified example that only serves to clarify the definition of expectation. See this tutorial
                      from MEASURE Evaluation for the actual computation of life expectancy.
                                                                  113
        Assuming 1000 babies are born at age 0, with a mortality rate of 1% at that age, we find that
        99% of the babies survive to age 20. Thus, the number of babies that survive to age 20 is:
        1000√ó(1‚àí1%)=990. Wecanapplysimilarcalculations to determine the number of survivors
        at each subsequent age.
        The number of individuals who die at a specific age (Column 5) is the difference between the
        number of survivors at that age and the next (Column 4). To find the probability of living to
        a specific age, we compute: Ì†µÌ±É(Age) = Column 4/1000.
        Finally, we compute the expected value of age (or life expectancy) as follows:
             Ì†µÌ∞∏(Ì†µÌ∞¥Ì†µÌ±îÌ†µÌ±í) = 0 √ó 1% + 20 √ó 2% + 40 √ó 10% +60√ó17%+80√ó70% = 70.6.
        This figure differs from the average age. Since the mortality rate is low at younger ages,
        the probabilities Ì†µÌ±É(Age) for these ages are also low, while they are higher for older ages.
        This example illustrates the distinction between average and expected values. In everyday
        conversation, we may use these terms interchangeably, but in certain contexts, expected values
        can significantly differ from averages.
                            114
                  34 Two envelope paradox*
                  Example 34.1 (Two-envelope paradox). Imagine you are given two identical envelopes, each
                  containing money. One contains twice as much as the other. You may pick one envelope and
                  keep the money it contains. Having chosen an envelope at will, but before inspecting it, you
                  are given the chance to switch envelopes. Should you switch?
                  The paradox arises when you try to solve the expectation. Let Ì†µÌ∞¥ denote the amount of money
                  in the envelope you have chosen, and Ì†µÌ∞µ denote the amount of money in the other envelope.
                  We know Ì†µÌ∞µ is either twice as much as Ì†µÌ∞¥, or half as much as Ì†µÌ∞¥. Each with probability 1/2.
                  So
                                                           1         1         5
                                                  Ì†µÌ∞∏(Ì†µÌ∞µ) =   (2Ì†µÌ∞¥) +  (Ì†µÌ∞¥/2) =   Ì†µÌ∞¥
                                                           2         2         4
                  Since Ì†µÌ∞∏(Ì†µÌ∞µ) > Ì†µÌ∞¥, you should always switch! However, after you switch to Ì†µÌ∞µ, by the same
                  argument, you should switch back to Ì†µÌ∞¥. You you switch back and forth indefinitely!
                  Wheredothingsgowrong? Theerrorinthiscalculationliesinasubtlemisunderstanding:
                  the two Ì†µÌ∞¥s in the calculation actually represent different values, that are incorrectly equated.
                  In particular, the 2Ì†µÌ∞¥ represents the expected value in the other envelope given that it is the
                  larger one, and the Ì†µÌ∞¥/2 represents the expected value in the other envelope given that it is
                  the smaller one.
                                     Ì†µÌ∞∏(Ì†µÌ∞µ) = Ì†µÌ∞∏(Ì†µÌ∞µ|Ì†µÌ∞µ < Ì†µÌ∞¥)Ì†µÌ±É(Ì†µÌ∞µ < Ì†µÌ∞¥) + Ì†µÌ∞∏(Ì†µÌ∞µ|Ì†µÌ∞µ > Ì†µÌ∞¥)Ì†µÌ±É(Ì†µÌ∞µ > Ì†µÌ∞¥)
                  Suppose the amount of money in the two envelopes are Ì†µÌ±é and 2Ì†µÌ±é respectively. Ì†µÌ∞∏(Ì†µÌ∞µ|Ì†µÌ∞µ < Ì†µÌ∞¥) = Ì†µÌ±é
                  and Ì†µÌ∞∏(Ì†µÌ∞µ|Ì†µÌ∞µ > Ì†µÌ∞¥) = 2Ì†µÌ±é. Therefore,
                                                               1     1      3
                                                      Ì†µÌ∞∏(Ì†µÌ∞µ) =  Ì†µÌ±é +  2Ì†µÌ±é =  Ì†µÌ±é.
                                                               2     2      2
                  The same calculation applies to Ì†µÌ∞∏(Ì†µÌ∞¥). Thus, Ì†µÌ∞∏(Ì†µÌ∞¥) = Ì†µÌ∞∏(Ì†µÌ∞µ).
                                                                 115
                            35 Linearity and indicators
                            Definition 35.1 (Indicator variable). An indicator variable Ì†µÌµÄ                                       for an event Ì†µÌ∞¥ is a random
                                                                                                                             Ì†µÌ∞¥
                            variable defined as:
                                                                                   1 if event Ì†µÌ∞¥ occurs,
                                                                       Ì†µÌµÄ   ={
                                                                         Ì†µÌ∞¥
                                                                                   0 if event Ì†µÌ∞¥ does not occur.
                            The indicator variable Ì†µÌµÄ               ‚Äúindicates‚Äù whether the event Ì†µÌ∞¥ happens (1) or not (0).
                                                                Ì†µÌ∞¥
                            The expected value of an indicator variable is equal to the probability of the event Ì†µÌ∞¥:
                                                                                                                 Ì†µÌ±ê
                                                                        Ì†µÌ∞∏[Ì†µÌµÄ  ] = 1 ‚ãÖ Ì†µÌ±É(Ì†µÌ∞¥) + 0 ‚ãÖ Ì†µÌ±É(Ì†µÌ∞¥ ) = Ì†µÌ±É(Ì†µÌ∞¥)
                                                                             Ì†µÌ∞¥
                            This is known as the fundamental bridge, as it allows us to convert between probability and
                            expectation.
                            Indicator variables are often used in linearity of expectation calculations. This allows us to
                                                                                                                                                                  Ì†µÌ±õ
                            break down a problem into easy-to-solve small problems. For example, if Ì†µÌ±ã = ‚àë                                                             Ì†µÌµÄ   ,
                                                                                                                                                                        Ì†µÌ∞¥
                                                                                                                                                                  Ì†µÌ±ñ=1
                                                                                                                                                                           Ì†µÌ±ñ
                            then:
                                                                                           Ì†µÌ±õ                 Ì†µÌ±õ
                                                                            Ì†µÌ∞∏[Ì†µÌ±ã] = ‚àëÌ†µÌ∞∏[Ì†µÌµÄ            ] = ‚àëÌ†µÌ±É(Ì†µÌ∞¥ )
                                                                                                    Ì†µÌ∞¥                   Ì†µÌ±ñ
                                                                                                      Ì†µÌ±ñ
                                                                                         Ì†µÌ±ñ=1                Ì†µÌ±ñ=1
                            Example 35.1. In a group of Ì†µÌ±õ people, what is the expected number of distinct birthdays
                            among the Ì†µÌ±õ people (the expected number of days on which at least one of the people was
                            born)? What is the expected number of people sharing a birthday (any day)?
                            Solution. Let Ì†µÌ±ã be the number of distinct birthdays, and write Ì†µÌ±ã = Ì†µÌ∞º + ‚ãØ + Ì†µÌ∞º                                                , where
                                                                                                                                         1             365
                                                                               1 if someone was born on day Ì†µÌ±ó
                                                                    Ì†µÌ∞º  ={                                                         .
                                                                      Ì†µÌ±ó
                                                                               0 otherwise
                            Then
                                                                    Ì†µÌ∞∏(Ì†µÌ∞º ) = Ì†µÌ±É(someone was born on day Ì†µÌ±ó)
                                                                         Ì†µÌ±ó
                                                                             =1‚àíÌ†µÌ±É(no one was born on day Ì†µÌ±ó)
                                                                                                  Ì†µÌ±õ
                                                                                          364
                                                                             =1‚àí( ) .
                                                                                          365
                                                                                                 116
                 Then by linearity,
                                                                       Ì†µÌ±õ
                                                                  364
                                               Ì†µÌ∞∏(Ì†µÌ±ã) = 365(1‚àí(       ) ).
                                                                  365
                 Let Ì†µÌ±å be the number of people sharing a birthday, and Ì†µÌ±å = Ì†µÌ∞Ω + ‚ãØ + Ì†µÌ∞Ω   where Ì†µÌ∞Ω is an
                                                                               1         Ì†µÌ±õ       Ì†µÌ±ò
                 indicator that the Ì†µÌ±ó-th person shares his birthday with somebody else.
                                       Ì†µÌ∞∏(Ì†µÌ∞Ω ) = Ì†µÌ±É(someone shares birthday with Ì†µÌ±ò)
                                           Ì†µÌ±ò
                                             =1‚àíÌ†µÌ±É(no one shares birthday with Ì†µÌ±ò)
                                                          Ì†µÌ±õ‚àí1
                                                     364
                                             =1‚àí( ) .
                                                     365
                 Therefore,
                                                                           Ì†µÌ±õ‚àí1
                                                  Ì†µÌ±õ
                                                                      364
                                         Ì†µÌ∞∏(Ì†µÌ±å ) = ‚àëÌ†µÌ∞∏(Ì†µÌ∞Ω ) = Ì†µÌ±õ(1‚àí(     )    ).
                                                        Ì†µÌ±ò
                                                                      365
                                                 Ì†µÌ±ò=1
                 For some numeric values, Ì†µÌ∞∏(Ì†µÌ±å) = 2.3 if Ì†µÌ±õ = 30; Ì†µÌ∞∏(Ì†µÌ±å ) = 6.3 if Ì†µÌ±õ = 50.
                 Example35.2. LetŒ†beapermutationover{1,2,‚Ä¶,Ì†µÌ±õ}. Thatisareorderingofthenumbers.
                 Afixed point of a permutation are the points not moved by the permutation. For example, in
                 the permutation below
                                                          1 2 3 4
                                                      Œ† 2 4 3 1
                 The fixed point is 3. Find the expected number of fixed points of a random permutation.
                 Solution. Let Ì†µÌ±ã be the number of fixed points of a random permutation.            Then
                          Ì†µÌ±õ
                 Ì†µÌ±ã = ‚àë       1      where 1        indicates the Ì†µÌ±ò-th number stays the same after the
                               Œ†(Ì†µÌ±ò)=Ì†µÌ±ò      Œ†(Ì†µÌ±ò)=Ì†µÌ±ò
                          Ì†µÌ±ò=1
                 permutation. By linearity,
                                               Ì†µÌ±õ             Ì†µÌ±õ               Ì†µÌ±õ
                                                                                  1
                                              ‚àë         )=‚àë               ) = ‚àë
                                  Ì†µÌ∞∏(Ì†µÌ±ã) = Ì†µÌ∞∏ (   1             Ì†µÌ∞∏ (1               =1.
                                                   Œ†(Ì†µÌ±ò)=Ì†µÌ±ò          Œ†(Ì†µÌ±ò)=Ì†µÌ±ò
                                                                                  Ì†µÌ±õ
                                              Ì†µÌ±ò=1           Ì†µÌ±ò=1             Ì†µÌ±ò=1
                 Example 35.3 (Buffon‚Äôs needle). A plan is ruled by the lines Ì†µÌ±¶ = 0,¬±1,¬±2,... and a needle
                 of unit length is cast randomly on to the plane. What is the probability that it intersects some
                 line?
                 Solution. Here we sketch an intuitive approach. A more rigorous one will be given in Exam-
                 ple 44.2.
                 Let Ì†µÌ±ã be the number of times the needle crosses a line. The needle can cross a line either 1
                 or 0 times. Thus, Ì†µÌ±É(intersection) = Ì†µÌ∞∏(Ì†µÌ±ã).
                                                           117
                    Consider dropping any (continuous) curve of unit length onto the surface. Divide the curve
                    into Ì†µÌ±Å straight line segments, each of length 1/Ì†µÌ±Å. Let Ì†µÌ±ã be the indicator for the Ì†µÌ±ñ-th segment
                                                                                 Ì†µÌ±ñ
                    crossing a line. Then,
                                             Ì†µÌ∞∏(Ì†µÌ±ã) = Ì†µÌ∞∏ (‚àëÌ†µÌ±ã ) = ‚àëÌ†µÌ∞∏(Ì†µÌ±ã ) = Ì†µÌ±Å ‚ãÖÌ†µÌ∞∏(Ì†µÌ±ã ).
                                                                 Ì†µÌ±ñ            Ì†µÌ±ñ            Ì†µÌ±ñ
                   Wedon‚Äôt necessarily have to compute this expectation, but by this line of reasoning: Ì†µÌ∞∏(Ì†µÌ±ã) is
                    proportional to the length of the curve, regardless the shape of the curve. If we can compute
                    Ì†µÌ∞∏(Ì†µÌ±ã) for some curve, then Ì†µÌ∞∏(Ì†µÌ±ã) is the same for all curves with the same length.
                    Consider a circle of diameter Ì†µÌ±ë = 1. The circle always crosses the lines twice for sure. That is,
                    Ì†µÌ∞∏(Ì†µÌ±ã    ) = 2. The length of the circle is Ì†µÌºã. Therefore, for any curve (including a needle) of
                         circle
                    length Ì†µÌºã we have Ì†µÌ∞∏(Ì†µÌ±ã ) = 2.
                                            Ì†µÌºã
                    For a needle of unit length, scale it down by Ì†µÌºã, we have Ì†µÌ∞∏(Ì†µÌ±ã) = 2/Ì†µÌºã. Therefore,
                                                                               2
                                                           Ì†µÌ±É(intersection) =    .
                                                                               Ì†µÌºã
                    # number of simulations
                    N <- 10000
                    # number of crossings
                    X <- numeric(N)
                    set.seed(0)
                    # run simulation
                    for (i in 1:N) {
                      # randomly generate the position of the needle's midpoint
                      # distance from the nearest line
                      y <- runif(1, min = 0, max = 1/2)
                      # randomly generate the angle of the needle (in radians)
                      Ôøø <- runif(1, min = 0, max = pi/2)
                      # check if the needle crosses a line
                      if (y <= 1/2 * sin(Ôøø)) X[i] <- 1
                      else X[i] <- 0
                    }
                    Ôøø <- 2 / mean(X)
                    cat("Estimated value of Ôøø:", Ôøø)
                                                                     118
        Estimated value of Ôøø: 3.140704
                            119
                       36 Median and mode
                       Themeaniscalledameasureofcentral tendency because it tells us something about the center
                       of a distribution, specifically its center of mass. Other measures of central tendency that are
                       commonly used in statistics are the median and the mode, which we now define.
                       Definition 36.1 (Median). We say that Ì†µÌ±ê is a median of a random variable Ì†µÌ±ã if
                                                           Ì†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±ê) ‚â• 1/2 and Ì†µÌ±É(Ì†µÌ±ã ‚â• Ì†µÌ±ê) ‚â• 1/2.
                       Intuitively, the median is a value Ì†µÌ±ê such that half the mass of the distribution falls on either
                       side of Ì†µÌ±ê (or as close to half as possible, for discrete random variables). Note that the condition
                       given above is more general than
                                                                                                   1
                                                                  Ì†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±ê) = Ì†µÌ±É(Ì†µÌ±ã ‚â• Ì†µÌ±ê) =
                                                                                                   2
                       Consider a discrete distribution as follows:
                                                                                     1
                                                                                  ‚éß
                                                                                       ,   Ì†µÌ±ò = 1
                                                                                     3
                                                                                  {
                                                                                     1
                                                                  Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±ò) =
                                                                                       ,   Ì†µÌ±ò = 2
                                                                                     2
                                                                                  ‚é®
                                                                                  {
                                                                                     1
                                                                                       ,   Ì†µÌ±ò = 3
                                                                                  ‚é©
                                                                                     6
                       In this case, 2 is a median since Ì†µÌ±É(Ì†µÌ±ã ‚â§ 2) = 5/6 ‚â• 1/2 and Ì†µÌ±É(Ì†µÌ±ã ‚â• 2) = 2/3 ‚â• 1/2. However,
                       Ì†µÌ±É(Ì†µÌ±ã ‚â§ 2) ‚â† Ì†µÌ±É(Ì†µÌ±ã ‚â• 2). For strictly continuous random variable Ì†µÌ±ã, Definition 36.1 does
                       imply
                                                                                                   1
                                                                  Ì†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±ê) = Ì†µÌ±É(Ì†µÌ±ã ‚â• Ì†µÌ±ê) =
                                                                                                   2
                       Since the CDF of Ì†µÌ±ã satisfies Ì†µÌ∞π(Ì†µÌ±ê) ‚â• 1/2 and 1 ‚àí Ì†µÌ∞π(Ì†µÌ±ê) ‚â• 1/2, which implies Ì†µÌ∞π(Ì†µÌ±ê) = 1/2.
                                                                                         ‚àí1
                       Moreover, if the CDF of Ì†µÌ±ã is strictly increasing, Ì†µÌ∞π                (1/2) is the unique median.
                       Definition 36.2 (Mode). For a discrete random variable Ì†µÌ±ã, we say that Ì†µÌ±ê is a mode of Ì†µÌ±ã if
                       it maximizes the PMF:
                                                             Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±ê) ‚â• Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•)         for all Ì†µÌ±•.
                       For a continuous random variable Ì†µÌ±ã with PDF Ì†µÌ±ì, we say that Ì†µÌ±ê is a mode if it maximizes the
                       PDF:
                                                                    Ì†µÌ±ì(Ì†µÌ±ê) ‚â• Ì†µÌ±ì(Ì†µÌ±•)    for all Ì†µÌ±•.
                       Intuitively, the mode is a value that has the greatest mass or density out of all values in the
                       support of Ì†µÌ±ã.
                                                                                 120
             ƒπ Note
             A distribution can have multiple medians and multiple modes. Medians have to occur
             side by side; modes can occur all over the distribution.
            Example36.1(Incomedistribution). Themainreasonwhythemedianissometimespreferred
            over the mean is that the median is more robust to extreme values. A typical example is the
            income distribution. Higher incomes are rare, but their absolute values are high. Thus, the
            mean income tends be higher than what the mass of the population would earn. But the
            median is more robust to extreme values and is closer to the earnings of an ‚Äúaverage‚Äù person.
            For example, the average monthly income in China is ¬•2,500 in 2019, but the median is only
            ¬•1,000.
            Theorem 36.1. Let Ì†µÌ±ã be an random variable with mean Ì†µÌºá , and let Ì†µÌ±ö be a median of Ì†µÌ±ã.
                                                      2
             ‚Ä¢ A value of Ì†µÌ±ê that minimizes the mean squared error Ì†µÌ∞∏(Ì†µÌ±ã ‚àíÌ†µÌ±ê) is Ì†µÌ±ê = Ì†µÌºá.
             ‚Ä¢ A value of Ì†µÌ±ê that minimizes the mean absolute error Ì†µÌ∞∏|Ì†µÌ±ã ‚àí Ì†µÌ±ê| is Ì†µÌ±ê = Ì†µÌ±ö.
            Proof.
                                            2
             1) Minimizing the mean squared error Ì†µÌ∞∏[(Ì†µÌ±ã ‚àíÌ†µÌ±ê) ]. Expand the mean squared error:
                               2     2      2     2         2
                         Ì†µÌ∞∏[(Ì†µÌ±ã ‚àí Ì†µÌ±ê) ] = Ì†µÌ∞∏[Ì†µÌ±ã ‚àí 2Ì†µÌ±êÌ†µÌ±ã + Ì†µÌ±ê ] = Ì†µÌ∞∏[Ì†µÌ±ã ] ‚àí 2Ì†µÌ±êÌ†µÌ∞∏[Ì†µÌ±ã] + Ì†µÌ±ê .
                                        121
                               To find the value of Ì†µÌ±ê that minimizes this expression, take the derivative with respect to
                               Ì†µÌ±ê and set it to zero:
                                                                   Ì†µÌ±ë
                                                                                   2
                                                                      Ì†µÌ∞∏[(Ì†µÌ±ã ‚àí Ì†µÌ±ê) ] = ‚àí2Ì†µÌ∞∏[Ì†µÌ±ã] + 2Ì†µÌ±ê = 0
                                                                   Ì†µÌ±ëÌ†µÌ±ê
                               This implies Ì†µÌ±ê = Ì†µÌºá. We can confirm with second-order condition that Ì†µÌ±ê = Ì†µÌºá is indeed a
                               minimizer.
                           2) Minimizing the mean absolute error Ì†µÌ∞∏|Ì†µÌ±ã ‚àíÌ†µÌ±ê|. We prove this assuming Ì†µÌ±ã is continuous.
                                                                          Ì†µÌ±ê                          ‚àû
                                                        Ì†µÌ∞∏|Ì†µÌ±ã ‚àí Ì†µÌ±ê| = ‚à´ (Ì†µÌ±ê ‚àí Ì†µÌ±•)Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±• + ‚à´          (Ì†µÌ±• ‚àí Ì†µÌ±ê)Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•
                                                                         ‚àí‚àû                         Ì†µÌ±ê
                               Take derivative with respect to Ì†µÌ±ê, applying the Leibniz‚Äôs rule:
                                                                      Ì†µÌ±ê                                         ‚àû
                                                            Ì†µÌ±ë                                        Ì†µÌ±ë
                                           (Ì†µÌ±ê ‚àí Ì†µÌ±•)Ì†µÌ±ì(Ì†µÌ±•)    Ì†µÌ±ê + ‚à´     Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±• ‚àí (Ì†µÌ±• ‚àí Ì†µÌ±ê)Ì†µÌ±ì(Ì†µÌ±•)    Ì†µÌ±ê + ‚à´ (‚àíÌ†µÌ±ì(Ì†µÌ±•))Ì†µÌ±ëÌ†µÌ±• = 0
                                                           Ì†µÌ±ëÌ†µÌ±ê                                       Ì†µÌ±ëÌ†µÌ±ê
                                                                     ‚àí‚àû                                        Ì†µÌ±ê
                               The first-order condition resolves to
                                                                           Ì†µÌ±ê                 ‚àû
                                                                        ‚à´ Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±• = ‚à´         Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•
                                                                         ‚àí‚àû                 Ì†µÌ±ê
                               which is exactly the definition of a median.
                                                                                   122
                                 37 Variance
                                 Expectation is the most commonly used summary of a distribution, as it indicates where values
                                 are likely centered. However, it provides limited insight into the distribution‚Äôs overall shape.
                                 For example, two random variables might have the same mean, yet one could have values
                                 spread far from the mean while the other has values tightly clustered around it. Variance, on
                                 the other hand, describes how far values in a distribution typically deviate from the mean,
                                 offering a measure of the distribution‚Äôs dispersion.
                                 Definition 37.1 (Variance). The variance of a random variable Ì†µÌ±ã is defined as
                                                                                                                                              2
                                                                                               Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) = Ì†µÌ∞∏ [Ì†µÌ±ã ‚àí Ì†µÌ∞∏(Ì†µÌ±ã)] .
                                                                                                                                            2
                                                                                                                                                                     ‚àö
                                 By convention, variance is also denoted by Greek letter Ì†µÌºé , where Ì†µÌºé =                                                                 Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) is called the
                                 standard deviation.
                                 Variance measures how far Ì†µÌ±ã typically deviates from its mean, but instead of averaging the
                                 differences, we average the squared differences to ensure both positive and negative deviations
                                 contribute. The expected deviation, Ì†µÌ∞∏(Ì†µÌ±ã ‚àí Ì†µÌ∞∏(Ì†µÌ±ã)), is always zero, so squaring avoids this
                                 cancellation. Since variance is in squared units, we take the square root to get the standard
                                 deviation, restoring the original units.
                                     ƒπ Why squared deviation?
                                     Wecan measure the dispersion of a distribution in different ways. For example, Ì†µÌ∞∏(|Ì†µÌ±ã ‚àí
                                     Ì†µÌ∞∏(Ì†µÌ±ã)|) is also a possible choice. But it is less common because the absolute value function
                                     isn‚Äôt differentiable. Besides, squaring connects to geometric concepts like the distance
                                     formula and Pythagorean theorem, which have useful statistical meanings.
                                     ƒπ Sample variance
                                     Definition 37.1 gives the theoretical variance of a distribution. With finite sample from
                                     the distribution, we estimate the variance with sample observations:
                                                                                                                        Ì†µÌ±õ
                                                                                                              1
                                                                                                  2                                          2
                                                                                                                                           ÃÑ
                                                                                                Ì†µÌ±†    =               ‚àë(Ì†µÌ±ã ‚àíÌ†µÌ±ã)
                                                                                                                                 Ì†µÌ±ñ
                                                                                                           Ì†µÌ±õ ‚àí 1
                                                                                                                      Ì†µÌ±ñ=1
                                     Note that we divide by Ì†µÌ±õ‚àí1 not Ì†µÌ±õ. Why? Because we want an unbiased estimator. We
                                                                                                                     123
                 will discuss this later in detail. But here is a sketch of the reasoning.
                 First note:
                                     Ì†µÌ±õ             Ì†µÌ±õ
                                               2             2          2
                                              ÃÑ                    ÃÑ
                                     ‚àë(Ì†µÌ±ã ‚àíÌ†µÌ±ã) =‚àë(Ì†µÌ±ã ‚àíÌ†µÌºá) ‚àíÌ†µÌ±õ(Ì†µÌ±ã‚àíÌ†µÌºá)
                                          Ì†µÌ±ñ            Ì†µÌ±ñ
                                     Ì†µÌ±ñ=1          Ì†µÌ±ñ=1
                 Take expectations of both sides:
                                      Ì†µÌ±õ
                                                              2
                                                             Ì†µÌºé
                                                2      2                   2
                                               ÃÑ
                                  Ì†µÌ∞∏ [‚àë(Ì†µÌ±ã ‚àíÌ†µÌ±ã) ] = Ì†µÌ±õÌ†µÌºé ‚àíÌ†µÌ±õ(  )=(Ì†µÌ±õ‚àí1)Ì†µÌºé
                                          Ì†µÌ±ñ
                                                              Ì†µÌ±õ
                                     Ì†µÌ±ñ=1
                                          2    2
                 Dividing by Ì†µÌ±õ ‚àí 1 makes Ì†µÌ∞∏(Ì†µÌ±† ) = Ì†µÌºé .
               Theorem 37.1. For any random variable Ì†µÌ±ã,
                                                         2       2
                                           Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) = Ì†µÌ∞∏(Ì†µÌ±ã ) ‚àí (Ì†µÌ∞∏Ì†µÌ±ã) .
               Proof. Let Ì†µÌºá = Ì†µÌ∞∏(Ì†µÌ±ã). By definition,
                                                    2      2          2
                                  Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) = Ì†µÌ∞∏(Ì†µÌ±ã ‚àí Ì†µÌºá) = Ì†µÌ∞∏(Ì†µÌ±ã ‚àí 2Ì†µÌºáÌ†µÌ±ã + Ì†µÌºá )
                                               2              2      2    2
                                         =Ì†µÌ∞∏(Ì†µÌ±ã )‚àí2Ì†µÌºáÌ†µÌ∞∏(Ì†µÌ±ã)+Ì†µÌºá = Ì†µÌ∞∏(Ì†µÌ±ã )‚àíÌ†µÌºá .
               Example 37.1. Find the variance for Ì†µÌ±ã ‚àº Bern(Ì†µÌ±ù).
                                                2    2           2
                                   Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) = Ì†µÌ∞∏(Ì†µÌ±ã ) ‚àí Ì†µÌ∞∏ (Ì†µÌ±ã) = Ì†µÌ±ù ‚àí Ì†µÌ±ù = Ì†µÌ±ù(1 ‚àí Ì†µÌ±ù).
               Proposition 37.1. Variance has the following properties:
                  ‚Ä¢ Ì†µÌ±âÌ†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) ‚â• 0
                  ‚Ä¢ Ì†µÌ±âÌ†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã + Ì†µÌ±ê) = Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã)
                               2
                  ‚Ä¢ Ì†µÌ±âÌ†µÌ±éÌ†µÌ±ü(Ì†µÌ±êÌ†µÌ±ã) = Ì†µÌ±ê Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã)
                  ‚Ä¢ If Ì†µÌ±ã,Ì†µÌ±å are independent, Ì†µÌ±âÌ†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã + Ì†µÌ±å ) = Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) + Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±å ).
                                                       Ì†µÌ±õ       Ì†µÌ±õ
                  ‚Ä¢ If Ì†µÌ±ã ,Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã are independent, Ì†µÌ±âÌ†µÌ±éÌ†µÌ±ü(‚àëÌ†µÌ±ã ) = ‚àëÌ†µÌ±âÌ†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã ).
                        1  2     Ì†µÌ±õ                        Ì†µÌ±ñ           Ì†µÌ±ñ
                                                      Ì†µÌ±ñ=1     Ì†µÌ±ñ=1
               Example 37.2 (Variance of Binomial distribution). Find the variance for Ì†µÌ±ã ‚àº Bin(Ì†µÌ±õ,Ì†µÌ±ù).
               Ì†µÌ±ã = Ì†µÌ±ã +‚ãØ+Ì†µÌ±ã where Ì†µÌ±ã are Ì†µÌ±ñ.Ì†µÌ±ñ.Ì†µÌ±ë Bernoulli distributions
                     1        Ì†µÌ±õ       Ì†µÌ±ñ
                                                  Ì†µÌ±õ
                                               Ì†µÌ±ñÌ†µÌ±ñÌ†µÌ±ë
                                       Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) = ‚àëÌ†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã ) = Ì†µÌ±õÌ†µÌ±ù(1 ‚àí Ì†µÌ±ù).
                                                          Ì†µÌ±ñ
                                                 Ì†µÌ±ñ=1
                                                      124
                       Example 37.3 (Variance of Poisson distribution). Let Ì†µÌ±ã ‚àº Pois(Ì†µÌºÜ). To find the variance, we
                                               2
                       first compute Ì†µÌ∞∏(Ì†µÌ±ã ). By LOTUS,
                                                                      ‚àû                          ‚àû
                                                                                ‚àíÌ†µÌºÜ Ì†µÌ±ò                    Ì†µÌ±ò
                                                                               Ì†µÌ±í   Ì†µÌºÜ                  Ì†µÌºÜ
                                                               2           2                ‚àíÌ†µÌºÜ       2
                                                                     ‚àë                          ‚àë
                                                         Ì†µÌ∞∏(Ì†µÌ±ã ) =        Ì†µÌ±ò ‚ãÖ          =Ì†µÌ±í          Ì†µÌ±ò
                                                                                  Ì†µÌ±ò!                   Ì†µÌ±ò!
                                                                     Ì†µÌ±ò=0                       Ì†µÌ±ò=1
                                                  Ì†µÌ±ò
                                           ‚àû
                                                Ì†µÌºÜ
                                                         Ì†µÌºÜ
                       Differentiate ‚àë              =Ì†µÌ±í on both sides with respect to Ì†µÌºÜ and multiply (replenish) again by
                                                 Ì†µÌ±ò!
                                           Ì†µÌ±ò=0
                       Ì†µÌºÜ:
                                                                           ‚àû
                                                                                   Ì†µÌ±ò
                                                                                 Ì†µÌºÜ
                                                                                           Ì†µÌºÜ
                                                                          ‚àëÌ†µÌ±ò        =Ì†µÌºÜÌ†µÌ±í
                                                                                 Ì†µÌ±ò!
                                                                          Ì†µÌ±ò‚àí1
                       Repeat:
                                                                     ‚àû
                                                                              Ì†µÌ±ò
                                                                            Ì†µÌºÜ
                                                                           2            Ì†µÌºÜ      Ì†µÌºÜ
                                                                    ‚àëÌ†µÌ±ò         =Ì†µÌºÜ(Ì†µÌ±í +Ì†µÌºÜÌ†µÌ±í )
                                                                             Ì†µÌ±ò!
                                                                    Ì†µÌ±ò‚àí1
                       Therefore, we have
                                                                    2       ‚àíÌ†µÌºÜ        2   Ì†µÌºÜ           2
                                                              Ì†µÌ∞∏(Ì†µÌ±ã ) = Ì†µÌ±í     (Ì†µÌºÜ + Ì†µÌºÜ )Ì†µÌ±í  =Ì†µÌºÜ+Ì†µÌºÜ
                       Finally,
                                                                        2               2           2      2
                                                    Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) = Ì†µÌ∞∏(Ì†µÌ±ã ) ‚àí (Ì†µÌ∞∏(Ì†µÌ±ã)) = Ì†µÌºÜ + Ì†µÌºÜ ‚àí Ì†µÌºÜ = Ì†µÌºÜ.
                                                                                 125
                  38 Covariance
                  For more than one random variable, it is also of interest to know the relationship between
                  them. Are they dependent? How strong is the dependence? Covariance and correlation are
                  intended to measure that dependence. But they only capture a particular type of dependence,
                  namely linear dependence.
                  Definition 38.1 (Covariance). The covariance between random variables Ì†µÌ±ã and Ì†µÌ±å is defined
                  as
                                               Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å ) = Ì†µÌ∞∏[(Ì†µÌ±ã ‚àí Ì†µÌ∞∏Ì†µÌ±ã)(Ì†µÌ±å ‚àí Ì†µÌ∞∏Ì†µÌ±å )].
                  The covariance between Ì†µÌ±ã and Ì†µÌ±å reflects how much Ì†µÌ±ã and Ì†µÌ±å simultaneously deviate from
                  their respective means.
                  Theorem 38.1. For any random variables Ì†µÌ±ã and Ì†µÌ±å,
                                                Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã, Ì†µÌ±å ) = Ì†µÌ∞∏(Ì†µÌ±ãÌ†µÌ±å ) ‚àí Ì†µÌ∞∏(Ì†µÌ±ã)Ì†µÌ∞∏(Ì†µÌ±å ).
                  Proof. Let Ì†µÌºá   =Ì†µÌ∞∏(Ì†µÌ±ã) and Ì†µÌºá   =Ì†µÌ∞∏(Ì†µÌ±å). By definition,
                                Ì†µÌ±ã               Ì†µÌ±å
                                        Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å ) = Ì†µÌ∞∏(Ì†µÌ±ãÌ†µÌ±å ‚àí Ì†µÌºá Ì†µÌ±å ‚àí Ì†µÌºá Ì†µÌ±ã + Ì†µÌºá Ì†µÌºá )
                                                                 Ì†µÌ±ã      Ì†µÌ±å     Ì†µÌ±ã  Ì†µÌ±å
                                                   =Ì†µÌ∞∏(Ì†µÌ±ãÌ†µÌ±å)‚àíÌ†µÌºá Ì†µÌ∞∏(Ì†µÌ±å)‚àíÌ†µÌºá Ì†µÌ∞∏(Ì†µÌ±ã)+Ì†µÌºá Ì†µÌºá
                                                                  Ì†µÌ±ã         Ì†µÌ±å          Ì†µÌ±ã Ì†µÌ±å
                                                   =Ì†µÌ∞∏(Ì†µÌ±ãÌ†µÌ±å)‚àíÌ†µÌ∞∏(Ì†µÌ±ã)Ì†µÌ∞∏(Ì†µÌ±å).
                  Theorem 38.2. If Ì†µÌ±ã,Ì†µÌ±å are independent, they are uncorrelated. But the converse is false.
                  Proof.
                     1. Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å ) = Ì†µÌ∞∏(Ì†µÌ±ãÌ†µÌ±å ) ‚àí Ì†µÌ∞∏(Ì†µÌ±ã)Ì†µÌ∞∏(Ì†µÌ±å ). Independence implies Ì†µÌ∞∏(Ì†µÌ±ãÌ†µÌ±å ) = Ì†µÌ∞∏(Ì†µÌ±ã)Ì†µÌ∞∏(Ì†µÌ±å ). Thus,
                        Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å ) = 0.
                     2. Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å ) = 0 does not necessarily imply independence. Consider the following counter
                        example. Let Ì†µÌ±ã be a random variable that takes three values -1, 0, 1 with equal prob-
                                             2
                        ability. And Ì†µÌ±å = Ì†µÌ±ã . Ì†µÌ±ã and Ì†µÌ±å are clearly dependent. But they their covariance is 0.
                                                                       3
                        Since Ì†µÌ∞∏(Ì†µÌ±ã) = 0, Ì†µÌ∞∏(Ì†µÌ±å ) = 2/3, Ì†µÌ∞∏(Ì†µÌ±ãÌ†µÌ±å ) = Ì†µÌ∞∏(Ì†µÌ±ã ) = 0, Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å ) = 0.
                                                                 126
                        ƒπ Linear dependency
                        Covariance and correlation provide measures of the extend to which two random variables
                        are linearly related.      It is possible that the covariance is 0 even when Ì†µÌ±ã and Ì†µÌ±å are
                        dependent but the relationship is nonlinear.
                                     Cov=1.645                         Cov=0.473                        Cov=0.192
                               4                                 5                                2
                               2                                                                  1
                                                                 3
                          Y1   0                            Y2                               Y3
                                                                 1                                ‚àí1
                               ‚àí4                                ‚àí1                               ‚àí3
                                   ‚àí2      0   1   2                  ‚àí1   0   1    2                  ‚àí2     0   1  2
                                          X1                                X2                               X3
                      Proposition 38.1. Covariance has the following properties:
                         ‚Ä¢ Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±ã) = Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã)
                         ‚Ä¢ Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å ) = Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±å ,Ì†µÌ±ã)
                         ‚Ä¢ Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±êÌ†µÌ±ã,Ì†µÌ±å ) = Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±êÌ†µÌ±å ) = Ì†µÌ±ê [Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å )]
                         ‚Ä¢ Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã + Ì†µÌ±å ,Ì†µÌ±ç) = Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±ç) + Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±å ,Ì†µÌ±ç)
                         ‚Ä¢ Ì†µÌ±âÌ†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã + Ì†µÌ±å ) = Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) + Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±å ) + 2Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å )
                      Proof. We only prove the variance-covariance property:
                                                                                     2
                                       Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã + Ì†µÌ±å ) = Ì†µÌ∞∏[(Ì†µÌ±ã + Ì†µÌ±å ‚àí Ì†µÌºá   ‚àíÌ†µÌºá ) ]
                                                                           Ì†µÌ±ã     Ì†µÌ±å
                                                                        2               2
                                                       =Ì†µÌ∞∏[(Ì†µÌ±ã ‚àíÌ†µÌºá ) +(Ì†µÌ±å ‚àíÌ†µÌºá ) +2(Ì†µÌ±ã ‚àíÌ†µÌºá )(Ì†µÌ±å ‚àíÌ†µÌºá )]
                                                                     Ì†µÌ±ã              Ì†µÌ±å                Ì†µÌ±ã         Ì†µÌ±å
                                                       =Ì†µÌ±âÌ†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) + Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±å ) + 2Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å ).
                      Theorem 38.3. For random variables Ì†µÌ±ã ,Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã , it holds that
                                                                        1    2        Ì†µÌ±õ
                                                         Ì†µÌ±õ            Ì†µÌ±õ
                                                Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü (‚àëÌ†µÌ±ã ) = ‚àëÌ†µÌ±âÌ†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã )+2‚àëÌ†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã ,Ì†µÌ±ã ).
                                                              Ì†µÌ±ñ                  Ì†µÌ±ñ                  Ì†µÌ±ñ   Ì†µÌ±ó
                                                                                          Ì†µÌ±ñ<Ì†µÌ±ó
                                                        Ì†µÌ±ñ=1          Ì†µÌ±ñ=1
                      If Ì†µÌ±ã ,Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã      are identically distributed and have the same covariance relationships (sym-
                           1    2       Ì†µÌ±õ
                      metric), then
                                                         Ì†µÌ±õ
                                                                                         Ì†µÌ±õ
                                                        ‚àë )=Ì†µÌ±õÌ†µÌ±âÌ†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã                    )Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã
                                                Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü (   Ì†µÌ±ã                   ) +2(                , Ì†µÌ±ã ).
                                                               Ì†µÌ±ñ               1                    1    2
                                                                                         2
                                                        Ì†µÌ±ñ=1
                                                                            127
                          While Cov(Ì†µÌ±ã,Ì†µÌ±å) quantifies how Ì†µÌ±ã and Ì†µÌ±å vary together, its magnitude also depends on the
                          absolute scales of Ì†µÌ±ã and Ì†µÌ±å (multiply Ì†µÌ±ã by a constant Ì†µÌ±ê, the covariance will be different). To
                          establish a measure of association between Ì†µÌ±ã and Ì†µÌ±å that is unaffected by arbitrary changes
                          in the scales of either variable, we introduce a ‚Äústandardized covariance‚Äù called correlation.
                          Definition 38.2 (Correlation). The correlation between random variables Ì†µÌ±ã and Ì†µÌ±å is defined
                          as
                                                                                                Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å )
                                                                     Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±üÌ†µÌ±ü(Ì†µÌ±ã, Ì†µÌ±å ) =                             .
                                                                                           ‚àö
                                                                                              Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã)Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±å )
                          By convention, we denote correlation by Greek letter Ì†µÌºå ‚â° Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±üÌ†µÌ±ü(Ì†µÌ±ã,Ì†µÌ±å ).
                          Unlike covariance, scaling Ì†µÌ±ã or Ì†µÌ±å has no effect on the correlation. We can verify this:
                                                                     Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±êÌ†µÌ±ã, Ì†µÌ±å )                Ì†µÌ±êÌ†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã, Ì†µÌ±å )
                                         Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±üÌ†µÌ±ü(Ì†µÌ±êÌ†µÌ±ã, Ì†µÌ±å ) =                               =                               =Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±üÌ†µÌ±ü(Ì†µÌ±ã,Ì†µÌ±å ).
                                                                ‚àö                                 ‚àö
                                                                   Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±êÌ†µÌ±ã)Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±å )    Ì†µÌ±ê   Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã)Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±å )
                          Theorem 38.4. For any random variable Ì†µÌ±ã and Ì†µÌ±å,
                                                                             ‚àí1‚â§Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±üÌ†µÌ±ü(Ì†µÌ±ã,Ì†µÌ±å) ‚â§ 1.
                          Proof. Without loss of generality, assume Ì†µÌ±ã,Ì†µÌ±å both have variance 1, since scaling does not
                          change the correlation. Let Ì†µÌºå = Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±üÌ†µÌ±ü(Ì†µÌ±ã,Ì†µÌ±å ) = Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å ). Then
                                                Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã + Ì†µÌ±å ) = Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) + Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±å ) + 2Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã, Ì†µÌ±å ) = 2 + 2Ì†µÌºå ‚â• 0,
                                                Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã ‚àí Ì†µÌ±å ) = Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) + Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±å ) ‚àí 2Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã, Ì†µÌ±å ) = 2 ‚àí 2Ì†µÌºå ‚â• 0.
                          Thus ‚àí1 ‚â§ Ì†µÌºå ‚â§ 1.
                              ‚Ä¢ Ì†µÌ±ã and Ì†µÌ±å are positively correlated if Ì†µÌºå                          >0;
                                                                                              Ì†µÌ±ãÌ†µÌ±å
                              ‚Ä¢ Ì†µÌ±ã and Ì†µÌ±å are negatively correlated if Ì†µÌºå                           <0;
                                                                                               Ì†µÌ±ãÌ†µÌ±å
                              ‚Ä¢ Ì†µÌ±ã and Ì†µÌ±å are uncorrelated if Ì†µÌºå                      =0.
                                                                                 Ì†µÌ±ãÌ†µÌ±å
                          Theorem 38.5. Suppose that Ì†µÌ±ã is a random variable and Ì†µÌ±å = Ì†µÌ±éÌ†µÌ±ã + Ì†µÌ±è for some constants
                          Ì†µÌ±é, Ì†µÌ±è, where Ì†µÌ±é ‚â† 0. If Ì†µÌ±é > 0, then Ì†µÌºå               =1. If Ì†µÌ±é < 0, then Ì†µÌºå             =‚àí1.
                                                                           Ì†µÌ±ãÌ†µÌ±å                                Ì†µÌ±ãÌ†µÌ±å
                          Proof. If Ì†µÌ±å = Ì†µÌ±éÌ†µÌ±ã + Ì†µÌ±è, then Ì†µÌ∞∏(Ì†µÌ±å ) = Ì†µÌ±éÌ†µÌ∞∏(Ì†µÌ±ã) + Ì†µÌ±è. Thus, Ì†µÌ±å ‚àí Ì†µÌ∞∏(Ì†µÌ±å ) = Ì†µÌ±é(Ì†µÌ±ã ‚àí Ì†µÌ∞∏(Ì†µÌ±ã)). Therefore,
                                                                                                       2
                                                               Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å ) = Ì†µÌ±éÌ†µÌ∞∏[(Ì†µÌ±ã ‚àí Ì†µÌ∞∏Ì†µÌ±ã) ] = Ì†µÌ±éÌ†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã).
                                                                              Ì†µÌ±é
                                                    2
                          Since Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±å ) = Ì†µÌ±é Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã), Ì†µÌºå         = . The theorem thus follows.
                                                                    Ì†µÌ±ãÌ†µÌ±å
                                                                              |Ì†µÌ±é|
                                                                                           128
                                                              ƒπ Correlation analysis
                                                             Acorrelation matrix shows the pairwise correlation coeÔ¨Äicients between variables. It‚Äôs
                                                              one of the most common tools for exploring relationships in multivariate data.
                                                              # variables for analysis
                                                              vars <- mtcars[, 1:4]
                                                              # compute the correlation matrix
                                                              print(cor(vars))
                                                                                           mpg                       cyl                    disp                             hp
                                                              mpg                   1.000 -0.852 -0.848 -0.776
                                                              cyl -0.852 1.000 0.902 0.832
                                                              disp -0.848                                     0.902 1.000 0.791
                                                              hp                -0.776 0.832 0.791 1.000
                                                       Example 38.1. Let Ì†µÌ±ã ‚àº HGeom(Ì†µÌ±§,Ì†µÌ±è,Ì†µÌ±õ). Find Ì†µÌ±âÌ†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã).
                                                       Solution. Interpret Ì†µÌ±ã as the number of white balls in a sample of size Ì†µÌ±õ from an box with Ì†µÌ±§
                                                      white and Ì†µÌ±è black balls. We can represent Ì†µÌ±ã as the sum of indicator variables, Ì†µÌ±ã = Ì†µÌ∞º +‚ãØ+Ì†µÌ∞º
                                                                                                                                                                                                                                                                                                                          1                         Ì†µÌ±õ
                                                       , where Ì†µÌ∞º is the indicator of the Ì†µÌ±ó-th ball in the sample being white. Each Ì†µÌ∞º has mean
                                                                                     Ì†µÌ±ó                                                                                                                                                                                                                            Ì†µÌ±ó
                                                       Ì†µÌ±ù = Ì†µÌ±§/(Ì†µÌ±§+Ì†µÌ±è) and variance Ì†µÌ±ù(1‚àíÌ†µÌ±ù), but because the Ì†µÌ∞º are dependent, we cannot simply add
                                                                                                                                                                                                                                Ì†µÌ±ó
                                                       their variances. Instead,
                                                                                                                                                                               Ì†µÌ±õ
                                                                                                                     Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) = Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü (‚àëÌ†µÌ∞º )
                                                                                                                                                                                          Ì†µÌ±ó
                                                                                                                                                                            Ì†µÌ±ó=1
                                                                                                                                                =Ì†µÌ±âÌ†µÌ±éÌ†µÌ±ü(Ì†µÌ∞º ) + ‚ãØ + Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ∞º ) + 2‚àëÌ†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ∞º ,Ì†µÌ∞º )
                                                                                                                                                                          1                                              Ì†µÌ±õ                                                Ì†µÌ±ñ      Ì†µÌ±ó
                                                                                                                                                                                                                                              Ì†µÌ±ñ<Ì†µÌ±ó
                                                                                                                                                                                                      Ì†µÌ±õ
                                                                                                                                                =Ì†µÌ±õÌ†µÌ±ù(1 ‚àíÌ†µÌ±ù) + 2( )Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ∞º ,Ì†µÌ∞º )
                                                                                                                                                                                                                                  Ì†µÌ±ñ      Ì†µÌ±ó
                                                                                                                                                                                                      2
                                                       In the last step, because of symmetry, for every pair Ì†µÌ±ñ and Ì†µÌ±ó, Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ∞º ,Ì†µÌ∞º ) are the same.
                                                                                                                                                                                                                                                                       Ì†µÌ±ñ       Ì†µÌ±ó
                                                                                                   Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ∞º , Ì†µÌ∞º ) = Ì†µÌ∞∏(Ì†µÌ∞º Ì†µÌ∞º ) ‚àí Ì†µÌ∞∏(Ì†µÌ∞º )Ì†µÌ∞∏(Ì†µÌ∞º )
                                                                                                                      Ì†µÌ±ñ      Ì†µÌ±ó                         Ì†µÌ±ñ   Ì†µÌ±ó                        Ì†µÌ±ñ              Ì†µÌ±ó
                                                                                                                                      =Ì†µÌ±É(Ì†µÌ±ñ and Ì†µÌ±ó both white) ‚àí Ì†µÌ±É(Ì†µÌ±ñ is white)Ì†µÌ±É(Ì†µÌ±ó is white)
                                                                                                                                                    Ì†µÌ±§                     Ì†µÌ±§ ‚àí 1
                                                                                                                                                                                                                2
                                                                                                                                      =                          ‚ãÖ                                   ‚àíÌ†µÌ±ù
                                                                                                                                              Ì†µÌ±§ + Ì†µÌ±è                 Ì†µÌ±§ + Ì†µÌ±è ‚àí 1
                                                                                                                                                  Ì†µÌ±ÅÌ†µÌ±ù ‚àí 1
                                                                                                                                                                                     2
                                                                                                                                      =Ì†µÌ±ù                                 ‚àíÌ†µÌ±ù
                                                                                                                                                   Ì†µÌ±Å ‚àí1
                                                                                                                                              Ì†µÌ±ù(Ì†µÌ±ù ‚àí 1)
                                                                                                                                      =
                                                                                                                                                 Ì†µÌ±Å ‚àí1
                                                                                                                                                                                                  129
                      where Ì†µÌ±Å = Ì†µÌ±§+Ì†µÌ±è. Plugging this into the above formula and simplifying, we eventually obtain
                                                                                  Ì†µÌ±ù(Ì†µÌ±ù ‚àí 1)     Ì†µÌ±Å ‚àíÌ†µÌ±õ
                                           Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) = Ì†µÌ±õÌ†µÌ±ù(1 ‚àí Ì†µÌ±ù) + Ì†µÌ±õ(Ì†µÌ±õ ‚àí 1)           =           Ì†µÌ±õÌ†µÌ±ù(1 ‚àí Ì†µÌ±ù).
                                                                                    Ì†µÌ±Å ‚àí1        Ì†µÌ±Å ‚àí1
                                                                                                             Ì†µÌ±Å‚àíÌ†µÌ±õ
                      This differs from the Binomial variance of Ì†µÌ±õÌ†µÌ±ù(1 ‚àí Ì†µÌ±ù) by a factor of                      .  This discrepancy
                                                                                                             Ì†µÌ±Å‚àí1
                      arises because the Hypergeometric story involves sampling without replacement. As Ì†µÌ±Å ‚Üí ‚àû,
                      it becomes extremely unlikely that we would draw the same ball more than once, so sampling
                      with or without replacement essentially become the same.
                                                                              130
                          39 Portfolio allocation*
                          Intheworldoffinance, oneofthemostwell-establishedprinciplesistheideaof diversification.
                          By combining assets with varying levels of risk and return, investors can reduce the overall
                          risk of their portfolio.
                          Consider a portfolio of two assets, Asset A and Asset B. Both assets have the same expected
                          return and individual risks (standard deviations), and they are weighted equally in the port-
                          folio.
                                                                  Asset       Return Ì†µÌºá        Risk Ì†µÌºé      Weight Ì†µÌ±§
                                                                  A           10%              15%          50%
                                                                  B           10%              15%          50%
                          The expected return of the portfolio is:
                                                                          Ì†µÌºá   =Ì†µÌ±§ Ì†µÌºá +Ì†µÌ±§ Ì†µÌºá =10%
                                                                            Ì†µÌ±É       Ì†µÌ∞¥  Ì†µÌ∞¥      Ì†µÌ∞µ   Ì†µÌ∞µ
                                                                                                                              Ì†µÌ∞ª
                          Let‚Äôs consider the portfolio risk. First, assuming high correlation, Ì†µÌºå                                   =0.8. The portfolio
                                                                                                                              Ì†µÌ∞¥Ì†µÌ∞µ
                          risk is:
                                                          Ì†µÌ∞ª
                                                                       2   2        2   2                         Ì†µÌ∞ª
                                                                 ‚àö
                                                        Ì†µÌºé    = Ì†µÌ±§ Ì†µÌºé +Ì†µÌ±§ Ì†µÌºé +2Ì†µÌ±§ Ì†µÌ±§ Ì†µÌºé Ì†µÌºé Ì†µÌºå                           ‚âà14.2%
                                                                                                  Ì†µÌ∞¥  Ì†µÌ∞µ   Ì†µÌ∞¥  Ì†µÌ∞µ
                                                          Ì†µÌ±É
                                                                                    Ì†µÌ∞µ  Ì†µÌ∞µ
                                                                       Ì†µÌ∞¥  Ì†µÌ∞¥                                     Ì†µÌ∞¥Ì†µÌ∞µ
                                                                     Ì†µÌ∞ø
                          If assuming low correlation, Ì†µÌºå                 =0.2. The portfolio risk is:
                                                                     Ì†µÌ∞¥Ì†µÌ∞µ
                                                          Ì†µÌ∞ø
                                                                       2   2       2   2                          Ì†µÌ∞ø
                                                                 ‚àö
                                                        Ì†µÌºé   = Ì†µÌ±§ Ì†µÌºé +Ì†µÌ±§ Ì†µÌºé +2Ì†µÌ±§ Ì†µÌ±§ Ì†µÌºé Ì†µÌºé Ì†µÌºå                           ‚âà11.6%
                                                                                                  Ì†µÌ∞¥  Ì†µÌ∞µ  Ì†µÌ∞¥  Ì†µÌ∞µ
                                                          Ì†µÌ±É
                                                                                   Ì†µÌ∞µ  Ì†µÌ∞µ
                                                                       Ì†µÌ∞¥  Ì†µÌ∞¥                                     Ì†µÌ∞¥Ì†µÌ∞µ
                          As we see, by reducing the correlation between the two assets, we reduced the portfolio risk,
                          though the expected return remains the same. Therefore, diversification is often referred to as
                          a ‚Äúfree lunch‚Äù in finance because it allows investors to reduce portfolio risk without sacrificing
                          expected returns.
                                                                                           131
                            132
                                        40 Conditional expectation
                                        Wehaveintroduced conditional expectation in Definition 26.2. Here we reiterate the definition
                                        with continuous random variables.
                                        Definition 40.1 (Conditional expectation). Let Ì†µÌ±ã and Ì†µÌ±å be continuous random variables
                                                                                                                                                                                                                                       Ì†µÌ±ì       (Ì†µÌ±•,Ì†µÌ±¶)
                                                                                                                                                                                                                                         Ì†µÌ±ã,Ì†µÌ±å
                                        with joint density Ì†µÌ±ì                              (Ì†µÌ±•, Ì†µÌ±¶), Ì†µÌ±ã‚Äôs density Ì†µÌ±ì                       (Ì†µÌ±•), and conditional density Ì†µÌ±ì                                         (Ì†µÌ±¶|Ì†µÌ±•) =                            .
                                                                                   Ì†µÌ±ã,Ì†µÌ±å                                               Ì†µÌ±ã                                                                    Ì†µÌ±å |Ì†µÌ±ã
                                                                                                                                                                                                                                          Ì†µÌ±ì    (Ì†µÌ±•)
                                                                                                                                                                                                                                            Ì†µÌ±ã
                                        The conditional expectation of Ì†µÌ±å given Ì†µÌ±ã = Ì†µÌ±• is
                                                                                                                                                 ‚àû
                                                                                                         Ì†µÌ∞∏(Ì†µÌ±å |Ì†µÌ±ã = Ì†µÌ±•) = ‚à´                          Ì†µÌ±¶ Ì†µÌ±ì          (Ì†µÌ±¶|Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±¶
                                                                                                                                                             Ì†µÌ±å |Ì†µÌ±ã
                                                                                                                                              ‚àí‚àû
                                                                                                                                                 ‚àû
                                                                                                                                                           Ì†µÌ±ì         (Ì†µÌ±•, Ì†µÌ±¶)
                                                                                                                                                              Ì†µÌ±ã,Ì†µÌ±å
                                                                                                                                       =‚à´ Ì†µÌ±¶                                      Ì†µÌ±ëÌ†µÌ±¶
                                                                                                                                                                Ì†µÌ±ì    (Ì†µÌ±•)
                                                                                                                                                                  Ì†µÌ±ã
                                                                                                                                              ‚àí‚àû
                                        When the denominator is zero, the expression is undefined.
                                        Note that conditioning on a continuous random variable is not the same as conditioning on
                                        the event {Ì†µÌ±ã = Ì†µÌ±•} as it was in the discrete case. The probability of the event is zero, but we
                                        define the conditional expectation in terms of the density function.
                                        Theorem 40.1 (Law of iterated expectation). For any random variable Ì†µÌ±ã and Ì†µÌ±å, it holds
                                       that
                                                                                                                         Ì†µÌ∞∏(Ì†µÌ∞∏(Ì†µÌ±å |Ì†µÌ±ã)) = Ì†µÌ∞∏(Ì†µÌ±å ).
                                        Proof. Note that Ì†µÌ∞∏(Ì†µÌ±å|Ì†µÌ±ã) = Ì†µÌ±î(Ì†µÌ±ã) is a function of Ì†µÌ±ã. Apply LOTUS:
                                                                                                   Ì†µÌ∞∏(Ì†µÌ∞∏(Ì†µÌ±å |Ì†µÌ±ã)) = ‚à´Ì†µÌ±î(Ì†µÌ±•)Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•
                                                                                                                               =‚à´(‚à´Ì†µÌ±¶Ì†µÌ±ì(Ì†µÌ±¶|Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±¶)Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•
                                                                                                                               =‚à´‚à´Ì†µÌ±¶Ì†µÌ±ì(Ì†µÌ±¶|Ì†µÌ±•)Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±¶Ì†µÌ±ëÌ†µÌ±•
                                                                                                                               =‚à´Ì†µÌ±¶‚à´Ì†µÌ±ì(Ì†µÌ±¶,Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•Ì†µÌ±ëÌ†µÌ±¶
                                                                                                                                         ‚àû
                                                                                                                               =‚à´ Ì†µÌ±¶Ì†µÌ±ì(Ì†µÌ±¶)Ì†µÌ±ëÌ†µÌ±¶
                                                                                                                                      ‚àí‚àû
                                                                                                                               =Ì†µÌ∞∏(Ì†µÌ±å).
                                                                                                                                             133
                  Theorem 40.2. For any random variable Ì†µÌ±ã and Ì†µÌ±å, and any function Ì†µÌ±î, we have
                                                  Ì†µÌ∞∏(Ì†µÌ±î(Ì†µÌ±ã)Ì†µÌ±å |Ì†µÌ±ã) = Ì†µÌ±î(Ì†µÌ±ã)Ì†µÌ∞∏(Ì†µÌ±å |Ì†µÌ±ã).
                  Proof. For any specific value of Ì†µÌ±ã = Ì†µÌ±•, Ì†µÌ±î(Ì†µÌ±•) is a constant.   Thus, Ì†µÌ∞∏(Ì†µÌ±î(Ì†µÌ±•)Ì†µÌ±å |Ì†µÌ±ã = Ì†µÌ±•) =
                  Ì†µÌ±î(Ì†µÌ±•)Ì†µÌ∞∏(Ì†µÌ±å |Ì†µÌ±ã = Ì†µÌ±•). This is true for all values of Ì†µÌ±•.
                  Theorem 40.3 (Best predictor). Conditional expectation Ì†µÌ∞∏(Ì†µÌ±å|Ì†µÌ±ã) is the best predictor for Ì†µÌ±å
                  using Ì†µÌ±ã (minimized the square loss function).
                  Proof. Let Ì†µÌ±î(Ì†µÌ±ã) be a predictor for Ì†µÌ±å using Ì†µÌ±ã. We want to find the Ì†µÌ±î such that minimizes
                               2
                  Ì†µÌ∞∏(Ì†µÌ±å ‚àí Ì†µÌ±î(Ì†µÌ±ã)) .
                                          2                                      2
                             Ì†µÌ∞∏(Ì†µÌ±å ‚àí Ì†µÌ±î(Ì†µÌ±ã)) = Ì†µÌ∞∏(Ì†µÌ±å ‚àí Ì†µÌ∞∏(Ì†µÌ±å |Ì†µÌ±ã) + Ì†µÌ∞∏(Ì†µÌ±å |Ì†µÌ±ã) ‚àí Ì†µÌ±î(Ì†µÌ±ã))
                                                              2
                                           =Ì†µÌ∞∏(Ì†µÌ±å ‚àíÌ†µÌ∞∏(Ì†µÌ±å|Ì†µÌ±ã)) +2Ì†µÌ∞∏(Ì†µÌ±å ‚àíÌ†µÌ∞∏(Ì†µÌ±å|Ì†µÌ±ã)((Ì†µÌ∞∏(Ì†µÌ±å|Ì†µÌ±ã)‚àíÌ†µÌ±î(Ì†µÌ±ã))
                                                                    ‚èü‚èü‚èü‚èü‚èü‚èü‚èü
                                                                    Ì†µÌ∞∏(Ì†µÌ±å )=Ì†µÌ∞∏(Ì†µÌ∞∏(Ì†µÌ±å |Ì†µÌ±ã))
                                                                   2
                                              +Ì†µÌ∞∏(Ì†µÌ∞∏(Ì†µÌ±å|Ì†µÌ±ã) ‚àíÌ†µÌ±î(Ì†µÌ±ã))
                                                              2                      2
                                           =Ì†µÌ∞∏(Ì†µÌ±å ‚àíÌ†µÌ∞∏(Ì†µÌ±å|Ì†µÌ±ã)) +Ì†µÌ∞∏(Ì†µÌ∞∏(Ì†µÌ±å|Ì†µÌ±ã)‚àíÌ†µÌ±î(Ì†µÌ±ã))
                                                              2
                                           ‚â•Ì†µÌ∞∏(Ì†µÌ±å ‚àíÌ†µÌ∞∏(Ì†µÌ±å|Ì†µÌ±ã)) .
                                          2
                  Therefore, Ì†µÌ∞∏(Ì†µÌ±å ‚àí Ì†µÌ±î(Ì†µÌ±ã)) is minimized when Ì†µÌ±î(Ì†µÌ±ã) = Ì†µÌ∞∏(Ì†µÌ±å|Ì†µÌ±ã).
                  Definition 40.2 (Linear conditional expectation model). An extremely widely used method
                  for data analysis in statistics is linear regression. In its most basic form, we want to predict
                  the mean of Ì†µÌ±å using a single explanatory variable Ì†µÌ±ã. A linear conditional expectation model
                  assumes that Ì†µÌ∞∏(Ì†µÌ±å|Ì†µÌ±ã) is linear in Ì†µÌ±ã:
                                                        Ì†µÌ∞∏(Ì†µÌ±å |Ì†µÌ±ã) = Ì†µÌ±é + Ì†µÌ±èÌ†µÌ±ã,
                  or equivalently,
                                                         Ì†µÌ±å = Ì†µÌ±é + Ì†µÌ±èÌ†µÌ±ã + Ì†µÌºñ,
                  with Ì†µÌ∞∏(Ì†µÌºñ|Ì†µÌ±ã) = 0. The intercept and the slope is given by
                                                   Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å )
                                               Ì†µÌ±è =           , Ì†µÌ±é = Ì†µÌ∞∏(Ì†µÌ±å ) ‚àí Ì†µÌ±èÌ†µÌ∞∏(Ì†µÌ±ã).
                                                     Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã)
                                                               134
                             Wefirst show the equivalence of the two expressions of the model. Let Ì†µÌ±å = Ì†µÌ±é + Ì†µÌ±èÌ†µÌ±ã + Ì†µÌºñ, with
                             Ì†µÌ∞∏(Ì†µÌºñ|Ì†µÌ±ã) = 0. Then by linearity,
                                                               Ì†µÌ∞∏(Ì†µÌ±å |Ì†µÌ±ã) = Ì†µÌ∞∏(Ì†µÌ±é|Ì†µÌ±ã) + Ì†µÌ∞∏(Ì†µÌ±èÌ†µÌ±ã|Ì†µÌ±ã) + Ì†µÌ∞∏(Ì†µÌºñ|Ì†µÌ±ã) = Ì†µÌ±é + Ì†µÌ±èÌ†µÌ±ã.
                             Conversely, suppose that Ì†µÌ∞∏(Ì†µÌ±å|Ì†µÌ±ã) = Ì†µÌ±é + Ì†µÌ±èÌ†µÌ±ã, and define
                                                                                         Ì†µÌºñ = Ì†µÌ±å ‚àí (Ì†µÌ±é + Ì†µÌ±èÌ†µÌ±ã).
                             Then Ì†µÌ±å = Ì†µÌ±é +Ì†µÌ±èÌ†µÌ±ã + Ì†µÌºñ, with
                                                      Ì†µÌ∞∏(Ì†µÌºñ|Ì†µÌ±ã) = Ì†µÌ∞∏(Ì†µÌ±å |Ì†µÌ±ã) ‚àí Ì†µÌ∞∏(Ì†µÌ±é + Ì†µÌ±èÌ†µÌ±ã|Ì†µÌ±ã) = Ì†µÌ∞∏(Ì†µÌ±å |Ì†µÌ±ã) ‚àí (Ì†µÌ±é + Ì†µÌ±èÌ†µÌ±ã) = 0.
                             To derive the expression for Ì†µÌ±é and Ì†µÌ±è, take covariance between Ì†µÌ±ã and Ì†µÌ±å,
                                                               Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã, Ì†µÌ±å ) = Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±é + Ì†µÌ±èÌ†µÌ±ã + Ì†µÌºñ)
                                                                                 =Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±é) + Ì†µÌ±èÌ†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±ã) + Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌºñ)
                                                                                 =Ì†µÌ±èÌ†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) + Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌºñ)
                             Note that Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌºñ) = 0 because
                                                                    Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã, Ì†µÌºñ) = Ì†µÌ∞∏(Ì†µÌ±ãÌ†µÌºñ) ‚àí Ì†µÌ∞∏(Ì†µÌ±ã)Ì†µÌ∞∏(Ì†µÌºñ)
                                                                                     =Ì†µÌ∞∏(Ì†µÌ∞∏(Ì†µÌ±ãÌ†µÌºñ|Ì†µÌ±ã)) ‚àí Ì†µÌ∞∏(Ì†µÌ±ã)Ì†µÌ∞∏(Ì†µÌ∞∏(Ì†µÌºñ|Ì†µÌ±ã))
                                                                                     =Ì†µÌ∞∏(Ì†µÌ±ãÌ†µÌ∞∏(Ì†µÌºñ|Ì†µÌ±ã)) ‚àí Ì†µÌ∞∏(Ì†µÌ±ã)Ì†µÌ∞∏(Ì†µÌ∞∏(Ì†µÌºñ|Ì†µÌ±ã))
                                                                                     =0
                             Therefore,
                                                                                      Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å ) = Ì†µÌ±èÌ†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã)
                             Thus,
                                                                         Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å )
                                                                  Ì†µÌ±è =                    ,
                                                                           Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã)
                                                                                                                   Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å )
                                                                 Ì†µÌ±é = Ì†µÌ∞∏(Ì†µÌ±å ) ‚àí Ì†µÌ±èÌ†µÌ∞∏(Ì†µÌ±ã) = Ì†µÌ∞∏(Ì†µÌ±å ) ‚àí                                Ì†µÌ∞∏(Ì†µÌ±ã).
                                                                                                                     Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã)
                             In practice, we don‚Äôt know the true value of Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å) or Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã). We have to estimate it
                                                                                                                  Ì†µÌ±õ
                                                                                                               ‚àë (Ì†µÌ±• ‚àíÌ†µÌ±•)(Ì†µÌ±¶ÃÑ     ‚àíÌ†µÌ±¶)ÃÑ
                                                                                                                         Ì†µÌ±ñ      Ì†µÌ±ñ
                                                                                                                  Ì†µÌ±ñ=1
                                                                                                          ÃÇ
                             with sample observations. Thus, we compute Ì†µÌ±è =                                                           .  By definition, Ì†µÌ±è gives the
                                                                                                                      Ì†µÌ±õ
                                                                                                                                  2
                                                                                                                   ‚àë (Ì†µÌ±• ‚àíÌ†µÌ±•)ÃÑ
                                                                                                                            Ì†µÌ±ñ
                                                                                                                      Ì†µÌ±ñ=1
                             marginal change of Ì†µÌ∞∏(Ì†µÌ±å|Ì†µÌ±ã) with respect to Ì†µÌ±ã.
                             # load data
                             exam <- read.csv("../dataset/exam.csv")
                             # midterm score
                             x <- exam$mid
                                                                                                     135
        # final score
        y <- exam$final
        # regress y on x, compute coefficients
        b <- cov(x,y)/var(x)
        a <- mean(y) - b*mean(x)
        # plot the data and the regression line
        plot(x,y)
        abline(a,b,col="red")
            90
         y  70
            50
            30
              40  50  60   70  80   90  100
                           x
        Linear regression is the simple yet powerful modeling tool in statistics. It is useful whenever
        we want to predict one variable with another. When the assumptions are met (though this is
        rare), the model gives the best predictor (conditional expectation). If the assumptions are not
        met, regression gives a linear approximation.
                            136
                 41 Moments and MGF
                                                                                                        2
                 Definition 41.1 (Moment). Let Ì†µÌ±ã be a random variable with mean Ì†µÌºá and variance Ì†µÌºé .
                                                                          Ì†µÌ±õ
                 For any positive integer Ì†µÌ±õ, the Ì†µÌ±õ-th moment of Ì†µÌ±ã is Ì†µÌ∞∏(Ì†µÌ±ã ), the Ì†µÌ±õ-th central moment is
                                                                             Ì†µÌ±õ
                                                                        Ì†µÌ±ã‚àíÌ†µÌºá
                          Ì†µÌ±õ
                 Ì†µÌ∞∏(Ì†µÌ±ã ‚àí Ì†µÌºá) , and the Ì†µÌ±õ-th standardized moment is Ì†µÌ∞∏(     ) .
                                                                         Ì†µÌºé
                 In accordance with this terminology, Ì†µÌ∞∏(Ì†µÌ±ã) is the first moment of Ì†µÌ±ã, Ì†µÌ±âÌ†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) is the second
                 central moment of Ì†µÌ±ã. It is natural to ask if there are higher order moments. The answer is
                 yes.
                 Definition 41.2 (Skewness). Let Ì†µÌ±ã be a random variable with mean Ì†µÌºá, standard deviation
                 Ì†µÌºé, and finite third moment. The skewness of Ì†µÌ±ã is defined as
                                                                        3
                                                                Ì†µÌ±ã ‚àíÌ†µÌºá
                                                Skew(Ì†µÌ±ã) = Ì†µÌ∞∏[(       ) ].
                                                                   Ì†µÌºé
                 Definition 41.3 (Kurtosis). The Kurtosis of Ì†µÌ±ã is defined as
                                                                       4
                                                               Ì†µÌ±ã ‚àíÌ†µÌºá
                                                                     ) ].
                                                 Kurt(Ì†µÌ±ã) = [(
                                                                 Ì†µÌºé
                 Skewness is the measure of the lopsidedness of the distribution; any symmetric distribution
                 will have a third central moment, if defined, of zero. A distribution that is skewed to the left
                 (the tail of the distribution is longer on the left) will have a negative skewness. A distribution
                 that is skewed to the right (the tail of the distribution is longer on the right), will have a
                 positive skewness.
                 Kurtosis is a measure of the heaviness of the tail of the distribution. If a distribution has
                 heavy tails, the kurtosis will be high; conversely, light-tailed distributions have low kurtosis.
                                                            137
                                           We see that moments give information about the shape of a distribution. Different orders
                                           of moments captures different aspects of the distribution. As higher and higher moments
                                           are calculated, they reveal more and more aspects of the distribution. Loosely speaking, it is
                                           somewhatliketheTaylortheoremintheprobabilitytheory. Wecanapproximateadistribution
                                                                                                                                                        2                  3
                                           by ‚Äúexpectation of polynomials‚Äù: Ì†µÌ∞∏(Ì†µÌ±ã),Ì†µÌ∞∏(Ì†µÌ±ã ),Ì†µÌ∞∏(Ì†µÌ±ã ),‚Ä¶
                                           Definition 41.4 (Moment generating function). Let Ì†µÌ±ã be a random variable. For each real
                                           number Ì†µÌ±°, define the moment generating function (MGF) as
                                                                                                                                                                       Ì†µÌ±°Ì†µÌ±ã
                                                                                                                                      Ì†µÌ±Ä (Ì†µÌ±°) = Ì†µÌ∞∏ (Ì†µÌ±í                       ).
                                                                                                                                           Ì†µÌ±ã
                                           Toseewhyitis‚Äúgenerating‚Äùmoments,taketheTaylorexpansionoftheexponentialfunction:
                                                                                                                                                            2      2            3      3
                                                                                                                                                          Ì†µÌ±°   Ì†µÌ±ã             Ì†µÌ±°  Ì†µÌ±ã
                                                                                                                     Ì†µÌ±°Ì†µÌ±ã
                                                                                                                   Ì†µÌ±í       =1+Ì†µÌ±°Ì†µÌ±ã+                                    +                   +‚ãØ
                                                                                                                                                              2!                  3!
                                           Hence,
                                                                                                                                                                                                       2
                                                                                                                                                                                                     Ì†µÌ±°
                                                                                                                                     Ì†µÌ±°Ì†µÌ±ã                                                      2
                                                                                                   Ì†µÌ±Ä (Ì†µÌ±°) = Ì†µÌ∞∏ (Ì†µÌ±í                       ) = 1+Ì†µÌ∞∏(Ì†µÌ±ã)Ì†µÌ±°+Ì†µÌ∞∏(Ì†µÌ±ã )                                           +‚ãØ
                                                                                                         Ì†µÌ±ã
                                                                                                                                                                                                     2!
                                                                                                                                                        138
                    Anatural question at this point is: What is the interpretation of Ì†µÌ±°? The answer is that Ì†µÌ±° has
                    no interpretation in particular; it‚Äôs just a bookkeeping device that we introduce in order to
                    encode the sequence of moments in a differentiable function.
                    Theorem 41.1. Let Ì†µÌ±Ä (Ì†µÌ±°) be the MGF of Ì†µÌ±ã. Then the Ì†µÌ±õ-th moment of Ì†µÌ±ã is given by
                                               Ì†µÌ±ã
                                  (Ì†µÌ±õ)             (Ì†µÌ±õ)
                         Ì†µÌ±õ
                    Ì†µÌ∞∏(Ì†µÌ±ã ) = Ì†µÌ±Ä    (0), where Ì†µÌ±Ä      denotes the Ì†µÌ±õ-th derivative of the MGF.
                                 Ì†µÌ±ã                Ì†µÌ±ã
                    Theorem 41.2. If the MGFs of two random variables Ì†µÌ±ã and Ì†µÌ±ã are finite and identical for
                                                                                   1        2
                    all values of Ì†µÌ±° in an open interval around the point Ì†µÌ±° = 0, then the probability distributions of
                    Ì†µÌ±ã and Ì†µÌ±ã must be identical.
                      1        2
                    Theorem 41.3. If Ì†µÌ±ã and Ì†µÌ±å are independent, then the MGF of Ì†µÌ±ã +Ì†µÌ±å is the product of the
                    individual MGFs:
                                                         Ì†µÌ±Ä      (Ì†µÌ±°) = Ì†µÌ±Ä (Ì†µÌ±°)Ì†µÌ±Ä (Ì†µÌ±°).
                                                            Ì†µÌ±ã+Ì†µÌ±å        Ì†µÌ±ã     Ì†µÌ±å
                                                            Ì†µÌ±°Ì†µÌ±ã                      Ì†µÌ±°
                    Example 41.1. For Ì†µÌ±ã ‚àº Bern(Ì†µÌ±ù), Ì†µÌ±í         takes on the value Ì†µÌ±í with probability Ì†µÌ±ù and the value
                                                             Ì†µÌ±°Ì†µÌ±ã      Ì†µÌ±°
                    1 with probability Ì†µÌ±û, so Ì†µÌ±Ä(Ì†µÌ±°) = Ì†µÌ∞∏ (Ì†µÌ±í  ) = Ì†µÌ±ùÌ†µÌ±í + Ì†µÌ±û. Since this is finite for all values of Ì†µÌ±°, the
                    MGFisdefined on the entire real line.
                                                                                                    Ì†µÌ±°    Ì†µÌ±õ
                    Example 41.2. The MGF of a Bin(Ì†µÌ±õ,Ì†µÌ±ù) random variable is Ì†µÌ±Ä(Ì†µÌ±°) = (Ì†µÌ±ùÌ†µÌ±í +Ì†µÌ±û) , since it is the
                    product of Ì†µÌ±õ independent Bernoulli MGFs.
                                                                      139
                                                       42 Inequalities*
                                                      This section introduces some of the most popular inequality in statistics and general math-
                                                       ematics. Interestingly, our probability theories can shed light on these inequalities that are
                                                       otherwise hard to explain. We don‚Äôt show formal proofs here, but just point out how these
                                                       inequalities can be useful in statistics.
                                                       Theorem 42.1 (Cauchy-Schwarz inequality).
                                                                                                                                                                                                                       2                        2
                                                                                                                                                                                                ‚àö                         ‚àö
                                                                                                                                                           ‚à£‚àëÌ†µÌ±• Ì†µÌ±¶ ‚à£ ‚â§                                 ‚àëÌ†µÌ±• ‚àëÌ†µÌ±¶
                                                                                                                                                                            Ì†µÌ±ñ    Ì†µÌ±ñ
                                                                                                                                                                                                                       Ì†µÌ±ñ                       Ì†µÌ±ñ
                                                       Proof. If Ì†µÌ±ã,Ì†µÌ±å have zero means, their correlation can be written as
                                                                                                                                                                                                       Ì†µÌ∞∏(Ì†µÌ±ãÌ†µÌ±å )
                                                                                                                                                                   Ì†µÌºå             =
                                                                                                                                                                       Ì†µÌ±ãÌ†µÌ±å
                                                                                                                                                                                                                2                   2
                                                                                                                                                                                           ‚àö
                                                                                                                                                                                                  Ì†µÌ∞∏(Ì†µÌ±ã )Ì†µÌ∞∏(Ì†µÌ±å )
                                                       Since |Ì†µÌºå                        | ‚â§ 1, we always have
                                                                               Ì†µÌ±ãÌ†µÌ±å
                                                                                                                                                                                                                      2                   2
                                                                                                                                                                                                ‚àö
                                                                                                                                                            |Ì†µÌ∞∏(Ì†µÌ±ãÌ†µÌ±å )| ‚â§                               Ì†µÌ∞∏(Ì†µÌ±ã )Ì†µÌ∞∏(Ì†µÌ±å ).
                                                       Consider {Ì†µÌ±• } and {Ì†µÌ±¶ } as realizations of Ì†µÌ±ã and Ì†µÌ±å with equal probabilities, such that Ì†µÌ∞∏(Ì†µÌ±ã) =
                                                                                            Ì†µÌ±ñ                          Ì†µÌ±ñ
                                                        1
                                                             ‚àëÌ†µÌ±• . The original inequality is thus proved.
                                                                          Ì†µÌ±ñ
                                                       Ì†µÌ±õ
                                                       Theorem 42.2 (Jensen‚Äôs inequality). For a convex function Ì†µÌ±ì, we have
                                                                                                                                                             1                                                     1
                                                                                                                                                                   ‚àëÌ†µÌ±ì(Ì†µÌ±•                                                ‚àëÌ†µÌ±•);
                                                                                                                                                                                           ) ‚â• Ì†µÌ±ì (
                                                                                                                                                                                         Ì†µÌ±ñ                                             Ì†µÌ±ñ
                                                                                                                                                             Ì†µÌ±õ                                                   Ì†µÌ±õ
                                                      If Ì†µÌ±ì is concave, then
                                                                                                                                                             1                                                     1
                                                                                                                                                                   ‚àëÌ†µÌ±ì(Ì†µÌ±• ) ‚â§ Ì†µÌ±ì (                                       ‚àëÌ†µÌ±•).
                                                                                                                                                                                         Ì†µÌ±ñ                                             Ì†µÌ±ñ
                                                                                                                                                             Ì†µÌ±õ                                                   Ì†µÌ±õ
                                                      We do not intend to prove it, but offer a special case in statistics that helps to understand
                                                      Jensen‚Äôs inequality. Consider
                                                                                                                                                                                                    2                                     2
                                                                                                                                                    Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) = Ì†µÌ∞∏(Ì†µÌ±ã ) ‚àí (Ì†µÌ∞∏(Ì†µÌ±ã)) ‚â• 0
                                                                                                                                                                                                  140
                    Wehave
                                                                  2            2
                                                             Ì†µÌ∞∏(Ì†µÌ±ã ) ‚â• (Ì†µÌ∞∏(Ì†µÌ±ã)) .
                                                                                       1
                                            2
                    Note that Ì†µÌ±ì(Ì†µÌ±ã) = Ì†µÌ±ã      is a convex function, and Ì†µÌ∞∏(‚àó) =         ‚àë‚àó, we have shown the first
                                                                                       Ì†µÌ±õ
                    inequality. The concave case is the opposite.
                    In general, if Ì†µÌ±î is a convex function, then Ì†µÌ∞∏(Ì†µÌ±î(Ì†µÌ±ã)) ‚â• Ì†µÌ±î(Ì†µÌ∞∏(Ì†µÌ±ã)). If Ì†µÌ±î is a concave function,
                    then Ì†µÌ∞∏(Ì†µÌ±î(Ì†µÌ±ã)) ‚â§ Ì†µÌ±î(Ì†µÌ∞∏(Ì†µÌ±ã)). In both cases, the only way that equality can hold is if there are
                    constants Ì†µÌ±é and Ì†µÌ±è such that Ì†µÌ±î(Ì†µÌ±ã) = Ì†µÌ±é + Ì†µÌ±èÌ†µÌ±ã with probability 1.
                    Theorem 42.3 (Markov inequality). Let Ì†µÌ±ã be a random variable, then
                                                                            Ì†µÌ∞∏|Ì†µÌ±ã|
                                                            Ì†µÌ±É(|Ì†µÌ±ã| ‚â• Ì†µÌ±é) ‚â§
                                                                              Ì†µÌ±é
                   That is, the probability of |Ì†µÌ±ã| deviating from its mean by a multiple of Ì†µÌ±é must be less than 1/Ì†µÌ±é.
                    Proof. Define a random variable
                                                                      1  if |Ì†µÌ±ã| ‚â• Ì†µÌ±é
                                                          Ì†µÌ∞º     ={
                                                           |Ì†µÌ±ã|‚â•Ì†µÌ±é
                                                                      0  if |Ì†µÌ±ã| < Ì†µÌ±é
                    Note that Ì†µÌ±É(|Ì†µÌ±ã| ‚â• Ì†µÌ±é) = Ì†µÌ∞∏(Ì†µÌ∞º     ). It always holds that
                                                   |Ì†µÌ±ã|‚â•Ì†µÌ±é
                                                               Ì†µÌ±é ‚ãÖ Ì†µÌ∞º   ‚â§|Ì†µÌ±ã|
                                                                   |Ì†µÌ±ã|‚â•Ì†µÌ±é
                    Therefore,
                                                           Ì†µÌ∞∏ [Ì†µÌ±é ‚ãÖ Ì†µÌ∞º  ] ‚â§ Ì†µÌ∞∏|Ì†µÌ±ã|
                                                                   |Ì†µÌ±ã|‚â•Ì†µÌ±é
                    Hence,
                                                                            Ì†µÌ∞∏|Ì†µÌ±ã|
                                                            Ì†µÌ±É(|Ì†µÌ±ã| ‚â• Ì†µÌ±é) ‚â§       .
                                                                              Ì†µÌ±é
                                                                     141
                    For an intuitive interpretation, let Ì†µÌ±ã be the income of a randomly selected individual from a
                    population. Taking Ì†µÌ±é = 2Ì†µÌ∞∏(Ì†µÌ±ã), Markov‚Äôs inequality says that Ì†µÌ±É(Ì†µÌ±ã ‚â• 2Ì†µÌ∞∏(Ì†µÌ±ã)) ‚â§ 1/2, i.e., it is
                    impossible for more than half the population to make at least twice the average income. This
                    is clearly true, since if over half the population were earning at least twice the average income,
                    the average income would be higher. Similarly, Ì†µÌ±É(Ì†µÌ±ã ‚â• 3Ì†µÌ∞∏(Ì†µÌ±ã)) ‚â§ 1/3: you can‚Äôt have more
                    than 1/3 of the population making at least three times the average income, since those people
                    would already drive the average above what it is.
                    Theorem42.4(Chebyshevinequality). Let Ì†µÌ±ã be a random variable with mean Ì†µÌºá and standard
                    deviation Ì†µÌºé, then
                                                                                    1
                                                            Ì†µÌ±É (|Ì†µÌ±ã ‚àí Ì†µÌºá| > Ì†µÌ±êÌ†µÌºé) ‚â§
                                                                                    2
                                                                                   Ì†µÌ±ê
                    That is, the probability of Ì†µÌ±ã deviating from its mean by Ì†µÌ±é times the standard deviation must
                                      2
                    be less than 1/Ì†µÌ±é .
                    Proof. We first show
                                                                                    2
                                                                                  Ì†µÌºé
                                                             Ì†µÌ±É(|Ì†µÌ±ã ‚àí Ì†µÌºá| > Ì†µÌ±é) ‚â§
                                                                                   2
                                                                                  Ì†µÌ±é
                    This is true by taking squares and applying the Markov inequality,
                                                                                                2      2
                                                                                     Ì†µÌ∞∏(Ì†µÌ±ã ‚àí Ì†µÌºá)      Ì†µÌºé
                                                                          2    2
                                         Ì†µÌ±É(|Ì†µÌ±ã ‚àí Ì†µÌºá| > Ì†µÌ±é) = Ì†µÌ±É((Ì†µÌ±ã ‚àí Ì†µÌºá) > Ì†µÌ±é ) ‚â§                = .
                                                                                            2          2
                                                                                          Ì†µÌ±é          Ì†µÌ±é
                    Substitute Ì†µÌ±êÌ†µÌºé for Ì†µÌ±é, we have the original inequality.
                    This gives us an upper bound on the probability of a random variable being more than Ì†µÌ±ê
                    standard deviations away from its mean, e.g., there can‚Äôt be more than a 25% chance of being
                    2 or more standard deviations from the mean. Given the mean and standard deviation of a
                    random variable Ì†µÌ±ã, we know that Ì†µÌºá¬±2Ì†µÌºé captures 75% of its possible values; Ì†µÌºá¬±3Ì†µÌºé captures
                    90% of the possible values.
                                                                       142
                           Part V
                Continuous Distributions
                             143
                  43 Continuous vs Discrete
                  Continuous random variables, in many ways, are more versatile and useful than discrete dis-
                  tributions. One key reason is that many quantities in the physical world, such as temperature,
                  height, weight, and time, are inherently continuous in nature. These variables can take on
                  any value within a range, providing a more accurate representation of real-world phenom-
                  ena compared to discrete variables, which are limited to distinct values. Additionally, the
                  probability density functions (PDFs) of continuous distributions are often defined by smooth,
                  differentiable functions. This mathematical structure allows us to apply calculus for analysis,
                  enabling precise calculations of probabilities, expected values, and other statistical measures.
                  The ability to integrate and differentiate these functions not only simplifies manipulation but
                  also makes continuous distributions a powerful tool for solving complex problems in physics,
                  engineering, and data analysis.
                  Definition 43.1. A random variable has a continuous distribution if its CDF is             .
                                                                                                 differentiable
                  Acontinuous random variable is a random variable with a continuous distribution.
                  Definition 43.2. For a continuous random variable Ì†µÌ±ã with CDF Ì†µÌ∞π, the probability density
                                                                                      ‚Ä≤
                  function (PDF) of Ì†µÌ±ã is the derivative of the CDF, given by Ì†µÌ±ì(Ì†µÌ±•) = Ì†µÌ∞π (Ì†µÌ±•). The support of Ì†µÌ±ã
                  is the set of all Ì†µÌ±• where Ì†µÌ±ì(Ì†µÌ±•) > 0.
                  Remark. By the fundamental theorem of calculus, we integrate a PDF to get the CDF:
                                                                Ì†µÌ±•
                                                      Ì†µÌ∞π(Ì†µÌ±•) = ‚à´  Ì†µÌ±ì(Ì†µÌ±°)Ì†µÌ±ëÌ†µÌ±°.
                                                               ‚àí‚àû
                  PDFdiffers from the discrete PMF in important ways:
                    ‚Ä¢ For a continuous random variable, Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•) = 0 for all Ì†µÌ±•;
                    ‚Ä¢ The quantity Ì†µÌ±ì(Ì†µÌ±•) is not a probability. To get the probability, we integrate the PDF
                       (probability is the area under the PDF):
                                                                               Ì†µÌ±è
                                             Ì†µÌ±É(Ì†µÌ±é < Ì†µÌ±ã ‚â§ Ì†µÌ±è) = Ì†µÌ∞π(Ì†µÌ±è) ‚àí Ì†µÌ∞π(Ì†µÌ±é) = ‚à´ Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•.
                                                                              Ì†µÌ±é
                    ‚Ä¢ Since any single value has probability 0, including or excluding endpoints does not mat-
                       ter.
                                  Ì†µÌ±É(Ì†µÌ±é < Ì†µÌ±ã < Ì†µÌ±è) = Ì†µÌ±É(Ì†µÌ±é < Ì†µÌ±ã ‚â§ Ì†µÌ±è) = Ì†µÌ±É(Ì†µÌ±é ‚â§ Ì†µÌ±ã < Ì†µÌ±è) = Ì†µÌ±É(Ì†µÌ±é ‚â§ Ì†µÌ±ã ‚â§ Ì†µÌ±è).
                                                              144
                    Theorem 43.1. The PDF Ì†µÌ±ì of a continuous random variable must satisfy the following
                    criteria:
                       ‚Ä¢ Nonnegative: Ì†µÌ±ì(Ì†µÌ±•) ‚â• 0;
                                               ‚àû
                                             ‚à´
                       ‚Ä¢ Integrates to 1:         Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±• = 1.
                                              ‚àí‚àû
                    Definition 43.3. The expectation of a continuous random variable Ì†µÌ±ã with PDF Ì†µÌ±ì is
                                                                         ‚àû
                                                            Ì†µÌ∞∏(Ì†µÌ±ã) = ‚à´     Ì†µÌ±•Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•.
                                                                       ‚àí‚àû
                    Theorem 43.2. If Ì†µÌ±ã is a continuous random variable with PDF Ì†µÌ±ì and Ì†µÌ±î ‚à∂ ‚Ñù ‚Üí ‚Ñù. The
                    LOTUS applies
                                                                         ‚àû
                                                         Ì†µÌ∞∏[Ì†µÌ±î(Ì†µÌ±ã)] = ‚à´    Ì†µÌ±î(Ì†µÌ±•)Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•.
                                                                       ‚àí‚àû
                                                        Discrete                            Continuous
                                                                                                                 Ì†µÌ±è
                                                                                                                ‚à´
                    PMF/PDF                             Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•) = Ì†µÌ±ù(Ì†µÌ±•)                Ì†µÌ±É(Ì†µÌ±é ‚â§ Ì†µÌ±ã ‚â§ Ì†µÌ±è) =     Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•
                                                                                                                Ì†µÌ±é
                    CDF                                 Ì†µÌ∞π(Ì†µÌ±•) = Ì†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±•) =              Ì†µÌ∞π(Ì†µÌ±•) = Ì†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±•) =
                                                                                              Ì†µÌ±•
                                                                                            ‚à´
                                                        ‚àë Ì†µÌ±ù(Ì†µÌ±ò)                                 Ì†µÌ±ì(Ì†µÌ±°)Ì†µÌ±ëÌ†µÌ±°
                                                           Ì†µÌ±ò‚â§Ì†µÌ±•
                                                                                             ‚àí‚àû
                                                                                                        +‚àû
                                                                                                      ‚à´
                    Expectation                         Ì†µÌ∞∏(Ì†µÌ±•) = ‚àë Ì†µÌ±•Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•)            Ì†µÌ∞∏(Ì†µÌ±ã) =        Ì†µÌ±•Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•
                                                                    Ì†µÌ±•
                                                                                                       ‚àí‚àû
                                                                                                          +‚àû
                                                                                                        ‚à´
                    LOTUS                               Ì†µÌ∞∏[Ì†µÌ±î(Ì†µÌ±•)] = ‚àë Ì†µÌ±î(Ì†µÌ±•)Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±•)    Ì†µÌ∞∏[Ì†µÌ±î(Ì†µÌ±•)] =      Ì†µÌ±î(Ì†µÌ±•)Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•
                                                                       Ì†µÌ±•
                                                                                                         ‚àí‚àû
                                                                       145
                         44 Uniform distribution
                         Definition 44.1 (Uniform distribution). Let Ì†µÌ±é and Ì†µÌ±è be two given real numbers such that
                         Ì†µÌ±é < Ì†µÌ±è. Let Ì†µÌ±ã be a random variable such that it is known that Ì†µÌ±é ‚â§ Ì†µÌ±ã ‚â§ Ì†µÌ±è and, for every
                         subinterval of [Ì†µÌ±é,Ì†µÌ±è], the probability that Ì†µÌ±ã will belong to that subinterval is proportional to
                         the length of that subinterval. We then say that the random variable Ì†µÌ±ã has the Uniform
                         distribution on the interval [Ì†µÌ±é,Ì†µÌ±è]. The PDF of Ì†µÌ±ã is
                                                                                     1
                                                                                           for Ì†µÌ±é ‚â§ Ì†µÌ±• ‚â§ Ì†µÌ±è
                                                                                    Ì†µÌ±è‚àíÌ†µÌ±é
                                                                      Ì†µÌ±ì(Ì†µÌ±•) = {
                                                                                   0       otherwise
                         This is a valid PDF since
                                                            +‚àû                    Ì†µÌ±è                           Ì†µÌ±è
                                                                                       1               1
                                                         ‚à´ Ì†µÌ±ì(Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±• = ‚à´                   Ì†µÌ±ëÌ†µÌ±• =          ‚à´ Ì†µÌ±ëÌ†µÌ±• = 1.
                                                                                    Ì†µÌ±è ‚àí Ì†µÌ±é         Ì†µÌ±è ‚àí Ì†µÌ±é
                                                          ‚àí‚àû                    Ì†µÌ±é                           Ì†µÌ±é
                         The CDF of Ì†µÌ±ã is
                                                                                                     ‚éß
                                                                                                        0       Ì†µÌ±• < Ì†µÌ±é
                                                                   Ì†µÌ±•                 Ì†µÌ±•
                                                                                                     {
                                                                                                        Ì†µÌ±•‚àíÌ†µÌ±é
                                                     Ì†µÌ∞π(Ì†µÌ±•) = ‚à´       Ì†µÌ±ì(Ì†µÌ±°)Ì†µÌ±ëÌ†µÌ±° = ‚à´ Ì†µÌ±ì(Ì†µÌ±°)Ì†µÌ±ëÌ†µÌ±° =                             .
                                                                                                                Ì†µÌ±é ‚â§ Ì†µÌ±• ‚â§ Ì†µÌ±è
                                                                                                        Ì†µÌ±è‚àíÌ†µÌ±é
                                                                                                     ‚é®
                                                                 ‚àí‚àû                 Ì†µÌ±é
                                                                                                     {
                                                                                                        1       Ì†µÌ±• > Ì†µÌ±è
                                                                                                     ‚é©
                         The expectation of Ì†µÌ±ã:
                                                                                                            Ì†µÌ±è
                                                                         Ì†µÌ±è
                                                                                                        2
                                                                                1              1       Ì†µÌ±•         Ì†µÌ±é + Ì†µÌ±è
                                                          Ì†µÌ∞∏(Ì†µÌ±ã) = ‚à´ Ì†µÌ±•             Ì†µÌ±ëÌ†µÌ±• =          [     ] =            .
                                                                             Ì†µÌ±è ‚àí Ì†µÌ±é         Ì†µÌ±è ‚àí Ì†µÌ±é   2             2
                                                                       Ì†µÌ±é
                                                                                                            Ì†µÌ±é
                         To figure out the variance, first compute
                                                                                                         Ì†µÌ±è
                                                                     Ì†µÌ±è
                                                                                                     3           2            2
                                                                             1              1      Ì†µÌ±•          Ì†µÌ±é  +Ì†µÌ±éÌ†µÌ±è + Ì†µÌ±è
                                                           2            2
                                                    Ì†µÌ∞∏(Ì†µÌ±ã ) = ‚à´ Ì†µÌ±•               Ì†µÌ±ëÌ†µÌ±• =          [     ] =
                                                                          Ì†µÌ±è ‚àí Ì†µÌ±é         Ì†µÌ±è ‚àí Ì†µÌ±é   3                 3
                                                                   Ì†µÌ±é
                                                                                                         Ì†µÌ±é
                         Thus,
                                                                                        2             2              2               2
                                                                                      Ì†µÌ±é  +Ì†µÌ±éÌ†µÌ±è + Ì†µÌ±è        (Ì†µÌ±é + Ì†µÌ±è)       (Ì†µÌ±è ‚àí Ì†µÌ±é)
                                                                  2        2
                                            Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) = Ì†µÌ∞∏(Ì†µÌ±ã ) ‚àí Ì†µÌ∞∏ (Ì†µÌ±ã) =                           ‚àí               =              .
                                                                                              3                 4               12
                                                                                        146
                        Example 44.1 (Longer piece of a broken stick). A stick of unit length is broken at a random
                        point X. What is the expected length of the longer piece?
                        Solution. The lengths of the two pieces are Ì†µÌ±ã and 1‚àíÌ†µÌ±ã, with Ì†µÌ±ã ‚àº Ì†µÌ±à(0,1). The longer piece
                        is max(Ì†µÌ±ã,1 ‚àí Ì†µÌ±ã). For Ì†µÌ±ã < 0.5, the longer piece is 1 ‚àí Ì†µÌ±ã, and for Ì†µÌ±ã ‚â• 0.5, it is Ì†µÌ±ã. The
                        expected value is:
                                                                                    0.5                      1
                                                                                                                          3
                                                    Ì†µÌ∞∏[max(Ì†µÌ±ã,1‚àíÌ†µÌ±ã)] = ‚à´               (1‚àíÌ†µÌ±ã)Ì†µÌ±ëÌ†µÌ±•+‚à´ Ì†µÌ±ãÌ†µÌ±ëÌ†µÌ±• = .
                                                                                                                          4
                                                                                  0                        0.5
                        Intuition might suggest that since the stick is broken at a random point, the longer piece
                        should be ‚Äúsomewhat larger‚Äù than the shorter piece, but not as large as 3/4. However, the
                        uniform distribution of the break point means that the longer piece can sometimes be much
                        larger than the shorter piece, especially when the break point is close to one end.
                        # number of simulations
                        N <- 1000
                        # simulate random break point
                        X <- runif(N, min = 0, max = 1)
                        # length of the longer piece
                        L <- pmax(X, 1 - X)
                        cat("Expected Length of Longer Piece:", mean(L))
                        Expected Length of Longer Piece: 0.7521517
                        Example 44.2 (Buffon‚Äôs needle revisited). A plan is ruled by the lines Ì†µÌ±¶ = 0,¬±1,¬±2,... and
                        a needle of unit length is cast randomly on to the plane. What is the probability that it
                        intersects some line?
                        Solution. Let Ì†µÌ±ç be the distance from the needle‚Äôs center to the nearest line beneath it. Let
                        Œòbethe angle made by the needle and the Ì†µÌ±•-axis. The fact that the needle is cast randomly
                        meansÌ†µÌ±ç ‚àº Ì†µÌ±à(0,1), Œò ‚àº Ì†µÌ±à(0,Ì†µÌºã) and Ì†µÌ±ç and Œò are independent. Thus the joint density function
                        of (Ì†µÌ±ç,Œò) is
                                                                              1
                                                                Ì†µÌ±ì(Ì†µÌ±ß, Ì†µÌºÉ) =    ,   0 ‚â§ Ì†µÌ±ß ‚â§ 1,0 ‚â§ Ì†µÌºÉ ‚â§ Ì†µÌºã.
                                                                             Ì†µÌºã
                        An intersection occurs if and only if (draw a diagram to see this):
                                                                           1                        1
                                                                     Ì†µÌ±ß ‚â§    sinÌ†µÌºÉ or 1 ‚àí Ì†µÌ±ß ‚â§        sinÌ†µÌºÉ
                                                                           2                        2
                                                                                      147
                     Hence
                                                                             1
                                                                      Ì†µÌºã      sinÌ†µÌºÉ        1
                                                                             2
                                                                 1
                                           Ì†µÌ±É(intersection) =      ‚à´ (‚à´            Ì†µÌ±ëÌ†µÌ±ß + ‚à´       Ì†µÌ±ëÌ†µÌ±ß)Ì†µÌ±ëÌ†µÌºÉ
                                                                 Ì†µÌºã
                                                                                             1
                                                                    0      0              1‚àí sinÌ†µÌºÉ
                                                                                             2
                                                                 2
                                                              = .
                                                                 Ì†µÌºã
                                                                         148
                                45 Special integrals
                               There are many reasons to learn integrals. But the most compelling reason is that math is no
                                longer the same with integrals. We can have many amazing results with integrals that were
                                otherwise not imaginable. This section introduces two integrals that are of special importance
                                to continuous distributions.
                                                                                                             ‚àö
                                                                                     +‚àû
                                                                                                 2
                                                                                             ‚àíÌ†µÌ±•
                                                                                  ‚à´
                                Example 45.1. Show that                                    Ì†µÌ±í      Ì†µÌ±ëÌ†µÌ±• =       Ì†µÌºã.
                                                                                   ‚àí‚àû
                                Proof. This is known as Gaussian integral, which is the kernel of the PDF of the normal
                                distribution. It also amazingly relates two of the most famous constants in mathematics. It
                                is not integrable by normal integration techniques. But it can be solved by switching to the
                                polar coordinate.
                                                                                    2
                                                              +‚àû                               +‚àû                     +‚àû
                                                                          2                                2                      2
                                                                      ‚àíÌ†µÌ±•                              ‚àíÌ†µÌ±•                    ‚àíÌ†µÌ±¶
                                                      (‚à´ Ì†µÌ±í Ì†µÌ±ëÌ†µÌ±•) =‚à´                                 Ì†µÌ±í      Ì†µÌ±ëÌ†µÌ±• ‚à´         Ì†µÌ±í      Ì†µÌ±ëÌ†µÌ±¶
                                                           ‚àí‚àû                               ‚àí‚àû                      ‚àí‚àû
                                                                                               +‚àû +‚àû
                                                                                                                      2     2
                                                                                                                 ‚àí(Ì†µÌ±• +Ì†µÌ±¶ )
                                                                                       =‚à´ ‚à´ Ì†µÌ±í                                 Ì†µÌ±ëÌ†µÌ±•Ì†µÌ±ëÌ†µÌ±¶
                                                                                            ‚àí‚àû ‚àí‚àû
                                                                                               2Ì†µÌºã     ‚àû
                                                                                                                 2
                                                                                                             ‚àíÌ†µÌ±ü
                                                                                       =‚à´ ‚à´ Ì†µÌ±í Ì†µÌ±üÌ†µÌ±ëÌ†µÌ±üÌ†µÌ±ëÌ†µÌºÉ                                   Ì†µÌ±ëÌ†µÌ∞¥ = Ì†µÌ±ëÌ†µÌ±•Ì†µÌ±ëÌ†µÌ±¶ = Ì†µÌ±üÌ†µÌ±ëÌ†µÌ±üÌ†µÌ±ëÌ†µÌºÉ
                                                                                            0        0
                                                                                               2Ì†µÌºã     ‚àû
                                                                                                            1
                                                                                                                ‚àíÌ†µÌ±¢                                                          2
                                                                                       =‚à´ ‚à´ Ì†µÌ±í Ì†µÌ±ëÌ†µÌ±¢Ì†µÌ±ëÌ†µÌºÉ                                                       let Ì†µÌ±¢ = Ì†µÌ±ü
                                                                                                            2
                                                                                            0        0
                                                                                                  2Ì†µÌºã
                                                                                          1
                                                                                       = ‚à´ Ì†µÌ±ëÌ†µÌºÉ=Ì†µÌºã.
                                                                                          2
                                                                                               0
                                                                                     ‚àû
                                                                                          Ì†µÌ±õ  ‚àíÌ†µÌ±°
                                                                                  ‚à´
                                Example 45.2. Show that                                  Ì†µÌ±° Ì†µÌ±í    Ì†µÌ±ëÌ†µÌ±° = Ì†µÌ±õ!
                                                                                   0
                                                             ‚àû
                                                                  Ì†µÌ±ß‚àí1 ‚àíÌ†µÌ±°
                                                          ‚à´
                                Proof. Œì(Ì†µÌ±ß) =                   Ì†µÌ±°     Ì†µÌ±í   Ì†µÌ±ëÌ†µÌ±° is known as the Gamma function, which is definitely one of the
                                                           0
                                most interesting functions in mathematics. It is the extension of factorials to real numbers
                                or even complex numbers. It also has many interesting properties, such as Œì(Ì†µÌ±õ) = (Ì†µÌ±õ ‚àí 1)!,
                                                 ‚àö                         ‚àö
                                                                                          ‚Ä≤
                                Œì(1/2) =            Ì†µÌºã, Œì(3/2) =              Ì†µÌºã/2, Œì (1) = ‚àíÌ†µÌªæ and so on. The (Ì†µÌ±õ ‚àí 1) in the Gamma function is
                                                                                                               149
                                          due to historical reasons and does not matter in our case. We will prove the integral with Ì†µÌ±õ
                                          instead of (Ì†µÌ±õ ‚àí 1).
                                          There are many ways to prove this. One is to discover the recursive relationship Œì(Ì†µÌ±õ + 1) =
                                          Ì†µÌ±õŒì(Ì†µÌ±õ). But it does not give a clue why we need this integral to approximate the factorial. We
                                          start with an elementary integral
                                                                                                                                           ‚àû
                                                                                                                                                                         1
                                                                                                                                                   Ì†µÌ±éÌ†µÌ±°
                                                                                                                                      ‚à´ Ì†µÌ±í Ì†µÌ±ëÌ†µÌ±° = ‚àí
                                                                                                                                                                         Ì†µÌ±é
                                                                                                                                        0
                                          where Ì†µÌ±é < 0. Differentiate both sides Ì†µÌ±õ times with respect to Ì†µÌ±é:
                                                                                                                       ‚àû
                                                                                                                               Ì†µÌ±éÌ†µÌ±°                                                           ‚àí2
                                                                                                                  ‚à´ Ì†µÌ±í Ì†µÌ±°Ì†µÌ±ëÌ†µÌ±° =                                            ‚àí(‚àí1)Ì†µÌ±é
                                                                                                                    0
                                                                                                                     ‚àû
                                                                                                                             Ì†µÌ±éÌ†µÌ±°  2                                                          ‚àí3
                                                                                                               ‚à´ Ì†µÌ±í Ì†µÌ±° Ì†µÌ±ëÌ†µÌ±° =                                   ‚àí(‚àí1)(‚àí2)Ì†µÌ±é
                                                                                                                  0
                                                                                                                     ‚àû
                                                                                                                             Ì†µÌ±éÌ†µÌ±°  3                                                          ‚àí4
                                                                                                               ‚à´ Ì†µÌ±í Ì†µÌ±° Ì†µÌ±ëÌ†µÌ±° =                        ‚àí(‚àí1)(‚àí2)(‚àí3)Ì†µÌ±é
                                                                                                                  0
                                                                                                                                            ‚ãÆ
                                                                                                                    ‚àû
                                                                                                                            Ì†µÌ±éÌ†µÌ±°   Ì†µÌ±õ                                Ì†µÌ±õ+1            ‚àí(Ì†µÌ±õ+1)
                                                                                                               ‚à´ Ì†µÌ±í Ì†µÌ±° Ì†µÌ±ëÌ†µÌ±° =                             (‚àí1)               Ì†µÌ±õ!Ì†µÌ±é
                                                                                                                 0
                                          Let Ì†µÌ±é = ‚àí1, we have
                                                                                                                                              ‚àû
                                                                                                                                                      Ì†µÌ±°  Ì†µÌ±õ
                                                                                                                                         ‚à´ Ì†µÌ±í Ì†µÌ±° = Ì†µÌ±õ!
                                                                                                                                           0
                                                                                                                                                     150
         46 Sum of random variables
         We have seen the sum of coin heads and the sum of dice points follow a ‚Äúbell-shaped‚Äù distri-
         bution. This is not a coincidence. The pattern does not exist when the number of coins or
         dice are small, but becomes apparent when the numbers get large.
         # Function to simulate the sum of rolling n fair dice
         simulate_dice_sum <- function(n, n_simulations) {
          # Simulate rolling n dice n_simulations times
          dice_rolls <- matrix(sample(1:6, n * n_simulations, replace = TRUE),
                      nrow = n_simulations, ncol = n)
          # Compute the sum of each roll
          sums <- rowSums(dice_rolls)
          return(sums)
         }
         # Set seed for reproducibility
         set.seed(123)
         # Rolling 2 dice
         sums <- simulate_dice_sum(2, 1000)
         # Plot the distribution of sums
         hist(sums, prob = TRUE, col = "lightblue")
                              151
                      Histogram of sums
            0.15
            0.10
         Density
            0.05
            0.00
              2    4     6    8    10   12
                          sums
        # Rolling 10 dice
        sums <- simulate_dice_sum(10, 1000)
        # Plot the distribution of sums
        hist(sums, prob = TRUE, ylim = c(0,.08), col = "lightblue")
        # Overlay the normal curve
        curve(dnorm(x, mean = mean(sums), sd = sd(sums)),
           col = "red", lwd = 2, add = TRUE)
                      Histogram of sums
            0.08
         Density0.04
            0.00
                 20     30    40     50
                          sums
                            152
                 This does not only hold for dice points.  In fact, the sum of random variables from any
                 distribution would reveal a similar pattern.
                 # Sum of uniform random variables
                 simulate_uniform_sum <- function(n, n_simulations) {
                   sums <- numeric(n_simulations)
                   # Simulate the experiment n_simulations times
                   for (i in 1:n_simulations) {
                      # Sum of n Uniform random variables
                      sums[i] <- sum(runif(n))
                   }
                   return(sums)
                 }
                 # Sum of 2 Uniform random variables
                 sums <- simulate_uniform_sum(2, 1000)
                 # Plot the histogram
                 hist(sums, prob = TRUE, col = "lightblue")
                                              Histogram of sums
                         0.8
                   Density0.4
                         0.0
                             0.0           0.5           1.0           1.5           2.0
                                                        sums
                 # Sum of 10 Uniform random variables
                 sums <- simulate_uniform_sum(10, 1000)
                 # Plot the histogram
                                                            153
                          hist(sums, prob = TRUE, col = "lightblue")
                          # Overlay the normal curve
                          curve(dnorm(x, mean = mean(sums), sd = sd(sums)),
                                    col = "red", lwd = 2, add = TRUE)
                                                                     Histogram of sums
                                     0.4
                            Density  0.2
                                     0.0
                                                   2            3            4             5            6            7            8
                                                                                    sums
                             ƒπ Sum of random variables approaches Normal distribution
                             Let Ì†µÌ±ã ,Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã be a sequence of i.i.d random variables with mean Ì†µÌºá = Ì†µÌ∞∏(Ì†µÌ±ã ) and
                                      1    2          Ì†µÌ±õ                                                                                          Ì†µÌ±ñ
                                            2
                             variance Ì†µÌºé = Ì†µÌ±âÌ†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã ). Let
                                                           Ì†µÌ±ñ
                                                                          Ì†µÌ±Ü   =Ì†µÌ±ã +Ì†µÌ±ã +‚ãØ+Ì†µÌ±ã
                                                                            Ì†µÌ±õ       1       2              Ì†µÌ±õ
                             Then, as Ì†µÌ±õ ‚Üí ‚àû, Ì†µÌ±Ü converge in distribution to a normal distribution. That is
                                                         Ì†µÌ±õ
                                                                                      Ì†µÌ±ë              2
                                                                              Ì†µÌ±Ü   ‚Üí Ì†µÌ±Å(Ì†µÌ±õÌ†µÌºá,Ì†µÌ±õÌ†µÌºé ).
                                                                                Ì†µÌ±õ
                                                                                          154
                      47 Normal distribution
                      The most widely used model for random variables with continuous distributions is the family
                      of normal distributions. One reason is that many real world samples appears to be normally
                      distributed (the mass centered around the mean). The other reason is because of the Central
                      Limit Theorem (will be discussed in later chapters), which essentially says the sum (or mean)
                      or any random samples are approximately normal.
                      Definition 47.1. A random variable Ì†µÌ±ç has the standard Normal distribution with mean 0
                      and variance 1, denoted as Ì†µÌ±ç ‚àº Ì†µÌ±Å(0,1), if Ì†µÌ±ç has a PDF that follows
                                                                               1
                                                                                      2
                                                                                    ‚àíÌ†µÌ±ß /2
                                                                             ‚àö
                                                                    Ì†µÌ±ì(Ì†µÌ±ß) =      Ì†µÌ±í      .
                                                                               2Ì†µÌºã
                                                            ‚àû
                                                          ‚à´
                      This is a valid PDF because              Ì†µÌ±ì(Ì†µÌ±ß)Ì†µÌ±ëÌ†µÌ±ß = 1, which directly follows from Example 45.1. We
                                                           ‚àí‚àû
                      further verify its mean and variance:
                                                             +‚àû
                                                                       1
                                                                               2
                                                                            ‚àíÌ†µÌ±ß /2
                                                                     ‚àö
                                                Ì†µÌ∞∏(Ì†µÌ±ç) = ‚à´        Ì†µÌ±ß ‚ãÖ     Ì†µÌ±í     Ì†µÌ±ëÌ†µÌ±ß = 0   by symmetry.
                                                                        2Ì†µÌºã
                                                            ‚àí‚àû
                                                                  2            2         2
                                                Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ç) = Ì†µÌ∞∏(Ì†µÌ±ç ) ‚àí (Ì†µÌ∞∏Ì†µÌ±ç) = Ì†µÌ∞∏(Ì†µÌ±ç )
                                                                +‚àû
                                                                           1
                                                                                   2
                                                                      2         ‚àíÌ†µÌ±ß /2
                                                                         ‚àö
                                                          =‚à´ Ì†µÌ±ß ‚ãÖ              Ì†µÌ±í     Ì†µÌ±ëÌ†µÌ±ß
                                                                           2Ì†µÌºã
                                                              ‚àí‚àû
                                                                      ‚àû
                                                               2
                                                                                  2
                                                                               ‚àíÌ†µÌ±ß /2
                                                             ‚àö
                                                          =         ‚à´ Ì†µÌ±ß‚ãÖÌ†µÌ±ßÌ†µÌ±í        Ì†µÌ±ëÌ†µÌ±ß
                                                                         ‚èü ‚èü‚èü‚èü‚èü‚èü
                                                                2Ì†µÌºã
                                                                         Ì†µÌ±¢
                                                                     0
                                                                                 Ì†µÌ±ëÌ†µÌ±£
                                                                    ‚éß                                      ‚é´
                                                                    {                                      }
                                                                                               ‚àû
                                                                    {                                      }
                                                                                     ‚àû
                                                               2
                                                                                2
                                                                                                     2
                                                                             ‚àíÌ†µÌ±ß /2
                                                                                                   ‚àíÌ†µÌ±ß /2
                                                             ‚àö
                                                          =           [Ì†µÌ±ß(‚àíÌ†µÌ±í      )]   +‚à´ Ì†µÌ±í            Ì†µÌ±ëÌ†µÌ±ß
                                                                    ‚é®                                      ‚é¨
                                                                                     0
                                                                2Ì†µÌºã
                                                                                             0
                                                                                           ‚èü‚èü‚èü‚èü‚èü
                                                                    {                                      }
                                                                                                ‚àö
                                                                    {                                      }
                                                                                                  2Ì†µÌºã/2
                                                                    ‚é©                                      ‚é≠
                                                          =1.
                      Definition 47.2. The CDF of standard normal distribution is usually denoted by Œ¶. There-
                      fore,
                                                                                  Ì†µÌ±ß
                                                                           1
                                                                                        2
                                                                                      ‚àíÌ†µÌ±° /2
                                                                         ‚àö
                                                               Œ¶(Ì†µÌ±ß) =         ‚à´ Ì†µÌ±í        Ì†µÌ±ëÌ†µÌ±°.
                                                                           2Ì†µÌºã
                                                                                ‚àí‚àû
                                                                             155
                     By symmetry, we have Œ¶(‚àíÌ†µÌ±ß) = 1‚àíŒ¶(Ì†µÌ±ß).
                     Definition 47.3. Let Ì†µÌ±ã = Ì†µÌºá + Ì†µÌºéÌ†µÌ±ç where Ì†µÌ±ç ‚àº Ì†µÌ±Å(0,1). Then we say Ì†µÌ±ã has the Normal
                                                                     2                             2
                     distribution with mean Ì†µÌºá and variance Ì†µÌºé , denoted as Ì†µÌ±ã ‚àº Ì†µÌ±Å(Ì†µÌºá,Ì†µÌºé ). The PDF of Ì†µÌ±ã is given
                     by
                                                                                             2
                                                                   1             1 Ì†µÌ±•‚àíÌ†µÌºá
                                                                ‚àö
                                                       Ì†µÌ±ì(Ì†µÌ±•) =          exp[‚àí (            ) ].
                                                                       2
                                                                                 2     Ì†µÌºé
                                                                  2Ì†µÌºãÌ†µÌºé
                     The mean and variance of Ì†µÌ±ã can be easily verified by the properties of expectation and
                     variance.
                                                     Ì†µÌ∞∏(Ì†µÌ±ã) = Ì†µÌ∞∏(Ì†µÌºá + Ì†µÌºéÌ†µÌ±ç) = Ì†µÌºá + Ì†µÌºéÌ†µÌ∞∏(Ì†µÌ±ç) = Ì†µÌºá,
                                                                                    2              2
                                                   Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) = Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌºá + Ì†µÌºéÌ†µÌ±ç) = Ì†µÌºé Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ç) = Ì†µÌºé .
                     To verify the PDF, we utilize the standard normal CDF:
                                                                    Ì†µÌ±ã ‚àíÌ†µÌºá     Ì†µÌ±• ‚àí Ì†µÌºá          Ì†µÌ±• ‚àí Ì†µÌºá
                                                                                      )=Œ¶(            )
                                                Ì†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±•) = Ì†µÌ±É (          ‚â§
                                                                       Ì†µÌºé         Ì†µÌºé              Ì†µÌºé
                     The PDF is the derivative of the CDF,
                                                                                                      2
                                                       1      Ì†µÌ±• ‚àí Ì†µÌºá        1            1 Ì†µÌ±•‚àíÌ†µÌºá
                                                           ‚Ä≤
                                                                            ‚àö
                                              Ì†µÌ±ì(Ì†µÌ±•) =   Œ¶ (         )=           exp[‚àí (            ) ].
                                                       Ì†µÌºé       Ì†µÌºé                        2     Ì†µÌºé
                                                                          Ì†µÌºé  2Ì†µÌºã
                     The shape of the normal distribution is the famous bell-shaped curve.
                                                                          156
                 ƒπ Three-sigma rule
                 The normal distribution has the ‚Äúthree-sigma rule‚Äù:
                                             Ì†µÌ±É(|Ì†µÌ±ã ‚àí Ì†µÌºá| ‚â§ Ì†µÌºé) ‚âà 0.68
                                            Ì†µÌ±É(|Ì†µÌ±ã ‚àí Ì†µÌºá| ‚â§ 2Ì†µÌºé) ‚âà 0.95
                                            Ì†µÌ±É(|Ì†µÌ±ã ‚àí Ì†µÌºá| ‚â§ 3Ì†µÌºé) ‚âà 0.997
                 Critical values: Œ¶(‚àí1) ‚âà 0.16,Œ¶(‚àí2) ‚âà 0.025,Œ¶(‚àí3) ‚âà 0.0015.
                                                                                     2
               Theorem 47.1. Let Ì†µÌ±ã have the Normal distribution with mean Ì†µÌºá and variance Ì†µÌºé . Let Ì†µÌ∞π be
               the CDF of Ì†µÌ±ã. Then the standardization of Ì†µÌ±ã
                                                      Ì†µÌ±ã ‚àíÌ†µÌºá
                                                  Ì†µÌ±ç =
                                                        Ì†µÌºé
               has the standard normal distribution, and, for all Ì†µÌ±•:
                                                        Ì†µÌ±• ‚àí Ì†µÌºá
                                                             ).
                                               Ì†µÌ∞π(Ì†µÌ±•) = Œ¶(
                                                          Ì†µÌºé
               To find the value of Œ¶(Ì†µÌ±ß), we need to use the normal probability table or statistical software.
               Example 47.1. Suppose the test score of a class of 50 students is normally distributed with
               mean 80 and standard deviation 20 (the total mark is 100). A student has scored 90. What
               is his percentile in the class?
               Solution. Ì†µÌ±ã ‚àº Ì†µÌ±Å(80,20). We want to find Ì†µÌ±É(Ì†µÌ±ã < 90). Standardize the distribution:
                                               Ì†µÌ±ã ‚àí80   90‚àí80
                                Ì†µÌ±É(Ì†µÌ±ã < 90) = Ì†µÌ±É (    <       )=Œ¶(0.5)‚âà0.69.
                                                 20       20
               # Exam scores from past students
               scores <- read.csv("../dataset/exam.csv")$final
               # Histogram of the exam scores
               hist(scores, prob = TRUE, col = "lightblue")
               # Overlay the normal curve
               curve(dnorm(x, mean(scores), sd(scores)), col = "red", lwd = 2, add = TRUE)
                                                      157
                                       Histogram of scores
                     0.030
                Density0.015
                     0.000
                         30     40     50     60    70     80     90     100
                                               scores
                                              2
               Theorem 47.2. Suppose Ì†µÌ±ã ‚àº Ì†µÌ±Å(Ì†µÌºá,Ì†µÌºé ). If Ì†µÌ±å = Ì†µÌ±éÌ†µÌ±ã + Ì†µÌ±è, then Ì†µÌ±å has the Normal distribution
                            2 2
               Ì†µÌ±å ‚àº Ì†µÌ±Å(Ì†µÌ±éÌ†µÌºá + Ì†µÌ±è,Ì†µÌ±é Ì†µÌºé ).
                                                                                        2
               Theorem 47.3. If the random variables Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã are independent and Ì†µÌ±ã ‚àº Ì†µÌ±Å(Ì†µÌºá ,Ì†µÌºé ).
                                                   1     Ì†µÌ±ò                    Ì†µÌ±ñ     Ì†µÌ±ñ
                                                                                        Ì†µÌ±ñ
              Then
                                                             2       2
                                  Ì†µÌ±ã +‚ãØ+Ì†µÌ±ã ‚àºÌ†µÌ±Å(Ì†µÌºá +‚ãØ+Ì†µÌºá ,Ì†µÌºé +‚ãØ+Ì†µÌºé ).
                                   1        Ì†µÌ±ò     1      Ì†µÌ±ò
                                                             1
                                                                     Ì†µÌ±ò
               Example 47.2. Suppose the heights (in centimeters) of women and men independently follow
               the normal distribution, Ì†µÌ±ã ‚àº Ì†µÌ±Å(165,25), Ì†µÌ±å ‚àº Ì†µÌ±Å(170,25). Determine the probability that a
               randomly selected woman will be taller than a man.
               Solution. Let Ì†µÌ±ä = Ì†µÌ±å ‚àíÌ†µÌ±ã ‚àº Ì†µÌ±Å(170‚àí165,25+25). Then Ì†µÌ±ä ‚àº Ì†µÌ±Å(5,50). Therefore,
                                     Ì†µÌ±ä ‚àí5   ‚àí5               1
                                      ‚àö      ‚àö               ‚àö
                                                 )=Ì†µÌ±É(Ì†µÌ±ç <‚àí     )=Œ¶(‚àí0.707)‚âà0.24.
                       Ì†µÌ±É(Ì†µÌ±ä < 0) = Ì†µÌ±É (   <
                                       50     50
                                                               2
                                                   158
                        48 Multivariate normal
                        Definition 48.1 (Bivariate normal distribution). (Ì†µÌ±ã,Ì†µÌ±å) is said to have a Bivariate Normal
                        distribution if the joint PDF satisfies
                                                                      1                        1
                                                                                                         2      2
                                                  Ì†µÌ±ì(Ì†µÌ±•, Ì†µÌ±¶) =                 exp(‚àí                  (Ì†µÌ±•  +Ì†µÌ±¶ ‚àí2Ì†µÌºåÌ†µÌ±•Ì†µÌ±¶))
                                                                                                   2
                                                                             2
                                                                                         2(1‚àíÌ†µÌºå )
                                                                   ‚àö
                                                               2Ì†µÌºã    1‚àíÌ†µÌºå
                        where Ì†µÌºå ‚àà (‚àí1,1) is the correlation between Ì†µÌ±ã and Ì†µÌ±å.
                        Multivariate Normal (MVN) is an extension of the bivariate normal distribution to Ì†µÌ±õ-
                        dimensional variables. We skip the joint PDF here since it is too complicated. But like the
                        bivariate case, an MVN is fully specified by knowing the mean of each component, the variance
                        of each component, and the covariance between any two components.
                            ¬æ Marginal normality does not imply joint normality
                            If (Ì†µÌ±ã ,...,Ì†µÌ±ã ) is MVN, then the marginal distribution of every Ì†µÌ±ã is Normal. However,
                                   1        Ì†µÌ±ò                                                                        Ì†µÌ±ó
                            the converse is false: it is possible to have Normally distributed Ì†µÌ±ã ,...,Ì†µÌ±ã                              such that
                                                                                                                           1        Ì†µÌ±ò
                            (Ì†µÌ±ã ,...,Ì†µÌ±ã ) is not Multivariate Normal.
                               1         Ì†µÌ±ò
                        # Load necessary library
                        library(MASS)
                                                                                      159
                     # Set seed for reproducibility
                     set.seed(123)
                     # Generate bivariate normal data
                     bvn_data <- mvrnorm(n = 1000,
                                                  mu = c(0, 0),
                                                  Sigma = matrix(c(1, 0.5, 0.5, 1), nrow = 2))
                     # Modify the joint distribution: apply a nonlinear transformation
                     bvn_data[, 2] <- bvn_data[, 2] + 2 * sin(bvn_data[, 1])
                     # The marginal distribution remains normal
                     par(mfrow = c(1, 3))
                     hist(bvn_data[, 1], main = "Marginal X1", col = "lightblue")
                     hist(bvn_data[, 2], main = "Marginal X2", col = "lightblue")
                     # But the joint distribution is not normal
                     plot(bvn_data, main = "Joint Distribution", pch = 16, col = rgb(1,0,0,.2))
                                 Marginal X1                     Marginal X2                   Joint Distribution
                                                                                             4
                            150                                                              2
                                                            100                              0
                       Frequency                        Frequency50                     vn_data[,2]
                            50                                                          b    ‚àí4
                            0                               0
                              ‚àí3    ‚àí1    1 2 3                ‚àí6   ‚àí2     2 4 6                  ‚àí2    0 1 2 3
                                  bvn_data[, 1]                    bvn_data[, 2]                    bvn_data[,1]
                     Theorem 48.1. A random vector (Ì†µÌ±ã ,...,Ì†µÌ±ã ) is Multivariate Normal if every linear combi-
                                                                   1       Ì†µÌ±ò
                     nation of the Ì†µÌ±ã has a Normal distribution (Ì†µÌ±ã do not have to be independent). That is, we
                                        Ì†µÌ±ó                                   Ì†µÌ±ó
                     require Ì†µÌ±° Ì†µÌ±ã + ‚ãÖ ‚ãÖ ‚ãÖ + Ì†µÌ±° Ì†µÌ±ã  to have a Normal distribution for any choice of constants Ì†µÌ±° ,...,Ì†µÌ±° .
                               1  1            Ì†µÌ±ò Ì†µÌ±ò                                                                      1      Ì†µÌ±ò
                     Theorem 48.2. In general, uncorrelated does not imply independent. But with an MVN
                     random vector, uncorrelated implies independent. In particular, if (Ì†µÌ±ã,Ì†µÌ±å) is Bivariate Normal
                     and Ì†µÌºå     =0, then X and Y are independent.
                            Ì†µÌ±ãÌ†µÌ±å
                     Theorem 48.3. If (Ì†µÌ±ã,Ì†µÌ±å) is Bivariate Normal, then the conditional expectation satisfies
                                                                          Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å )
                                                   Ì†µÌ∞∏(Ì†µÌ±å |Ì†µÌ±ã) = Ì†µÌ∞∏(Ì†µÌ±å ) +              (Ì†µÌ±ã ‚àíÌ†µÌ∞∏(Ì†µÌ±ã)).
                                                                            Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã)
                                                                          160
           In other words,
                                   Ì†µÌ∞∏(Ì†µÌ±å |Ì†µÌ±ã) = Ì†µÌ±é + Ì†µÌ±èÌ†µÌ±ã
                  Ì†µÌ∞∂Ì†µÌ±úÌ†µÌ±£(Ì†µÌ±ã,Ì†µÌ±å )
           where Ì†µÌ±è =   and Ì†µÌ±é = Ì†µÌ∞∏(Ì†µÌ±å ) ‚àí Ì†µÌ±èÌ†µÌ∞∏(Ì†µÌ±ã).
                   Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã)
           This is exactly the case in Definition 40.2, where we assume the conditional expectation
            Ì†µÌ∞∏(Ì†µÌ±å |Ì†µÌ±ã) is a linear function of Ì†µÌ±ã. This assumption is true when (Ì†µÌ±ã,Ì†µÌ±å) are jointly normal.
            Otherwise, the assumption might not be reasonable. In practice, we don‚Äôt know precisely the
            joint distribution of variables. The linear model is just a simplified assumption.
                                        161
                     49 Exponential distribution
                     Imagine you are a shop owner that waits for your next customer. The customers arrive
                     randomly, with no preference for any specific time interval. What interests us is the waiting
                     time until the next customer arrives. Since the customers arrives randomly, the likelihood
                     of it coming in the next moment is the same whether you‚Äôve been waiting for one minute
                     or ten minutes. In other words, the waiting time between events that occur randomly and
                     independently over time. The exponential distribution is the mathematical model that best
                     describes such scenarios.
                     To model the waiting time, let Ì†µÌ±ã represent the time until the next event. A crucial feature
                     of this process is that the waiting time has no ‚Äúmemory.‚Äù                   That is, no matter how long
                     you‚Äôve already waited, the probability of waiting an additional amount of time is the same.
                     Mathematically, this memoryless property is expressed as:
                                              Ì†µÌ±É(Ì†µÌ±ã ‚â• Ì†µÌ±† + Ì†µÌ±° ‚à£ Ì†µÌ±ã ‚â• Ì†µÌ±†) = Ì†µÌ±É(Ì†µÌ±ã ‚â• Ì†µÌ±°),    for all Ì†µÌ±†,Ì†µÌ±° ‚â• 0.
                     Theconditional probability can be rewritten using the definition of conditional probabilities:
                                                                                   Ì†µÌ±É(Ì†µÌ±ã ‚â• Ì†µÌ±† + Ì†µÌ±°)
                                                      Ì†µÌ±É(Ì†µÌ±ã ‚â• Ì†µÌ±† + Ì†µÌ±° ‚à£ Ì†µÌ±ã ‚â• Ì†µÌ±†) =                 .
                                                                                     Ì†µÌ±É(Ì†µÌ±ã ‚â• Ì†µÌ±†)
                     Thus, the memoryless property implies:
                                                             Ì†µÌ±É(Ì†µÌ±ã ‚â• Ì†µÌ±† + Ì†µÌ±°)
                                                                              =Ì†µÌ±É(Ì†µÌ±ã ‚â• Ì†µÌ±°).
                                                               Ì†µÌ±É(Ì†µÌ±ã ‚â• Ì†µÌ±†)
                     Let the survival function Ì†µÌ±Ü(Ì†µÌ±•) represent Ì†µÌ±É(Ì†µÌ±ã ‚â• Ì†µÌ±•) . Substituting Ì†µÌ±Ü(Ì†µÌ±•) into the equation
                     gives:
                                                                    Ì†µÌ±Ü(Ì†µÌ±† + Ì†µÌ±°)
                                                                              =Ì†µÌ±Ü(Ì†µÌ±°).
                                                                      Ì†µÌ±Ü(Ì†µÌ±†)
                     This reminds us of the exponential function. In fact, the only continuous and non-negative
                     solution to this equation is:
                                                                          ‚àíÌ†µÌºÜÌ†µÌ±•
                                                                Ì†µÌ±Ü(Ì†µÌ±•) = Ì†µÌ±í    ,   Ì†µÌºÜ > 0,
                     where Ì†µÌºÜ is a positive constant. This solution represents the probability that the waiting time
                     exceeds Ì†µÌ±• , and Ì†µÌºÜ determines how quickly the probability decreases over time.
                     The CDF of Ì†µÌ±ã is exactly the opposite of Ì†µÌ±Ü(Ì†µÌ±•):
                                                                                         ‚àíÌ†µÌºÜÌ†µÌ±•
                                                            Ì†µÌ∞π(Ì†µÌ±•) = 1 ‚àí Ì†µÌ±Ü(Ì†µÌ±•) = 1 ‚àí Ì†µÌ±í     .
                                                                           162
                        Take derivative to get the PDF:
                                                                                     ‚Ä≤           ‚àíÌ†µÌºÜÌ†µÌ±•
                                                                        Ì†µÌ±ì(Ì†µÌ±•) = Ì†µÌ∞π (Ì†µÌ±•) = Ì†µÌºÜÌ†µÌ±í       .
                        Definition 49.1 (Exponential distribution). A random variable Ì†µÌ±ã is said to have the Expo-
                        nential distribution with parameter Ì†µÌºÜ if its PDF is
                                                                                    ‚àíÌ†µÌºÜÌ†µÌ±•
                                                                       Ì†µÌ±ì(Ì†µÌ±•) = Ì†µÌºÜÌ†µÌ±í     ,      Ì†µÌ±• > 0.
                        We denote this as Ì†µÌ±ã ‚àº Expo(Ì†µÌºÜ).Ì†µÌºÜ is interpreted as the ‚Äúrate‚Äù, i.e. number of events per unit
                        of time.
                        To compute the expectation and variance, we first standardize the exponential distribution.
                        Let Ì†µÌ±å = Ì†µÌºÜÌ†µÌ±ã, then Ì†µÌ±å ‚àº Expo(1), because
                                                                                                            ‚àíÌ†µÌ±¶
                                                               Ì†µÌ±É(Ì†µÌ±å ‚â§ Ì†µÌ±¶) = Ì†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±¶/Ì†µÌºÜ) = 1 ‚àí Ì†µÌ±í           .
                        It follows that,
                                                                     ‚àû                                    ‚àû
                                                                                                ‚àû
                                                                           ‚àíÌ†µÌ±¶              ‚àíÌ†µÌ±¶               ‚àíÌ†µÌ±¶
                                                       Ì†µÌ∞∏(Ì†µÌ±å ) = ‚à´      Ì†µÌ±¶Ì†µÌ±í   Ì†µÌ±ëÌ†µÌ±¶ = [‚àíÌ†µÌ±¶Ì†µÌ±í   ]    +‚à´ Ì†µÌ±í Ì†µÌ±ëÌ†µÌ±¶ = 1;
                                                                                                0
                                                                   0                                    0
                                                                                               ‚àû
                                                                        2             2            2 ‚àíÌ†µÌ±¶
                                                    Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±å ) = Ì†µÌ∞∏(Ì†µÌ±å  ) ‚àí(Ì†µÌ∞∏Ì†µÌ±å) = ‚à´ Ì†µÌ±¶ Ì†µÌ±í            Ì†µÌ±ëÌ†µÌ±¶ ‚àí 1 = 1.
                                                                                             0
                                                                    1                   1
                        For Ì†µÌ±ã = Ì†µÌ±å /Ì†µÌºÜ, we have Ì†µÌ∞∏(Ì†µÌ±ã) =             , Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) =     .
                                                                                        2
                                                                    Ì†µÌºÜ                 Ì†µÌºÜ
                        Theorem 49.1 (Memoryless property). If Ì†µÌ±ã has the exponential distribution with parameter
                        Ì†µÌºÜ, and let Ì†µÌ±° > 0, ‚Ñé > 0, then
                                                                 Ì†µÌ±É(Ì†µÌ±ã ‚â• Ì†µÌ±° + ‚Ñé|Ì†µÌ±ã ‚â• Ì†µÌ±°) = Ì†µÌ±É(Ì†µÌ±ã ‚â• ‚Ñé).
                        Proof. For Ì†µÌ±° > 0 we have
                                                                                     ‚àû
                                                                                           ‚àíÌ†µÌºÜÌ†µÌ±•         ‚àíÌ†µÌºÜÌ†µÌ±°
                                                                 Ì†µÌ±É(Ì†µÌ±ã ‚â• Ì†µÌ±°) = ‚à´        Ì†µÌºÜÌ†µÌ±í    Ì†µÌ±ëÌ†µÌ±• = Ì†µÌ±í    .
                                                                                   Ì†µÌ±°
                        Hence for each Ì†µÌ±° > 0 and each ‚Ñé > 0,
                                                                                                 ‚àíÌ†µÌºÜ(Ì†µÌ±°+‚Ñé)
                                                                        Ì†µÌ±É(Ì†µÌ±ã ‚â• Ì†µÌ±° + ‚Ñé)
                                                                                               Ì†µÌ±í
                                                                                                                ‚àíÌ†µÌºÜ‚Ñé
                                        Ì†µÌ±É(Ì†µÌ±ã ‚â• Ì†µÌ±° + ‚Ñé|Ì†µÌ±ã ‚â• Ì†µÌ±°) =                           =              =Ì†µÌ±í       =Ì†µÌ±É(Ì†µÌ±ã ‚â• ‚Ñé).
                                                                                                   ‚àíÌ†µÌºÜÌ†µÌ±°
                                                                           Ì†µÌ±É(Ì†µÌ±ã ‚â• Ì†µÌ±°)            Ì†µÌ±í
                                                                                     163
                 What are the implications of the memoryless property? If human lifetimes were Exponential,
                 then conditional on having survived to the age of 80, your remaining lifetime would have the
                 same distribution as that of a newborn baby! Clearly, the memoryless property is not an
                 appropriate description for human lifetimes.
                 The memoryless property is a very special property of the Exponential distribution. In fact,
                 the Exponential is the only memoryless continuous distribution (with support (0,‚àû)); and
                 Geometric distribution is the only memoryless discrete distribution (with support 0,1,‚Ä¶).
                 Example 49.1 (Waiting time). We try to model the waiting time at a bus station. Suppose
                 the bus arrives at random time but on average there will be one bus per 10 minutes. You
                 arrive at the bus stop at a random time, not knowing how long ago the previous bus came.
                 What is the distribution of your waiting time for the next bus? What is the mean waiting
                 time? What is the median waiting time?
                 Solution. Let Ì†µÌ±ã be the waiting time and we know it is an Exponential distribution. Since
                 Ì†µÌ∞∏(Ì†µÌ±ã) = 1/Ì†µÌºÜ = 10, the parameter Ì†µÌºÜ = 1/10. Thus Ì†µÌ±ã ‚àº Expo(0.1). By the memoryless
                 property, how much longer the next bus will take to arrive is independent of how long ago the
                 previous bus arrived. The average waiting time is always 10 minutes.
                                                  ‚àíÌ†µÌºÜÌ†µÌ±•
                 The CDF of Ì†µÌ±ã is: Ì†µÌ∞π(Ì†µÌ±•) = 1 ‚àí Ì†µÌ±í   .  The median Ì†µÌ±ö satisfies Ì†µÌ∞π(Ì†µÌ±ö) = 1/2. Thus, Ì†µÌ±ö =
                 log(2)/Ì†µÌºÜ ‚âà 6.9 minutes. So the typical waiting experienced by most passengers is less than 10
                 minutes.
                 Theorem 49.2 (Poisson-Exponential connection). Let Ì†µÌ±á be the time between two consecutive
                 events in Poisson process Pois(Ì†µÌºÜÌ†µÌ±°). Then Ì†µÌ±á follows Exponential distribution Ì†µÌ±á ‚àº Expo(Ì†µÌºÜ).
                 Proof. The waiting time Ì†µÌ±á > Ì†µÌ±° is equivalent to no event occurred during period Ì†µÌ±°. Therefore,
                                                                         0
                                                                     (Ì†µÌºÜÌ†µÌ±°)
                                                                  ‚àíÌ†µÌºÜÌ†µÌ±°       ‚àíÌ†µÌºÜÌ†µÌ±°
                                         Ì†µÌ±É(Ì†µÌ±á > Ì†µÌ±°) = Ì†µÌ±É(Ì†µÌ±Å = 0) = Ì†µÌ±í     =Ì†µÌ±í
                                                         Ì†µÌ±°
                                                                       0!
                 where Ì†µÌ±Å is the number of events occurred in [0,Ì†µÌ±°], which follows a Poisson distribution. The
                         Ì†µÌ±°
                 CDFof Ì†µÌ±á is
                                                                         ‚àíÌ†µÌºÜÌ†µÌ±°
                                              Ì†µÌ∞π(Ì†µÌ±°) = 1 ‚àí Ì†µÌ±É(Ì†µÌ±á > Ì†µÌ±°) = 1 ‚àí Ì†µÌ±í
                 The PDF of Ì†µÌ±á is
                                                            ‚Ä≤       ‚àíÌ†µÌºÜÌ†µÌ±°
                                                   Ì†µÌ±ì(Ì†µÌ±°) = Ì†µÌ∞π (Ì†µÌ±°) = Ì†µÌºÜÌ†µÌ±í
                 This indicates Ì†µÌ±á ‚àº Expo(Ì†µÌºÜ).
                                                           164
        #
        # Simulate random arrivals and inter-arrival time
        #
        T <- 1000 # Total time horizon
        rate <- 1 # rate of occurrence per unit time
        # Total number of arrivals
        n_arrivals <- rpois(1, lambda = rate * T)
        # Time of each arrivals
        t_arrivals <- sort(runif(n_arrivals, min = 0, max = T))
        # Plot the timeline of arrivals
        plot(t_arrivals[1:20], rep(1, 20), type = "h", col = "red", ann = F)
         1.4
         1.2
         1.0
         0.8
         0.6
           0       5       10      15
        # Compute inter-arrival time
        inter_arrival_time <- diff(t_arrivals)
        # Plot the distribution of inter-arrival time
        hist(inter_arrival_time, prob = TRUE, breaks = 20)
        # Overlay the exponential function
        curve(exp(-x), col = "red", add = TRUE)
                            165
                  Histogram of inter_arrival_time
            0.8
            0.6
         Density0.4
            0.2
            0.0
              0   1   2   3   4   5    6
                       inter_arrival_time
                            166
                           50 Gamma distribution
                          The Gamma distribution is a continuous distribution on the positive real line; it is a general-
                           ization of the Exponential distribution. While an Exponential RV represents the waiting time
                           for the first event to occur, we shall see that a Gamma RV represents the total waiting time
                           for Ì†µÌ±õ events to occur.
                           Let‚Äôs start with a simple case. Suppose we want to find out the total waiting until the 2nd
                           event occurred. Let Ì†µÌ±å = Ì†µÌ±ã + Ì†µÌ±ã where Ì†µÌ±ã ,Ì†µÌ±ã ‚àº Expo(Ì†µÌºÜ) independently. If Ì†µÌ±å is discrete,
                                                                    1        2               1     2
                                                              Ì†µÌ±¶
                          we have Ì†µÌ±É(Ì†µÌ±å = Ì†µÌ±¶) = ‚àë                  Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±ò,Ì†µÌ±ã = Ì†µÌ±¶ ‚àí Ì†µÌ±ò). For continuous Ì†µÌ±¶, we have
                                                                          1           2
                                                              Ì†µÌ±ò=0
                                                                       Ì†µÌ±¶                                   Ì†µÌ±¶
                                                                                                                  ‚àíÌ†µÌºÜÌ†µÌ±•    ‚àíÌ†µÌºÜ(Ì†µÌ±¶‚àíÌ†µÌ±•)
                                                       Ì†µÌ±ì  (Ì†µÌ±¶) = ‚à´ Ì†µÌ±ì (Ì†µÌ±•)Ì†µÌ±ì (Ì†µÌ±¶ ‚àí Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±• = ‚à´ Ì†µÌºÜÌ†µÌ±í                   Ì†µÌºÜÌ†µÌ±í          Ì†µÌ±ëÌ†µÌ±•
                                                         Ì†µÌ±å                Ì†µÌ±ã       Ì†µÌ±ã
                                                                     0                                    0
                                                                       Ì†µÌ±¶
                                                                           2 ‚àíÌ†µÌºÜÌ†µÌ±¶            2 ‚àíÌ†µÌºÜÌ†µÌ±¶
                                                                =‚à´ Ì†µÌºÜ Ì†µÌ±í            Ì†µÌ±ëÌ†µÌ±• = Ì†µÌºÜ Ì†µÌ±í      Ì†µÌ±¶.
                                                                     0
                           If there is a third variable,
                                                                 Ì†µÌ±ß                                   Ì†µÌ±ß
                                                                                                            ‚àíÌ†µÌºÜÌ†µÌ±•  2 ‚àíÌ†µÌºÜ(Ì†µÌ±ß‚àíÌ†µÌ±•)
                                                 Ì†µÌ±ì  (Ì†µÌ±ß) = ‚à´ Ì†µÌ±ì (Ì†µÌ±•)Ì†µÌ±ì (Ì†µÌ±ß ‚àí Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±• = ‚à´ Ì†µÌºÜÌ†µÌ±í                   Ì†µÌºÜ Ì†µÌ±í          (Ì†µÌ±ß ‚àí Ì†µÌ±•)Ì†µÌ±ëÌ†µÌ±•
                                                   Ì†µÌ±ç                Ì†µÌ±ã       Ì†µÌ±å
                                                               0                                    0
                                                                            Ì†µÌ±ß
                                                                3 ‚àíÌ†µÌºÜÌ†µÌ±ß                           3 ‚àíÌ†µÌºÜÌ†µÌ±ß 2
                                                          =Ì†µÌºÜ Ì†µÌ±í         ‚à´(Ì†µÌ±ß‚àíÌ†µÌ±•)Ì†µÌ±ëÌ†µÌ±• = Ì†µÌºÜ Ì†µÌ±í             Ì†µÌ±ß  /2.
                                                                          0
                          The general pattern is the Gamma distribution.
                           Definition 50.1 (Exponential distribution). An random variable X is said to have the Gamma
                           distribution with parameters Ì†µÌ±é and Ì†µÌºÜ, Ì†µÌ±é > 0 and Ì†µÌºÜ > 0, if it has the PDF
                                                                                        Ì†µÌ±é
                                                                                      Ì†µÌºÜ
                                                                                              Ì†µÌ±é‚àí1 ‚àíÌ†µÌºÜÌ†µÌ±•
                                                                         Ì†µÌ±ì(Ì†µÌ±•) =           Ì†µÌ±•     Ì†µÌ±í    ,    Ì†µÌ±• > 0
                                                                                     Œì(Ì†µÌ±é)
                          Wewrite Ì†µÌ±ã ‚àº Gamma(Ì†µÌ±é,Ì†µÌºÜ).
                          Verify this is a valid PDF:
                                                       ‚àû                                                 ‚àû
                                                             1                   Ì†µÌ±ëÌ†µÌ±•           1                    Ì†µÌ±ëÌ†µÌ±¢      Œì(Ì†µÌ±é)
                                                                                      Ì†µÌ±¢=Ì†µÌºÜÌ†µÌ±•
                                                                        Ì†µÌ±é ‚àíÌ†µÌºÜÌ†µÌ±•                              Ì†µÌ±é ‚àíÌ†µÌ±¢
                                                   ‚à´              (Ì†µÌºÜÌ†µÌ±•) Ì†µÌ±í             =            ‚à´ Ì†µÌ±¢ Ì†µÌ±í               =          =1.
                                                           Œì(Ì†µÌ±é)                  Ì†µÌ±•          Œì(Ì†µÌ±é)                   Ì†µÌ±¢       Œì(Ì†µÌ±é)
                                                     0                                                 0
                                                                                             167
                                                                                                                                              ‚àíÌ†µÌºÜÌ†µÌ±•
                                    Taking Ì†µÌ±é = 1, the Gamma(1,Ì†µÌºÜ) PDF is Ì†µÌ±ì(Ì†µÌ±•) = Ì†µÌºÜÌ†µÌ±í                                                              , which is the same as Expo(Ì†µÌºÜ). So
                                    Exponential distribution is a special case of Gamma distribution.
                                    Let‚Äôs find the expectation and variance of the Gamma distribution. Let Ì†µÌ±å ‚àº Gamma(Ì†µÌ±é,1).
                                    Recall Œì function has the property Œì(Ì†µÌ±é + 1) = Ì†µÌ±éŒì(Ì†µÌ±é).
                                                                                  ‚àû                                                              ‚àû
                                                                                                1                                    1                                      Œì(Ì†µÌ±é + 1)
                                                                                                         Ì†µÌ±é‚àí1 ‚àíÌ†µÌ±¶                                       Ì†µÌ±é  ‚àíÌ†µÌ±¶
                                                             Ì†µÌ∞∏(Ì†µÌ±å ) = ‚à´               Ì†µÌ±¶ ‚ãÖ            Ì†µÌ±¶      Ì†µÌ±í     Ì†µÌ±ëÌ†µÌ±¶ =                ‚à´ Ì†µÌ±¶ Ì†µÌ±í              Ì†µÌ±ëÌ†µÌ±¶ =                        =Ì†µÌ±é.
                                                                                             Œì(Ì†µÌ±é)                                Œì(Ì†µÌ±é)                                         Œì(Ì†µÌ±é)
                                                                               0                                                              0
                                    Apply LOTUS to evaluate the second moment:
                                                                         ‚àû                                                               ‚àû
                                                                                         1                                    1                                          Œì(Ì†µÌ±é + 2)
                                                          2                     2                 Ì†µÌ±é‚àí1 ‚àíÌ†µÌ±¶                                       Ì†µÌ±é+1 ‚àíÌ†µÌ±¶
                                                 Ì†µÌ∞∏(Ì†µÌ±å ) = ‚à´                  Ì†µÌ±¶   ‚ãÖ           Ì†µÌ±¶       Ì†µÌ±í     Ì†µÌ±ëÌ†µÌ±¶ =                ‚à´ Ì†µÌ±¶              Ì†µÌ±í     Ì†µÌ±ëÌ†µÌ±¶ =                        =(Ì†µÌ±é+1)Ì†µÌ±é.
                                                                                      Œì(Ì†µÌ±é)                                Œì(Ì†µÌ±é)                                             Œì(Ì†µÌ±é)
                                                                      0                                                                0
                                    Therefore,
                                                                                                                                                   2
                                                                                                     Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±å ) = (Ì†µÌ±é + 1)Ì†µÌ±é ‚àí Ì†µÌ±é                   =Ì†µÌ±é.
                                    So for Ì†µÌ±å ‚àº Gamma(Ì†µÌ±é,1), Ì†µÌ∞∏(Ì†µÌ±å) = Ì†µÌ±âÌ†µÌ±éÌ†µÌ±ü(Ì†µÌ±å) = Ì†µÌ±é. For the general case Ì†µÌ±ã ‚àº Gamma(Ì†µÌ±é,Ì†µÌºÜ), we
                                                                              Ì†µÌ±å
                                    now show that Ì†µÌ±ã =                           . Note that
                                                                              Ì†µÌºÜ
                                                                                    Ì†µÌ∞π    (Ì†µÌ±•) = Ì†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±•) = Ì†µÌ±É(Ì†µÌ±å ‚â§ Ì†µÌ±•/Ì†µÌºÜ) = Ì†µÌ∞π (Ì†µÌ±•/Ì†µÌºÜ)
                                                                                      Ì†µÌ±ã                                                                          Ì†µÌ±å
                                                                                                      Ì†µÌ±ëÌ†µÌ∞π            Ì†µÌºïÌ†µÌ∞π      Ì†µÌ±ëÌ†µÌ±¶
                                                                                                            Ì†µÌ±ã              Ì†µÌ±å
                                                                                    Ì†µÌ±ì   (Ì†µÌ±•) =                  =                     =Ì†µÌ±ì (Ì†µÌ±¶)Ì†µÌºÜ
                                                                                      Ì†µÌ±ã                                                      Ì†µÌ±å
                                                                                                        Ì†µÌ±ëÌ†µÌ±•            Ì†µÌºïÌ†µÌ±¶    Ì†µÌ±ëÌ†µÌ±•
                                    Therefore,
                                                                                                                                                   Ì†µÌ±é
                                                                                                               1                                Ì†µÌºÜ
                                                                                                                       Ì†µÌ±é‚àí1 ‚àíÌ†µÌ±¶                           Ì†µÌ±é‚àí1 ‚àíÌ†µÌºÜÌ†µÌ±•
                                                                                         Ì†µÌ±ì    (Ì†µÌ±•) =                Ì†µÌ±¶       Ì†µÌ±í     Ì†µÌºÜ =              Ì†µÌ±•        Ì†µÌ±í       .
                                                                                           Ì†µÌ±ã
                                                                                                           Œì(Ì†µÌ±é)                              Œì(Ì†µÌ±é)
                                                                                      Ì†µÌ±é                           Ì†µÌ±é
                                    Hence, we have Ì†µÌ∞∏(Ì†µÌ±ã) =                              , Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã) =               .
                                                                                                                     2
                                                                                      Ì†µÌºÜ                          Ì†µÌºÜ
                                    Theorem 50.1 (Exponential-Gamma connection). Let Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã be independent and identi-
                                                                                                                                                         1              Ì†µÌ±õ
                                   cal Expo(Ì†µÌºÜ). Then
                                                                                                   Ì†µÌ±ã +‚ãØ+Ì†µÌ±ã ‚àºGamma(Ì†µÌ±õ,Ì†µÌºÜ).
                                                                                                       1                    Ì†µÌ±õ
                                    Proof. Let‚Äôs prove by showing the MGFs are equivalent.
                                                                                                                       ‚àû
                                                                                                                                                            Ì†µÌºÜ
                                                                                                     Ì†µÌ±°Ì†µÌ±ã                    Ì†µÌ±°Ì†µÌ±•     ‚àíÌ†µÌºÜÌ†µÌ±•
                                                                           Ì†µÌ±Ä (Ì†µÌ±°) = Ì†µÌ∞∏(Ì†µÌ±í                ) = ‚à´ Ì†µÌ±í Ì†µÌºÜÌ†µÌ±í                      Ì†µÌ±ëÌ†µÌ±• =                     for Ì†µÌ±° < Ì†µÌºÜ
                                                                               Ì†µÌ±ã
                                                                                                                                                         Ì†µÌºÜ ‚àí Ì†µÌ±°
                                                                                                                    0
                                                                                                                                                                                Ì†µÌ±õ
                                                                                                                                                            Ì†µÌ±õ
                                                                                                                                                                         Ì†µÌºÜ
                                    Thus, the MGF of Ì†µÌ±å = Ì†µÌ±ã +‚ãØ+Ì†µÌ±ã is Ì†µÌ±Ä (Ì†µÌ±°) = (Ì†µÌ±Ä (Ì†µÌ±°)) = (                                                                                 ) . We verify this is the
                                                                                         1                    Ì†µÌ±õ            Ì†µÌ±å                   Ì†µÌ±ã
                                                                                                                                                                        Ì†µÌºÜ‚àíÌ†µÌ±°
                                    MGFofaGammadistribution. Suppose Ì†µÌ±å ‚àº Gamma(Ì†µÌ±õ,Ì†µÌºÜ), it has MGF:
                                                                                                                               168
                                                                                                                                              ‚àû
                                                                                                                                                                Ì†µÌ±õ
                                                                                                                                                             Ì†µÌºÜ
                                                                                                                         Ì†µÌ±°Ì†µÌ±å                         Ì†µÌ±°Ì†µÌ±¶               Ì†µÌ±õ‚àí1 ‚àíÌ†µÌºÜÌ†µÌ±¶
                                                                                        Ì†µÌ±Ä (Ì†µÌ±°) = Ì†µÌ∞∏(Ì†µÌ±í                       ) = ‚à´ Ì†µÌ±í                                Ì†µÌ±¶          Ì†µÌ±í         Ì†µÌ±ëÌ†µÌ±¶
                                                                                              Ì†µÌ±å
                                                                                                                                                           Œì(Ì†µÌ±é)
                                                                                                                                          0
                                                                                                                                           ‚àû
                                                                                                                          Ì†µÌ±õ
                                                                                                                       Ì†µÌºÜ                            1
                                                                                                                                                                                     Ì†µÌ±õ‚àí1 ‚àí(Ì†µÌºÜ‚àíÌ†µÌ±°)Ì†µÌ±¶
                                                                                                         =                           ‚à´                       ((Ì†µÌºÜ ‚àí Ì†µÌ±°)Ì†µÌ±¶)                   Ì†µÌ±í                 (Ì†µÌºÜ ‚àí Ì†µÌ±°)Ì†µÌ±ëÌ†µÌ±¶
                                                                                                                                 Ì†µÌ±õ
                                                                                                                (Ì†µÌºÜ ‚àí Ì†µÌ±°)                        Œì(Ì†µÌ±é)
                                                                                                                                        0
                                                                                                                                           ‚àû
                                                                                                                          Ì†µÌ±õ
                                                                                                                       Ì†µÌºÜ                            1
                                                                                                                                                                Ì†µÌ±õ‚àí1 ‚àíÌ†µÌ±¢
                                                                                                         =                           ‚à´                       Ì†µÌ±¢          Ì†µÌ±í      Ì†µÌ±ëÌ†µÌ±¢              Ì†µÌ±¢ = (Ì†µÌºÜ ‚àí Ì†µÌ±°)Ì†µÌ±¶
                                                                                                                                 Ì†µÌ±õ
                                                                                                                (Ì†µÌºÜ ‚àí Ì†µÌ±°)                        Œì(Ì†µÌ±é)
                                                                                                                                        0
                                                                                                                                    Ì†µÌ±õ
                                                                                                                        Ì†µÌºÜ
                                                                                                         =(                      ) .
                                                                                                                    Ì†µÌºÜ ‚àí Ì†µÌ±°
                                            Thus, if Ì†µÌ±ã represents the i.i.d inter-arrival time. Ì†µÌ±å has the interpretation of the arrival time
                                                                      Ì†µÌ±ñ
                                            until the Ì†µÌ±õ-th event.
                                                                                                     Ì†µÌ±õ                     Ì†µÌ±õ
                                                                                      Ì†µÌ±å = ‚àëÌ†µÌ±ã =‚àë(time of the i-th arrival) ‚àº Gamma(Ì†µÌ±õ,Ì†µÌºÜ).
                                                                                                                Ì†µÌ±ñ
                                                                                                   Ì†µÌ±ñ=1                   Ì†µÌ±ñ=1
                                            Example 50.1 (Service time in a queue). Customer Ì†µÌ±ñ must wait time Ì†µÌ±ã for service once
                                                                                                                                                                                                                                        Ì†µÌ±ñ
                                            reaching the head of the queue. The average service rate is 1 customer per 10 minutes. Assume
                                            the service for each customer is independent. If you are the 5th in the queue. What is the
                                            expected waiting to be served?
                                            Solution. Ì†µÌ±ã ‚àº Expo(0.1). Then Ì†µÌ∞∏(Ì†µÌ±ã ) = 10. Let Y be the time until you are served. Then
                                                                         Ì†µÌ±ñ                                                                Ì†µÌ±ñ
                                                                                                                                              5
                                            Ì†µÌ±å     ‚àº Gamma(5,0.1). Thus, Ì†µÌ∞∏(Ì†µÌ±å) =                                                                    = 50 minutes. The probabilities of some selected
                                                                                                                                            0.1
                                            values:
                                                                                                                                                           ‚éß
                                                                                                                                                                5%              Ì†µÌ±° = 20
                                                                                                                                                           {
                                                                                                                            Ì†µÌ±É(Ì†µÌ±å ‚â§ Ì†µÌ±°) =                                                        .
                                                                                                                                                                18% Ì†µÌ±° = 30
                                                                                                                                                           ‚é®
                                                                                                                                                           {
                                                                                                                                                                71% Ì†µÌ±° = 60
                                                                                                                                                           ‚é©
                                                                                                                                                           169
                     51 Beta distribution
                     The Beta distribution is a continuous distribution on the interval (0,1). It is a generalization
                     of the Unif(0,1) distribution, allowing the PDF to be non-constant on (0,1).
                     Definition 51.1 (Beta distribution). A random variable Ì†µÌ±ã is said to have the Beta distribu-
                     tion with parameters Ì†µÌ±é and Ì†µÌ±è, Ì†µÌ±é > 0 and Ì†µÌ±è > 0, if its PDF is
                                                               1
                                                                      Ì†µÌ±é‚àí1        Ì†µÌ±è‚àí1
                                                   Ì†µÌ±ì(Ì†µÌ±•) =         Ì†µÌ±•    (1‚àíÌ†µÌ±•)     ,   0 < Ì†µÌ±• < 1
                                                            Ì†µÌªΩ(Ì†µÌ±é, Ì†µÌ±è)
                     where the constant Ì†µÌªΩ(Ì†µÌ±é,Ì†µÌ±è) is chosen to make the PDF integrate to 1. We write this as Ì†µÌ±ã ‚àº
                     Beta(Ì†µÌ±é,Ì†µÌ±è).
                     TheBetadistribution takes different shapes for different Ì†µÌ±é and Ì†µÌ±è values. Here are some general
                     patterns:
                        ‚Ä¢ If Ì†µÌ±é = Ì†µÌ±è = 1, the Beta(1,1) PDF is constant on (0,1), equivalent to Unif(0,1).
                        ‚Ä¢ If Ì†µÌ±é < 1 and Ì†µÌ±è < 1, the PDF is U-shaped and opens upward. If Ì†µÌ±é > 1 and Ì†µÌ±è > 1, the
                            PDFopens downward.
                        ‚Ä¢ If Ì†µÌ±é = Ì†µÌ±è, the PDF is symmetric about 1/2. If Ì†µÌ±é > Ì†µÌ±è, the PDF favors values larger than
                            1/2. If Ì†µÌ±é < Ì†µÌ±è, the PDF favors values smaller than 1/2.
                     To make the PDF integrates to 1, the constant Ì†µÌªΩ(Ì†µÌ±é,Ì†µÌ±è) has to satisfy
                                                                        1
                                                                           Ì†µÌ±é‚àí1        Ì†µÌ±è‚àí1
                                                          Ì†µÌªΩ(Ì†µÌ±é, Ì†µÌ±è) = ‚à´ Ì†µÌ±•    (1‚àíÌ†µÌ±•)     Ì†µÌ±ëÌ†µÌ±•.
                                                                      0
                     Wenowtry to find this integral:
                                                                          170
                                                        1
                                                            Ì†µÌ±é‚àí1        Ì†µÌ±è‚àí1
                                           Ì†µÌªΩ(Ì†µÌ±é, Ì†µÌ±è) = ‚à´ Ì†µÌ±•    (1‚àíÌ†µÌ±•)      Ì†µÌ±ëÌ†µÌ±•
                                                           ‚èü
                                                                ‚èü‚èü‚èü‚èü‚èü
                                                       0
                                                            Ì†µÌ±ì
                                                                      ‚Ä≤
                                                                     Ì†µÌ±î
                                                                          1
                                                                                 1
                                                                       Ì†µÌ±è                               Ì†µÌ±è
                                                               (1‚àíÌ†µÌ±•)                           (1‚àíÌ†µÌ±•)
                                                           Ì†µÌ±é‚àí1                             Ì†µÌ±é‚àí2
                                                   =[‚àíÌ†µÌ±•                ] +‚à´ (Ì†µÌ±é‚àí1)Ì†µÌ±•                    Ì†µÌ±ëÌ†µÌ±•
                                                                   Ì†µÌ±è                               Ì†µÌ±è
                                                                               0
                                                                          0
                                                      Ì†µÌ±é ‚àí 1
                                                   =        Ì†µÌªΩ(Ì†µÌ±é ‚àí 1,Ì†µÌ±è + 1)
                                                        Ì†µÌ±è
                                                      Ì†µÌ±é ‚àí 1   Ì†µÌ±é ‚àí 2
                                                   =         ‚ãÖ       Ì†µÌªΩ(Ì†µÌ±é ‚àí 2,Ì†µÌ±è + 2)
                                                        Ì†µÌ±è     Ì†µÌ±è + 1
                                                      Ì†µÌ±é ‚àí 1   Ì†µÌ±é ‚àí 2  Ì†µÌ±é ‚àí 3
                                                   =         ‚ãÖ       ‚ãÖ       Ì†µÌªΩ(Ì†µÌ±é ‚àí 3,Ì†µÌ±è + 3)
                                                        Ì†µÌ±è     Ì†µÌ±è + 1  Ì†µÌ±è + 2
                                                   ‚ãÆ
                                                                  (Ì†µÌ±é ‚àí 1)!
                                                   =                                   Ì†µÌªΩ(1,Ì†µÌ±é + Ì†µÌ±è ‚àí 1)
                                                                                       ‚èü‚èü‚èü‚èü‚èü‚èü‚èü
                                                      Ì†µÌ±è(Ì†µÌ±è + 1)(Ì†µÌ±è + 2)‚ãØ(Ì†µÌ±è + Ì†µÌ±é ‚àí 2)
                                                                                               1
                                                                                             Ì†µÌ±é+Ì†µÌ±è‚àí1
                                                      (Ì†µÌ±é ‚àí 1)!       1
                                                   =            ‚ãÖ
                                                      (Ì†µÌ±è+Ì†µÌ±é‚àí2)!
                                                                 Ì†µÌ±é + Ì†µÌ±è ‚àí 1
                                                        (Ì†µÌ±è‚àí1)!
                                                      (Ì†µÌ±é ‚àí 1)!(Ì†µÌ±è ‚àí 1)!
                                                   =
                                                        (Ì†µÌ±é + Ì†µÌ±è ‚àí 1)!
                                                      Œì(Ì†µÌ±é)Œì(Ì†µÌ±è)
                                                   =            .
                                                      Œì(Ì†µÌ±é + Ì†µÌ±è)
                     Example51.1. LetÌ†µÌ±ã ,‚Ä¶,Ì†µÌ±ã beindependentrandomvariableswiththeuniformdistribution
                                               1        Ì†µÌ±õ
                     on the interval [0,1]. Find the distribution of Ì†µÌ±å = max(Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã ).
                                                                                         1       Ì†µÌ±õ
                     Solution. Let‚Äôs find the CDF of Ì†µÌ±å:
                                               Ì†µÌ±É(Ì†µÌ±å ‚â§ Ì†µÌ±¶) = Ì†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±¶ ‚à© Ì†µÌ±ã ‚â§ Ì†µÌ±¶ ‚à© ‚ãØ ‚à© Ì†µÌ±ã ‚â§ Ì†µÌ±¶)
                                                                    1          2               Ì†µÌ±õ
                                                           Ì†µÌ±ñÌ†µÌ±ñÌ†µÌ±ë
                                                            =Ì†µÌ±É(Ì†µÌ±ã ‚â§Ì†µÌ±¶)Ì†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±¶)‚ãØÌ†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±¶)
                                                                    1            2              Ì†µÌ±õ
                                                                Ì†µÌ±õ
                                                           =Ì†µÌ±¶
                     for Ì†µÌ±¶ ‚àà [0,1]. Hence,
                                                                              ‚éß
                                                                                 0    Ì†µÌ±¶ < 0
                                                                              {
                                                                                  Ì†µÌ±õ
                                                     Ì†µÌ∞π  (Ì†µÌ±¶) = Ì†µÌ±É(Ì†µÌ±å ‚â§ Ì†µÌ±¶) =
                                                                                 Ì†µÌ±¶   0 ‚â§ Ì†µÌ±¶ ‚â§ 1
                                                       Ì†µÌ±å
                                                                              ‚é®
                                                                              {
                                                                                 1    Ì†µÌ±¶ > 1
                                                                              ‚é©
                     The PDF of Ì†µÌ±å is
                                                                               Ì†µÌ±õ‚àí1
                                                                            Ì†µÌ±õÌ†µÌ±¶      0 ‚â§ Ì†µÌ±¶ ‚â§ 1
                                                                  ‚Ä≤
                                                     Ì†µÌ±ì  (Ì†µÌ±¶) = Ì†µÌ∞π (Ì†µÌ±¶) = {
                                                       Ì†µÌ±å
                                                                 Ì†µÌ±å
                                                                            0         otherwise
                     Thus, Ì†µÌ±å ‚àº Ì†µÌ∞µÌ†µÌ±íÌ†µÌ±°Ì†µÌ±é(Ì†µÌ±õ,1).
                                                                         171
                     Beta distributions are often used as priors for parameters in Bayesian inference. We do not
                     cover Bayesian inference in this book. Nonetheless we illustrate this with an example.
                     Example 51.2 (Beta-Binomial conjugacy). We have a coin that lands Heads with probability
                     Ì†µÌ±ù, but we don‚Äôt know what Ì†µÌ±ù is. Our goal is to infer the value of Ì†µÌ±ù after observing the outcomes
                     of Ì†µÌ±õ tosses of the coin. The larger that Ì†µÌ±õ is, the more accurately we should be able to estimate
                     Ì†µÌ±ù.
                     Solution. We model the unknown parameter Ì†µÌ±ù as a Beta distribution, Ì†µÌ±ù ‚àº Beta(Ì†µÌ±é,Ì†µÌ±è). Since
                    we are completely ignorant about this Ì†µÌ±ù, we can also model it as the uniform distribution. But
                    we will see that using the Beta distribution is even simpler than the uniform distribution. Let
                     Ì†µÌ±ã be the number of heads in Ì†µÌ±õ tosses of the coin. Then
                                                                 Ì†µÌ±ã|Ì†µÌ±ù ‚àº Bin(Ì†µÌ±õ,Ì†µÌ±ù)
                    Apply the Bayes‚Äô rule to inverse the conditioning:
                                                             Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±ò|Ì†µÌ±ù)Ì†µÌ±ì(Ì†µÌ±ù)
                                            Ì†µÌ±ì(Ì†µÌ±ù|Ì†µÌ±ã = Ì†µÌ±ò) =
                                                                 Ì†µÌ±É(Ì†µÌ±ã = Ì†µÌ±ò)
                                                              Ì†µÌ±õ
                                                                                   1
                                                                  Ì†µÌ±ò       Ì†µÌ±õ‚àíÌ†µÌ±ò        Ì†µÌ±é‚àí1        Ì†µÌ±è‚àí1
                                                             ( )Ì†µÌ±ù (1 ‚àí Ì†µÌ±ù)     ‚ãÖ      Ì†µÌ±ù   (1‚àíÌ†µÌ±ù)
                                                              Ì†µÌ±ò                 Ì†µÌªΩ(Ì†µÌ±é,Ì†µÌ±è)
                                                          =
                                                                     1
                                                                        Ì†µÌ±õ
                                                                            Ì†µÌ±ò       Ì†µÌ±õ‚àíÌ†µÌ±ò
                                                                    ‚à´
                                                                       ( )Ì†µÌ±ù (1 ‚àí Ì†µÌ±ù)    Ì†µÌ±ì(Ì†µÌ±ù)Ì†µÌ±ëÌ†µÌ±ù
                                                                        Ì†µÌ±ò
                                                                     0
                                                              Ì†µÌ±é+Ì†µÌ±ò‚àí1       Ì†µÌ±è+Ì†µÌ±õ‚àíÌ†µÌ±ò‚àí1
                                                          ‚àùÌ†µÌ±ù       (1‚àíÌ†µÌ±ù)
                    This the kernel of Beta(Ì†µÌ±é + Ì†µÌ±ò,Ì†µÌ±è + Ì†µÌ±õ ‚àí Ì†µÌ±ò). The rest is just a normalizing constant. Therefore,
                                                       Ì†µÌ±ù|Ì†µÌ±ã = Ì†µÌ±ò ‚àº Beta(Ì†µÌ±é + Ì†µÌ±ò,Ì†µÌ±è + Ì†µÌ±õ ‚àí Ì†µÌ±ò).
                    The posterior distribution of Ì†µÌ±ù after observing Ì†µÌ±ã = Ì†µÌ±ò is still a Beta distribution! This is a
                     special relationship between the Beta and Binomial distributions called conjugacy: if we have
                     a Beta prior distribution on Ì†µÌ±ù and data that are conditionally Binomial given Ì†µÌ±ù, then when
                     going from prior to posterior, we don‚Äôt leave the family of Beta distributions. We say that the
                     Beta is the conjugate prior of the Binomial.
                                                                        172
                           52 Multivariate problems
                           Weextend the concepts of joint, marginal and conditional distribution to continuous random
                           variables.
                                                                                                                                                             2       2
                           Example 52.1. Suppose Ì†µÌ±ã and Ì†µÌ±å are uniformly distributed on a disk {(Ì†µÌ±•,Ì†µÌ±¶) ‚à∂ Ì†µÌ±• + Ì†µÌ±¶ ‚â§
                           1}. Find the joint PDF, marginal distributions and conditional distributions. Are Ì†µÌ±ã and Ì†µÌ±å
                           independent?
                           Solution. The area of the disk is Ì†µÌºã, therefore
                                                                                                 1
                                                                                                        2       2
                                                                                                      Ì†µÌ±•  +Ì†µÌ±¶ ‚â§1
                                                                                                Ì†µÌºã
                                                                              Ì†µÌ±ì(Ì†µÌ±•, Ì†µÌ±¶) = {
                                                                                                0     otherwise
                           The marginal distributions are
                                                                                                173
                                                                                         ‚àö
                                                                                                 2
                                                                                           1+Ì†µÌ±•
                                                                                                                    ‚àö
                                                                                                    1            2
                                                                                                                                2
                                                                     Ì†µÌ±ì   (Ì†µÌ±•) = ‚à´                    Ì†µÌ±ëÌ†µÌ±¶ =           1‚àíÌ†µÌ±• ,               ‚àí1‚â§Ì†µÌ±•‚â§1
                                                                       Ì†µÌ±ã
                                                                                        ‚àö
                                                                                                    Ì†µÌºã           Ì†µÌºã
                                                                                                2
                                                                                      ‚àí 1‚àíÌ†µÌ±•
                                                                                                 2
                                                                                         ‚àö
                                                                                           1+Ì†µÌ±¶
                                                                                                    1            2
                                                                                                                                 2
                                                                                                                    ‚àö
                                                                     Ì†µÌ±ì   (Ì†µÌ±¶) = ‚à´                     Ì†µÌ±ëÌ†µÌ±• =           1‚àíÌ†µÌ±¶ ,               ‚àí1‚â§Ì†µÌ±¶‚â§1
                                                                       Ì†µÌ±å
                                                                                                    Ì†µÌºã           Ì†µÌºã
                                                                                                 2
                                                                                        ‚àö
                                                                                      ‚àí 1‚àíÌ†µÌ±¶
                                The conditional distributions are
                                                                                                                           1
                                                                                                 Ì†µÌ±ì(Ì†µÌ±•, Ì†µÌ±¶)                                       1
                                                                                                                           Ì†µÌºã
                                                                                                                      ‚àö                      ‚àö
                                                                          Ì†µÌ±ì      (Ì†µÌ±¶|Ì†µÌ±•) =                   =                       =
                                                                            Ì†µÌ±å |Ì†µÌ±ã
                                                                                                                    2
                                                                                                                                   2                      2
                                                                                                  Ì†µÌ±ì   (Ì†µÌ±•)
                                                                                                                         1‚àíÌ†µÌ±•              2 1‚àíÌ†µÌ±•
                                                                                                    Ì†µÌ±ã
                                                                                                                    Ì†µÌºã
                                                                             ‚àö               ‚àö
                                                                                          2               2
                                Therefore, Ì†µÌ±å |Ì†µÌ±ã ‚àº Unif(‚àí 1‚àíÌ†µÌ±• ,                                1‚àíÌ†µÌ±• ).
                                Since Ì†µÌ±ì(Ì†µÌ±•,Ì†µÌ±¶) ‚â† Ì†µÌ±ì (Ì†µÌ±•)Ì†µÌ±ì (Ì†µÌ±¶), Ì†µÌ±ã and Ì†µÌ±å are not independent. This is because knowing the value
                                                              Ì†µÌ±ã        Ì†µÌ±å
                                of Ì†µÌ±ã constrains the value of Ì†µÌ±å .
                                                                                        Ì†µÌ±ñÌ†µÌ±ñÌ†µÌ±ë
                                                                                                                                                                       1
                                Example 52.2. Suppose Ì†µÌ±ã,Ì†µÌ±å ‚àº Unif(0,1). Find the probability Ì†µÌ±É (Ì†µÌ±å ‚â§                                                                     ).
                                                                                                                                                                      2Ì†µÌ±ã
                                Solution. The joint distribution is
                                                                                                        1 0‚â§Ì†µÌ±•‚â§1,0‚â§Ì†µÌ±¶‚â§1
                                                                                   Ì†µÌ±ì(Ì†µÌ±•, Ì†µÌ±¶) = {
                                                                                                        0 otherwise
                                                                            1/2       1                     1      1/2Ì†µÌ±•                               1
                                                                                                                                                                                     ‚àö
                                                           1                                                                                1               1             1
                                          Ì†µÌ±É (Ì†µÌ±å ‚â§              )=‚à´ ‚à´ 1Ì†µÌ±ëÌ†µÌ±¶Ì†µÌ±ëÌ†µÌ±•+‚à´ ‚à´                                        1Ì†µÌ±ëÌ†µÌ±¶Ì†µÌ±ëÌ†µÌ±• =         +‚à´              Ì†µÌ±ëÌ†µÌ±• =        +ln 2.
                                                          2Ì†µÌ±ã                                                                               2              2Ì†µÌ±•            2
                                                                          0        0                      1/2 0                                      1/2
                                                                                Ì†µÌ±ñÌ†µÌ±ñÌ†µÌ±ë
                                Example 52.3. For Ì†µÌ±ã,Ì†µÌ±å ‚àº Unif(0,1), find Ì†µÌ∞∏(|Ì†µÌ±ã ‚àíÌ†µÌ±å|).
                                Solution. Apply 2D LOTUS:
                                                                                               1      1
                                                                  Ì†µÌ∞∏(|Ì†µÌ±ã ‚àí Ì†µÌ±å |) =‚à´ ‚à´ |Ì†µÌ±•‚àíÌ†µÌ±¶|Ì†µÌ±ëÌ†µÌ±•Ì†µÌ±ëÌ†µÌ±¶
                                                                                             0     0
                                                                                               1      1                               1      Ì†µÌ±¶
                                                                                       =‚à´ ‚à´(Ì†µÌ±•‚àíÌ†µÌ±¶)Ì†µÌ±ëÌ†µÌ±•Ì†µÌ±ëÌ†µÌ±¶ +‚à´ ‚à´ (Ì†µÌ±¶‚àíÌ†µÌ±•)Ì†µÌ±ëÌ†µÌ±•Ì†µÌ±ëÌ†µÌ±¶
                                                                                             0     Ì†µÌ±¶                               0     0
                                                                                                 1      1
                                                                                       =2‚à´ ‚à´ (Ì†µÌ±•‚àíÌ†µÌ±¶)Ì†µÌ±ëÌ†µÌ±•Ì†µÌ±ëÌ†µÌ±¶
                                                                                               0      Ì†µÌ±¶
                                                                                           1
                                                                                       = .
                                                                                           3
                                                                                                                174
                                             Ì†µÌ±ñÌ†µÌ±ñÌ†µÌ±ë
                    Example 52.4. Ì†µÌ±ã,Ì†µÌ±å ‚àº Ì†µÌ±Å(0,1), find Ì†µÌ∞∏(|Ì†µÌ±ã ‚àíÌ†µÌ±å|).
                    Solution. Since the sum or difference of independent Normal variables is also Normal, Ì†µÌ±ã‚àíÌ†µÌ±å ‚àº
                                                                                          ‚àö
                    Ì†µÌ±Å(0,2). Let Ì†µÌ±ç = Ì†µÌ±ã ‚àí Ì†µÌ±å . Then Ì†µÌ±ç ‚àº Ì†µÌ±Å(0,1), and Ì†µÌ∞∏(|Ì†µÌ±ã ‚àí Ì†µÌ±å |) =     2Ì†µÌ∞∏(|Ì†µÌ±ç|). Apply LOTUS,
                                                  ‚àû                           ‚àû
                                                         1                          1                  2
                                                                2                         2
                                                              ‚àíÌ†µÌ±ß /2                    ‚àíÌ†µÌ±ß /2
                                                                                                    ‚àö
                                                        ‚àö                         ‚àö
                                     Ì†µÌ∞∏(|Ì†µÌ±ç|) = ‚à´    |Ì†µÌ±ß|    Ì†µÌ±í     Ì†µÌ±ëÌ†µÌ±ß = 2‚à´   Ì†µÌ±ß     Ì†µÌ±í     Ì†µÌ±ëÌ†µÌ±ß =    ,
                                                                                                       Ì†µÌºã
                                                          2Ì†µÌºã                       2Ì†µÌºã
                                                 ‚àí‚àû                         0
                                                2
                                                ‚àö
                    Therefore, Ì†µÌ¥º(|Ì†µÌ±ã ‚àí Ì†µÌ±å |) =   .
                                                 Ì†µÌºã
                                                                     175
                        53 Transformation
                        Example 53.1 (Min/max of random variables). Let Ì†µÌ±ã ,Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã                                       be i.i.d random vari-
                                                                                                        1    2         Ì†µÌ±õ
                        ables, each following a uniform distribution on the interval [0,1]. Find the distribution of
                        max(Ì†µÌ±ã ,Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã ).
                                  1    2         Ì†µÌ±õ
                        Solution. Let Ì†µÌ±Ä = max(Ì†µÌ±ã ,Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã ). The CDF of Ì†µÌ±Ä, denoted Ì†µÌ∞π (Ì†µÌ±ö), is the probability
                                                             1    2         Ì†µÌ±õ                                        Ì†µÌ±Ä
                        that Ì†µÌ±Ä ‚â§ Ì†µÌ±ö. For Ì†µÌ±Ä ‚â§ Ì†µÌ±ö to hold, all Ì†µÌ±ã must satisfy Ì†µÌ±ã ‚â§ Ì†µÌ±ö. Since the Ì†µÌ±ã are independent
                                                                               Ì†µÌ±ñ                    Ì†µÌ±ñ                         Ì†µÌ±ñ
                        and identically distributed:
                                             Ì†µÌ∞π  (Ì†µÌ±ö) = Ì†µÌ±É(Ì†µÌ±Ä ‚â§ Ì†µÌ±ö) = Ì†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±ö,Ì†µÌ±ã ‚â§ Ì†µÌ±ö,‚Ä¶,Ì†µÌ±ã ‚â§ Ì†µÌ±ö)
                                               Ì†µÌ±Ä                                   1            2               Ì†µÌ±õ
                                                                           =Ì†µÌ±É(Ì†µÌ±ã ‚â§Ì†µÌ±ö)‚ãÖÌ†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±ö)‚ãØÌ†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±ö)
                                                                                    1                 2                  Ì†µÌ±õ
                        For a uniform distribution on [0,1], Ì†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±ö) = Ì†µÌ±ö for 0 ‚â§ Ì†µÌ±ö ‚â§ 1. Thus:
                                                                                Ì†µÌ±ñ
                                                                                               Ì†µÌ±õ
                                                                              Ì†µÌ∞π   (Ì†µÌ±ö) = Ì†µÌ±ö .
                                                                                Ì†µÌ±Ä
                        The PDF of Ì†µÌ±Ä, denoted Ì†µÌ±ì (Ì†µÌ±ö), is the derivative of the CDF:
                                                            Ì†µÌ±Ä
                                                                          Ì†µÌ±ë                 Ì†µÌ±ë
                                                                                                    Ì†µÌ±õ          Ì†µÌ±õ‚àí1
                                                           Ì†µÌ±ì  (Ì†µÌ±ö) =        Ì†µÌ∞π   (Ì†µÌ±ö) =        (Ì†µÌ±ö ) = Ì†µÌ±õÌ†µÌ±ö         .
                                                            Ì†µÌ±Ä                 Ì†µÌ±Ä
                                                                        Ì†µÌ±ëÌ†µÌ±ö                Ì†µÌ±ëÌ†µÌ±ö
                                                                                                              2
                        Example 53.2 (Chi-square PDF). Let Ì†µÌ±ã ‚àº Ì†µÌ±Å(0,1), Ì†µÌ±å = Ì†µÌ±ã . The distribution of Ì†µÌ±å is an
                        example of a Chi-Square distribution. Find the PDF of Ì†µÌ±å.
                        Solution. Again, we try to find the CDF first, and differentiate to the PDF.
                                                                        ‚àö              ‚àö             ‚àö              ‚àö             ‚àö
                                                      2
                                   Ì†µÌ∞π  (Ì†µÌ±¶) = Ì†µÌ±É(Ì†µÌ±ã     ‚â§Ì†µÌ±¶) = Ì†µÌ±É(‚àí Ì†µÌ±¶ ‚â§ Ì†µÌ±ã ‚â§             Ì†µÌ±¶) = Œ¶( Ì†µÌ±¶) ‚àí Œ¶(‚àí Ì†µÌ±¶) = 2Œ¶( Ì†µÌ±¶)‚àí1
                                     Ì†µÌ±å
                        Therefore,
                                                                                1
                                                                        ‚àö                       ‚àö
                                                                                   ‚àí1/2                ‚àí1/2
                                                       Ì†µÌ±ì  (Ì†µÌ±¶) = 2Ì†µÌºë(    Ì†µÌ±¶) ‚ãÖ  Ì†µÌ±¶      =Ì†µÌºë( Ì†µÌ±¶)Ì†µÌ±¶         ,    Ì†µÌ±¶ > 0.
                                                        Ì†µÌ±å
                                                                                2
                        Theorem53.1(Transformation). Let Ì†µÌ±ã be a continuous r.v. with PDF Ì†µÌ±ì , and let Ì†µÌ±å = Ì†µÌ±î(Ì†µÌ±ã),
                                                                                                                            Ì†µÌ±ã
                        where Ì†µÌ±î is differentiable and strictly increasing (or strictly decreasing). Then the PDF of Ì†µÌ±å is
                        given by
                                                                                                Ì†µÌ±ëÌ†µÌ±•
                                                                                                    ‚à£ ,
                                                                          Ì†µÌ±ì  (Ì†µÌ±¶) = Ì†µÌ±ì   (Ì†µÌ±•)‚à£
                                                                            Ì†µÌ±å          Ì†µÌ±ã
                                                                                                Ì†µÌ±ëÌ†µÌ±¶
                                        ‚àí1
                        where Ì†µÌ±• = Ì†µÌ±î       (Ì†µÌ±¶).
                                                                                      176
                     Proof. Let Ì†µÌ±î be strictly increasing. The CDF of Ì†µÌ±å is
                                                                                     ‚àí1               ‚àí1
                               Ì†µÌ∞π  (Ì†µÌ±¶) = Ì†µÌ±É(Ì†µÌ±å ‚â§ Ì†µÌ±¶) = Ì†µÌ±É(Ì†µÌ±î(Ì†µÌ±ã) ‚â§ Ì†µÌ±¶) = Ì†µÌ±É(Ì†µÌ±ã ‚â§ Ì†µÌ±î    (Ì†µÌ±¶)) = Ì†µÌ∞π (Ì†µÌ±î   (Ì†µÌ±¶)) = Ì†µÌ∞π (Ì†µÌ±•)
                                 Ì†µÌ±å                                                               Ì†µÌ±ã              Ì†µÌ±ã
                     By the chain rule, the PDF of Ì†µÌ±å is
                                                                                  Ì†µÌ±ëÌ†µÌ±•
                                                                 Ì†µÌ±ì (Ì†µÌ±¶) = Ì†µÌ±ì (Ì†µÌ±•)   .
                                                                  Ì†µÌ±å        Ì†µÌ±ã
                                                                                  Ì†µÌ±ëÌ†µÌ±¶
                     If Ì†µÌ±î is strictly decreasing,
                                                                                 ‚àí1                   ‚àí1
                          Ì†µÌ∞π  (Ì†µÌ±¶) = Ì†µÌ±É(Ì†µÌ±å ‚â§ Ì†µÌ±¶) = Ì†µÌ±É(Ì†µÌ±î(Ì†µÌ±ã) ‚â§ Ì†µÌ±¶) = Ì†µÌ±É(Ì†µÌ±ã ‚â• Ì†µÌ±î    (Ì†µÌ±¶)) = 1 ‚àí Ì†µÌ∞π (Ì†µÌ±î    (Ì†µÌ±¶)) = 1 ‚àí Ì†µÌ∞π (Ì†µÌ±•)
                            Ì†µÌ±å                                                                    Ì†µÌ±ã                   Ì†µÌ±ã
                     Then the PDF of Ì†µÌ±å is
                                                                                   Ì†µÌ±ëÌ†µÌ±•
                                                                Ì†µÌ±ì (Ì†µÌ±¶) = ‚àíÌ†µÌ±ì (Ì†µÌ±•)    .
                                                                 Ì†µÌ±å          Ì†µÌ±ã
                                                                                   Ì†µÌ±ëÌ†µÌ±¶
                     But in this case, Ì†µÌ±ëÌ†µÌ±•/Ì†µÌ±ëÌ†µÌ±¶ < 0. So taking absolute value covers both cases.
                                                                                            Ì†µÌ±ã
                     Example 53.3 (Log-Normal PDF). Let Ì†µÌ±ã ‚àº Ì†µÌ±Å(0,1), Ì†µÌ±å = Ì†µÌ±í . Then the distribution of Ì†µÌ±å is
                                                                                    Ì†µÌ±å
                     called the Log-Normal distribution. Find the PDF of              .
                                                  Ì†µÌ±•                                       Ì†µÌ±•                                   Ì†µÌ±•
                     Solution. Since Ì†µÌ±î(Ì†µÌ±•) = Ì†µÌ±í     is strictly increasing. Let Ì†µÌ±¶ = Ì†µÌ±í , so Ì†µÌ±• = logÌ†µÌ±¶ and Ì†µÌ±ëÌ†µÌ±¶/Ì†µÌ±ëÌ†µÌ±• = Ì†µÌ±í .
                     Then
                                                                Ì†µÌ±ëÌ†µÌ±•          1              1
                                             Ì†µÌ±ì  (Ì†µÌ±¶) = Ì†µÌ±ì (Ì†µÌ±•)‚à£   ‚à£ = Ì†µÌºë(Ì†µÌ±•)    =Ì†µÌºë(logÌ†µÌ±¶) ,     Ì†µÌ±¶ > 0.
                                               Ì†µÌ±å        Ì†µÌ±ã
                                                                              Ì†µÌ±•
                                                                Ì†µÌ±ëÌ†µÌ±¶         Ì†µÌ±í              Ì†µÌ±¶
                     Notethatafter applying the change of variables formula, we write everything on the right-hand
                     side in terms of Ì†µÌ±¶, and we specify the support of the distribution. To determine the support,
                                                                               Ì†µÌ±•
                     we just observe that as Ì†µÌ±• ranges from ‚àí‚àû to‚àû, Ì†µÌ±í ranges from 0 to ‚àû.
                     Theorem 53.2 (Transformation of multi-variables). Let X = (Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã ) be a continuous
                                                                                                   1        Ì†µÌ±õ
                     random vector with joint PDF Ì†µÌ±ì , and let Y = Ì†µÌ±î(X) where Ì†µÌ±î is an invertible function from
                                                           X
                       Ì†µÌ±õ     Ì†µÌ±õ
                     ‚Ñù to ‚Ñù . Let y = Ì†µÌ±î(x). Define the Jacobian matrix:
                                                                    Ì†µÌºïÌ†µÌ±•   Ì†µÌºïÌ†µÌ±•       Ì†µÌºïÌ†µÌ±•
                                                                      1      1          1
                                                                                 ‚Ä¶
                                                                    Ì†µÌºïÌ†µÌ±¶   Ì†µÌºïÌ†µÌ±¶       Ì†µÌºïÌ†µÌ±¶
                                                                      1      2          Ì†µÌ±õ
                                                          Ì†µÌºïx
                                                                 ‚éõ                        ‚éû
                                                                 ‚éú                        ‚éü
                                                                     ‚ãÆ      ‚ãÆ          ‚ãÆ
                                                              =                              .
                                                                 ‚éú                        ‚éü
                                                          Ì†µÌºïy
                                                                    Ì†µÌºïÌ†µÌ±•  Ì†µÌºïÌ†µÌ±•        Ì†µÌºïÌ†µÌ±•
                                                                      Ì†µÌ±õ     Ì†µÌ±õ         Ì†µÌ±õ
                                                                                 ‚Ä¶
                                                                 ‚éù                        ‚é†
                                                                    Ì†µÌºïÌ†µÌ±¶   Ì†µÌºïÌ†µÌ±¶       Ì†µÌºïÌ†µÌ±¶
                                                                      1      2          Ì†µÌ±õ
                     Also assume that the determinant of the Jacobian matrix is never 0. Then the joint PDF of
                     Yis
                                                                                  Ì†µÌºïx
                                                               Ì†µÌ±ì  (y) = Ì†µÌ±ì (x)‚à£      ‚à£ ,
                                                                 Y         X
                                                                                  Ì†µÌºïy
                             Ì†µÌºïx
                     where ‚à£    ‚à£ is the absolute value of the determinant of the Jacobian matrix.
                             Ì†µÌºïy
                                                          Ì†µÌ±ñÌ†µÌ±ñÌ†µÌ±ë
                     Example 53.4. Suppose Ì†µÌ±ã,Ì†µÌ±å ‚àº Ì†µÌ∞∏Ì†µÌ±•Ì†µÌ±ùÌ†µÌ±ú(1). Find the distribution of Ì†µÌ±ã/(Ì†µÌ±ã +Ì†µÌ±å).
                                                                         177
                                                     Ì†µÌ±ã
                         Solution. Let Ì†µÌ±à =               , Ì†µÌ±â = Ì†µÌ±ã + Ì†µÌ±å . Then Ì†µÌ±ã = Ì†µÌ±àÌ†µÌ±â , Ì†µÌ±å = Ì†µÌ±â ‚àí Ì†µÌ±àÌ†µÌ±â . The determinant of the
                                                    Ì†µÌ±ã+Ì†µÌ±å
                         Jacobian matrix is
                                                                      Ì†µÌºï(Ì†µÌ±•, Ì†µÌ±¶)
                                                                                         Ì†µÌ±£      Ì†µÌ±¢
                                                                     ‚à£          ‚à£ = ‚à£                  ‚à£ = Ì†µÌ±£
                                                                                       ‚àíÌ†µÌ±£    1‚àíÌ†µÌ±¢
                                                                      Ì†µÌºï(Ì†µÌ±¢, Ì†µÌ±£)
                         Thus, the joint distribution of (Ì†µÌ±à,Ì†µÌ±â ) is
                                                                                                             ‚àí(Ì†µÌ±•+Ì†µÌ±¶)        ‚àíÌ†µÌ±£
                                                 Ì†µÌ±ì   (Ì†µÌ±¢, Ì†µÌ±£) = Ì†µÌ±ì    (Ì†µÌ±•, Ì†µÌ±¶)|Ì†µÌ±£| = Ì†µÌ±ì  (Ì†µÌ±•)Ì†µÌ±ì  (Ì†µÌ±¶)Ì†µÌ±£ = Ì†µÌ±í         Ì†µÌ±£ = Ì†µÌ±í   Ì†µÌ±£.
                                                  Ì†µÌ±àÌ†µÌ±â             Ì†µÌ±ãÌ†µÌ±å                 Ì†µÌ±ã      Ì†µÌ±å
                         The distribution of Ì†µÌ±ã/(Ì†µÌ±ã + Ì†µÌ±å) is equivalent to the marginal distribution of Ì†µÌ±à:
                                                                                        ‚àû
                                                                                             ‚àíÌ†µÌ±£
                                                                         Ì†µÌ±ì (Ì†µÌ±¢) = ‚à´       Ì†µÌ±í   Ì†µÌ±£Ì†µÌ±ëÌ†µÌ±£ = 1
                                                                          Ì†µÌ±à
                                                                                      0
                         for 0 ‚â§ Ì†µÌ±¢ ‚â§ 1. Hence Ì†µÌ±à is a Uniform distribution over [0,1].
                                                                                       178
                            Part VI
                   Sampling distributions
                               179
                          54 Law of large numbers
                          Wenowintroduce two important theorems describing the behavior of the sample mean as the
                          sample size grows. Throughout this section and the next, we assume Ì†µÌ±ã ,Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã are i.i.d
                                                                                                                                   1     2          Ì†µÌ±õ
                                                                                                               2
                          RVs drawn from a population with mean Ì†µÌºá and variance Ì†µÌºé . The sample mean is defined as
                                                                                        Ì†µÌ±ã +‚ãØ+Ì†µÌ±ã
                                                                                           1              Ì†µÌ±õ
                                                                                  ÃÑ
                                                                               Ì†µÌ±ã =                         .
                                                                                  Ì†µÌ±õ
                                                                                                 Ì†µÌ±õ
                          As we have discussed previously, the sample mean is itself a random variable with mean and
                          variance:
                                                          1                               1
                                                  ÃÑ
                                           Ì†µÌ∞∏(Ì†µÌ±ã ) =        Ì†µÌ∞∏(Ì†µÌ±ã +‚ãØ+Ì†µÌ±ã )= (Ì†µÌ∞∏(Ì†µÌ±ã )+‚ãØ+Ì†µÌ∞∏(Ì†µÌ±ã ))=Ì†µÌºá,
                                                  Ì†µÌ±õ               1              Ì†µÌ±õ                1                   Ì†µÌ±õ
                                                          Ì†µÌ±õ                              Ì†µÌ±õ
                                                                                                                                                 2
                                                           1                                    1                                              Ì†µÌºé
                                                                                           Ì†µÌ±ñÌ†µÌ±ñÌ†µÌ±ë
                                                  ÃÑ
                                        Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã ) =        Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã  +‚ãØ+Ì†µÌ±ã ) =                (Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã ) + ‚ãØ + Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã )) =             .
                                                  Ì†µÌ±õ                    1              Ì†µÌ±õ                      1                      Ì†µÌ±õ
                                                            2                                    2
                                                          Ì†µÌ±õ                                   Ì†µÌ±õ                                               Ì†µÌ±õ
                                                                                                                                       ÃÑ
                          The law of large numbers (LLN) says that as n grows, the sample mean Ì†µÌ±ã converges to the
                                                                                                                                       Ì†µÌ±õ
                          true mean Ì†µÌºá.
                                                                                                                                ÃÑ
                          Theorem 54.1. (Strong law of large numbers). The sample mean Ì†µÌ±ã converges to the true
                                                                                                                                Ì†µÌ±õ
                                                                                                                                                 ÃÑ
                          mean Ì†µÌºá point-wise as Ì†µÌ±õ ‚Üí ‚àû, with probability 1. In other words, the event Ì†µÌ±ã                                             ‚ÜíÌ†µÌºá has
                                                                                                                                                  Ì†µÌ±õ
                          probability 1.
                                                                                                                           ÃÑ
                          Theorem 54.2. (Weak law of large numbers). For all Ì†µÌºñ > 0, Ì†µÌ±É(|Ì†µÌ±ã ‚àíÌ†µÌºá| > Ì†µÌºñ) ‚Üí 0 as Ì†µÌ±õ ‚Üí ‚àû.
                                                                                                                           Ì†µÌ±õ
                          (this is known as converge in probability).
                                                                                                                                                           ÃÑ
                          Wedon‚Äôtneedarigorousproof here. But an intuitive proof is obvious. As Ì†µÌ±õ ‚Üí ‚àû, Ì†µÌ±âÌ†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã ) =
                                                                                                                                                           Ì†µÌ±õ
                            2
                          Ì†µÌºé
                                                                         ÃÑ
                               ‚Üí0. The random variable Ì†µÌ±ã becomes fixed at Ì†µÌºá as Ì†µÌ±õ becomes large. Thus, it converges
                                                                          Ì†µÌ±õ
                           Ì†µÌ±õ
                          to Ì†µÌºá in a probabilistic sense.
                          It seems that the LLN just states the obvious. But it has wide applications in daily time that
                          you might not even realize. What it says is essentially this: the uncertainty at the individual
                          level becomes certain when aggregating together; the risks that are unmanageable at the
                          individual level becomes manageable collectively. Think about a rare disease, it happens at 1
                          out of a million probability. For each individual, no one knows if they will get the disease or
                          not. But as the sample size gets large, suppose we have one billion population, the LLN says
                                                                                           180
            the sample mean will be very close the true mean. That is, there will be almost surely 1000
            people being infected by the disease. We provide two more examples.
            Example 54.1. (Lottery). A lottery company is designing a game with a 6-digit format.
            Each time someone buys a ticket, they receive a randomly generated 6-digit number. Only
            one number will win the grand prize of 10 million dollars. What should the company charge
            per ticket to break even?
                                                 6
            Solution. The probability of winning the game is Ì†µÌ±ù = 1/10 . Suppose the company has sold
            Ì†µÌ±õ tickets. The price for each ticket is Ì†µÌ±•. The revenue for the company is therefore Ì†µÌ±•Ì†µÌ±õ. By
                                                   7
            the LLN, the cost of the company should be very close to 10 Ì†µÌ±õÌ†µÌ±ù. The break even point is
                 7         7
            Ì†µÌ±•Ì†µÌ±õ = 10 Ì†µÌ±õÌ†µÌ±ù. So Ì†µÌ±• = 10 Ì†µÌ±ù = 10. Therefore, if the company sells each ticket above 10 dollars.
           The business is surely profitable as long as Ì†µÌ±õ is large. If the company is a monopoly, it can
            reap as much profit as it desires as long as they know the basic probability theory! The same
            can be said about gambling companies.
            Example54.2. (Insurance). Insurance is anther great application of the LLN. It is essentially
            the same as the the lottery game but most people do not realize it. Suppose there is a disease
           with infection rate of 1 out of 1 million. The medical expenditure to cure the disease is 10
            million dollars. How much the insurance company should charge per customer to cover this
            disease?
            Solution. The solution is essentially the same as above. Suppose the premium for the insurance
            product is Ì†µÌ±•. The revenue of the company by selling the premium is Ì†µÌ±•Ì†µÌ±õ. The cost is ‚Äî when
                                                         7
            one customer is infected, the company has to pay the medical cost ‚Äî10 Ì†µÌ±õÌ†µÌ±ù. The break even
            price for the insurance premium is thus 10 dollars.
           What is the implication of this insurance? Without the insurance, each individual either
            chooses to set aside 10 million dollars pre-cautiously for the disease (if he is rich enough) or
            be exposed to the risk completely uncovered. The insurance product enables everyone to get
            covered at a cost of just 10 dollars. It is a typical example that the unmanageable risk at the
            individual level becomes manageable collectively.
                                        181
                                                   55 Central limit theorem
                                                   The LLN shows the convergence of the sample mean to the population mean. What about
                                                   the entire sample distribution? This is addressed by the central limit theorem (CLT), which,
                                                   as its name suggests, is a limit theorem of central importance in statistics.
                                                                                                                                                                                                                     ÃÑ
                                                   The CLT states that for large Ì†µÌ±õ, the distribution of Ì†µÌ±ã                                                                                                                 after standardization approaches a
                                                                                                                                                                                                                      Ì†µÌ±õ
                                                   standard Normal distribution, regardless of the underlying distribution of Ì†µÌ±ã . By standardiza-
                                                                                                                                                                                                                                                                              Ì†µÌ±ñ
                                                                                                                                                                                                                                                                                ‚àö
                                                                                                                                                                                                                         ÃÑ
                                                   tion, we mean that we subtract Ì†µÌºá, the expected value of Ì†µÌ±ã , and divide by Ì†µÌºé/ Ì†µÌ±õ, the standard
                                                                                                                                                                                                                         Ì†µÌ±õ
                                                                                             ÃÑ
                                                   deviation of Ì†µÌ±ã .
                                                                                             Ì†µÌ±õ
                                                   Theorem 55.1. (Central limit theorem). As Ì†µÌ±õ ‚Üí ‚àû,
                                                                                                                                                    ÃÑ
                                                                                                                               ‚àö
                                                                                                                                               Ì†µÌ±ã ‚àíÌ†µÌºá
                                                                                                                                                    Ì†µÌ±õ
                                                                                                                                                                     )‚ÜíÌ†µÌ±Å(0,1)
                                                                                                                                    Ì†µÌ±õ (                                                                   in distribution.
                                                                                                                                                       Ì†µÌºé
                                                   In other words, the CDF of the left-hand side approaches the CDF of the standard normal
                                                   distribution.
                                                   Proof. We will prove the CLT assuming the MGF of the Ì†µÌ±ã exists, though the theorem holds
                                                                                                                                                                                                                               Ì†µÌ±ñ
                                                                                                                                                                                                                                                                    2
                                                   under much weaker conditions. Without loss of generality let Ì†µÌºá = 1,Ì†µÌºé = 1 (since we stan-
                                                                                                                                                                                             ‚àö                                                                         ‚àö
                                                                                                                                                                                                           ÃÑ
                                                   dardize it anyway). We show that the MGF of                                                                                                    Ì†µÌ±õÌ†µÌ±ã           =(Ì†µÌ±ã +‚ãØ+Ì†µÌ±ã )/ Ì†µÌ±õ converges to the
                                                                                                                                                                                                            Ì†µÌ±õ                  1                             Ì†µÌ±õ
                                                   MGFofthe Ì†µÌ±Å(0,1).
                                                   The MGF of Ì†µÌ±Å(0,1) is
                                                                                                                                                                            ‚àû
                                                                                                                                                                                                     1
                                                                                                                                                                                                                       2
                                                                                                                                                   Ì†µÌ±°Ì†µÌ±ã                              Ì†µÌ±°Ì†µÌ±•                        ‚àíÌ†µÌ±• /2
                                                                                                                                                                                                ‚àö
                                                                                                                                        Ì†µÌ∞∏(Ì†µÌ±í            ) = ‚à´ Ì†µÌ±í ‚ãÖ                                           Ì†µÌ±í               Ì†µÌ±ëÌ†µÌ±•
                                                                                                                                                                                                      2Ì†µÌºã
                                                                                                                                                                        ‚àí‚àû
                                                                                                                                                                            ‚àû
                                                                                                                                                                                        1
                                                                                                                                                                                                          2
                                                                                                                                                                                                    ‚àíÌ†µÌ±• /2+Ì†µÌ±°Ì†µÌ±•
                                                                                                                                                                                   ‚àö
                                                                                                                                                              =‚à´                                Ì†µÌ±í                         Ì†µÌ±ëÌ†µÌ±•
                                                                                                                                                                                        2Ì†µÌºã
                                                                                                                                                                        ‚àí‚àû
                                                                                                                                                                            ‚àû
                                                                                                                                                                                        1
                                                                                                                                                                                                        1                     1
                                                                                                                                                                                                                       2           2
                                                                                                                                                                                                    ‚àí (Ì†µÌ±•‚àíÌ†µÌ±°) + Ì†µÌ±°
                                                                                                                                                                                                        2                     2
                                                                                                                                                                                   ‚àö
                                                                                                                                                              =‚à´                                Ì†µÌ±í                                    Ì†µÌ±ëÌ†µÌ±•
                                                                                                                                                                                        2Ì†µÌºã
                                                                                                                                                                        ‚àí‚àû
                                                                                                                                                                                      ‚àû
                                                                                                                                                                           2
                                                                                                                                                                                                  1
                                                                                                                                                                         Ì†µÌ±°                                       1
                                                                                                                                                                                                                                 2
                                                                                                                                                                                                              ‚àí (Ì†µÌ±•‚àíÌ†µÌ±°)
                                                                                                                                                                          2                                       2
                                                                                                                                                                                             ‚àö
                                                                                                                                                              =Ì†µÌ±í              ‚à´                           Ì†µÌ±í                       Ì†µÌ±ëÌ†µÌ±•
                                                                                                                                                                                                   2Ì†µÌºã
                                                                                                                                                                                  ‚àí‚àû
                                                                                                                                                                          2
                                                                                                                                                                        Ì†µÌ±°   /2
                                                                                                                                                              =Ì†µÌ±í
                                                                                                                                                                                      182
                                                                                                  ‚àö
                                                                                                              ÃÑ
                                           Compute the MGF of                                         Ì†µÌ±õÌ†µÌ±ã :
                                                                                                              Ì†µÌ±õ
                                                                                                      ‚àö                                                          ‚àö
                                                                                                               ÃÑ
                                                                                                         Ì†µÌ±õÌ†µÌ±ã                        Ì†µÌ±°(Ì†µÌ±ã +‚ãØ+Ì†µÌ±ã )/ Ì†µÌ±õ
                                                                                                               Ì†µÌ±õ                           1              Ì†µÌ±õ
                                                                                             Ì†µÌ∞∏(Ì†µÌ±í                ) = Ì†µÌ∞∏(Ì†µÌ±í                                            )
                                                                                                                                              ‚àö                          ‚àö                                 ‚àö
                                                                                                                                     Ì†µÌ±°Ì†µÌ±ã /       Ì†µÌ±õ            Ì†µÌ±°Ì†µÌ±ã /       Ì†µÌ±õ                  Ì†µÌ±°Ì†µÌ±ã    / Ì†µÌ±õ
                                                                                                                                          1                          2                                Ì†µÌ±õ
                                                                                                                      =Ì†µÌ∞∏(Ì†µÌ±í                         )Ì†µÌ∞∏(Ì†µÌ±í                     )‚ãØÌ†µÌ∞∏(Ì†µÌ±í                          )
                                                                                                                                                ‚àö
                                                                                                                                                           Ì†µÌ±õ
                                                                                                                                       Ì†µÌ±°Ì†µÌ±ã /       Ì†µÌ±õ
                                                                                                                                             Ì†µÌ±ñ
                                                                                                                      =[Ì†µÌ∞∏(Ì†µÌ±í                          )]                 since Ì†µÌ±ñ.Ì†µÌ±ñ.Ì†µÌ±ë
                                                                                                                                                                                                             Ì†µÌ±õ
                                                                                                                                                                     2      2
                                                                                                                                                  Ì†µÌ±°Ì†µÌ±ã             Ì†µÌ±°  Ì†µÌ±ã
                                                                                                                                                         Ì†µÌ±ñ
                                                                                                                                                                            Ì†µÌ±ñ
                                                                                                                                                                                              ‚àí1
                                                                                                                                                   ‚àö
                                                                                                                      =[Ì†µÌ∞∏(1+                                +                  +Ì†µÌ±ú(Ì†µÌ±õ              ))]
                                                                                                                                                                      2Ì†µÌ±õ
                                                                                                                                                       Ì†µÌ±õ
                                                                                                                                                                                                                        Ì†µÌ±õ
                                                                                                                                                                           2
                                                                                                                                            Ì†µÌ±°                           Ì†µÌ±°
                                                                                                                                                                                          2                   ‚àí1
                                                                                                                                         ‚àö
                                                                                                                      =[1+                        Ì†µÌ∞∏(Ì†µÌ±ã ) +                    Ì†µÌ∞∏(Ì†µÌ±ã ) + Ì†µÌ±ú(Ì†µÌ±õ                     )]
                                                                                                                                                             Ì†µÌ±ñ
                                                                                                                                                                                          Ì†µÌ±ñ
                                                                                                                                                                        2Ì†µÌ±õ
                                                                                                                                              Ì†µÌ±õ
                                                                                                                                                                         Ì†µÌ±õ
                                                                                                                                            2
                                                                                                                                          Ì†µÌ±°
                                                                                                                                                               ‚àí1
                                                                                                                      =[1+                       +Ì†µÌ±ú(Ì†µÌ±õ             )]
                                                                                                                                         2Ì†µÌ±õ
                                                                                                                                                                             Ì†µÌ±õ
                                                                                                                                           2
                                                                                                                                         Ì†µÌ±°  /2
                                                                                                                                                                   ‚àí1
                                                                                                                      =[1+                           +Ì†µÌ±ú(Ì†µÌ±õ             )]
                                                                                                                                            Ì†µÌ±õ
                                                                                                                                 2
                                                                                                                                Ì†µÌ±°  /2
                                                                                                                      ‚ÜíÌ†µÌ±í                          as Ì†µÌ±õ ‚Üí ‚àû
                                                                                                       ‚àö
                                                                                                                    ÃÑ
                                           Therefore, the MGF of                                            Ì†µÌ±õÌ†µÌ±ã         approaches the MGF of the standard normal. Since MGF
                                                                                                                    Ì†µÌ±õ
                                                                                                                                                                 ‚àö
                                                                                                                                                                             ÃÑ
                                           determines the distribution, the distribution of                                                                          Ì†µÌ±õÌ†µÌ±ã         also approaches the standard normal
                                                                                                                                                                             Ì†µÌ±õ
                                           distribution.
                                                                                                                                                                                   ÃÑ
                                           The CLT tells us about the limiting distribution of Ì†µÌ±ã                                                                                        as Ì†µÌ±õ ‚Üí ‚àû. That means, we can
                                                                                                                                                                                   Ì†µÌ±õ
                                                                                                                                                   ÃÑ
                                           reasonably approximate the distribution Ì†µÌ±ã with normal distribution when Ì†µÌ±õ is a finite large
                                                                                                                                                   Ì†µÌ±õ
                                           number.
                                                                                                                                                      2
                                                                                                                           ÃÑ
                                                                                                                       Ì†µÌ±ã ‚âàÌ†µÌ±Å(Ì†µÌºá,Ì†µÌºé /Ì†µÌ±õ)                              for large Ì†µÌ±õ.
                                                                                                                           Ì†µÌ±õ
                                           The Central Limit Theorem was first proved by Pierre-Simon Laplace in 1810. Let‚Äôs take a
                                           moment to admire the generality of this result. The distribution of the individual Ì†µÌ±ã can
                                                                                                                                                                                                                                                                 Ì†µÌ±ñ
                                           be anything in the world, as long as the mean and variance are finite. This does mean the
                                           distribution of Ì†µÌ±ã is irrelevant, however. If the distribution is fairly close to normal, the result
                                                                                    Ì†µÌ±ñ
                                           would hold for smaller Ì†µÌ±õ. If the distribution is far away from normal, it would take larger Ì†µÌ±õ
                                           to converge.
                                           The CLT gives the distribution of the sample mean regardless of the underlying distribution.
                                           This allows to assess the ‚Äúquality‚Äù of the sample mean ‚Äî how close it is to the true mean.
                                           The LLN tells us the larger the sample, the closer the sample mean to the population mean.
                                           The CLT tells us the distribution of the sample mean for sample size Ì†µÌ±õ. For smaller Ì†µÌ±õ, the
                                                                                                                                                                                                            2
                                           distribution is more spread-out (a normal distribution with large Ì†µÌºé ); hence the uncertainty is
                                           huge, other values are more likely. For larger Ì†µÌ±õ, the uncertainty is reduced, most values would
                                           be centered around the true mean. We will delve deeper into this when we get to hypothesis
                                           testing.
                                                                                                                                                        183
                 Example 55.1. Suppose that a fair coin is tossed 900 times. Approximate the probability of
                 obtaining more than 395 heads.
                                     900
                                                                                   1
                 Solution. Let Ì†µÌ∞ª = ‚àë   Ì†µÌ±ã be the number of heads, where Ì†µÌ±ã ‚àº Bern( ). We could compute
                                          Ì†µÌ±ñ                              Ì†µÌ±ñ
                                                                                   2
                                     Ì†µÌ±ñ=1
                 the probability by
                                                                      Ì†µÌ±ò    900‚àíÌ†µÌ±ò
                                                       900
                                                             900    1    1
                                        Ì†µÌ±É(Ì†µÌ∞ª > 495) = ‚àë (      )( ) ( )
                                                              Ì†µÌ±ò    2    2
                                                      Ì†µÌ±ò=496
                 But this is quite tedious. Because Ì†µÌ±õ = 900 is reasonably large, we can apply the CLT:
                                                  900
                                                1
                                                                 2
                                                  ‚àëÌ†µÌ±ã ‚àºÌ†µÌ±Å(Ì†µÌºá,Ì†µÌºé /Ì†µÌ±õ)   or
                                                       Ì†µÌ±ñ
                                                Ì†µÌ±õ
                                                  Ì†µÌ±ñ=1
                                                  900
                                                                    2
                                                  ‚àëÌ†µÌ±ã ‚àºÌ†µÌ±Å(Ì†µÌ±õÌ†µÌºá,Ì†µÌ±õÌ†µÌºé )
                                                       Ì†µÌ±ñ
                                                  Ì†µÌ±ñ=1
                                       1                  1
                                           2
                 WeknowÌ†µÌºá=Ì†µÌ∞∏(Ì†µÌ±ã )= , Ì†µÌºé =Ì†µÌ±âÌ†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã ) = . Thus Ì†µÌ∞ª ‚àº Ì†µÌ±Å(450,225). Therefore,
                                  Ì†µÌ±ñ                 Ì†µÌ±ñ
                                       2                  4
                                                                      495‚àí450
                               Ì†µÌ±É(Ì†µÌ∞ª > 495) = 1‚àíÌ†µÌ±É(Ì†µÌ∞ª ‚â§ 495) ‚âà 1‚àíŒ¶(            )=0.0013.
                                                                          15
                                                           184
                   56 Samples and statistics
                   Wemodelreal-world uncertain events with random variables. We have also introduced various
                   distributions suitable to model different kinds of events. However, we never observe the full
                   distribution or the true parameters of the assumed distribution. Instead, we only observe a
                   sample of that random variable. We can only infer the properties of the distribution from a
                   limited sample. For example, suppose we model the height of an Asian women with a normal
                   distribution. But we never know exactly what the mean and variance are. We can only observe
                   a sample of the distribution.
                   In statistics, the conceptual distribution Ì†µÌ∞π is called the population distribution, or just
                                     1
                   the population.     It is tempting to think of the population as all the observations (e.g. all
                   the population on the planet), but this is not exactly correct. The population distribution is
                   more of a mathematical abstraction or an assumption. Suppose we are modeling the height
                   of human being, even if we have all the observations on the planet, that does not include the
                   people that have died or yet to be born. Thus, it is still a sample of the assumed distribution.
                   Acollection of random variables {Ì†µÌ±ã ,Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã } is a random sample from the population
                                                         1   2       Ì†µÌ±õ
                   Ì†µÌ∞π if Ì†µÌ±ã are independent and identically distributed (i.i.d) with distribution Ì†µÌ∞π. What
                          Ì†µÌ±ñ
                   we mean by i.i.d is that Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã       are mutually independent and have exactly the same
                                                 1       Ì†µÌ±õ
                   distribution Ì†µÌ±ã ‚àº Ì†µÌ∞π. Survey sampling is an useful metaphor to understand random sampling,
                                  Ì†µÌ±ñ
                   in which we randomly select a subset of the population with equal probability. The sample
                   size Ì†µÌ±õ is the number of individuals in the sample.
                   Adata set is a collection of numbers, typically organized by observation. We sometimes call
                   a data set also as a sample. But it should not be confused with the random sample defined
                   above. As the former is a collection of random variables, whereas the latter is one realization
                   of the random variables.
                   Typically, we will use Ì†µÌ±ã without the subscript to denote a random variable or vector with
                   distribution Ì†µÌ∞π, Ì†µÌ±ã with a subscript to denote a random observation in the sample, and Ì†µÌ±• or
                                     Ì†µÌ±ñ                                                                          Ì†µÌ±ñ
                   Ì†µÌ±• to denote a specific or realized value.
                   The problem of statistical inference is to learn about the underlying process ‚Äî the popula-
                   tion distribution or data generating process ‚Äî by examining the observations. In most cases,
                   we assume the population distribution and want to learn about the its parameters (e.g. Ì†µÌºá and
                    2
                   Ì†µÌºé in the normal distribution). As a convention, we use greek letters to denote population
                   parameters.
                    1
                     This section is based on Bruce Hansen‚Äôs Probability and Statistics for Economists.
                                                                  185
                            A statistic is a function of the random sample {Ì†µÌ±ã ,Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã }. Recall that there is a
                                                                                                                  1     2          Ì†µÌ±õ
                            distinction between random variables and their realizations. Similarly there is a distinction
                            between a statistic as a function of a random sample ‚Äî and is therefore a random variable as
                            well ‚Äî and a statistic as a function of the realized sample, which is a realized value. When
                            we treat a statistic as random we are viewing it is a function of a sample of random variables.
                            When we treat it as a realized value we are viewing it as a function of a set of realized values.
                            One way of viewing the distinction is to think of ‚Äúbefore viewing the data‚Äù and ‚Äúafter viewing
                            the data‚Äù. When we think about a statistic ‚Äúbefore viewing‚Äù we do not know what value it
                            will take. From our vantage point it is unknown and random. After viewing the data and
                            specifically after computing and viewing the statistic the latter is a specific number and is
                            therefore a realization. It is what it is and it is not changing. The randomness is the process
                            by which the data was generated ‚Äî and the understanding that if this process were repeated
                            the sample would be different and the specific realization would be therefore different. The
                            distribution of a statistic is called the sampling distribution, since it is the distribution
                            induced by sampling.
                                                     ÃÇ
                            AnestimatorÌ†µÌºÉforapopulationparameterÌ†µÌºÉisastatisticintendedtoinferÌ†µÌºÉ. Itisconventional
                                                                     ÃÇ                                                           ÃÇ
                            to use the hat notation Ì†µÌºÉ to denote an estimator. Note that Ì†µÌºÉ is a statistic and hence also
                                                                              ÃÇ
                            a random variable. We call Ì†µÌºÉ an estimate when it is a specific value (or realized value)
                            calculated in a specific sample.
                            A standard way to construct an estimator is by the analog principle. The idea is to express
                                                                                                                                                                    ÃÇ
                            the parameter Ì†µÌºÉ as a function of the population Ì†µÌ∞π, and then express the estimator Ì†µÌºÉ as the
                            analog function in the sample.
                            For example, suppose we want to construct an estimator for the population mean Ì†µÌºá = Ì†µÌ∞∏(Ì†µÌ±ã).
                            By definition, if each value of Ì†µÌ±ã is of equal probability, Ì†µÌºá is simply the average. By analogy,
                                                                                                       Ì†µÌ±õ
                                                                                                 1
                                                                                        ÃÑ
                            we construct the sample mean as Ì†µÌ±ã =                                   ‚àë Ì†µÌ±ã. It is conventional to denote a sample
                                                                                         Ì†µÌ±õ                    Ì†µÌ±ñ
                                                                                                Ì†µÌ±õ
                                                                                                       Ì†µÌ±ñ=1
                                                                                                                                                                             ÃÑ
                            average by the notation ‚ÄúX bar‚Äù. Because it is an estimator for Ì†µÌºá, we also denote it as Ì†µÌºáÃÇ = Ì†µÌ±ã .
                                                                                                                                                                             Ì†µÌ±õ
                            Note that from different samples we calculate different estimates. In one sample, Ì†µÌºáÃÇ = 6.5; in
                                                       Ì†µÌºáÃÇ = 6.7                                                                                                   Ì†µÌºá
                            another sample,                        .  All of them are erroneous estimate of the true parameter                                        .  The
                            question is therefore how close they are to the true parameter. To answer this question, we
                            need to study the distribution of the sample mean.
                                                                                                   186
                  57 Estimator accuracy
                  Thecentral question of statistics is we want to learn about the population from a finite sample.
                  Weknowsample mean is different from the population mean. But we also want to know how
                  large the error could be, that is, how far or close the sample mean is from the true population
                  mean. Thequestionis exceedingly diÔ¨Äicult to answer because the population mean is unknown.
                  Fortunately, with the help of the CLT, we can say more about the distribution of the sample
                  mean. This chapter bridges our probability theory with statistics. We use the theorems we
                  have derived to infer the properties of a statistic.
                  As the purpose of statistics is to learn about the population, we want our sample estimator
                  to be as good as possible. But what is a ‚Äúgood‚Äù estimator? This section we discuss two
                  properties that we usually demand from a good estimator, namely, unbiased and consistency.
                  Next section will tackle the more challenging concept of confidence interval.
                                                                ÃÇ
                  Definition 57.1. The bias of an estimator Ì†µÌºÉ of a parameter Ì†µÌºÉ is
                                                              ÃÇ      ÃÇ
                                                       Bias[Ì†µÌºÉ] = Ì†µÌ∞∏(Ì†µÌºÉ) ‚àí Ì†µÌºÉ.
                  We say that an estimator is biased if its sampling is incorrectly centered. We say that an
                  estimator is unbiased is the bias is zero.
                                      ÃÑ
                  Theorem 57.1. Ì†µÌ±ã is unbiased for Ì†µÌºá = Ì†µÌ∞∏(Ì†µÌ±•) if Ì†µÌ∞∏(Ì†µÌ±ã) < ‚àû.
                                      Ì†µÌ±õ
                  Proof.
                                                       Ì†µÌ±õ           Ì†µÌ±õ              Ì†µÌ±õ
                                                    1            1               1
                                          ÃÑ
                                     Ì†µÌ∞∏(Ì†µÌ±ã ) = Ì†µÌ∞∏ (   ‚àëÌ†µÌ±ã)= ‚àëÌ†µÌ∞∏(Ì†µÌ±ã)= ‚àëÌ†µÌºá=Ì†µÌºá.
                                          Ì†µÌ±õ               Ì†µÌ±ñ               Ì†µÌ±ñ
                                                    Ì†µÌ±õ           Ì†µÌ±õ              Ì†µÌ±õ
                                                      Ì†µÌ±ñ=1          Ì†µÌ±ñ=1           Ì†µÌ±ñ=1
                                                                              ÃÇ
                                        ÃÇ                                          ÃÇ
                  Theorem 57.2. If Ì†µÌºÉ is an unbiased estimator of Ì†µÌºÉ, then Ì†µÌªΩ = Ì†µÌ±éÌ†µÌºÉ + Ì†µÌ±è is an unbiased estimator
                  of Ì†µÌªΩ = Ì†µÌ±éÌ†µÌºÉ + Ì†µÌ±è.
                  But obtaining an unbiased estimator is not always as straightforward as it seems. Consider
                  the sample variance as an estimator for the population variance. By the analog principle, the
                  sample variance should be
                                                               187
                                                                                                 Ì†µÌ±õ
                                                                                          1
                                                                               2                                               2
                                                                                                                          ÃÑ
                                                                            Ì†µÌºéÃÇ    = ‚àë(Ì†µÌ±ã ‚àíÌ†µÌ±ã )
                                                                                                             Ì†µÌ±ñ           Ì†µÌ±õ
                                                                                         Ì†µÌ±õ
                                                                                               Ì†µÌ±ñ=1
                                                                                                 Ì†µÌ±õ
                                                                                          1
                                                                                                                                                   2
                                                                                                                                              ÃÑ
                                                                                   = ‚àë(Ì†µÌ±ã ‚àíÌ†µÌºá+Ì†µÌºá‚àíÌ†µÌ±ã )
                                                                                                             Ì†µÌ±ñ                               Ì†µÌ±õ
                                                                                         Ì†µÌ±õ
                                                                                               Ì†µÌ±ñ=1
                                                                                          1                                          2                                                             1
                                                                                                                           2                                                                                                         2
                                                                                                                                                                                       ÃÑ                                        ÃÑ
                                                                                   = ‚àë(Ì†µÌ±ã ‚àíÌ†µÌºá) + ‚àë(Ì†µÌ±ã ‚àíÌ†µÌºá)(Ì†µÌºá‚àíÌ†µÌ±ã )+ ‚àë(Ì†µÌºá‚àíÌ†µÌ±ã )
                                                                                                             Ì†µÌ±ñ                                         Ì†µÌ±ñ                            Ì†µÌ±õ                                        Ì†µÌ±õ
                                                                                         Ì†µÌ±õ                                         Ì†µÌ±õ                                                            Ì†µÌ±õ
                                                                                          1
                                                                                                                           2                                                                                  2
                                                                                                                                              ÃÑ                              ÃÑ                            ÃÑ
                                                                                   = ‚àë(Ì†µÌ±ã ‚àíÌ†µÌºá) +2(Ì†µÌ±ã ‚àíÌ†µÌºá)(Ì†µÌºá‚àíÌ†µÌ±ã )+(Ì†µÌºá‚àíÌ†µÌ±ã )
                                                                                                             Ì†µÌ±ñ                              Ì†µÌ±õ                              Ì†µÌ±õ                          Ì†µÌ±õ
                                                                                         Ì†µÌ±õ
                                                                                                 Ì†µÌ±õ
                                                                                          1
                                                                                                                           2                             2
                                                                                                                                          ÃÑ
                                                                                   = ‚àë(Ì†µÌ±ã ‚àíÌ†µÌºá) ‚àí(Ì†µÌ±ã ‚àíÌ†µÌºá)
                                                                                                             Ì†µÌ±ñ                           Ì†µÌ±õ
                                                                                         Ì†µÌ±õ
                                                                                               Ì†µÌ±ñ=1
                                                                                            2                              2
                                                                                                            ÃÑ
                                                                                   =Ì†µÌºéÃÉ ‚àí(Ì†µÌ±ã ‚àíÌ†µÌºá)
                                                                                                            Ì†µÌ±õ
                                          Weknowthat
                                                                                                                                                 Ì†µÌ±õ
                                                                                                                                          1
                                                                                                                             2                                                  2            2
                                                                                                                    Ì†µÌ∞∏(Ì†µÌºéÃÉ ) =                ‚àëÌ†µÌ∞∏(Ì†µÌ±ã ‚àíÌ†µÌºá) =Ì†µÌºé
                                                                                                                                                                  Ì†µÌ±ñ
                                                                                                                                          Ì†µÌ±õ
                                                                                                                                               Ì†µÌ±ñ=1
                                          Thus, if we compute the bias of this estimator:
                                                                                                                                                            2
                                                                                                                                                        Ì†µÌºé                          1
                                                                                                                                2              2                                               2
                                                                                                                       Ì†µÌ∞∏[Ì†µÌºéÃÇ ] = Ì†µÌºé              ‚àí             =(1‚àí )Ì†µÌºé
                                                                                                                                                          Ì†µÌ±õ                        Ì†µÌ±õ
                                                                                                                                                    2
                                                                                                                                                Ì†µÌºé
                                                                                                                                2
                                                                                                                 Bias[Ì†µÌºéÃÇ ] = ‚àí                         ‚â†0
                                                                                                                                                  Ì†µÌ±õ
                                                                                                          2                                                                    2
                                          Therefore, the estimator Ì†µÌºéÃÇ                                         is a biased estimator for Ì†µÌºé ! To correct the bias, we divide the
                                          sample sum of squares by (Ì†µÌ±õ‚àí1).
                                                                                                                                                                     Ì†µÌ±õ
                                                                                                                              Ì†µÌ±õ                         1
                                                                                                               2                          2                                                        2
                                                                                                                                                                                              ÃÑ
                                                                                                            Ì†µÌ±†     =                  Ì†µÌºéÃÇ     =                   ‚àë(Ì†µÌ±ã ‚àíÌ†µÌ±ã ) .
                                                                                                                                                                                 Ì†µÌ±ñ           Ì†µÌ±õ
                                                                                                                         Ì†µÌ±õ ‚àí 1                     Ì†µÌ±õ ‚àí 1
                                                                                                                                                                   Ì†µÌ±ñ=1
                                                                                                                              2                                                                              2                               2
                                          It is straightforward to see that Ì†µÌ±†                                                     is an unbiased estimator for Ì†µÌºé . We call Ì†µÌ±†                                                                   the bias-
                                          corrected variance estimator.
                                                                                       2                                                                           2                    2
                                          Theorem 57.3. Ì†µÌ±† is an unbiased estimator for Ì†µÌºé if Ì†µÌ∞∏(Ì†µÌ±ã ) < ‚àû.
                                                                                                                                                                                        ÃÇ
                                          Definition 57.2. The mean square error of an estimator Ì†µÌºÉ for Ì†µÌºÉ is
                                                                                                                                                                              2
                                                                                                                                              ÃÇ                    ÃÇ
                                                                                                                              MSE[Ì†µÌºÉ] = Ì†µÌ∞∏ [(Ì†µÌºÉ ‚àí Ì†µÌºÉ) ].
                                                                                                                                                     188
            By expanding the square we find that
                                 2
                         ÃÇ     ÃÇ
                     MSE[Ì†µÌºÉ] = Ì†µÌ∞∏ [(Ì†µÌºÉ ‚àí Ì†µÌºÉ) ]
                                          2
                               ÃÇ   ÃÇ   ÃÇ
                          =Ì†µÌ∞∏[(Ì†µÌºÉ‚àíÌ†µÌ∞∏[Ì†µÌºÉ]+Ì†µÌ∞∏[Ì†µÌºÉ]‚àíÌ†µÌºÉ) ]
                                   2                         2
                               ÃÇ   ÃÇ      ÃÇ   ÃÇ  ÃÇ       ÃÇ
                          =Ì†µÌ∞∏[(Ì†µÌºÉ‚àíÌ†µÌ∞∏[Ì†µÌºÉ]) ]+2Ì†µÌ∞∏(Ì†µÌºÉ ‚àíÌ†µÌ∞∏[Ì†µÌºÉ])(Ì†µÌ∞∏[Ì†µÌºÉ] ‚àí Ì†µÌºÉ) + (Ì†µÌ∞∏[Ì†µÌºÉ] ‚àí Ì†µÌºÉ)
                                      2
                               ÃÇ     ÃÇ
                          =Ì†µÌ±âÌ†µÌ±éÌ†µÌ±ü[Ì†µÌºÉ] + (Bias[Ì†µÌºÉ]) .
           Thus the MSE is the variance plus the squared bias. The MSE as a measure of accuracy
            combines the variance and bias.
            Theorem 57.4. For any estimator with a finite variance, we have
                                                 2
                                    ÃÇ     ÃÇ      ÃÇ
                                MSE[Ì†µÌºÉ] = Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü[Ì†µÌºÉ] + (Bias[Ì†µÌºÉ]) .
                                              ÃÇ
            Definition 57.3. An estimator is consistent if MSE[Ì†µÌºÉ] ‚Üí 0 as Ì†µÌ±õ ‚Üí ‚àû.
            Bias is the property of an estimator for finite samples. Consistency is the property of an
            estimator when the sample size gets large. It means that for any given data distribution, there
                                                   ÃÇ
            is a sample size Ì†µÌ±õ suÔ¨Äiciently large such that the estimator Ì†µÌºÉ will be arbitrarily close to the
            true value Ì†µÌºÉ with high probability. In practice, we usually do not know how large this Ì†µÌ±õ has
            to be. But it is a desirable property for an estimator to be considered a ‚Äúgood‚Äù estimator.
            For unbiased estimator, MSE is solely determined by the variance of the estimator. Recall
                                                2
                                           ÃÑ
            that the variance for the sample mean is Ì†µÌ±âÌ†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã ) = Ì†µÌºé /Ì†µÌ±õ. But this is not a very useful
                                           Ì†µÌ±õ
                                               2
            formula because the it depends on unknown parameter Ì†µÌºé . We need to replace these unknown
            parameters by estimators. To put the latter in the same units as the parameter estimate we
            typically take the square root before reporting. We thus arrive at the following concept.
                                             ÃÇ
            Definition 57.4. A standard error of an estimator Ì†µÌºÉ is defined as
                                            1/2
                                        ÃÇ
                                           ÃÇ
                                     Ì†µÌ±ÜÌ†µÌ∞∏(Ì†µÌºÉ) = Ì†µÌ±â
                                  ÃÇ
                 ÃÇ
           where Ì†µÌ±â is the estimator for Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü[Ì†µÌºÉ].
                                       ÃÑ
            Definition 57.5. The standard error for Ì†µÌ±ã is
                                       Ì†µÌ±õ
                                            Ì†µÌ±†
                                        ÃÑ
                                            ‚àö
                                    Ì†µÌ±ÜÌ†µÌ∞∏(Ì†µÌ±ã ) =
                                        Ì†µÌ±õ
                                             Ì†µÌ±õ
           where Ì†µÌ±† is the bias-corrected estimator for Ì†µÌºé.
            Note the difference between standard error and standard deviation. Standard deviation
            describes the dispersion of a distribution. Standard error is the standard deviation of an
           estimator. It indicates the ‚Äúprecision‚Äù of the estimator, thereby carrying a sense of ‚Äúerror‚Äù.
           The smaller the standard error, the more precise the estimator.
                                        189
                     58 Confidence intervals
                                                                                                                           ÃÇ
                     Confidence intervals provide a method of adding more information to an estimator Ì†µÌºÉ when we
                     wish to estimate an unknown parameter Ì†µÌºÉ. We can find an interval (Ì†µÌ∞¥,Ì†µÌ∞µ) that we think has
                     high probability of containing Ì†µÌºÉ. The length of such an interval gives us an idea of how closely
                     we can estimate Ì†µÌºÉ.
                     Definition 58.1. A 100(1‚àíÌ†µÌªº)% confidence interval (CI) for Ì†µÌºÉ is an interval [Ì†µÌ∞ø(Ì†µÌºÉ),Ì†µÌ±à(Ì†µÌºÉ)] such
                     that the probability that the interval contains the true Ì†µÌºÉ is (1 ‚àí Ì†µÌªº).
                     Due to randomness we rarely seek a confidence interval with 100% coverage as this would
                     typically need to be the entire parameter space. Instead we seek an interval which includes
                     the true value with reasonably high probability. Standard choices are Ì†µÌªº = 0.05 and 0.10,
                     corresponding to 95% and 90% confidence.
                     Confidence intervals are reported to indicate the degree of precision of our estimates. The
                     narrower the confidence interval, the more precise the estimate. Because a small range of
                     values contains the true parameter with high probability.
                                                                                                                       ÃÑ
                     With the help of the CLT, it is not hard to find the CI for the sample mean Ì†µÌ±ã . Let‚Äôs set
                                                                                                                       Ì†µÌ±õ
                     Ì†µÌªº = 5%, that is, we are trying to find the CI that contains the true mean 95% of the times.
                     Assume our sample size Ì†µÌ±õ is large enough to invoke the CLT, we thus have
                                                                    ÃÑ
                                                                  Ì†µÌ±ã ‚àíÌ†µÌºá
                                                                    Ì†µÌ±õ
                                                                      ‚àö
                                                                            ‚àºÌ†µÌ±Å(0,1)
                                                                   Ì†µÌºé/  Ì†µÌ±õ
                     Let‚Äôs find the interval [Ì†µÌ±é,Ì†µÌ±è] such that
                                                                 ÃÑ
                                                              Ì†µÌ±ã ‚àíÌ†µÌºá
                                                                 Ì†µÌ±õ
                                                                  ‚àö
                                                    Ì†µÌ±É (Ì†µÌ±é ‚â§            ‚â§Ì†µÌ±è)=1‚àí2Œ¶(Ì†µÌ∞ø)=0.95
                                                               Ì†µÌºé/   Ì†µÌ±õ
                     since the normal distribution is symmetric, Ì†µÌ±è = ‚àíÌ†µÌ±é. By looking at the CDF of standard
                     normal, we get Ì†µÌ±é = ‚àí1.96, Ì†µÌ±è = 1.96. Thus,
                                                                         ÃÑ
                                                                       Ì†µÌ±ã ‚àíÌ†µÌºá
                                                                         Ì†µÌ±õ
                                                                           ‚àö
                                                        Ì†µÌ±É (‚àí1.96 ‚â§              ‚â§1.96)=0.95
                                                                        Ì†µÌºé/  Ì†µÌ±õ
                     With a little rearrangement, we have
                                                                           190
                                                                             Ì†µÌºé                           Ì†µÌºé
                                                                ÃÑ                            ÃÑ
                                                                            ‚àö                            ‚àö
                                                        Ì†µÌ±É (Ì†µÌ±ã ‚àí1.96             ‚â§Ì†µÌºá ‚â§ Ì†µÌ±ã +1.96               )=0.95
                                                                Ì†µÌ±õ                           Ì†µÌ±õ
                                                                              Ì†µÌ±õ                            Ì†µÌ±õ
                                                                            Ì†µÌºé                 Ì†µÌºé
                                                               ÃÑ                  ÃÑ
                                                                           ‚àö                  ‚àö
                         Therefore, the interval [Ì†µÌ±ã ‚àí1.96                    , Ì†µÌ±ã   +1.96        ] contains the true mean 95% of the
                                                               Ì†µÌ±õ                 Ì†µÌ±õ
                                                                            Ì†µÌ±õ                  Ì†µÌ±õ
                         times.
                                                                                                                                                      Ì†µÌºé
                                                                                                                                  ÃÑ       ÃÑ
                                                                                                                                                     ‚àö
                         Theorem58.1. The 100(1‚àíÌ†µÌªº)% confidence interval for the sample mean Ì†µÌ±ã is Ì†µÌ±ã ¬±Ì†µÌ±ß                                                ,
                                                                                                                                  Ì†µÌ±õ      Ì†µÌ±õ     Ì†µÌªº/2
                                                                                                                                                       Ì†µÌ±õ
                                                                                                Ì†µÌªº
                        where Ì†µÌ±ß        is the critical value such that Œ¶(Ì†µÌ±ß              ) =     .
                                   Ì†µÌªº/2                                               Ì†µÌªº/2
                                                                                                2
                                                                               ‚àö                                                           ‚àö
                         In practice, because we do not know Ì†µÌºé/ Ì†µÌ±õ, we replace it with the standard error Ì†µÌ±†/ Ì†µÌ±õ. Thus,
                                                                                ÃÑ
                         we compute the confidence interval as Ì†µÌ±ã ¬±Ì†µÌ±ß                     Ì†µÌ±ÜÌ†µÌ∞∏. However, this replacement is not without
                                                                                Ì†µÌ±õ    Ì†µÌªº/2
                         risk. When the sample size is small, Ì†µÌ±† is a very poor estimate of Ì†µÌºé. For the approximation to
                         be valid, we require either the sample size is large enough (Ì†µÌ±õ ‚â• 30 at least) or the population
                         distribution is nearly normal. Some commonly used confidence levels:
                            ‚Ä¢ 90% CI: Ì†µÌªº = 0.1, Ì†µÌ±ß              =1.645
                                                          0.05
                            ‚Ä¢ 95% CI: Ì†µÌªº = 0.05, Ì†µÌ±ß                =1.96
                                                            0.025
                            ‚Ä¢ 99% CI: Ì†µÌªº = 0.01, Ì†µÌ±ß                =2.58
                                                            0.005
                        Wegothrough some common misunderstandings about confidence intervals through an exam-
                         ple. Suppose we have a sample fo size 50 with mean 3.2 and standard deviation 1.74. We
                         construct the 95% confidence interval as
                                                                              1.74
                                                                ÃÑ
                                                                              ‚àö
                                                             Ì†µÌ±ã ¬±1.96√ó               ‚âà3.2¬±0.5=(2.7,3.7).
                                                                                50
                         Now check the following interpretations (true or false):
                            1. We are 95% confident that the sample mean is between 2.7 and 3.7.
                                                                                                          ÃÑ
                                 False. The CI definitely contains the sample mean Ì†µÌ±ã.
                            2. 95% of the population observations are in 2.7 to 3.7.
                                 False. The CI is about covering the population mean, not for covering 95% of the entire
                                 population.
                            3. The true mean falls in the interval (2.7, 3.7) with probability 95%.
                                 False.    The true mean Ì†µÌºá is a fixed number, not a random one that happens with a
                                 probability.
                            4. If a new random sample is taken, we are 95% confident that the new sample mean will
                                 be between 2.7 and 3.7.
                                 False. The confidence interval is for covering the population mean, not for covering the
                                 mean of another sample.
                                                                                       191
          5. This confidence interval is not valid if the population or sample is not normally dis-
           tributed.
           False. The construction of the CI only uses the normality of the sampling distribution
           of the sample mean (by the CLT). Neither the population nor the sample is required to
           be normally distributed.
        So what is exactly the thing that has a 95% change to happen? It is the procedure to con-
        struct the 95% interval. About 95% of the intervals constructed following the procedure will
        cover the true population mean Ì†µÌºá. After taking the sample and an interval is constructed,
        the constructed interval either covers Ì†µÌºá or it doesn‚Äôt. But if we were able to take many such
        samples and reconstruct the interval many times, 95% of the intervals will contain the true
        mean.
                            192
                          59 Hypothesis testing
                          Confidence interval allows us to construct an interval estimate of a population parameter.
                          Hypothesis testing allows us to test specific hypothesis about a population parameter with the
                          evidence obtained from a sample. The earliest use of statistical hypothesis testing is generally
                          credited to the question of whether male and female births are equally likely (null hypothesis),
                          which was addressed in the 1700s by John Arbuthnot and later by Pierre-Simon Laplace.
                          Let Ì†µÌ±ù be the population ratio (defined as the ratio of boys to the total number of babies). We
                          hypotheses that
                                                                                       Ì†µÌ∞ª ‚à∂ Ì†µÌ±ù = 0.5
                                                                                         0
                          This is called the null hypothesis, which is the hypothesis we want to test. If the null
                          hypothesis is false, we have
                                                                                       Ì†µÌ∞ª ‚à∂ Ì†µÌ±ù ‚â† 0.5
                                                                                         1
                          This is called the alternative hypothesis. How am I able to test which hypothesis is true?
                          I can answer this question by collecting a small sample. Suppose I have collected a sample of
                          50 babies computed a sample ratio of Ì†µÌ±ùÃÇ = 0.55. Does it prove or disprove the hypothesis?
                          Note that the ratio Ì†µÌ±ùÃÇ can be regarded as a sample mean. Let Ì†µÌ±ã be a random variable that
                                                                                                                           Ì†µÌ±ñ
                                                                                                                            Ì†µÌ±õ
                                                                                                                      1
                          equals 1 if the Ì†µÌ±ñ-th baby is a boy and 0 otherwise. Then, Ì†µÌ±ùÃÇ =                               ‚àë Ì†µÌ±ã. Thevariance of Ì†µÌ±ùÃÇ is
                                                                                                                                    Ì†µÌ±ñ
                                                                                                                      Ì†µÌ±õ
                                                                                                                            Ì†µÌ±ñ=1
                          given by
                                                                                         Ì†µÌ±õ
                                                                                    1                          Ì†µÌ±ù(1 ‚àí Ì†µÌ±ù)
                                                                    Ì†µÌ±â Ì†µÌ±éÌ†µÌ±ü(Ì†µÌ±ùÃÇ) =      ‚àëÌ†µÌ±âÌ†µÌ±éÌ†µÌ±ü(Ì†µÌ±ã ) =
                                                                                                       Ì†µÌ±ñ
                                                                                     2
                                                                                   Ì†µÌ±õ                               Ì†µÌ±õ
                                                                                        Ì†µÌ±ñ=1
                          since Ì†µÌ±ã is a Bernoulli random variable. By the Central Limit Theorem, we have
                                      Ì†µÌ±ñ
                                                                                    Ì†µÌ±ùÃÇ ‚àí Ì†µÌ±ù
                                                                                               ‚ÜíÌ†µÌ±Å(0,1)
                                                                                     Ì†µÌ±ù(1‚àíÌ†µÌ±ù)
                                                                                  ‚àö
                                                                                        Ì†µÌ±õ
                          Suppose Ì†µÌ∞ª is true, then we know the distribution of Ì†µÌ±ùÃÇ. In particular, there is 95% chance
                                           0
                          that Ì†µÌ±ùÃÇ would be in the interval
                                                                                        Ì†µÌ±ù(1 ‚àí Ì†µÌ±ù)
                                                                                    ‚àö
                                                                        Ì†µÌ±ù ¬± 1.96                    =0.5¬±0.14
                                                                                             Ì†µÌ±õ
                          Our observed sample mean Ì†µÌ±ùÃÇ = 0.55 is not outrageous. It is well within this interval. That
                          means the evidence is not against the null hypothesis. It does not mean Ì†µÌ∞ª is true. But it is
                                                                                                                                         0
                          reasonable given we have observed a sample mean Ì†µÌ±ùÃÇ = 0.55.
                                                                                             193
            Suppose we have observed Ì†µÌ±ùÃÇ = 0.65. This piece of evidence does not seem to be consistent with
            the null hypothesis. Because if Ì†µÌ∞ª is true, we only have less than 5% chance of observing this
                                 0
            sample mean. It is extremely unlikely. Based on this sample, we are more inclined to reject
            the Ì†µÌ∞ª . Rejecting the null hypothesis does not mean it is false, but it means our evidence does
               0
            not support this hypothesis.
            Ì†µÌ±ù-value: the probability of obtaining test results at least as extreme as the result actually
            observed, under the assumption that the null hypothesis is correct. A very small Ì†µÌ±ù-value means
            that such an extreme observed outcome would be very unlikely under the null hypothesis. Thus,
            The smaller the Ì†µÌ±ù-value, the stronger the evidence against the Ì†µÌ∞ª .
                                                    0
            In some studies, we can simply report the Ì†µÌ±ù-value and let people judge whether the evidence
            is strong enough. In other studies, we prefer to select a cut-off value Ì†µÌªº, call the significance
            level, and follow the rule:
             ‚Ä¢ If the Ì†µÌ±ù-value < Ì†µÌªº, reject Ì†µÌ∞ª ;
                                0
             ‚Ä¢ If the Ì†µÌ±ù-value > Ì†µÌªº, do not reject Ì†µÌ∞ª .
                                     0
            Commonly used significance levels: 0.05 and 0.01. And we like to use the word ‚Äúsignificant‚Äù
            to describe the test result:
             ‚Ä¢ A test with Ì†µÌ±ù-value < 0.05 is said to be (statistically) significant;
             ‚Ä¢ A test with Ì†µÌ±ù-value < 0.01 is said to be highly significant.
           Whenwemakeadecision about accepting or rejecting a hypothesis, there are chances that we
            make a mistake. There are two types of mistakes: Type 1 error and Type 2 error.
                                        194
                                                           Decision
                                                          Reject Ì†µÌ∞ª     Fail to reject Ì†µÌ∞ª
                                                                   0                    0
                                            Ì†µÌ∞ª is true   Type 1 error   ‚úì
                                              0
                                            Ì†µÌ∞ª is false       ‚úì         Type 2 error
                                              0
                  Type 1 error is rejecting the Ì†µÌ∞ª when it is true. Type 2 error is failing to reject the Ì†µÌ∞ª
                                                    0                                                          0
                  when it is false. Usually, it is more important to control the Type 1 error than the the Type
                  2 error. That is, we want to minimize the chance of falsely rejecting the null hypothesis.
                  In the example above, we reject the null hypothesis on the ground that there is only 2.3% of
                  the chance that we could observe this sample. Therefore, the probability of Type 1 error is
                  only 2.3%.
                  If we make decisions based on a significance level, the significance level is the Type 1 error
                  rate. In other words, when using a 5% significance level, there is 5% chance of making a Type
                  1 error.
                                                 Ì†µÌ±É(Type 1 error|Ì†µÌ∞ª is true) = Ì†µÌªº
                                                                   0
                  This is why we prefer small values of Ì†µÌªº‚Äîsmaller Ì†µÌªº reduces the Type 1 error rate. However,
                  significance level doesn‚Äôt control Type 2 error rate.
                  Hypothesis testing with Ì†µÌ±ß-statistics
                  We may have noticed that, in the above example, the assumption that the population Ì†µÌºé is
                                                                                                      ‚àö
                  known is unrealistic. In practice, we approximate it with the standard error Ì†µÌ±†/ Ì†µÌ±õ. The
                  approximate is valid if the the sample size is large enough or the underlying distribution is
                  nearly normal. If this is not the case, we would opt for a Ì†µÌ±°-test. Here we summarize the steps
                  of testing for a population mean with Ì†µÌ±ß-statistics.
                  We notice that the two-sided hypothesis tests are very closed related to the concept of con-
                  fidence intervals. A two-sided test means we are interested in rejection regions on both sides
                  of the tail distribution. Typically, the alternative hypothesis is Ì†µÌ∞ª ‚à∂ Ì†µÌºá ‚â† Ì†µÌºá .
                                                                                    1        0
                                                               195
                  Suppose we are doing a hypothesis test under the significance level Ì†µÌªº, the region of accepting
                  the Ì†µÌ∞ª is
                         0
                                                                  ÃÑ
                                                                Ì†µÌ±ã ‚àíÌ†µÌºá
                                                       ‚àíÌ†µÌ±ß   ‚â§          ‚â§Ì†µÌ±ß
                                                          Ì†µÌªº/2             Ì†µÌªº/2
                                                                  Ì†µÌ±ÜÌ†µÌ∞∏
                  such that the rejection region (Ì†µÌ±ù-value) has probability Ì†µÌªº. This is equivalent to
                                                    ÃÑ                   ÃÑ
                                                  Ì†µÌ±ã ‚àíÌ†µÌ±ß   Ì†µÌ±ÜÌ†µÌ∞∏ ‚â§ Ì†µÌºá ‚â§ Ì†µÌ±ã + Ì†µÌ±ß Ì†µÌ±ÜÌ†µÌ∞∏
                                                        Ì†µÌªº/2                Ì†µÌªº/2
                                                                             ÃÑ
                  which is exactly the 100(1‚àíÌ†µÌªº)% confidence interval of Ì†µÌ±ã. Therefore, for a two-sided test, we
                  have the rule:
                                                                         ÃÑ
                     ‚Ä¢ Reject Ì†µÌ∞ª if Ì†µÌºá is not in the 100(1 ‚àí Ì†µÌªº)% CI: Ì†µÌ±ã ¬± Ì†µÌ±ß   Ì†µÌ±ÜÌ†µÌ∞∏
                                  0                                          Ì†µÌªº/2
                  Weconclude this chapter by reiterating a couple of critical points that could be easily misun-
                  derstood.
                  Rejecting Ì†µÌ∞ª doesn‚Äôt means we are 100% sure that Ì†µÌ∞ª is false. We might make Type 1 errors.
                               0                                         0
                  Setting a significance level just guarantee we won‚Äôt make Type 1 error too often.
                  Failing to reject Ì†µÌ∞ª does not necessarily mean Ì†µÌ∞ª is true. We could make a type 2 error when
                                     0                              0
                  failing to reject Ì†µÌ∞ª . Moreover, unlike type 1 error rate is controlled at a low level, type 2 error
                                     0
                  rate is usually quite high. When we fail to reject Ì†µÌ∞ª , it just means the data are not able to
                                                                        0
                  distinguish between Ì†µÌ∞ª and Ì†µÌ∞ª . That‚Äôs why we say fail to reject. Ì†µÌ±ù-value is not the probability
                                         0       1
                  that the Ì†µÌ∞ª is true.
                              0
                                                                 196
                        Saying that results are statistically significant just informs the reader that the findings are
                        unlikely due to chance alone. However, it says nothing about the practical importance of the
                        finding. For example, rejecting the Ì†µÌ∞ª : Ì†µÌºá = Ì†µÌºá does not tell us how big the difference |Ì†µÌºá‚àíÌ†µÌºá |
                                                                           0          0                                                              0
                        is. Mostly in practice we care more about the magnitude of this difference, rather than the
                        fact that they are indeed different. It is possible that the difference is too small to be relevant
                        even if it is significant.
                        Hypothesis testing with Ì†µÌ±°-statistics
                        When the sample size is small, we opt for Ì†µÌ±°-test for more reliable hypothsis testing. Define
                        test statistics
                                                                                         ÃÑ
                                                                                       Ì†µÌ±ã ‚àíÌ†µÌºá
                                                                                          ‚àö
                                                                                Ì†µÌ±á =
                                                                                       Ì†µÌ±†/   Ì†µÌ±õ
                        where Ì†µÌ±† is the sample standard deviation. For small samples, this test statistics follows a
                        Student Ì†µÌ±°-distribution with Ì†µÌ±õ degrees of freedom, Ì†µÌ±á ‚àº Ì†µÌ±°(Ì†µÌ±õ).
                        Why Student-Ì†µÌ±° distribution? Recall the definition of Student-Ì†µÌ±° distribution: when the under-
                                                                                                               2                2
                        lying distribution of Ì†µÌ±ã ,Ì†µÌ±ã ,‚Ä¶,Ì†µÌ±ã is Normal, sample variance Ì†µÌ±† follows a Ì†µÌºí distribution. Ì†µÌ±á
                                                       1    2         Ì†µÌ±õ
                        follows Ì†µÌ±° distribution by definition regardless of the sample size. However, if the underlying
                        distribution is not normal, this argument loses ground. We use Ì†µÌ±°-test mainly as a convention.
                        But Ì†µÌ±° distribution has heavier tails than standard normal, meaning that we are more likely to
                        reject a hypothesis based on Ì†µÌ±° distribution. In other words, Ì†µÌ±°-test is a more conservative choice
                        than Ì†µÌ±ß-test for small samples.
                                                                 one-tail Ì†µÌªº       0.05     0.025      0.005
                                                                 two-tail Ì†µÌªº       0.10      0.05       0.01
                                                                      d.f.
                                                                      10          1.812     2.228      3.169
                                                                      20          1.725     2.086      2.845
                                                                      30          1.697     2.042      2.750
                                                                   Ì†µÌ±ß value       1.645     1.960      2.576
                        The table shows a few critical values for Ì†µÌ±°-test with different degrees of freedom (d.f.). We can
                        see as the sample size gets larger, Ì†µÌ±° distribution converges to standard normal.
                                                                                     197
