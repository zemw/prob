# Binomial distribution

::: {#def-bin}
## Binomial distribution

Suppose $X_{1},X_{2},\dots,X_{n}$ are independent and identical
$\text{Bern}(p)$ distributions. Let $X$ be the total number of successes
of the $n$ independent trials. That is, $X=X_{1}+X_{2}+\cdots+X_{n}$.
Then $X$ has the Binomial distribution, $X\sim \text{Bin}(n,p)$.
:::

The PMF of $X$ directly follows from the combination theory:
$$P(X=k)=\binom{n}{k}p^{k}(1-p)^{n-k}.$$

This is a valid PMF because, by the Binomial theorem, we have
$$\sum_{k=0}^{n}\binom{n}{k}p^{k}(1-p)^{n-k}=(p+(1-p))^{n}=1.$$

::: callout-note
## Binomial distribution and Binomial theorem

You may have noticed the connection between Binomial distribution and
Binomial theorem. Consider using the polynomial $px+q$ to represent the
outcome of a single Bernoulli trial, where $x$ is the indicator for a
success. Then $(px+q)^n$ is the outcome for $n$ independent trials. The
coefficient of $x^k$ gives the probability of there being exactly $k$
successes.
:::

::: {#exm-3.3}
In the previous example of tossing two coins, we compute the
distribution of $X$ by counting the equally likely outcomes in an event.
We can get the same result by realizing it is a Binomial distribution.
$X\sim\textrm{Bin}(2,1/2)$. Since each coin tossing is an independent
Bernoulli trial. The probabilities come directly from the PMF.
$$\begin{aligned}
P(X=0) & =\binom{2}{0}\left(\frac{1}{2}\right)^{0}\left(\frac{1}{2}\right)^{2}=\frac{1}{4};\\
P(X=1) & =\binom{2}{1}\left(\frac{1}{2}\right)^{1}\left(\frac{1}{2}\right)^{1}=\frac{1}{2};\\
P(X=2) & =\binom{2}{2}\left(\frac{1}{2}\right)^{2}\left(\frac{1}{2}\right)^{0}=\frac{1}{4}.\end{aligned}$$

Utilizing the Binomial distribution also allows us to generalize the
problem. Suppose we are tossing $n$ coins, we want to find the
probability of getting $k$ heads. It is almost impossible to count all
the possible outcomes, but the answer immediately follows from the
Binomial PMF.
:::

::: {#thm-bin}
Let $X\sim \text{Bin}(n,p)$ and $Y\sim \text{Bin}(m,p)$ be two
independent Binomial random variables. Then $X+Y\sim \text{Bin}(n+m,p)$.
:::

::: proof
By the definition of the Binomial distribution, $X=\sum_{i=1}^{n}X_{i}$
where $X_{i}\sim \text{Bern}(p)$; $Y=\sum_{j=1}^{m}Y_{j}$ where
$Y_{j}\sim \text{Bern}(p)$. Therefore,
$$X+Y=\sum_{i=1}^{n}X_{i}+\sum_{j=1}^{m}Y_{j}=\sum_{k=1}^{n+m}Z_{k}$$
where $Z_{k}\sim \text{Bern}(p)$. Since $X_{i}$ and $Y_{j}$ are
identical Bernoulli random variables,
$$Z_k = \begin{cases}X_k,&\text{ for }k=1,\dots,n\\ 
Y_{k-n}, &\text{ for }k=n+1,\dots,n+m
\end{cases}$$ By definition,
$X+Y=\sum_{k=1}^{n+m}Z_{k}\sim \text{Bin}(n+m,p)$.
:::

<!-- Another way to prove it is to leverage the PMF: $$\begin{aligned} -->

<!-- P(X+Y=k) & =\sum_{i+j=k}P(X=i)P(Y=j)\\ -->

<!--  & =\sum_{i+j=k}\binom{n}{i}p^{i}(1-p)^{n-i}\binom{m}{j}p^{j}(1-p)^{m-j}\\ -->

<!--  & =\sum_{i+j=k}\binom{n}{i}\binom{m}{j}p^{i+j}(1-p)^{m+n-i-j}\\ -->

<!--  & =p^{k}(1-p)^{m+n-k}\sum_{i=0}^{k}\binom{n}{i}\binom{m}{k-i}\\ -->

<!--  & =p^{k}(1-p)^{m+n-k}\binom{n+m}{k}.\end{aligned}$$ -->

<!-- The last step: $\binom{n+m}{k}=\sum_{i=0}^{k}\binom{n}{i}\binom{m}{k-i}$ -->

<!-- is known as the Vandermonde's identity. -->

There are built-in functions in R to work with Binomial distributions.

```{r, fig.asp=0.5}
# computes P(X=5) for Bin(10,0.5)
p <- dbinom(5, 10, 0.5)

par(mfrow=c(1,2))

# plot the PMF for Bin(10,0.5)
curve(dbinom(x, 10, 0.5), from=0, to=10, n=11, type="b", ann=F)

# `pbinom` computes the CDF 
curve(pbinom(x, 10, 0.5), from=0, to=10, n=11, type="b",ann=F)
```

```{r}
# draw a random value from a given Binomial distribution
# this allows us to simulate a random experiment
# e.g. the number of heads when flipping 10 fair coins
outcome <- rbinom(1, 10, 0.5)

# Repeat the experiment 1000 times
heads <- rbinom(1000, 10, 0.5)

# the histogram will converge to the ideal Binomial distribution
# if the experiment is repeated a large number of times
hist(heads)
```
