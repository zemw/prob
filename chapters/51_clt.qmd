## Central limit theorem

The LLN shows the convergence of the sample mean to the population mean.
What about the entire sample distribution? This is addressed by the
central limit theorem (CLT), which, as its name suggests, is a limit
theorem of central importance in statistics.

The CLT states that for large $n$, the distribution of $\bar{X}_{n}$
after standardization approaches a standard Normal distribution,
regardless of the underlying distribution of $X_{i}$. By
standardization, we mean that we subtract $\mu$, the expected value of
$\bar{X}_{n}$, and divide by $\sigma/\sqrt{n}$, the standard deviation
of $\bar{X}_{n}$.

::: {#thm-7.3}
(Central limit theorem). As $n\to\infty$,
$$\sqrt{n}\left(\frac{\bar{X}_{n}-\mu}{\sigma}\right)\to N(0,1)\textrm{ in distribution.}$$

In other words, the CDF of the left-hand side approaches the CDF of the
standard normal distribution.
:::

::: proof
We will prove the CLT assuming the MGF of the $X_{i}$ exists, though the
theorem holds under much weaker conditions. Without loss of generality
let $\mu=1,\sigma^{2}=1$ (since we standardize it anyway). We show that
the MGF of $\sqrt{n}\bar{X}_{n}=(X_{1}+\cdots+X_{n})/\sqrt{n}$ converges
to the MGF of the $N(0,1)$.

The MGF of $N(0,1)$ is

$$\begin{aligned}
E(e^{tX}) & =\int_{-\infty}^{\infty}e^{tx}\cdot\frac{1}{\sqrt{2\pi}}e^{-x^{2}/2}dx\\
 & =\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}e^{-x^{2}/2+tx}dx\\
 & =\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x-t)^{2}+\frac{1}{2}t^{2}}dx\\
 & =e^{\frac{t^{2}}{2}}\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x-t)^{2}}dx\\
 & =e^{t^{2}/2}\end{aligned}$$

Compute the MGF of $\sqrt{n}\bar{X}_{n}$:

$$\begin{aligned}
E(e^{\sqrt{n}\bar{X}_{n}}) & =E(e^{t(X_{1}+\cdots+X_{n})/\sqrt{n}})\\
 & =E(e^{tX_{1}/\sqrt{n}})E(e^{tX_{2}/\sqrt{n}})\cdots E(e^{tX_{n}/\sqrt{n}})\\
 & =\left[E(e^{tX_{i}/\sqrt{n}})\right]^{n}\qquad\textrm{since }i.i.d\\
 & =\left[E\left(1+\frac{tX_{i}}{\sqrt{n}}+\frac{t^{2}X_{i}^{2}}{2n}+o(n^{-1})\right)\right]^{n}\\
 & =\left[1+\frac{t}{\sqrt{n}}E(X_{i})+\frac{t^{2}}{2n}E(X_{i}^{2})+o(n^{-1})\right]^{n}\\
 & =\left[1+\frac{t^{2}}{2n}+o(n^{-1})\right]^{n}\\
 & =\left[1+\frac{t^{2}/2}{n}+o(n^{-1})\right]^{n}\\
 & \to e^{t^{2}/2}\qquad\textrm{as }n\to\infty\end{aligned}$$

Therefore, the MGF of $\sqrt{n}\bar{X}_{n}$ approaches the MGF of the
standard normal. Since MGF determines the distribution, the distribution
of $\sqrt{n}\bar{X}_{n}$ also approaches the standard normal
distribution.
:::

The CLT tells us about the limiting distribution of $\bar{X}_{n}$ as
$n\to\infty$. That means, we can reasonably approximate the distribution
$\bar{X}_{n}$ with normal distribution when $n$ is a finite large
number.
$$\bar{X}_{n}\approx N(\mu,\sigma^{2}/n)\quad\textrm{for large }n.$$

The Central Limit Theorem was first proved by Pierre-Simon Laplace in
1810. Let's take a moment to admire the generality of this result. The
distribution of the individual $X_{i}$ can be anything in the world, as
long as the mean and variance are finite. This does mean the
distribution of $X_{i}$ is irrelevant, however. If the distribution is
fairly close to normal, the result would hold for smaller $n$. If the
distribution is far away from normal, it would take larger $n$ to
converge.

The CLT gives the distribution of the sample mean regardless of the
underlying distribution. This allows to assess the "quality" of the
sample mean --- how close it is to the true mean. The LLN tells us the
larger the sample, the closer the sample mean to the population mean.
The CLT tells us the distribution of the sample mean for sample size
$n$. For smaller $n$, the distribution is more spread-out (a normal
distribution with large $\sigma^{2}$); hence the uncertainty is huge,
other values are more likely. For larger $n$, the uncertainty is
reduced, most values would be centered around the true mean. We will
delve deeper into this when we get to hypothesis testing.

::: {#exm-7.3}
Suppose that a fair coin is tossed 900 times. Approximate the
probability of obtaining more than 395 heads.
:::

::: solution
Let $H=\sum_{i=1}^{900}X_{i}$ be the number of heads, where
$X_{i}\sim\textrm{Bern}(\frac{1}{2})$. We could compute the probability
by

$$P(H>495)=\sum_{k=496}^{900}\binom{900}{k}\left(\frac{1}{2}\right)^{k}\left(\frac{1}{2}\right)^{900-k}$$

But this is quite tedious. Because $n=900$ is reasonably large, we can
apply the CLT:

$$\begin{aligned}
\frac{1}{n}\sum_{i=1}^{900}X_{i} & \sim N(\mu,\sigma^{2}/n)\quad\textrm{or}\\
\sum_{i=1}^{900}X_{i} & \sim N(n\mu,n\sigma^{2})\end{aligned}$$

We know $\mu=E(X_{i})=\frac{1}{2}$, $\sigma^{2}=Var(X_{i})=\frac{1}{4}$.
Thus $H\sim N(450,225)$. Therefore,

$$P(H>495)=1-P(H\leq495)\approx1-\Phi\left(\frac{495-450}{15}\right)=0.0013.$$
:::

