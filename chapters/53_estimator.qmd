## Estimator accuracy

The central question of statistics is we want to learn about the
population from a finite sample. We know sample mean is different from
the population mean. But we also want to know how large the error could
be, that is, how far or close the sample mean is from the true
population mean. The question is exceedingly difficult to answer because
the population mean is unknown. Fortunately, with the help of the CLT,
we can say more about the distribution of the sample mean. This chapter
bridges our probability theory with statistics. We use the theorems we
have derived to infer the properties of a statistic.

As the purpose of statistics is to learn about the population, we want
our sample estimator to be as good as possible. But what is a "good"
estimator? This section we discuss two properties that we usually demand
from a good estimator, namely, unbiased and consistency. Next section
will tackle the more challenging concept of confidence interval.

::: {#def-7.1}
The bias of an estimator $\hat{\theta}$ of a parameter $\theta$ is
$$\textrm{Bias}[\hat{\theta}]=E(\hat{\theta})-\theta.$$ We say that an
estimator is **biased** if its sampling is incorrectly centered. We say
that an estimator is **unbiased** is the bias is zero.
:::

::: {#thm-7.4}
$\bar{X}_{n}$ is **unbiased** for $\mu=E(x)$ if $E(X)<\infty$.
:::

::: proof
$$E(\bar{X}_{n})=E\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right)=\frac{1}{n}\sum_{i=1}^{n}E(X_{i})=\frac{1}{n}\sum_{i=1}^{n}\mu=\mu.$$
:::

::: {#thm-7.5}
If $\hat{\theta}$ is an unbiased estimator of $\theta$, then
$\hat{\beta}=a\hat{\theta}+b$ is an unbiased estimator of
$\beta=a\theta+b$.
:::

But obtaining an unbiased estimator is not always as straightforward as
it seems. Consider the sample variance as an estimator for the
population variance. By the analog principle, the sample variance should
be

$$\begin{aligned}
\hat{\sigma}^{2} & =\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\bar{X_{n}})^{2}\\
 & =\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\mu+\mu-\bar{X_{n}})^{2}\\
 & =\frac{1}{n}\sum(X_{i}-\mu)^{2}+\frac{2}{n}\sum(X_{i}-\mu)(\mu-\bar{X_{n}})+\frac{1}{n}\sum(\mu-\bar{X_{n}})^{2}\\
 & =\frac{1}{n}\sum(X_{i}-\mu)^{2}+2(\bar{X_{n}}-\mu)(\mu-\bar{X_{n}})+(\mu-\bar{X_{n}})^{2}\\
 & =\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\mu)^{2}-(\bar{X}_{n}-\mu)^{2}\\
 & =\tilde{\sigma}^{2}-(\bar{X}_{n}-\mu)^{2}\end{aligned}$$

We know that
$$E(\tilde{\sigma}^{2})=\frac{1}{n}\sum_{i=1}^{n}E(X_{i}-\mu)^{2}=\sigma^{2}$$
Thus, if we compute the bias of this estimator:

$$\begin{aligned}
E[\hat{\sigma}^{2}] & =\sigma^{2}-\frac{\sigma^{2}}{n}=\left(1-\frac{1}{n}\right)\sigma^{2}\\
\textrm{Bias}[\hat{\sigma}^{2}] & =-\frac{\sigma^{2}}{n}\neq0\end{aligned}$$

Therefore, the estimator $\hat{\sigma}^{2}$ is a biased estimator for
$\sigma^{2}$! To correct the bias, we divide the sample sum of squares
by $(n-1)$.

$$s^{2}=\frac{n}{n-1}\hat{\sigma}^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\bar{X_{n}})^{2}.$$

It is straightforward to see that $s^{2}$ is an unbiased estimator for
$\sigma^{2}$. We call $s^{2}$ the **bias-corrected variance estimator**.

::: {#thm-7.6}
$s^{2}$ is an unbiased estimator for $\sigma^{2}$ if $E(X^{2})<\infty$.
:::

::: {#def-7.2}
The mean square error of an estimator $\hat{\theta}$ for $\theta$ is

$$\textrm{MSE}[\hat{\theta}]=E\left[(\hat{\theta}-\theta)^{2}\right].$$
:::

By expanding the square we find that

$$\begin{aligned}
\textrm{MSE}[\hat{\theta}] & =E\left[(\hat{\theta}-\theta)^{2}\right]\\
 & =E\left[(\hat{\theta}-E[\hat{\theta}]+E[\hat{\theta}]-\theta)^{2}\right]\\
 & =E\left[(\hat{\theta}-E[\hat{\theta}])^{2}\right]+2E(\hat{\theta}-E[\hat{\theta}])(E[\hat{\theta}]-\theta)+(E[\hat{\theta}]-\theta)^{2}\\
 & =Var[\hat{\theta}]+(\textrm{Bias}[\hat{\theta}])^{2}.\end{aligned}$$

Thus the MSE is the variance plus the squared bias. The MSE as a measure
of accuracy combines the variance and bias.

::: {#thm-7.7}
For any estimator with a finite variance, we have
$$\textrm{MSE}[\hat{\theta}]=Var[\hat{\theta}]+(\textrm{Bias}[\hat{\theta}])^{2}.$$
:::

::: {#def-7.3}
An estimator is consistent if $\textrm{MSE}[\hat{\theta}]\to0$ as
$n\to\infty$.
:::

Bias is the property of an estimator for finite samples. Consistency is
the property of an estimator when the sample size gets large. It means
that for any given data distribution, there is a sample size $n$
sufficiently large such that the estimator $\hat{\theta}$ will be
arbitrarily close to the true value $\theta$ with high probability. In
practice, we usually do not know how large this $n$ has to be. But it is
a desirable property for an estimator to be considered a "good"
estimator.

For unbiased estimator, MSE is solely determined by the variance of the
estimator. Recall that the variance for the sample mean is
$Var(\bar{X}_{n})=\sigma^{2}/n$. But this is not a very useful formula
because the it depends on unknown parameter $\sigma^{2}$. We need to
replace these unknown parameters by estimators. To put the latter in the
same units as the parameter estimate we typically take the square root
before reporting. We thus arrive at the following concept.

::: {#def-7.4}
A standard error of an estimator $\hat{\theta}$ is defined as
$$SE(\hat{\theta})=\hat{V}^{1/2}$$ where $\hat{V}$ is the estimator for
$Var[\hat{\theta}]$.
:::

::: {#def-7.5}
The standard error for $\bar{X}_{n}$ is
$$SE(\bar{X}_{n})=\frac{s}{\sqrt{n}}$$ where $s$ is the bias-corrected
estimator for $\sigma$.
:::

Note the difference between **standard error** and **standard
deviation**. Standard deviation describes the dispersion of a
distribution. Standard error is the standard deviation of an
*estimator*. It indicates the "precision" of the estimator, thereby
carrying a sense of "error". The smaller the standard error, the more
precise the estimator.

