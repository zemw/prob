[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Probability",
    "section": "",
    "text": "Overview\nThis book is adapted from MAT921: Probability at Southwest University of Finance and Economics (RIEM). It is an introductory probability course that aims to be not boring. In this course, we try to blend conventional teaching, interesting puzzles and data-oriented practical skills.\nCourse Instructor’s email: gamma12@126.com. Please indicate your class and student ID when you email me.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Introduction to Probability",
    "section": "Syllabus",
    "text": "Syllabus\nTopic 1: Classical probabilities\nHow likely were some of your classmates born on the same day as you?\nTopic 2: Data and random variables\nWhy is your exam score in this class a random variable?\nTopic 3: Discrete distributions\nHow many earthquakes are likely to happen in a random year?\nTopic 4: Expectation and variance\nHow old are you expected to live?\nTopic 5: Continuous distributions\nHow long are you expected to wait in the queue at a restaurant?\nTopic 6: Limiting theorems\nWhy a lottery company never loses?\nTopic 7: Sampling distribution\nHow do I know I am taller than an average person?",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#assessment",
    "href": "index.html#assessment",
    "title": "Introduction to Probability",
    "section": "Assessment",
    "text": "Assessment\nQuiz (25%). There will be an arbitrary number of in-class quizzes. The date for each quiz will be announced in advance. Each quiz will consist of 1-2 questions based on material covered in previous weeks. Every quiz is mandatory; there will be no make-up quizzes under any circumstances.\nProject (25%). The goal of the projects is to encourage students to apply the knowledge learned in this course to solve practical problems. Projects are usually open-ended and may involve data analysis, simulations, or exploring real-world applications of probability. Essays that present innovative perspectives and use the data persuasively to support their conclusions will receive higher marks. Selective students may be invited to present their findings to the class.\nFinal exam (50%). The final exam will be a closed-book, paper-and-pencil exam scheduled for Week 17. It will not simply repeat lecture material but will assess your ability to apply the knowledge you have gained to solve novel problems. To perform well, you must have a deep understanding of the concepts and acquire some degree of problem-solving skills. The average score of the past exam is 69 with a standard deviation 15. The pass rate (&gt;=60) is about 80%.\nClass participation (5%). Additional 5 marks for class participation on top of the above. Regular attendance and constructive participation are are encouraged (though not mandatory) and will be recognized. Active engagement in class discussions or asking insightful questions will also be considered.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#lecture-notes",
    "href": "index.html#lecture-notes",
    "title": "Introduction to Probability",
    "section": "Lecture notes",
    "text": "Lecture notes\nAll lecture materials will be published through this online website. You are not required to read any textbook. For students who insist on a textbook, it would be DeGroot and Schervish’s Probability and Statistics (4th edition).\nIt is recommended to use the textbook as a supplement not a replacement of the lecture note. For students who prefer to read the textbook. There are two key differences between this lecture note and the textbook. First, the sections are arranged differently. Second, the examples and exercises are entirely different despite the key definitions and theorems are the same.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#homework",
    "href": "index.html#homework",
    "title": "Introduction to Probability",
    "section": "Homework",
    "text": "Homework\nThere is no homework assignment in this course. We will do in-class exercises instead. However, problem solving is essential for learning math. You are encouraged to practice the exercises in DeGroot and Schervish’s textbook after class. But it is not mandatory.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#statistical-software",
    "href": "index.html#statistical-software",
    "title": "Introduction to Probability",
    "section": "Statistical software",
    "text": "Statistical software\nStatistical software is indispensable for modern statistics. For practical consideration, it is beneficial to start learn it as early as possible. We will demonstrate how to do statistics in R, which is a widely-used open-source statistical programming language. It is highly recommended that you try it yourself while learning this course.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#students-evaluation",
    "href": "index.html#students-evaluation",
    "title": "Introduction to Probability",
    "section": "Students’ evaluation",
    "text": "Students’ evaluation\nIf you have not decided whether to enroll in this course or not. Here are some surveys from the past students for your reference. In general, this is not an easy course especially for freshmen. We will deal with serious maths, though I will try to convey the beauty of the subject as much as possible.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#reference",
    "href": "index.html#reference",
    "title": "Introduction to Probability",
    "section": "Reference",
    "text": "Reference\n\nSchervish, M. J., & DeGroot, M. H. (2014). Probability and statistics. Pearson Education.\nBlitzstein, J. K., & Hwang, J. (2019). Introduction to probability. Chapman and Hall/CRC.\nGrimmett, G., & Stirzaker, D. (2020). Probability and random processes. Oxford University Press.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#online-playground",
    "href": "index.html#online-playground",
    "title": "Introduction to Probability",
    "section": "Online playground",
    "text": "Online playground\nProbability Playground: Interactive Probability Distributions\nStatKey: Statistics Unlocking the Power of Data\nMathIsFun: Standard Normal Distribution Table\nCentral Limit Theorem Demo",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#exam-score-lookup",
    "href": "index.html#exam-score-lookup",
    "title": "Introduction to Probability",
    "section": "Exam score lookup",
    "text": "Exam score lookup\n\nStudent number\n\n Find scores\n\n\nLoading exam scores…",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#copyright",
    "href": "index.html#copyright",
    "title": "Introduction to Probability",
    "section": "Copyright ©",
    "text": "Copyright ©\nThe content on this website is made available for online viewing by the public. Redistribution, reproduction, or any other use of the content, in whole or in part, is prohibited without prior written permission from the author.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "chapters/01_intro.html",
    "href": "chapters/01_intro.html",
    "title": "1  What is probability?",
    "section": "",
    "text": "Probability is the most important concept in modern science, especially as nobody has the slightest notion of what it means. —— B. Russell\n\nWhat is probability? We all talk about probabilities in everyday life, but mostly in vague languages. This course is to introduce probability as a logical framework for quantifying uncertainty and randomness.\n\nMathematics is the logic of certainty; probability is the logic of uncertainty. —— J. Blitzstein\n\nThe earliest development of probability is rooted in gambling. The famous Monte Carlo method in statistics, invented by Stanislaw Ulam in the late 1940s, takes its name from the Monte Carlo Casino in Monaco, where Ulam’s uncle went to gamble.\nToday, probability theory has been applied to almost every field of human knowledge. It is the foundation of statistics, machine learning, and artificial intelligence. It also plays a crucial role in everyday decision-making, from stock investments to effective strategies to combat an infectious disease.\nThe first formal definition of probability is often attributed to Pierre-Simon Laplace in the 18th century. In his work Theorie analytique des probabilites, published in 1812,\n\nThe probability of an event is the ratio of the number of cases favorable to it, to the number of all cases possible when nothing leads us to expect that any one of these cases should occur more than any other, which renders them, for us, equally possible.\n\nWe will soon discover that this definition is obsolete. We start the journey of modern probability theory by introducing the basic concepts of events and sample spaces.",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is probability?</span>"
    ]
  },
  {
    "objectID": "chapters/02_event.html",
    "href": "chapters/02_event.html",
    "title": "2  Event and sample space",
    "section": "",
    "text": "Use set language to describe events\nAnything (a gamble, an exam, a financial year, …) with uncertain outcomes can be an experiment. The sample space can be finite, countably infinite, or uncountably infinite. It is convenient to visualize events with Venn diagrams.",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Event and sample space</span>"
    ]
  },
  {
    "objectID": "chapters/02_event.html#simulating-coin-flipping",
    "href": "chapters/02_event.html#simulating-coin-flipping",
    "title": "2  Event and sample space",
    "section": "Simulating coin flipping",
    "text": "Simulating coin flipping\nRandomly sampling from the set \\(\\{H,T\\}\\):\n\n# 10 draws with equal prob with replacement\nsample(c('H', 'T'), 10, replace = T)\n\n [1] \"H\" \"T\" \"H\" \"H\" \"H\" \"T\" \"H\" \"H\" \"H\" \"T\"\n\n\nCompute the probability of \\(HH\\) when tossing two coins:\n\n# simulate coin tossing 10000 times\ntoss &lt;- sample(c('H','T'), 10000, replace =T)\n\n# group them into pairs\ntoss.pair &lt;- paste0(toss[-length(toss)], toss[-1])\n\n# number of HH\nn_HH &lt;- sum(toss.pair == 'HH')\n\n# total number of tosses\nn_total &lt;- length(toss.pair)\n\n# compute the probability\nprob &lt;- n_HH / n_total\n\ncat(\"Prob of HH: \", prob)\n\nProb of HH:  0.2462246",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Event and sample space</span>"
    ]
  },
  {
    "objectID": "chapters/03_classic.html",
    "href": "chapters/03_classic.html",
    "title": "3  Classical probability",
    "section": "",
    "text": "Counting methods\nCalculating the naive probability of an event \\(A\\) often involves counting the number of outcomes in \\(A\\) and the number of outcomes in the sample space \\(S\\), which usually involve some counting methods. We now review some of the counting methods (multiplications, factorials, permutations, combinations) that was introduced in high schools.\nMultiplications. Consider a compound experiment consisting of two sub-experiments, Experiment A and Experiment B. Suppose that Experiment A has \\(a\\) possible outcomes, and for each of those outcomes Experiment B has \\(b\\) possible outcomes. Then the compound experiment has \\(a\\times b\\) possible outcomes.\nExponentiation. Consider \\(n\\) objects and making \\(k\\) choices from them, one at a time with replacement. Then there are \\(n^{k}\\) possible outcomes.\nFactorials. Consider \\(n\\) objects \\(1,2,\\dots,n\\). A permutation of \\(1,2,\\dots,n\\) is an arrangement of them in some order, e.g., \\(3,5,1,2,4\\) is a permutation of \\(1,2,3,4,5\\). The are \\(n!\\) permutations of \\(1,2,\\dots,n\\).\nPermutations. Consider \\(n\\) objects and making \\(k\\) choices from them, one at a time without replacement. Then there are \\(P_{n}^{k}=n(n-1)\\cdots(n-k+1)=\\frac{n!}{(n-k)!}\\) possible outcomes, for \\(k\\leq n\\). (Ordering matters in this case, e.g. \\(1,2,3\\) is considered different from \\(2,3,1\\))\nCombinations. Consider \\(n\\) objects and making \\(k\\) choices from them, one at a time without replacement, without distinguishing between the different orders in which they could be chosen (e.g. \\(1,2,3\\) is considered no different from \\(2,3,1\\)). Then there are \\(C_{n}^{k}=\\frac{n(n-1)\\cdots(n-k+1)}{k!}\\) possible outcomes. In modern math, we prefer the notation \\[\\binom{n}{k}\\equiv C_n^k,\\] which reads as “\\(n\\) choose \\(k\\)”.\nThe following table summarizes the counting methods.\nThe upper-right corner case is equivalent to putting \\(k\\) indistinguishable balls into \\(n\\) distinguishable baskets (e.g. two balls in Basket 3 means the 3rd object is chosen twice). Therefore, the number of possible arrangements is \\(\\binom{k+n-1}{n-1}\\).",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classical probability</span>"
    ]
  },
  {
    "objectID": "chapters/03_classic.html#counting-methods",
    "href": "chapters/03_classic.html#counting-methods",
    "title": "3  Classical probability",
    "section": "",
    "text": "order matters\norder doesn’t matter\n\n\n\n\nwith replacement\n\\(n^{k}\\)\n\\(\\binom{n+k-1}{k}\\)\n\n\nnon-replacement\n\\(\\frac{n!}{(n-k)!}\\)\n\\(\\binom{n}{k}\\)",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classical probability</span>"
    ]
  },
  {
    "objectID": "chapters/03_classic.html#binomial-coefficient",
    "href": "chapters/03_classic.html#binomial-coefficient",
    "title": "3  Classical probability",
    "section": "Binomial coefficient",
    "text": "Binomial coefficient\nThe Binomial coefficient \\(\\binom{n}{k}\\) counts the number of subsets of size \\(k\\) for a set of size \\(n\\). It is also the coefficient of \\(x^k\\) when expanding the polynomial \\((x+y)^n\\).\n\nTheorem 3.1 (Binomial theorem) \\[(x+y)^{n}=\\sum_{k=0}^{n}\\binom{n}{k}x^{k}y^{n-k}.\\]\n\n\n\n\n\n\nThe coefficients form an infinite triangle called the Pascal triangle. By observing the patterns in the triangle, it is not hard to conclude the following recursive formula:\n\\[\\binom{n}{k} = \\binom{n-1}{k-1} + \\binom{n-1}{k}\\]\nThe value of the binomial coefficient is only defined for non-negative integers \\(n\\) and \\(k\\) with \\(0 \\leq k \\leq n\\). But mathematics is about generalization. We can generalize the notion of “\\(n\\) choose \\(k\\)” for negative \\(n\\): \\[\\begin{aligned}\n\\binom{-n}{k} &= \\prod_{i=0}^{k-1} \\frac{-n-i}{k-i}\n=(-1)^k \\prod_{i=0}^{k-1} \\frac{n+i}{k-i} \\\\\n&=(-1)^k\\frac{n(n+1)\\dots(n+k-1)}{k!}\\\\\n&=(-1)^k\\frac{(n+k-1)!}{k!(n-1)!}\\\\\n&=(-1)^k\\binom{n+k-1}{k}\n\\end{aligned}\\]\nWe can also extend the formula to real numbers and even complex numbers:\n\\[\\binom{x}{y}=\\frac{\\Gamma(x+1)}{\\Gamma(y+1)\\Gamma(x-y+1)}\\]where \\(\\Gamma(x+1)=x!\\) is a generalization of factorials. We will come back to the Gamma function when we discuss Gamma distributions.",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classical probability</span>"
    ]
  },
  {
    "objectID": "chapters/04_gambling.html",
    "href": "chapters/04_gambling.html",
    "title": "4  Gambling problems",
    "section": "",
    "text": "Find probability by simulation\nLet’s redo Example 4.1 by simulation.\n# generate a deck of cards\ndeck.grid &lt;- expand.grid(c(1:10,'J','Q','K','A'), c('♠','♥','♦','♣'))\n\n# convert the grid to a vector\ndeck &lt;- do.call(paste0, deck.grid)\n\n# total number of simulations\nN &lt;- 100000 \n\n# number of target hand\nK &lt;- 0\n\n# for random generator\nset.seed(1000)\n\nfor (j in 1:N) {\n\n  # a random five-cards hand\n  hand &lt;- sample(deck, 5)\n\n  # drop the color\n  num &lt;- substr(hand, 1, nchar(hand)-1)\n  \n  # Full House have only two distinguished numbers\n  if ( length(unique(num)) == 2 ) {\n    K &lt;- K + 1\n  }\n}\n\n# compute the probability\nP &lt;- K / N\n\nsprintf(\"Prob of Full House: %.3f %%\", P*100)\n\n[1] \"Prob of Full House: 0.146 %\"",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Gambling problems</span>"
    ]
  },
  {
    "objectID": "chapters/05_birthday.html",
    "href": "chapters/05_birthday.html",
    "title": "5  Birthday paradox",
    "section": "",
    "text": "R simulation\nThe birthday problem is a classic probability puzzle that demonstrates how likely it is for at least two people in a group to share the same birthday. While it might seem intuitive that the probability is low in small groups, the results are surprising.\n# a class of k people \nk &lt;- 30\n\n# number of experiments\nN &lt;- 1000\n\n# number of matches\nm &lt;- 0\n\nfor (i in 1:N) {\n  \n  # draw k random numbers from 1 to 365 with replacement\n  birthdays &lt;- sample(1:365, k, replace = T) \n    \n  # number of duplicated birthdays\n  dups &lt;- duplicated(birthdays)\n    \n  # if duplicated birthdays found\n  if (any(dups)) m &lt;- m + 1\n  \n}\n\ncat(\"Prob of at least one match: \", m / N)\n\nProb of at least one match:  0.718\nThere is even a built-in function pbirthday that computes the probability of birthday coincidence. We may utilize this function to plot the probability as the number of people increases.\n# compute the probability of birthday match for 30 people\nprob &lt;- pbirthday(30) \n\n# compute a vector of probabilities for 1,2...100 people\nprobs &lt;- sapply(1:100, pbirthday)\n\n# make a plot\nplot(1:100, probs, type=\"l\",main = \"Probability of &gt;1 people with the same birthday\")",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Birthday paradox</span>"
    ]
  },
  {
    "objectID": "chapters/06_axioms.html",
    "href": "chapters/06_axioms.html",
    "title": "6  Axiomatic probability",
    "section": "",
    "text": "Definition 6.1 A probability space consists of \\(S\\) and \\(P\\), where \\(S\\) is a sample space, and \\(P\\) is a function which takes an event \\(A\\subseteq S\\) as input and returns \\(P(A)\\in[0,1]\\) such that\n\n\\(P(\\phi)=0\\),\n\\(P(S)=1\\),\n\\(P(\\cup_{n=1}^{\\infty}A_{n})=\\sum_{n=1}^{\\infty}P(A_{n})\\) if \\(A_{1},A_{2},\\ldots,A_{n}\\) are disjoint.\n\n\nNote that this Definition does not imply any particular interpretation of probability. In fact, any function \\(P\\) that satisfies the axioms are valid “probabilities”. Thus, the theories of probability do not depend on any particular interpretation. It is purely axiomatic. From the three axioms, we can derive any property of probabilities. The interpretation also matters, but it is more of a philosophical debate.\n\n\n\n\n\n\nTwo interpretations of probability\n\n\n\n\nThe frequentist view of probability is that it represents a long-run frequency over a large number of repetitions of an experiment: if we say a coin has probability 1/2 of Heads, that means the coin would land Heads 50% of the time if we tossed it over and over and over.\nThe Bayesian view of probability is that it represents a degree of belief about the event in question, so we can assign probabilities to hypotheses like “candidate A will win the election” or “the defendant is guilty” even if it isn’t possible to repeat the same election or the same crime over and over again.\n\n\n\n\nProposition 6.1 For any events \\(A\\) and \\(B\\), we have\n\n\\(P(A^{c})=1-P(A)\\)\nIf \\(A\\subseteq B\\), then \\(P(A)\\leq P(B).\\)\n\\(P(A\\cup B)=P(A)+P(B)-P(A\\cap B)\\).\n\n\n\nProof. We prove the three properties by just using the Axioms.\n\nSince \\(A\\) and \\(A^{c}\\) are disjoint and their union is \\(S\\), apply the third axiom: \\[P(S)=P(A\\cup A^{c})=P(A)+P(A^{c});\\] By the second axiom, \\(P(S)=1\\). So \\(P(A)+P(A^{c})=1\\).\nThe key is to break up the set into disjoint sets. If \\(A\\subseteq B\\), then \\(B=A\\cup(B\\cap A^{c})\\) where \\(A\\) and \\(B\\cap A^{c}\\) are disjoint (draw a Venn diagram for intuition). By the third axiom, we have \\[P(B)=P(A\\cup(B\\cap A^{c}))=P(A)+P(B\\cap A^{c})\\geq P(A).\\]\nWe can write \\(A\\cup B\\) as the union of the disjoint set \\(A\\) and \\(B\\cap A^{c}\\). Then by the third axiom, \\[P(A\\cup B)=P(A\\cup(B\\cap A^{c}))=P(A)+P(B\\cap A^{c}).\\] It suffices to show that \\(P(B\\cap A^{c})=P(B)-P(A\\cap B)\\). Since \\(B\\cap A\\) and \\(B\\cap A^{c}\\) are disjoint, we have \\[P(B)=P(B\\cap A)+P(B\\cap A^{c}).\\] So \\(P(B\\cap A^{c})=P(B)-P(A\\cap B)\\) as desired. \n\n\nThe last property is a very useful formula for finding the probability of a union of events when the events are not necessarily disjoint. We can generalize it to \\(n\\) events.\n\nTheorem 6.1 For any events \\(A_{1},A_{2},\\dots,A_{n}\\), it holds that \\[\\begin{aligned}\nP(A_{1}\\cup A_{2}\\cdots\\cup A_{n}) & =\\sum_{j=1}^{n}P(A_{j})-\\sum_{i&lt;j}P(A_{i}\\cap A_{j})+\\sum_{i&lt;j&lt;k}P(A_{i}\\cap A_{j}\\cap A_{k})-\\cdots\\\\\n& \\quad(-1)^{n+1}P(A_{1}\\cap\\cdots\\cap A_{n}).\\end{aligned}\\]\n\nThis formula can be proved by induction using the axioms. Below is a famous application (known as de Montmort’s problem, named after French mathematician Pierre Remond de Montmort) of the inclusion-exclusion theorem.\n\nExample 6.1 (Matching problem) Suppose there are \\(n\\) people who each check in a hat at a party. The hats are randomly returned to them without any concern for whose hat is whose. What is the probability that at least one person gets their own hat back?\n\n\nSolution. Let \\(A_j\\) be the event: the \\(j\\)-th person gets his own hat. The problem is equivalent to find \\(P(A_{1}\\cup A_{2}\\cup\\cdots\\cup A_{n})\\).\nSince all position are equally likely, \\(P(A_{j})=\\frac{1}{n}\\). The probability of there being two matches is: \\(P(A_{1}\\cap A_{2})=\\frac{(n-2)!}{n!}=\\frac{1}{n(n-1)}\\). Similarly, the probability of there being \\(k\\) matches is: \\(P(A_{1}\\cap\\cdots\\cap A_{k})=\\frac{(n-k)!}{n!}=\\frac{1}{n(n-1)\\cdots(n-k+1)}\\). Using the property of the union of events, \\[\\begin{aligned}P(A_{1}\\cup A_{2}\\cup\\cdots\\cup A_{n})= & n\\cdot\\frac{1}{n}-\\binom{n}{2}\\frac{1}{n(n-1)}+\\binom{n}{3}\\frac{1}{n(n-1)(n-2)}-\\cdots\\\\\n= & 1-\\frac{1}{2!}+\\frac{1}{3!}-\\frac{1}{4!}+\\cdots+(-1)^{n+1}\\frac{1}{n!}\\\\ \\approx & 1-\\frac{1}{e}.\n\\end{aligned}\\]\n\n\n\n\n\n\n\nPattern matching with Taylor series\n\n\n\nPattern matching is a very useful technique. In the last step, we recognize that the Taylor series of \\(e^x\\) is \\[e^x = 1 + x + \\frac{x^2}{2!} +\\frac{x^3}{3!}+\\cdots \\] Therefore, \\(e^{-1} = 1 -1  + \\frac{1}{2!} - \\frac{1}{3!}+\\cdots\\)\n\n\n\nExample 6.2 (Infinite monkey theorem) A monkey hitting keys independently and at random on a typewriter keyboard for an infinite amount of time will almost surely type any given text (e.g. the complete works of William Shakespeare). In other words, an infinite random sequence of letters contain every finite string infinitely often with probability 1.\n\n\nProof. Let’s compute the probability of the monkey typing “banana” correctly with random strokes. Suppose there are 50 keys on the keyboard. The monkey typed correctly “banana” is \\(\\frac{1}{50^6}\\). Suppose the monkey tried \\(n\\) times, the probability that he did not typed the correct text is \\[X_n = \\left(1-\\frac{1}{50^6}\\right)^n\\] For finite \\(n\\) (even \\(n\\) is very large), \\(X_n\\) would be very close to 1. For example, when \\(n=10^6\\), \\(X_n\\approx 0.9999\\). But as \\(n\\to\\infty\\), \\(X_n\\to 0\\). That means, if \\(n\\) is infinitely large, the probability that the monkey produced the correct text goes to 1.\nThe theorem reminds us that infinite limits behave very differently from large finite numbers. It is also used as a metaphor: given enough time, randomness can generate order, structure, or meaning.",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Axiomatic probability</span>"
    ]
  },
  {
    "objectID": "chapters/07_cond.html",
    "href": "chapters/07_cond.html",
    "title": "7  Conditional probability",
    "section": "",
    "text": "The probability of A conditioned on B is the updated probability of event A after we learn that event B has occurred. Since events contain information, the occurring of a certain event may change our believes on probabilities of other relevant events. The updated probability of event A after we learn that event B has occurred is the conditional probability of A given B.\n\nDefinition 7.1 If \\(A\\) and \\(B\\) are events with \\(P(B)&gt;0\\), then the conditional probability of \\(A\\) given \\(B\\) is defined as \\[P(A|B)=\\frac{P(A\\cap B)}{P(B)}.\\]\n\n\n\n\n\n\n\nDon’t confuse P(A|B) with P(A,B)\n\n\n\n\\(P(A|B)\\) is the probability of \\(A\\) occurring given that \\(B\\) has already occurred. While \\(P(A,B)=P(A\\cap B)\\) is the probability that \\(A\\) and \\(B\\) occur simultaneously.\n\n\n\nProposition 7.1 Properties of conditional probability:\n\n\\(P(A\\cap B)=P(B)P(A|B)=P(A)P(B|A)\\)\n\\(P(A_{1}\\cap\\cdots\\cap A_{n})=P(A_{1})P(A_{2}|A_{1})P(A_{3}|A_{1},A_{2})\\cdots P(A_{n}|A_{1}\\ldots A_{n-1})\\)\n\n\n\nTheorem 7.1 (Bayes’ rule) Assume \\(P(B)&gt;0\\), we have \\[P(A|B)=\\dfrac{P(B|A)P(A)}{P(B)}\\]\n\n\n\n\n\n\n\nThomas Bayes and “causes and effects”\n\n\n\nThe Bayes’ rule is named after Thomas Bayes (18th century) who wanted to know how to infer causes from effects. Human intelligence wants to know the cause given its effects. However, we are only able to observe the effects given the cause. Here is Bayes’ reasoning.\nSuppose we have a prior belief about the cause of something we want to learn. We may not be able to learn the true cause directly, but after we observe its effects (the Data), we would update our belief based on the new information we have learned from the data. The updated belief (the posterior) is therefore somewhat closer to the “truth”.\n\\[\\underbrace{P(\\text{Belief | Data})}_{\\text{Posterier}}=\n\\frac{\\overbrace{P(\\text{Data | Belief})}^{\\text{Likelihood}}\\;\n\\overbrace{P(\\text{Belief})}^{\\text{Prior}} }{P(\\text{Data})}\\]\n\n\n\nTheorem 7.2 (Law of total probability (LOTP)) Let \\(B_{1},...,B_{n}\\) be a partition of the sample space \\(S\\) (i.e., the \\(B_{i}\\) are disjoint events and their union is \\(S\\)), with \\(P(B_{i})&gt;0\\) for all \\(i\\). Then \\[P(A)=\\sum_{i=1}^{n}P(A|B_{i})P(B_{i}).\\]\n\n\nTheorem 7.3 (Conditional version of LOTP) The law of total probability has an analog conditional on another event \\(C\\), namely, \\[P(A|C) = \\sum_{i=1}^{n} P(A|B_i\\cap C)P(B_i|C).\\]\n\n\nExample 7.1 Get a random 2-card hand from a standard deck. Find the probability of (a) Both cards are aces given that at least one of them (not necessarily the first one) is an ace; (b) Getting another ace given the first draw is an ace of spade.\n\n\nSolution. The example shows the subtleness of conditional probabilities. The seemingly indifferent probabilities are in fact different: \\[\\begin{aligned}\nP(\\textrm{two aces | one ace})\n&=\\frac{P(\\textrm{both aces})}{P(\\textrm{one ace})}\\\\\n&=\\frac{\\binom{4}{2}/\\binom{52}{2}}{1-\\binom{48}{2}/\\binom{52}{2}}\\\\\n&=\\frac{1}{33};\\\\\n& \\\\\nP(\\textrm{another ace | ace of spade})\n&=\\frac{P(\\textrm{ace of spade \\& another ace})}{P(\\textrm{ace of spade})}\\\\\n&=\\frac{\\binom{3}{1}/\\binom{52}{2}}{\\binom{51}{1}/\\binom{52}{2}}\\\\\n&=\\frac{1}{17}.\n\\end{aligned}\\]\nNote that, in the first case, the denominator is interpreted as “at least one ace”; whereas in the second case, it is “ace of space + another card”.\n\n\nExample 7.2 A disease has a prevalence rate of 10% (i.e., the probability of being infected is 10%). A diagnostic test for the disease has an accuracy of 98%, meaning it correctly identifies infected individuals as positive 98% of the time. Calculate the probability that an individual is infected given that the test result is positive.\n\n\nSolution. Let \\(D\\) denote being actually infected by the disease; and \\(T\\) denote a positive test. The test accuracy means: \\(P(T|D)=98\\%\\). It also means \\(P(T|D^{C})=2\\%\\). We also know that \\(P(D)=0.1\\). We want to find \\(P(D|T)\\). Note they are two different conditional probabilities, though we mostly confuse the two in everyday life. The two conditional probabilities are associated with Bayes’ rule: \\[\\begin{aligned}P(D|T)= & \\frac{P(T|D)P(D)}{P(T)}\\\\\n= & \\frac{P(T|D)P(D)}{P(T|D)P(D)+P(T|D^{C})P(D^{C})}\\\\\n= & \\frac{0.98\\times 0.1}{0.98\\times 0.1+0.02\\times 0.9}\\approx 84\\%.\n\\end{aligned}\\]\n\nNote that how \\(P(T|D)\\) is far away from \\(P(D|T)\\)!\n\n\n\n\n\n\nThinking conditionally\n\n\n\nAbraham Wald, the renowned statistician, was hired by the Statistical Research Group (SRG) at Columbia University to figure out how to minimize the damage to bomber aircraft. The data they had comprised aircraft returning from missions with bullet holes on their bodies. If asked which parts of the aircraft should be armored to enhance survivability, the obvious answer seemed to be to armor the damaged parts. However, Wald suggested the exact opposite—to armor the parts that were not damaged. Why? Because the observed damage was conditioned on the aircraft returning. If an aircraft had been damaged on other parts, it likely would not have returned. Thinking conditionally completely changes the answer!\n\n\n\n\n\nSee The Soul of Statistics by Professor Joseph Blitzstein.",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conditional probability</span>"
    ]
  },
  {
    "objectID": "chapters/08_montyhall.html",
    "href": "chapters/08_montyhall.html",
    "title": "8  Monty Hall problem",
    "section": "",
    "text": "R simulation\n# Number of simulations\nN &lt;- 1000 \n\n# Number of total wins\nW &lt;- 0\n\n# Prizes offered\nprizes &lt;- c('Car', 'Goat 1', 'Goat 2')\n\nfor (i in 1:N) {\n  \n  # Prizes are in random order behind the doors\n  doors &lt;- sample(prizes, 3)\n  \n  # Guest picks a random door\n  pick &lt;- sample(doors, 1)\n  \n  # Monty opens the door that is not picked by the guest \n  # nor does it has the Car behind it\n  open &lt;- sample(setdiff(doors, c(pick, 'Car')), 1)\n  \n  # If the Guest swithes, he chooses the door that is not\n  # his initial pick nor the one opened by Monty\n  switch &lt;- setdiff(doors, c(pick, open))\n  \n  # The guest wins if his final choice has the Car\n  win &lt;- switch == 'Car'\n  \n  # Increase the winning counter\n  W &lt;- W + win\n}\n\n# Frequency of winning the game if always switching\nW/N\n\n[1] 0.683",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Monty Hall problem</span>"
    ]
  },
  {
    "objectID": "chapters/09_simpson.html",
    "href": "chapters/09_simpson.html",
    "title": "9  Simpson’s paradox",
    "section": "",
    "text": "R demostration\nA confounder is a variable that influences with both explanatory variable and the outcome variable, which therefore “confounds” the correlation between the two. In our example, the type of surgery (\\(E\\)) is associated with both the doctor and the outcome. Without the confounder being controlled, it is impossible to draw valid conclusions from the statistics.\nIn general terms, Simpson’s paradox refers to the paradox in which a trend that appears across different groups of aggregate data is the reverse of the trend that appears when the aggregate data is broken up into its components. It is one of the most common sources of statistical misuse. Here is another example.\nSee https://setosa.io/simpsons for a really good illustration of the Simpson’s paradox.\n# R has a built-in dataset `UCBAdmissions`\n# we convert it to data frame for analysis\ndata &lt;- as.data.frame(UCBAdmissions)\n\n# browse the first a few rows\nhead(data)\n\n     Admit Gender Dept Freq\n1 Admitted   Male    A  512\n2 Rejected   Male    A  313\n3 Admitted Female    A   89\n4 Rejected Female    A   19\n5 Admitted   Male    B  353\n6 Rejected   Male    B  207\n\n# subset of the data with only admissions\ndata &lt;- subset(data, Admit == 'Admitted')\n\n# number of admissions by Gender\naggregate(Freq ~ Gender, data = data, FUN = sum)\n\n  Gender Freq\n1   Male 1198\n2 Female  557\n\n# number of admissions by Gender and Department\naggregate(Freq ~ Gender + Dept, data = data, FUN = sum)\n\n   Gender Dept Freq\n1    Male    A  512\n2  Female    A   89\n3    Male    B  353\n4  Female    B   17\n5    Male    C  120\n6  Female    C  202\n7    Male    D  138\n8  Female    D  131\n9    Male    E   53\n10 Female    E   94\n11   Male    F   22\n12 Female    F   24",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simpson's paradox</span>"
    ]
  },
  {
    "objectID": "chapters/09_simpson.html#r-demostration",
    "href": "chapters/09_simpson.html#r-demostration",
    "title": "9  Simpson’s paradox",
    "section": "",
    "text": "The importance of conditional thinking\n\n\n\nWhenever we talk about probability or statistics, always remind ourselves what we are the conditioning on. Any statistical reasoning without specifying the conditions can be misleading. We are prone to such fallacies everyday everywhere.\n\n“10 millions new jobs were added during the term of President X.” But it doesn’t tell you this was achieved conditioned on that the economy had just had the worst recession.\n“Private schools’ graduates earned 50% more than those graduated from public schools.” But it doesn’t tell you the background of those students who enrolled in private schools.\n\nBe vigilant to these claims when you see them next time.",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simpson's paradox</span>"
    ]
  },
  {
    "objectID": "chapters/10_indep.html",
    "href": "chapters/10_indep.html",
    "title": "10  Independence",
    "section": "",
    "text": "Common confusions\n\\(A\\) and \\(B\\) are independent means they do not provide information to each other in the sense that conditional probability is not different from the unconditional probability. It is not an intuitive idea as it seems. It will become clearer when we discuss random variables in later chapters.",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Independence</span>"
    ]
  },
  {
    "objectID": "chapters/10_indep.html#common-confusions",
    "href": "chapters/10_indep.html#common-confusions",
    "title": "10  Independence",
    "section": "",
    "text": "Independence is not the same as disjointness.\n\n\n\n\\(A\\) and \\(B\\) are disjoint means if \\(A\\) occurs, \\(B\\) cannot occur. But independence means \\(A\\) occurs has nothing to do with \\(B\\).\n\n\n\n\n\n\n\n\nPairwise independence does not imply independence.\n\n\n\nIn Definition 10.2, If the first three conditions hold, we say that \\(A\\), \\(B\\), and \\(C\\) are pairwise independent. Pairwise independence does not imply independence. Convince yourself with the following example.\n\n\n\nExample 10.1 Consider two fair, independent coin tosses, and let \\(A\\) be the event that the first is Heads, \\(B\\) the event that the second is Heads, and \\(C\\) the event that both tosses have the same result. Show that \\(A\\), \\(B\\), and \\(C\\) are pairwise independent but not independent.\n\n\nSolution. For each event, \\(P(A)=\\frac{1}{2}\\), \\(P(B)=\\frac{1}{2}\\). Consider the two events together, there are four possible outcomes: HH, HT, TH, TT. \\(P(C)=P(HH)+P(TT)=\\frac{1}{2}\\). Thus, \\[\\begin{aligned}\nP(A\\cap B) & =P(HH)=\\frac{1}{4}=P(A)P(B)\\\\\nP(A\\cap C) & =P(HH)=\\frac{1}{4}=P(A)P(C)\\\\\nP(B\\cap C) & =P(HH)=\\frac{1}{4}=P(B)P(C)\\end{aligned}\\] But \\(A,B,C\\) are not independent, because \\[P(A\\cap B\\cap C)=P(HH)=\\frac{1}{4}\\neq P(A)P(B)P(C).\\]",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Independence</span>"
    ]
  },
  {
    "objectID": "chapters/11_dating.html",
    "href": "chapters/11_dating.html",
    "title": "11  Optimal mating problem*",
    "section": "",
    "text": "The question\nHow many people should you date before you settle down with someone for marriage? The answer is you should date 37% of your potential options and choose the next one who is better. 1\nMathematical framework\nLet’s assume there’s a pool of \\(N\\) people out there from which you are choosing. We’ll also assume that you have a clear-cut way of rating people. You know who is the best to be your partner. We will call that person Mr/Ms \\(X\\). The people that you will meet show up one by one in random order. \\(X\\) may show up anywhere in the sequence. Sadly, a person you have dated and then rejected isn’t available to you any longer later on. So you cannot date all of them and pick the best one.\nYour dating strategy is to date \\(M\\) of the \\(N\\) people and then settle with the next person who is better. Our task is to find the optimal \\(M\\). If \\(M\\) is too small, you will likely land with someone before \\(X\\) shows up. If \\(M\\) is too large, \\(X\\) will likely pass \\(X\\) and pick someone less optimal. Of course, there is no perfect solution. We want to find the \\(M\\) that maximizes the probability of landing \\(X\\).\nLet \\(P(M,N)\\) be the probability of successfully picking \\(X\\) if you date \\(M\\) people out of \\(N\\) and then go for the next person who is better than the previous ones. Let \\(S\\) be the event of successfully picking \\(X\\), and \\(X_j\\) means \\(X\\) is in the \\(j\\)th position in the sequence. The overall probability is: \\[P(M,N) = P(S|X_1)P(X_1)+P(S|X_2)P(X_2)+\\cdots+P(S|X_n)P(X_n)\\]\nFor a given value of \\(M\\), if \\(X\\) is among the first \\(M\\) people you date, then you have missed your chance. The probability of settling with \\(X\\) is zero. Therefore, the first \\(M\\) terms are all zero.\nIf \\(X\\) is in \\(M+1\\), you’re in luck: since \\(X\\) is better than all others so far, you will pick \\(X\\) for sure. Therefore, \\[P(S|X_{M+1})P(X_{M+1}) = 1\\cdot P(X_{M+1})= \\frac{1}{N}\\] Since \\(X\\) is equally likely to be in any position, the probability of \\(X\\) being in \\(M+1\\) out of \\(N\\) people is \\(1/N\\).\nIf \\(X\\) is in \\(M+2\\), you’ll pick him/her up as long as the \\((M+1)\\)st person didn’t have a higher rating than all the previous \\(M\\) people. In other words, you would pass the \\((M+1)\\)st person and pick \\(X\\) if the best one out of the \\((M+1)\\) people has shown up among the first \\(M\\) people. The change is \\(M/(M+1)\\). Thus, \\[\nP(S|X_{M+2})P(X_{M+2})=\\frac{M}{M+1}\\frac{1}{N}\n\\]\nSimilarly, if \\(X\\) shows up in \\(M+3\\), you’ll pick him/her up to as long as neither the \\((M+1)\\)st nor the \\((M+2)\\)nd person have a higher rating than all the previous \\(M\\) people. In other words, the best one out of the first \\((M+2)\\) people has to show up among the first \\(M\\) people. The chance is \\(M/(M+2)\\). Thus,\n\\[\nP(S|X_{M+3})P(X_{M+3})=\\frac{M}{M+2}\\frac{1}{N}\n\\]\nPutting them all together, we have \\[\n\\begin{aligned}\nP(M,N) &=\\frac{1}{N} +\\frac{M}{N(M+1)} +\\frac{M}{N(M+2)}+\\dots +\\frac{M}{N(N-1)}\\\\\n&= \\frac{M}{N}\\left(\\frac{1}{M}+\\frac{1}{M+1} +\\frac{1}{M+2}+\\dots+\\frac{1}{N-1}\\right)\n\\end{aligned}\n\\]\nMaximizing your chance of success\nAssuming \\(P(M,N)\\) is strictly concave (fortunately this is the case), the \\(M\\) the maximizes the chance satisfies \\[\nP(M-1,N) &lt; P(M,N) \\text{ and } P(M+1,N) &lt; P(M,N)\n\\]\nWe can ask the computer to find the solution:\nFor \\(N=100\\), the highest probability if achieved at \\(M=37\\).\nThe limiting solution\nWe can find the solution analytically if \\(M,N\\) are large. For large \\(n\\), the harmonic sequence can be approximated by the logarithm function: \\[\nH_n=1+\\frac{1}{2}+\\frac{1}{3}+\\dots+\\frac{1}{n}\\approx \\ln(n)+\\gamma\n\\] where \\(\\gamma\\) is a constant.\nWe rewrite the function \\(P(M,N)\\) as \\[\nP(M,N)\n=\\sum_{k=M}^{N-1}\\frac{M}{N}\\cdot\\frac{1}{k}\n=\\frac{M}{N}\\sum_{k=M}^{N-1}\\frac{1}{k}\n=\\frac{M}{N}\\Big(H_{N-1}-H_{M-1}\\Big)\n\\]\nFor large \\(M\\) and \\(N\\), it is approximated by \\[\nP(M,N)=\\frac{M}{N}(H_{N-1}-H_{M-1})\\approx\n  \\frac{M}{N}\\left[\\ln(N-1)-\\ln(M-1)\\right]\\approx\n  \\frac{M}{N}\\ln\\left(\\frac{N}{M}\\right)\n\\]\nLet \\(x=\\frac{M}{N}\\in(0,1)\\). We want to maximize \\[\nf(x)=x\\ln\\left(\\frac{1}{x}\\right)= -x\\ln x\n\\]\nDifferentiate: \\[\nf'(x)=\\ln\\left(\\frac{1}{x}\\right)-1=-\\ln x -1\n\\]\nSet \\(f'(x)=0 \\Rightarrow -\\ln x -1=0 \\Rightarrow \\ln x=-1 \\Rightarrow x=e^{-1}.\\) Second derivative \\(f''(x)=-1/x&lt;0\\), so it’s a maximum.\nTherefore, the maximizing fraction satisfies \\[\n\\frac{M^\\star}{N}\\;\\longrightarrow\\;\\frac{1}{e}\\quad\\text{as }N\\to\\infty,\n\\]\nand the maximal success probability tends to \\[\nf(1/e)=\\frac{1}{e}.\n\\]",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Optimal mating problem\\*</span>"
    ]
  },
  {
    "objectID": "chapters/11_dating.html#footnotes",
    "href": "chapters/11_dating.html#footnotes",
    "title": "11  Optimal mating problem*",
    "section": "",
    "text": "See Kissing the frog: A mathematician’s guide to mating and Strategic dating: The 37% rule for reference.↩︎",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Optimal mating problem\\*</span>"
    ]
  },
  {
    "objectID": "chapters/11_calculus.html",
    "href": "chapters/11_calculus.html",
    "title": "12  Review of calculus*",
    "section": "",
    "text": "Calculus is a prerequisite to work with continuous distributions. The following chapters assume readers are proficient in calculus. We nonetheless review some basic concepts here as a warm-up. This review is not exhaustive, so please refer to a specific textbook if needed for a more comprehensive understanding.\n\nDifferentiation\nWe define the derivative of a function \\(f(x)\\) to be \\[f'(x)=\\lim_{h\\to0}\\frac{f(x+h)-f(x)}{h}\\]\nLoosely speaking, a function is continuous if there is no jump in the graph, differentiable if the curve is smooth. Some commonly used derivatives:\n\\[\\begin{aligned} & \\frac{d}{dx}(x^{n}) & = & nx^{n-1}\\\\\n& \\frac{d}{dx}(e^{x}) & = & e^{x}\\\\\n& \\frac{d}{dx}(\\ln(x)) & = & \\frac{1}{x}\\\\\n& \\frac{d}{dx}(\\sin(x)) & = & \\cos(x)\\\\\n& \\frac{d}{dx}(\\cos(x)) & = & -\\sin(x)\\\\\n& (fg)' & = & f'g+fg'\\\\\n& \\left(\\frac{f}{g}\\right)^{'} & = & \\frac{f'g-fg'}{g^{2}}\\\\\n& [f(g(x))]' & = & f'(g(x))g'(x)\n\\end{aligned}\\]\nWhen dealing with limits of the form “\\(\\frac{0}{0}\\)” or “\\(\\frac{\\infty}{\\infty}\\)”, the L’Hospital rule is very handy. \\[\\lim_{x\\to a}\\frac{f(x)}{g(x)}=\\lim_{x\\to a}\\frac{f'(x)}{g'(x)}.\\]\nOne important application of derivatives is the Taylor’s theorem, which gives the approximation of a function around a given point by polynomials. Assume function \\(f\\) is at least \\(k\\) times differentiable, then\n\\[f(x)=f(a)+f'(a)(x-a)+\\frac{f''(a)}{2!}(x-a)^{2}+\\cdots+\\frac{f^{(k)}(a)}{k!}(x-a)^{k}+\\cdots\\]\nwhich means we can approximate a function arbitrarily well by higher order polynomials. Some commonly used Taylor series (expanding around \\(a=0\\)):\n\\[\\begin{aligned} & \\frac{1}{1-x} & = & 1+x+x^{2}+x^{3}+\\cdots\\quad\\textrm{for }|x|&lt;1\\\\\n& e^{x} & = & 1+x+\\frac{x^{2}}{2!}+\\frac{x^{3}}{3!}+\\cdots\\\\\n& \\sin x & = & x-\\frac{x^{3}}{3!}+\\frac{x^{5}}{5!}-\\frac{x^{7}}{7!}+\\cdots\\\\\n& \\cos x & = & 1-\\frac{x^{2}}{2!}+\\frac{x^{4}}{4!}-\\frac{x^{6}}{6!}+\\cdots\\\\\n& \\ln(1+x) & = & x-\\frac{x^{2}}{2}+\\frac{x^{3}}{3}-\\frac{x^{4}}{4}+\\cdots\\quad\\textrm{for }|x|&lt;1\\\\\n& \\arctan(x) & = & x-\\frac{x^{3}}{3}+\\frac{x^{5}}{5}-\\frac{x^{7}}{7}+\\cdots\\quad\\textrm{for }|x|\\leq1\n\\end{aligned}\\]\n\n\n\n\n\n\nApproximating \\(\\pi\\) with Taylor series\n\n\n\nTaylor series are one of the most amazing results in calculus. For example, in the last formula, if we let \\(x=1\\): \\[\\frac{\\pi}{4}=\\arctan(1)=1-\\frac{1}{3}+\\frac{1}{5}-\\frac{1}{7}+\\cdots\\] Therefore, we can approximate \\(\\pi\\) by summing up a sequence of fractions: \\[\\pi=4\\left(1-\\frac{1}{3}+\\frac{1}{5}-\\frac{1}{7}+\\cdots\\right).\\]\n\n\n\n\nIntegration\nIntegration is the inverse operation of differentiation. Integral has the geometric interpretation as the area under the curve. Let \\(A(x)\\) be the area under the curve of \\(y=f(x)\\). Thus \\(A(x)=\\int_{0}^{x}f(t)dt\\). The change of the area resulted from a tiny little change of \\(x\\) is approximated by \\(A(x+h)-A(x)\\approx f(x)h\\). That is \\(\\frac{A(x+h)-A(x)}{h}=f(x)\\). If the change is infinitesimal, \\(h\\to0\\), we have \\(A'(x)=f(x)\\).\n\n\n\n\n\nThe Fundamental Theorem of Calculus: if \\(F\\) is the anti-derivative of \\(f\\), then\n\\[F(x)=\\int_{a}^{x}f(t)dt\\] \\[\\int_{a}^{b}f(x)dx=F(b)-F(a)\\]\nOne interpretation of the integral is — the integral of a rate of change of a quantity gives the net change in that quantity. Think about speed and distance: \\(\\int_{a}^{b}v(t)dt=s(b)-s(a)\\).\nBecause the integral is just a sum over infinitely many approximating rectangles, \\(\\int_{a}^{b}f(x)dx=\\lim_{n\\to\\infty}\\sum_{i=1}^{n}f(x_{i})\\Delta x\\). Integrals behave just like sums. For example, \\(\\frac{1}{b-a}\\int_{a}^{b}f(x)dx\\) has the interpretation of the average of \\(f(x)\\) from \\(a\\) to \\(b\\).\nIndefinite integrals are the general antiderivatives without specifying the interval of the integration. It always comes with a constant \\(C\\). Some commonly used integrals:\n\\[\\begin{aligned} & \\int dx & = & x+C\\\\\n& \\int x^{n}dx & = & \\frac{x^{n+1}}{n+1}+C\\\\\n& \\int e^{x}dx & = & e^{x}+C\\\\\n& \\int\\frac{1}{x}dx & = & \\ln|x|+C\\\\\n& \\int\\cos(x)dx & = & \\sin(x)+C\\\\\n& \\int\\sin(x)dx & = & -\\cos(x)+C\\\\\n& \\int\\frac{1}{1+x^{2}}dx & = & \\arctan(x)+C\n\\end{aligned}\\]\nTwo common integration techniques are substitution and integration by parts.\n\nExample 12.1 (Integration by substitution) Find \\(\\int\\sqrt{3x+2}dx\\).\n\n\nSolution. Let \\(u=3x+2\\), then \\(du=3dx\\). Then \\[\\int\\sqrt{3x+2}dx=\\frac{1}{3}\\int\\sqrt{u}du=\\frac{2}{9}u^{3/2}+C=\\frac{2}{9}(3x+2)^{3/2}+C.\\]\n\n\nExample 12.2 (Integration by parts) Find \\(\\int x\\sin xdx\\).\n\n\nSolution. Integration by parts follows the formula: \\[\\int f(x)g'(x)dx=f(x)g(x)-\\int f'(x)g(x)dx\\] Let \\(f(x)=x\\), \\(g'(x)=\\sin x\\). Then \\(g(x)=-\\cos x\\). Then, \\[\\int x\\sin xdx=-x\\cos x-\\int(-\\cos x)dx=-x\\cos x+\\sin x+C.\\]",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Review of calculus*</span>"
    ]
  },
  {
    "objectID": "chapters/01_baseR.html",
    "href": "chapters/01_baseR.html",
    "title": "13  R tutorial*",
    "section": "",
    "text": "Variables\n\na = 5\nb &lt;- 5\nc &lt;- \"Hello\"\n\nAssignment\n\na &lt;- a + 1 # assignment\na == a + 1 # math equal\n\nVectors\n\nu &lt;- c(1,2,3,4,5)\nv &lt;- 6:10\nb &lt;- c('good', 'night', '😊')\n\nMatrices\n\nA &lt;- matrix(c(1,2,3,4), nrow = 2, ncol = 2)\nB &lt;- matrix(c(5,6,7,8), nrow = 2, ncol = 2)\n\nLinear algebra\n\nu * v   # element-wise \nu %*% v # dot product\nA * B   # element-wise\nA %*% B # matrix multiplication\nt(A)    # transpose\ndet(A)  # determinant \n\nRandom numbers\n\n# a random number from 0 to 100\nrunif(1, 0, 100)\n\n# generate 10 random numbers\nrunif(10, 0, 100)\n\n# random sampling\nsample(1:100, 10)\n\nConditional statement\n\n# draw a random integer\nx &lt;- sample(1:100, 1)\n\n# if the remainder divided by 2 is 0\nif (x %% 2 == 0) {\n  \n  # display it is an even number\n  print(\"even number\")\n\n  } else {\n  \n    # otherwise, it is an odd number\n    cat(\"odd number\")\n\n}\n\nLoop\n\n# for-loop\n# loop for a given number of times\nfor (k in 1:10) {\n  \n  # the code to be repeated\n  print(\"Hello!\")\n  \n}\n\n# while-loop\n# loop on condition\nk &lt;- 0\nwhile (k &lt; 10) {\n  \n  # the code to be repeated\n  print(\"Hello!\")\n  \n  # keep track of the \n  k &lt;- k + 1\n}\n\n\n# 1+2+...+100=?\ns = 0;\nk = 1;\nwhile (k &lt;= 100) {\n  s &lt;- s + k;\n  k &lt;- k + 1;\n}\n\nFunctions\n\n# built-in functions\nsin(pi/2)\nlog(100)\nexp(2.3)\n\n\n# custom functions\nsquare &lt;- function(a) {\n  a * a\n}\n\n# call the function\nsquare(10)\n\n\n# function as a reusable code block\nseqsum &lt;- function(begin, end) {\n  s = 0;\n  k = begin;\n  while (k &lt;= end) {\n    s &lt;- s + k;\n    k &lt;- k + 1;\n  }\n  return(s)\n}\n\n# function call\nseqsum(1, 100)",
    "crumbs": [
      "Probability Basics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>R tutorial*</span>"
    ]
  },
  {
    "objectID": "chapters/12_rv.html",
    "href": "chapters/12_rv.html",
    "title": "14  What is a random variable?",
    "section": "",
    "text": "Numeric encoding of events\nIn the previous chapter, we have been working with events, which is a conceptualization of real world outcomes occurred with probabilities. In this chapter, we introduce a much more powerful conceptualization that deals with uncertain outcomes — random variables, which is the foundation of all probability and statistical studies.\nInformally, a random variable differs from a normal variable as it is “random”. A random variable, say \\(X\\), is never associated with a certain value. It could different values probabilistically. For example, \\(X\\) may take the value 1 with probability 0.4, and take the value 2 with probability 0.3. The formal definition of a random variable is as follows.\nAs an example, flipping a coin twice, let \\(X\\) be the number of heads. Then \\(X(\\cdot)\\) is a functions that maps events in \\(\\left\\{ HH,HT,TH,TT\\right\\}\\) into real numbers. In our case, the mapping goes like \\[\\begin{aligned}\nX(HH) & =2,X(HT)=1,X(TH)=1,X(TT)=0.\\end{aligned}\\] \\(X\\) is therefore an encoding of events in the sample space into real numbers. We could, of course, have different encoding. Consider the random variable \\(Y\\) as the number of tails. Then we have \\(Y=2-X\\). \\[\\begin{aligned}\nY(HH) & =0,Y(HT)=1,Y(TH)=2,Y(TT)=2.\\end{aligned}\\] We could also define \\(Z\\) as the number heads in the 1st toss only. The encoding goes like \\[Z(HH)=1,Z(HT)=1,Z(TH)=0,Z(TT)=0.\\] We have listed three ways of “encoding” the same experiment as random variables. All of them are valid random variables, but they map the outcomes into different numbers. We can say that, a random variable is a numeric “summary” of an aspect of an experiment.\nA distribution specifies the probabilities associated with all values of a random variable. In the above example, the distribution of \\(X\\) is given by \\[P(X=0)=\\frac{1}{4},P(X=1)=\\frac{1}{2},P(X=2)=\\frac{1}{4}.\\] The distribution of \\(Y\\) is given by \\[P(Y=0)=\\frac{1}{4},P(Y=1)=\\frac{1}{2},P(Y=2)=\\frac{1}{4}.\\] The distribution of \\(Z\\) is given by \\[P(Z=0)=\\frac{1}{2},P(Z=1)=\\frac{1}{2}.\\]\nYou may have noted that the probabilities in a distribution always sums up to \\(1\\), as all possible events constitute the entire sample space.",
    "crumbs": [
      "Random Variables",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>What is a random variable?</span>"
    ]
  },
  {
    "objectID": "chapters/12_rv.html#numeric-encoding-of-events",
    "href": "chapters/12_rv.html#numeric-encoding-of-events",
    "title": "14  What is a random variable?",
    "section": "",
    "text": "Definition 14.1 (Random variable) Given an experiment with sample space \\(S\\), a random variable is a function from the sample space \\(S\\) to the real numbers \\(\\mathbb{R}\\).\n\n\n\n\n\n\n\n\nNotation for random variables\n\n\n\nWe usually use capital letters, such as \\(X,Y,Z\\), to denote random variables. We use small letters, such as \\(x,y,z\\), to denote specific values. \\(P(X=x)\\) means the probability of \\(X\\) taking the value \\(x\\). Don’t confuse the random variable \\(X\\) with the number \\(x\\).\n\n\n\n\n\n\n\n\nDon’t confuse random variables, numbers, and events\n\n\n\nRandom variables are never fixed numbers. Functions of random variables, such as \\(X^2\\), \\(|X|\\), \\(e^X\\), are also random variables. Random variables are not events. It does not make sense to write \\(P(X)\\), because \\(X\\) is not an event. But \\(X=a\\) is an event, it makes sense to write \\(P(X=a)\\).\n\n\n\nDefinition 14.2 (Distribution) Let \\(X\\) be a random variable. The distribution of \\(X\\) is the collection of all probabilities of the form \\(P(X\\in C)\\) for all sets \\(C\\) of real numbers such that \\(\\left\\{ X\\in C\\right\\}\\) is an event.\n\n\n\n\n\n\n\n\n\nSpecifying the distribution\n\n\n\nListing all the values is not a smart way to specify a distribution. We like to use a function (if possible), such as \\(f(x) \\overset{?}{=} e^{-x}\\), to specify the probability of a random variable \\(X\\) taking the value \\(x\\). This is convenient, because once we know the function, we know all the probabilities. But how to specify this function depends on whether a random variable is discrete or continuous.",
    "crumbs": [
      "Random Variables",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>What is a random variable?</span>"
    ]
  },
  {
    "objectID": "chapters/12_rv.html#conceptualization-of-uncertain-outcomes",
    "href": "chapters/12_rv.html#conceptualization-of-uncertain-outcomes",
    "title": "14  What is a random variable?",
    "section": "Conceptualization of uncertain outcomes",
    "text": "Conceptualization of uncertain outcomes\nMany real-world processes have uncertain outcomes. For example, the outcome of tossing a coin or the temperature of tomorrow. In many applications like this, we simply do not have perfect information to predict the future with certainty. In such cases, we model the uncertain outcome as an RV, which takes uncertain values with probabilities. The exact distribution of many applications may be unknown, but we can approximate it with frequencies observed in samples.\n\n\n\n\n\n\n\n\nExperiment:\nTossing a coin\n\n\n\n\nConceptualization\nObservations\n\n\nRandom variable\n\\(X\\) with support \\(\\{0,1\\}\\)\n\\(\\{0,1,1,0,0,1, ...\\}\\)\n\n\nDistribution\n\\(P(X=i)=0.5,i\\in\\{0,1\\}\\)\nProportion of \\(1\\)s \\(= 0.45\\)\n\n\nExperiment:\nTaking an exam\n\n\n\n\nConceptualization\nObservations\n\n\nRandom variable\n\\(Z\\) with support \\(\\{0,1,2,...,100\\}\\)\n\\(\\{80,69,75,60,92, ...\\}\\)\n\n\nDistribution\n\\(Z\\sim N(80,10)\\) (assumed)\nProportion of 80+ \\(= 0.14\\)\n\n\n\n\n\n\n\n\n\nDeterministic vs probabilistic models\n\n\n\nIn high school, mathematical models are typically presented as if they operate with certainty. For example, the time it takes an object to fall from a height \\(h\\) to the ground is given by \\(t = \\sqrt{\\tfrac{2h}{g}}\\), where \\(g\\) denotes the gravitational constant. The outcome here is deterministic: once the values of the variables are specified, the result follows with certainty. While the variables may or may not be known in practice, they are not random in the sense that the outcome is fully determined once inputs are given. Errors can only arise from frictions or measurement inaccuracies.\nBy contrast, many real-world processes are inherently uncertain. Consider tomorrow’s temperature or stock market returns: such outcomes can only be predicted probabilistically. This uncertainty does not reflect randomness in the nature of the universe itself, but rather the limits of human knowledge. In principle, with perfect information about the climate system, tomorrow’s temperature could be predicted exactly. However, given informational constraints, the only feasible approach is to incorporate uncertainty into mathematical models. Probabilistic models thus arise from the deliberate or unavoidable abstraction from complete information. The concept of the random variable provides the mathematical foundation for formalizing such uncertainty.",
    "crumbs": [
      "Random Variables",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>What is a random variable?</span>"
    ]
  },
  {
    "objectID": "chapters/13_data.html",
    "href": "chapters/13_data.html",
    "title": "15  Data descriptives*",
    "section": "",
    "text": "A random variable is a mathematical abstraction that provides a bridge between theoretical probability and real-world data. Every dataset can be viewed as observations from random variables.\nDespite the outcome of any one event being uncertain, we can use patterns from past observations to predict the general behavior of these variables. By collecting data, we can figure out how often certain outcomes occur and connect them to theoretical distributions.\n\\[\\begin{array}{ccccccc}\n\\textrm{Question with} & \\rightarrow & \\textrm{Data} & \\rightarrow & \\textrm{Patterns}\\\\\n\\textrm{uncertainty} &  & \\downarrow &  & \\downarrow\\\\\n&  & \\textrm{RVs} & \\rightarrow & \\textrm{Distributions} & \\rightarrow & \\textrm{Predictions}\\\\\n\\end{array}\\]\nColumns as random variables\nIn a dataset, we view every column as a random variable.\n\n# Load a dataset from a CSV file\nexam &lt;- read.csv(\"../dataset/exam.csv\")\n\n# View the data: each column is a random variable\nhead(exam)\n\n  id gender     major hw mid final overall\n1  1 Female Economics 85  89    74      81\n2  2 Female   Finance 90  84    79      83\n3  3 Female Economics 90  71    51      65\n4  4 Female   Finance 86  84    68      76\n5  5   Male   Finance 80  84    67      75\n6  6 Female   Finance 96 100    99      99\n\n\nSummary statistics\nWe can describe the distribution of a variable with summary statistics: such as quartiles, deciles and percentiles.\n\nsummary(exam$overall)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  45.00   72.00   77.50   76.56   84.00   99.00 \n\n\nHistograms\nOne way to visualize the distribution of a variable is to plot a histogram. A histogram groups data points into intervals, showing how often data values fall within each range. The horizontal axis represents the intervals (or bins), and the vertical axis shows the frequency or count of data points in each bin.\nA histogram gives an approximation of the true (unknown) distribution. It is not itself the distribution. A distribution refers to the theoretical assignment of probabilities across all possible outcomes, whereas a histogram represents the empirical frequencies observed in a finite sample.\n\nhist(exam$final, prob = T, ann=F)\n\n\n\n\n\n\n\n\nBoxplots\nA boxplot, also known as a box-and-whisker plot, displays the median, quartiles, and range of the data. The box represents the interquartile range (IQR), which contains the middle 50% of the data, with the lower and upper edges corresponding to the first (Q1) and third quartiles (Q3). Whiskers extend from the box to indicate the range of values within 1.5 times the IQR from Q1 and Q3, while points beyond this range are considered outliers.\n\nboxplot(final ~ major, exam)\n\n\n\n\n\n\n\n\nScatter plots\nTo observe the relationship between variables, it is straightforward to make a scatter plot of \\(Y\\) against \\(X\\). An upward-sloping pattern in the scatter plot indicates that the variables tend to move together, whereas a downward-sloping pattern suggests that they move in opposite directions. A flat slope implies the absence of correlation between the two variables.\n\nplot(final ~ mid, exam)",
    "crumbs": [
      "Random Variables",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data descriptives*</span>"
    ]
  },
  {
    "objectID": "chapters/14_disc.html",
    "href": "chapters/14_disc.html",
    "title": "16  Discrete RVs",
    "section": "",
    "text": "Definition 16.1 (Discrete random variable) We say \\(X\\) is a discrete random variable if \\(X\\) can take a finite or countable number of values \\(x_1,x_2, \\ldots,x_n\\).\n\n\nDefinition 16.2 (Support) The finite or countably infinite set of values \\(x\\) such that \\(P(X=x)&gt;0\\) is called the support of \\(X\\).\n\n\nDefinition 16.3 (Probability mass function) If a random variable \\(X\\) has a discrete distribution, the probability mass function (PMF) of \\(X\\) is defined as the function \\(f:\\mathbb{R}\\to[0,1]\\) such that \\[f(x) \\equiv P(X=x).\\]\n\nNote that the PMF \\(f(x)\\) is a discrete function which can only take values in the support \\(\\{x_1,x_2, \\ldots,x_n\\}\\).\n\n\n\n\n\n\nNotation for PMF\n\n\n\nThroughout this course, we use PMF to refer to the probability function for a discrete random variable. Some textbooks may call it the probability function (p.f.), while others may use the term mass function. All these terms describe the same concept.\nNote that how \\(f(x)\\) differs from the probability function \\(P(\\cdot)\\). \\(f(x)\\) is a real-valued function, whereas \\(P(\\cdot)\\) is the probability operator. The two should not be confused even when the notation \\(p(x)\\) is used to represent a PMF.\nWe may want to use a subscript to distinguish PMFs for different RVs. For example, \\(f_{X}\\) is the PMF for random variable \\(X\\), \\(f_{Y}\\) is the PMF for random variable \\(Y\\).\n\n\n\nProposition 16.1 A probability mass function \\(f:\\mathbb{R}\\to[0,1]\\) satisfies\n\n\\(f(x)\\geq 0\\) for all \\(x\\) and \\(f(x)\\neq 0\\) if and only if \\(x\\) is in the support.\n\\(\\sum_{i} f(x_i)=1\\) where \\(i\\) indexes every value in the support.\n\n\nThere are different ways to represent a PMF. We can (1) list all the possible values and their associated probabilities; (2) write a formula for the PMF; or (3) visualize it in a graph.\n\nExample 16.1 (Bernoulli distribution) A random variable \\(X\\) is said to have the Bernoulli distribution if \\(X\\) has only two possible values, \\(0\\) and \\(1\\), and \\(P(X=1)=p\\), \\(P(X=0)=1-p\\).\nThe PMF of a Bernoulli random variable \\(X\\) is given by \\[f(k)=\\begin{cases}\np & \\textrm{if }k=1,\\\\\n1-p & \\textrm{if }k=0.\n\\end{cases}\\] This can also be expressed as \\[f(k)=p^{k}(1-p)^{1-k},\\quad k\\in\\left\\{ 0,1\\right\\} .\\]\n\n\nExample 16.2 A student is trying to connect to the campus Wi-Fi network. Each attempt is independent, and:\n\nWith probability \\(p\\) the attempt is successful.\nWith probability \\(1-p\\) the attempt fails, and the student tries again.\n\nThe student will keep trying until the first success.\n\nDefine \\(A_k\\) = “the first successful connection occurs on the \\(k\\)-th attempt.” Find \\(P(A_k)\\).\nDefine a random variable \\(X\\) = “the number of attempts needed until the first success.” What is the support of \\(X\\)?\nDerive the probability mass function (PMF) of \\(X\\).\nShow that this is a valid PMF (Proposition 16.1).",
    "crumbs": [
      "Random Variables",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Discrete RVs</span>"
    ]
  },
  {
    "objectID": "chapters/15_cont.html",
    "href": "chapters/15_cont.html",
    "title": "17  Continuous RVs",
    "section": "",
    "text": "Definition 17.1 (Continuous random variable) We say a random variable \\(X\\) has a continuous random variable if the possible values of \\(X\\) takes the form of a continuum.\n\n\nDefinition 17.2 (Probability density function) For a continuous random variable \\(X\\), the probability density function (PDF) of \\(X\\) is a real-valued function \\(f:\\mathbb{R}\\to[0,\\infty)\\) such that \\[P(a\\leq X\\leq b)=\\int_{a}^{b}f(x)dx.\\]\n\nContinuous random variables are usually measurements. Examples include height, weight, temperature, the amount of money and so on.\n\n\n\n\n\n\nDensity is not probability\n\n\n\nPDF differs from the discrete PMF in important ways:\n\nFor a continuous random variable, \\(P(X=x)=0\\) for all \\(x\\);\nThe quantity \\(f(x)\\) is not a probability. To get the probability, we integrate the PDF (probability is the area under the PDF): \\[P(a&lt;X\\leq b)=F(b)-F(a)=\\int_{a}^{b}f(x)dx.\\]\nSince any single value has probability 0, including or excluding endpoints does not matter. \\[P(a&lt;X&lt;b)=P(a&lt;X\\leq b)=P(a\\leq X&lt;b)=P(a\\leq X\\leq b).\\]\n\n\n\n\nProposition 17.1 If \\(X\\) has density function \\(f\\) then\n\n\\(P(X=x)=0\\) for all \\(x\\in\\mathbb{R}\\)\n\\(P(a\\leq X\\leq b) =\\int_a^b f(x) dx\\)\n\\(\\int_{-\\infty}^{\\infty}f(x)dx=1\\)\n\n\n\nExample 17.1 (Uniform distribution) A uniform distribution is a probability distribution where all values within a specified interval \\([a, b]\\) are equally likely to occur, and its probability density function (PDF) is given by: \\[f(x) = \\frac{1}{b - a} \\quad \\text{for} \\quad a \\leq x \\leq b\\] and \\(f(x) = 0\\) otherwise.\n\n\n\n\n\n\n\nDon’t confuse a random variable with its distribution\n\n\n\nIf random variable \\(X\\) has distribution \\(f(x)\\), the distribution of \\(X^2\\) is not \\(f^2(x)\\). To get the distribution of \\(X+Y\\), you can’t just add up \\(f_X(x)+f_Y(y)\\). The right way to do it will be discussed in later chapters (transformation and convolution).\n\n\n\nExample 17.2 Every morning, a student waits for the elevator in their dormitory. The waiting time (in minutes) is equally likely to be anywhere between 0 and 3 minutes, depending on when the elevator arrives.\n\nDefine \\(X\\) = “the student’s elevator waiting time.” What is the support of \\(X\\)?\nDerive the probability density function (PDF) of \\(X\\).\nCompute \\(P(X \\leq 1)\\), i.e. the probability that the waiting time is at most 1 minute.\nIf the student must wait more than 2 minutes, they decide to take the stairs instead. Define a new indicator random variable \\(Y\\), which equals \\(1\\) if \\(X&gt;2\\) and \\(0\\) otherwise. Compute \\(P(Y=1)\\).",
    "crumbs": [
      "Random Variables",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous RVs</span>"
    ]
  },
  {
    "objectID": "chapters/16_cdf.html",
    "href": "chapters/16_cdf.html",
    "title": "18  Cumulative distribution",
    "section": "",
    "text": "Definition 18.1 (Cumulative distribution function) The cumulative distribution function (CDF) of a random variable \\(X\\) is the function \\(F\\) given by \\(F(x)=P(X\\leq x)\\).\nFor discrete random variables, \\(F(x)=\\sum_{k\\leq x}p(k)\\).\nFor continuous random variables, \\(F(x)=\\int_{-\\infty}^{x}f(t)dt\\). We thus have \\(\\frac{dF(x)}{dx}=f(x)\\).\n\nUnlike PMF or PDF, a cumulative distribution function can be defined for both discrete and continuous random variables. CDF gives the full distribution of a random variable. Given the CDF, we can figure out any probability distribution of the random variable: \\[P(x_{1}&lt;x\\leq x_{2})=F(x_{2})-F(x_{1}).\\]\n\nProposition 18.1 Any CDF has the following properties:\n\n\\(P(X&gt;x)=1-F(x)\\)\n\\(P(x_{1}&lt;x\\leq x_{2})=F(x_{2})-F(x_{1})\\)\nIncreasing: if \\(x_{1}\\leq x_{2}\\), then \\(F(x_{1})\\leq F(x_{2})\\).\nRight-continuous: for any \\(a\\), \\(F(a)=\\lim_{x\\to a+}F(x)\\).\n\\(F(x)\\to0\\) as \\(x\\to-\\infty\\); \\(F(x)\\to1\\) as \\(x\\to+\\infty\\).\n\n\nThe CDF for a continuous random variable is differentiable, while the CDF for a discrete random variable consists of jumps and ﬂat regions.",
    "crumbs": [
      "Random Variables",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Cumulative distribution</span>"
    ]
  },
  {
    "objectID": "chapters/18_bin.html",
    "href": "chapters/18_bin.html",
    "title": "19  Binomial distribution",
    "section": "",
    "text": "Coin tossing problem\nThe PMF of \\(X\\) directly follows from the combination theory: \\[P(X=k)=\\binom{n}{k}p^{k}(1-p)^{n-k}.\\]\nThis is a valid PMF because, by the Binomial theorem, we have \\[\\sum_{k=0}^{n}\\binom{n}{k}p^{k}(1-p)^{n-k}=(p+(1-p))^{n}=1.\\]\nThe following code simulates the number of heads if tossing \\(N\\) coins:\n# Number of simulations\nk &lt;- 1000 \n\n# Number of coins\nn &lt;- 20\n\n# Store the results\nn_heads &lt;- numeric(k)\n\n# Initialize random generator\nset.seed(100)\n\n# Run simulations\nfor (i in 1:k) { \n  toss &lt;- sample(c('H','T'), n, replace = T)\n  n_heads[i] &lt;- sum(toss == 'H')\n}\n\n# Plot distribution\nhist(n_heads, probability=TRUE)\n\n# Overlay the Binomial PMF\ncurve(choose(n,x)* 0.5^x * 0.5^(n-x), from=1, to=n, n=n, col=2, add=T)",
    "crumbs": [
      "Discrete Distributions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Binomial distribution</span>"
    ]
  },
  {
    "objectID": "chapters/18_bin.html#coin-tossing-problem",
    "href": "chapters/18_bin.html#coin-tossing-problem",
    "title": "19  Binomial distribution",
    "section": "",
    "text": "Example 19.1 In the previous example of tossing two coins, we compute the distribution of \\(X\\) by counting the equally likely outcomes in an event. We can get the same result by realizing it is a Binomial distribution. \\(X\\sim\\textrm{Bin}(2,1/2)\\). Since each coin tossing is an independent Bernoulli trial. The probabilities come directly from the PMF. \\[\\begin{aligned}\nP(X=0) & =\\binom{2}{0}\\left(\\frac{1}{2}\\right)^{0}\\left(\\frac{1}{2}\\right)^{2}=\\frac{1}{4};\\\\\nP(X=1) & =\\binom{2}{1}\\left(\\frac{1}{2}\\right)^{1}\\left(\\frac{1}{2}\\right)^{1}=\\frac{1}{2};\\\\\nP(X=2) & =\\binom{2}{2}\\left(\\frac{1}{2}\\right)^{2}\\left(\\frac{1}{2}\\right)^{0}=\\frac{1}{4}.\\end{aligned}\\]\nUtilizing the Binomial distribution also allows us to generalize the problem. Suppose we are tossing \\(n\\) coins, we want to find the probability of getting \\(k\\) heads. It is almost impossible to count all the possible outcomes, but the answer immediately follows from the Binomial PMF.",
    "crumbs": [
      "Discrete Distributions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Binomial distribution</span>"
    ]
  },
  {
    "objectID": "chapters/18_bin.html#binomial-functions-in-r",
    "href": "chapters/18_bin.html#binomial-functions-in-r",
    "title": "19  Binomial distribution",
    "section": "Binomial functions in R",
    "text": "Binomial functions in R\nThere are built-in functions in R to work with Binomial distributions.\n\n# computes P(X=5) for Bin(10,0.5)\np &lt;- dbinom(5, 10, 0.5)\n\npar(mfrow=c(1,2))\n\n# plot the PMF for Bin(10,0.5)\ncurve(dbinom(x, 10, 0.5), from=0, to=10, n=11, type=\"b\", ann=F)\n\n# `pbinom` computes the CDF \ncurve(pbinom(x, 10, 0.5), from=0, to=10, n=11, type=\"b\",ann=F)\n\n\n\n\n\n\n\n\n\n# draw a random value from a given Binomial distribution\n# this allows us to simulate a random experiment\n# e.g. the number of heads when flipping 10 fair coins\noutcome &lt;- rbinom(1, 10, 0.5)\n\n# Repeat the experiment 1000 times\nheads &lt;- rbinom(1000, 10, 0.5)\n\n# the histogram will converge to the ideal Binomial distribution\n# if the experiment is repeated a large number of times\nhist(heads)",
    "crumbs": [
      "Discrete Distributions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Binomial distribution</span>"
    ]
  },
  {
    "objectID": "chapters/18_bin.html#exam-survival-problem",
    "href": "chapters/18_bin.html#exam-survival-problem",
    "title": "19  Binomial distribution",
    "section": "Exam survival problem",
    "text": "Exam survival problem\n\nExample 19.2 An exam consists of 20 multiple-choice questions, each with four choices and exactly one correct answer. Suppose a student answers every question by guessing at random. What is the probability that the student passes the exam, defined as answering more than 60% of the questions correctly?\n\n\nSolution. The probability of correctly answering one question is \\(p=1/4\\). Let \\(N\\) be the total number of questions, \\(N=20\\). Let \\(X\\) be the number of correctly answered questions, \\(X\\leq N\\). Then \\(X\\) follows the Binomial distribution \\(X \\sim B(N, 1/4)\\). The probability of passing the exam is therefore \\[P(X \\geq 12) = 1-P(X\\leq 11)=1-\\text{CDF}^{\\text{Bin}}(11)\\approx 0.001.\\]\n\nNow we compare the survival probability for different choice of \\(N\\) and \\(p\\) :\n\n# Percentage of correct answers\nx &lt;- seq(0, 1, .1)\n\n# Survival probabilities for different N\ny1 &lt;- 1 - pbinom(10*x, 10, .25)\ny2 &lt;- 1 - pbinom(20*x, 20, .25)\ny3 &lt;- 1 - pbinom(30*x, 30, .25)\n\n# Compare the curves for different N\nplot(x, y1, type=\"b\", col=1, ann=F)\nlines(x, y2, type=\"b\", col=2)\nlines(x, y3, type=\"b\", col=3)\n\n# Indicating passing the exam\nabline(v=0.6, lty=2)\n\n# Add a legend at the top right corner of the plot\nlegend(\"topright\", c(\"N=10\", \"N=20\", \"N=30\"), lty=1, col=1:3)\n\n\n\n\n\n\n\n\n\n# Percentage of correct answers\nx &lt;- seq(0, 1, .1)\n\n# Survival probabilities for different p (number of choices)\ny1 &lt;- 1 - pbinom(10*x, 10, .25)\ny2 &lt;- 1 - pbinom(10*x, 10, .33)\ny3 &lt;- 1 - pbinom(10*x, 10, .5)\n\n# Compare the curves for different p\nplot(x, y1, type=\"b\", col=1, ann=F)\nlines(x, y2, type=\"b\", col=2)\nlines(x, y3, type=\"b\", col=3)\n\n# Indicating passing the exam\nabline(v=0.6, lty=2)\n\n# Add a legend at the top right corner of the plot\nlegend(\"topright\", c(\"p=1/4\", \"p=1/3\", \"p=1/2\"), lty=1,col=1:3)",
    "crumbs": [
      "Discrete Distributions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Binomial distribution</span>"
    ]
  },
  {
    "objectID": "chapters/20_dexp.html",
    "href": "chapters/20_dexp.html",
    "title": "20  Discrete expectation",
    "section": "",
    "text": "Law of averages*\nIn other words, the expected value of \\(X\\) is a weighted average of the possible values that \\(X\\) can take on, weighted by their probabilities.\nSometimes, we would like to omit the parentheses for simplicity and write \\(EX := E(X)\\). We also like to denote expectation by the Greek letter \\(\\mu := E(X)\\).\nYou may wonder what is the difference between \\(E(X)\\) defined in Definition 20.1 and the average of values defined as \\(\\bar{X}=\\frac{1}{n}(X_1+X_2+\\dots+X_n)\\).\nThe short answer is this: \\(E(X)\\) is a theoretical value, while \\(\\bar{X}\\) is an approximation to \\(E(X)\\) with finite observations. They are associated by the following theorem.\nThere is another fundamental difference. In probability theory, we treat \\(E(X)\\) as a fixed number; while \\(\\bar X\\) is another random variable! Because the sample \\(\\{X_1,X_2,\\dots,X_n\\}\\) is generated randomly. Consider the coin flipping example, while \\(E(X)=0.5\\) is a constant, each time you compute the average of, say, 10 flips, you get a different number. We will come back to this point later.",
    "crumbs": [
      "Discrete Distributions",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Discrete expectation</span>"
    ]
  },
  {
    "objectID": "chapters/20_dexp.html#law-of-averages",
    "href": "chapters/20_dexp.html#law-of-averages",
    "title": "20  Discrete expectation",
    "section": "",
    "text": "Law of averages\n\n\n\nThe law of averages (or the law of large numbers) states that if you repeat a random experiment, such as tossing a coin or rolling a die, a very large number of times, your individual outcomes, when averaged, should be very close to the theoretical mean (a constant parameter). In mathematical language, \\[\\bar X_n \\to^p \\mu \\text{ when }n\\to\\infty.\\] where \\(\\to^p\\) reads as “converge in probability”.",
    "crumbs": [
      "Discrete Distributions",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Discrete expectation</span>"
    ]
  },
  {
    "objectID": "chapters/21_hgeom.html",
    "href": "chapters/21_hgeom.html",
    "title": "21  Hypergeometric dist",
    "section": "",
    "text": "Example 21.1 Let’s explore an example that appears to be Binomial but is, in fact, not a Binomial distribution. Given a 5-card hand. Find the distribution of the number of aces.\nLet \\(X\\) be the number of aces. It is tempting to say \\(X\\sim \\text{Bin}(5,p)\\). But this not correct. Because having one ace is NOT independent from having another ace. We need to use the classical approach: \\[P(X=k)=\\frac{\\binom{4}{k}\\binom{48}{5-k}}{\\binom{52}{5}}.\\] This is a Hypergeometric distribution.\n\nSuppose we have a box filled with \\(w\\) white and \\(b\\) black balls. We draw \\(n\\) balls out of the box with replacement. Let \\(X\\) be the number of white balls. Then \\(X\\sim \\text{Bin}(n,w/(w+b))\\). Since the draws are independent Bernoulli trials, each with probability \\(w/(w+b)\\) of success. If we instead sample without replacement, then the number of white balls follows a Hypergeometric distribution. We denote this by \\(X\\sim\\textrm{HGeom}(w,b,n)\\).\n\nTheorem 21.1 If \\(X\\sim\\textrm{HGeom}(w,b,n)\\), then the PMF of \\(X\\) is \\[p_{X}(k)=\\frac{\\binom{w}{k}\\binom{b}{n-k}}{\\binom{w+b}{n}},\\] for integers \\(k\\) satisfying \\(0\\leq k\\leq w\\) and \\(0\\leq n-k\\leq b\\), and \\(p_{X}(k)=0\\) otherwise.\n\nIn Example 21.1, the number of aces in the hand has the \\(\\textrm{HGeom}(4,48,5)\\) distribution, which can be seen by thinking of the aces as white balls and the non-aces as black balls. The probability of having exactly three aces is \\(0.0017\\%\\).\n\nExample 21.2 Let \\(X\\sim\\textrm{HGeom}(w,b,n)\\). Find \\(E(X)\\) the expected number of white balls. Similarly, we can decompose \\(X\\): \\[X=I_{1}+\\cdots+I_{n}\\] where \\(I_{j}\\) equals \\(1\\) if the \\(j\\)th ball is white and \\(0\\) otherwise. We have said that \\(\\{I_{j}\\}\\) are not independent, but the linearity of expectation still holds: \\[E(X)=E(I_{1}+\\cdots+I_{n})=E(I_{1})+\\cdots+E(I_{n}).\\] Meanwhile we have \\[E(I_{j})=P(j\\textrm{-th ball is white})=\\frac{w}{w+b}\\] since unconditionally the \\(j\\)th ball is equally likely to be any of the balls. Thus, \\(E(X)=\\frac{nw}{w+b}\\).\n\nThe Binomial and Hypergeometric distributions are often confused. Both are discrete distributions taking on integer values between \\(0\\) and \\(n\\) for some \\(n\\), and both can be interpreted as the number of successes in \\(n\\) Bernoulli trials. However, a crucial part of the Binomial story is that the Bernoulli trials involved are independent. The Bernoulli trials in the Hypergeometric story are dependent, since the sampling is done without replacement.",
    "crumbs": [
      "Discrete Distributions",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Hypergeometric dist</span>"
    ]
  },
  {
    "objectID": "chapters/22_geom.html",
    "href": "chapters/22_geom.html",
    "title": "22  Geometric distribution",
    "section": "",
    "text": "Definition 22.1 (Geometric distribution) Consider a sequence of independent Bernoulli trials, each with the same success probability \\(p\\). Let \\(X\\) be the number of failures before the first successful trial. Then \\(X\\) has a Geometric distribution: \\(X\\sim\\textrm{Geom}(p)\\).\n\nLet’s derive the PMF for the Geometric distribution. By definition,\n\\[P(X=k)=q^{k}p\\] where \\(q=1-p\\). This is a valid PMF because \\[\\sum_{k=0}^{\\infty}q^{k}p=p\\sum_{k=0}^{\\infty}q^{k}=\\frac{p}{1-q}=1.\\] The expectation of \\(X\\) is given by\n\\[E(X)=\\sum_{k=0}^{\\infty}k\\cdot q^{k}p=p\\sum_{k=0}^{\\infty}kq^{k}=p\\frac{q}{p^{2}}=\\frac{q}{p}.\\] To see why this holds, taking derivative with respect to \\(q\\) on both sides of \\(\\sum_{k=0}^{\\infty}q^{k}=\\frac{1}{1-q}\\) yields\n\\[\\sum_{k=1}^{\\infty}kq^{k-1}=\\frac{1}{(1-q)^{2}};\\] Then multiply both sides by \\(q\\):\n\\[\\sum_{k=1}^{\\infty}kq^{k}=\\frac{q}{(1-q)^{2}}=\\frac{q}{p^{2}}.\\]\nPlot the PMF and CDF\n\npar(mfrow=c(1,2))\n\n# PMF for Geom(0.5)\ncurve(dgeom(x, 0.5), from=0, to=10, n=11, type=\"b\", ann=F)\n\n# CDF for Geom(0.5)\ncurve(pgeom(x, 0.5), from=0, to=10, n=11, type=\"b\", ann=F)\n\n\n\n\n\n\n\n\n\nExample 22.1 (Coin flip until Head) Flipping a fair coin, what is the expected number of flips before the first Head?\nLet \\(X\\) be the number of flips until the first head. We know \\(X-1\\sim\\text{Geom}(0.5)\\) as geometric distribution models the number of failures excluding the success. Thus, \\(E(X-1)=0.5/0.5=1\\), \\(E(X)=2\\). Let’s compare the theoretical value with results from simulations.\n\n\n# number of simulations\nN &lt;- 1000  \n\n# X: number of flips until first head\n# stores value of X in each simulation\nX &lt;- numeric(N) \n\nset.seed(100)\n\n# run simulations\nfor (i in 1:N) {\n  x &lt;- 0\n  # repeat until first head\n  while(TRUE) {\n    x &lt;- x + 1\n    t &lt;- sample(c('H','T'), 1, F)\n    if (t == 'H') break\n  }\n  # record the number\n  X[i] &lt;- x\n}\n\n# plot distribution of X\nhist(X, probability=T)\n\n#overlay with geometric distribution\ncurve(dgeom(x-1,.5), from=1, to=10,n=10,add=T,col=2)\n\n\n\n\n\n\n\ncat(\"Average number of flips until Head:\", mean(X))\n\nAverage number of flips until Head: 2.021",
    "crumbs": [
      "Discrete Distributions",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Geometric distribution</span>"
    ]
  },
  {
    "objectID": "chapters/22_hhht.html",
    "href": "chapters/22_hhht.html",
    "title": "23  Coin flip: HH vs HT*",
    "section": "",
    "text": "Flip a coin indefinite times. Let \\(X\\) denote the number of flips until you see HH. Let \\(Y\\) denote the number of flips until you see HT. Find \\(E(X)\\) and \\(E(Y)\\).\nIt is tempting to think they are the same, since either H or T happens with probability 1/2. But the answer is extremely counter-intuitive: \\(E(X)&gt;E(Y)\\)!\nHH case. Let \\(E_0\\) = E(X|No H observed), and \\(E_1\\) = E(X|One H observed). Then \\[E_0 = 1 + \\frac{1}{2}E_1 + \\frac{1}{2}E_0\\]\nThe first term is we need to flip once. If the first flip is H, the additional expected number of flips is \\(E_1\\). If the first flip is T, we have to start over again (\\(E_0\\)).\n\\[E_1 = 1 + \\frac{1}{2}(0) + \\frac{1}{2}E_0\\]\nOnce we have observed an H, we do another flip. If it is another H, we are done. If it is a T, we have to start over again (\\(E_0\\)).\nSolve the two equations, we have \\(E_0=6\\), \\(E_1=4\\). Thus, \\(E(X)=6\\).\nHT case. Let \\(E_0\\) = E(Y|No H observed), and \\(E_1\\) = E(Y|One H observed). Then\n\\[E_0 = 1 + \\frac{1}{2}E_1 + \\frac{1}{2}E_0\\]\nIf the first flip is H, we need \\(E_1\\). If the first flip is T, we have wasted the flip, so it is \\(E_0\\) again.\n\\[E_1 = 1 + \\frac{1}{2}(0) + \\frac{1}{2}E_1\\]\nIf we have a T by 1/2 chance, we are done (the first term). If it is an H, we get another \\(E_1\\).\nIn this case, we have \\(E_0=4\\), \\(E_1=2\\). Thus, \\(E(Y)=4\\).\n\n# number of simulations\nN &lt;- 1000  \n\n# X: number of flips until HH\nX &lt;- numeric(N) \n\nset.seed(100)\n\n# run simulations\nfor (i in 1:N) {\n  x &lt;- 0\n  # repeat until first head\n  while(TRUE) {\n    x &lt;- x + 1\n    t &lt;- sample(c('H','T'), 1, F)\n    if (x &gt;=2 && t == 'H' && tt == 'H') break\n    else tt &lt;- t  # store last toss\n  }\n  # record the number\n  X[i] &lt;- x\n}\n\ncat(\"Average number of flips until HH:\", mean(X))\n\nAverage number of flips until HH: 5.789\n\n\n\n# number of simulations\nN &lt;- 1000  \n\n# X: number of flips until HT\nY &lt;- numeric(N) \n\nset.seed(100)\n\n# run simulations\nfor (i in 1:N) {\n  y &lt;- 0\n  # repeat until first head\n  while(TRUE) {\n    y &lt;- y + 1\n    t &lt;- sample(c('H','T'), 1, F)\n    if (y &gt;= 2 && t == 'T' && tt == 'H') break\n    else tt &lt;- t  # store last toss\n  }\n  # record the number\n  Y[i] &lt;- y\n}\n\ncat(\"Average number of flips until HT:\", mean(Y))\n\nAverage number of flips until HT: 4.047",
    "crumbs": [
      "Discrete Distributions",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Coin flip: HH vs HT\\*</span>"
    ]
  },
  {
    "objectID": "chapters/22_nbin.html",
    "href": "chapters/22_nbin.html",
    "title": "24  Negative Binomial",
    "section": "",
    "text": "Definition 24.1 (Negative Binomial distribution) In a sequence of independent Bernoulli trials with success probability \\(p\\), if \\(X\\) is the number of failures before the \\(r\\)-th success, then \\(X\\) is said to have a Negative Binomial distribution, denoted \\(X\\sim\\textrm{NBin}(r,p)\\).\n\nThe PMF for Negative Binomial distribution is given by\n\\[P(X=k)=\\binom{k+r-1}{r-1}q^{k}p^{r}.\\]\nTo compute the expectation, let \\(X=X_{1}+\\cdots+X_{r}\\) where \\(X_{i}\\) is the number of failures between the \\((i-1)\\)-th success and the \\(i\\)-th success, \\(1\\leq i\\leq r\\). Then \\(X_{i}\\sim\\textrm{Geom}(p)\\). By linearity of expectations,\n\\[E(X)=E(X_{1})+\\cdots+E(X_{r})=r\\frac{1-p}{p}.\\]\n\nTheorem 24.1 Let \\(X_1,X_2,...,X_n\\) be independent geometric distributions with the same parameter \\(p\\). Then \\[X=X_1+X_2+\\cdots+X_n\\] is a negative binomial distribution, namely \\(X\\sim\\text{NBin}(n,p)\\).\n\nThe theorem is straightforward if we interpret \\(X_1\\) as the number of failures before the 1st success, \\(X_2\\) as the number of failures between the 1st and 2nd successes, and \\(X_n\\) be the number of failures between the \\((n-1)\\)-th and \\(n\\)-th successes.\n\nExample 24.1 (Toy collection) There are \\(n\\) types of toys. Assume each time you buy a toy, it is equally likely to be any of the \\(n\\) types. What is the expected number of toys you need to buy until you have a complete set?\n\n\nSolution. Define the following random variables: \\[\\begin{aligned}T= & T_{1}+T_{2}+\\cdots+T_{n}\\\\\nT_{1}= & \\textrm{number of toys until 1st new type}\\\\\nT_{2}= & \\textrm{additional number of toys until 2nd new type}\\\\\nT_{3}= & \\textrm{additional number of toys until 3rd new type}\\\\\n\\vdots\n\\end{aligned}\\]\nWe know, \\(T_{1}=1\\), \\(T_{2}-1\\sim\\textrm{Geom}\\left(\\frac{n-1}{n}\\right)\\),…, \\(T_{j}-1\\sim\\textrm{Geom}\\left(\\frac{n-(j-1)}{n}\\right)\\). Thus, \\[\\begin{aligned}E(T)= & E(T_{1})+E(T_{2})+\\cdots+E(T_{n})\\\\\n= & 1+\\frac{n}{n-1}+\\frac{n}{n-2}+\\cdots+\\frac{1}{n}\\\\\n= & n(1+\\frac{1}{2}+\\frac{1}{3}+\\cdots+\\frac{1}{n})\\\\\n\\to & n(\\log n+0.577).\n\\end{aligned}\\]\nIf \\(n=5\\), \\(E(T)\\approx11\\); if \\(n=10\\), \\(E(T)\\approx29\\).\n\n\n# number of simulations\nN &lt;- 1000\n\n# number of toys bought in each simulation\nX &lt;- numeric(N)\n\n# the set of toys\nToys &lt;- c('🧸','🐢','🐣','🐤','🐼','🤖','🐸','🐷')\n\nset.seed(100)\n\n# run simulation\nfor (i in 1:N) {\n  C &lt;- c() # the collection of toys bought\n  # repeat until a full set is collected\n  while(TRUE) {\n    t &lt;- sample(Toys, 1, replace=T)\n    C &lt;- c(C, t)\n    if(setequal(C, Toys)) break\n  }\n  X[i] &lt;- length(C)\n}\n\nhist(X, probability = T)\nabline(v = mean(X), col = 2)",
    "crumbs": [
      "Discrete Distributions",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Negative Binomial</span>"
    ]
  },
  {
    "objectID": "chapters/23_joint.html",
    "href": "chapters/23_joint.html",
    "title": "25  Bivariate distribution",
    "section": "",
    "text": "We need a tool to study collections of variables. Knowledge of each individual PMF is of little help. Because variables can be dependent on each each other (they are not necessarily independent). We need to know their inter-relationship. Joint distribution gives the probability that two or more random variables simultaneously takes particular values.\n\nDefinition 25.1 (Joint distribution) The joint PMF of random variables \\((X,Y)\\) is given by \\[f(x,y) = P(X=x,Y=y).\\]\nThe joint CDF of random variables \\((X,Y)\\) is given by \\[F(x,y) = P(X\\leq x,Y\\leq y).\\]\n\n\nTheorem 25.1 The discrete random variables \\(X\\) and \\(Y\\) are independent if and only if \\[P(X=x,Y=y)=P(X=x)P(Y=y)\\] for all possible values of \\(x,y\\).\nEquivalently, the condition can be stated with CDF: the random variables \\(X\\) and \\(Y\\) are independent if and only if \\[P(X\\leq x,Y\\leq y)=P(X\\leq x)P(Y\\leq y)\\] for all possible values of \\(x,y\\).\n\n\nProof. \\(X\\) and \\(Y\\) are independent implies the event \\(\\{X=x\\}\\) and \\(\\{Y=y\\}\\) are independent for any \\(x,y\\). By Definition 10.1, we have \\[P(X=x,Y=y)= P(\\{X=x\\}\\cap\\{Y=y\\})=P(X=x)P(Y=y).\\]\n\n\n\n\n\n\n\nNote\n\n\n\nIf \\(X\\) and \\(Y\\) are independent, then any function of \\(X\\) is independent of any function of \\(Y\\).\n\n\n\nDefinition 25.2 (Marginal distribution) The marginal distribution gives the distribution of a subset of variables in a joint distribution without reference to the values of the other variables.\nThe marginal PMF of \\(X\\) given the joint PMF of \\((X,Y)\\) is given by \\[f_X(x) = \\sum_y P(X=x,Y=y)=\\sum_y f_{X,Y}(x,y).\\]\n\n\n\n\n\n\n\nNote\n\n\n\nIt is easy to compute the marginal distribution given the joint distribution. However, in general, we cannot deduce the joint distribution from the marginal distribution. Unless the random variables are independent, the joint distribution is not the product of marginal distributions.\n\n\n\n\n\n\n\n\nExample 25.1 Let \\(X\\) be an indicator of an individual being a current smoker. Let \\(Y\\) be the indicator of his developing lung cancer at some point in his life. The joint PMF of \\(X\\) and \\(Y\\) is as specified in the table below.\n\n\n\n\n\\(Y=1\\)\n\\(Y=0\\)\nTotal\n\n\n\\(X=1\\)\n0.05\n0.20\n0.25\n\n\n\\(X=0\\)\n0.03\n0.72\n0.75\n\n\nTotal\n0.08\n0.92\n1\n\n\n\nThe marginal PMF for having lung cancer is\n\\[\\begin{aligned}P(Y=1)= & P(Y=1,X=0)+P(Y=1,X=1)=0.08,\\\\\nP(Y=0)= & P(Y=0,X=0)+P(Y=0,X=1)=0.92.\n\\end{aligned}\\]\nIn this example, \\(X,Y\\) are not independent, because \\[P(X=1,Y=1)\\neq P(X=1)P(Y=1).\\]\n\n\nDefinition 25.3 If a given number of random variables are independent and have the same distribution, we call them independent and identically distributed, or i.i.d for short.\n\n\nIndependent and identically distributed (\\(X,Y\\) independent die rolls)\nIndependent and not identically distributed (\\(X\\): die roll; \\(Y\\): coin flip)\nDependent and identically distributed (\\(X\\): number of Heads; \\(Y\\): number of Tails)\nDependent and not identically distributed (\\(X\\): economic growth; \\(Y\\): presidential election)\n\n\n\n\n\n\n\nNote\n\n\n\nWe view random sample as a collection of i.i.d random variables from the same population distribution. For example, let \\(X_i\\) be the test score of student \\(i\\). We say \\(X_1,X_2,\\dots,X_n \\overset{iid}{\\sim} G\\) where \\(G\\) is the (unknown) population distribution for test scores.\nThe independent assumption means that one observation does not influence another, while the identically distributed assumption ensures all observations follow the same probability law. This perspective simplifies statistical analysis and is foundational for many statistical inference.",
    "crumbs": [
      "Discrete Distributions",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Bivariate distribution</span>"
    ]
  },
  {
    "objectID": "chapters/23_condist.html",
    "href": "chapters/23_condist.html",
    "title": "26  Conditional distribution",
    "section": "",
    "text": "Definition 26.1 (Conditional distribution) The conditional PMF of \\(Y\\) given \\(X=x\\) is defined as \\[f_{Y|X} (y|x) = P(Y=y | X=x)=\\frac{P(Y=y,X=x)}{P(X=x)}\\] for any \\(x\\) such that \\(P(X=x)&gt;0\\).\n\nPlot conditional distribution\n\nlibrary(lattice) # easy to make conditional plots\n\n# conditional distribution of exam scores \nexam &lt;- read.csv(\"../dataset/exam.csv\")\n\n# distribution of exam scores conditioned on major\nhistogram(~ final | major, data = exam)\n\n\n\n\n\n\n\n\n\nDefinition 26.2 (Conditional expectation) The conditional expectation of \\(Y\\) given \\(X=x\\) is defined as \\[\\psi(x) = E(Y|X=x)= \\sum_{\\text{all }y} y\\ f_{Y|X}(y|x).\\] \\(\\psi(x)\\) depends on the value of \\(x\\) taken by \\(X\\), so it can be thought of as a function \\(\\psi(X)\\) of \\(X\\) itself. \\[\\psi(X)= E(Y|X)\\] is called the conditional expectation of \\(Y\\) given \\(X\\).\n\n\n\n\n\n\n\nNote\n\n\n\nAlthough \\(E(X)\\) is a number, \\(E(Y|X)\\) is a random variable. It is a function of random variable \\(X\\), and therefore it is a random variable itself.\n\n\nConditional distribution is a key concept in probability, describing how the distribution of one random variable depends on the values of other variables—an idea central to many practical applications. For instance, we might be interested in how income distributions vary by education level or how the probability of a disease changes with age.\nConditional expectation gives the expected value of one variable given the value of another. It is frequently used for making predictions, such as predicting your earnings given that you graduate from a this college.\n\n\n\nGiven each value of X, there is a distribution of Y|X. E(Y|X) is a function of X.",
    "crumbs": [
      "Discrete Distributions",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Conditional distribution</span>"
    ]
  },
  {
    "objectID": "chapters/24_pois.html",
    "href": "chapters/24_pois.html",
    "title": "27  Poisson distribution",
    "section": "",
    "text": "Now we introduce arguably the most popular discrete distribution—Poisson distribution. Poisson distribution is used to model independent events occurring at a constant mean rate. It is like the Binomial distribution in the sense that they both model the number of occurrence of events, but it is parametrized on the “rate” of the event (how many times an event occurs in a unit of time on average) rather than the total number of events and the probability of each event. It is therefore more practical in real-world modeling since we mostly observe the rate rather than the totality. We introduce the Poisson distribution by showing that it is a limiting case of the Binomial distribution.\nQuestion: Suppose we are studying the distribution of the number of visitors to a certain website. Every day, a million people independently decide whether to visit the site, with probability \\(p=2\\times10^{-6}\\) of visiting. What is the probability of getting \\(k\\) visitors on a particular day?\nWe can model the problem with a Binomial distribution. Let \\(X\\sim \\text{Bin}(n,p)\\) be the number of visitors, where \\(n=10^{6}\\) and \\(p=2\\times10^{-6}\\). But it is easy to run into computational difficulties with such a large \\(n\\) and small \\(p\\). This is not uncommon, if we want to model the number of emails one receives per day, or the number of phone calls in a service center. In such cases, we could reasonably assume \\(n\\to\\infty\\) and \\(p\\to0\\) while \\(np=\\lambda\\) is a constant. We may call \\(\\lambda\\) — the “rate”, as it can be interpreted as the average visitors per day.\nTake limit of the Binomial distribution: \\[\\begin{aligned}P(X=k)= & \\lim_{n\\to\\infty}\\binom{n}{k}p^{k}(1-p)^{n-k}\\\\\n= & \\lim_{n\\to\\infty}\\binom{n}{k}\\left(\\frac{\\lambda}{n}\\right)^{k}\\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\\n= & \\lim_{n\\to\\infty}\\frac{n!}{(n-k)!k!}\\cdot \\frac{\\lambda^{k}}{n^{k}}\\underbrace{\\left(1-\\frac{\\lambda}{n}\\right)^{n}}_{\\to e^{-\\lambda}}\\underbrace{\\left(1-\\frac{\\lambda}{n}\\right)^{-k}}_{\\to1}\\\\\n= & \\lim_{n\\to\\infty}\\underbrace{\\frac{n!}{n^{k}(n-k)!}}_{\\to1}\\frac{\\lambda^{k}}{k!}e^{-\\lambda}\\\\\n= & \\frac{\\lambda^{k}}{k!}e^{-\\lambda}.\n\\end{aligned}\\] This is the PMF of the Poisson distribution.\n\n\n\n\n\n\nThe limiting definition of exponential function\n\n\n\n\\[e^x = \\lim_{n\\to\\infty}\\left(1+\\frac{x}{n}\\right)^n\\]\n\n\n\nDefinition 27.1 (Poisson distribution) A random variable \\(X\\) has the Poisson distribution with parameter \\(\\lambda\\) if the PMF of \\(X\\) is\n\\[P(X=k)=\\frac{e^{-\\lambda}\\lambda^{k}}{k!},\\quad k=0,1,2,\\ldots\\]\nWe denote this as \\(X\\sim\\textrm{Pois}(\\lambda)\\).\n\nWe can easily verify this is a valid PMF because \\(\\sum_{k=0}^{\\infty}\\frac{\\lambda^{k}}{k!}=e^{\\lambda}\\).\n\nTheorem 27.1 If \\(X\\sim \\text{Bin}(n,p)\\) and we let \\(n\\to\\infty\\) and \\(p\\to0\\) such that \\(\\lambda=np\\) remains fixed, then the PMF of \\(X\\) converges to the PMF of \\(\\text{Pois}(\\lambda)\\).\n\nThe expectation of the Poisson distribution is\n\\[\\begin{aligned}E(X)= & \\sum_{k=0}^{\\infty}k\\cdot\\frac{e^{-\\lambda}\\lambda^{k}}{k!}\\\\\n= & e^{-\\lambda}\\sum_{k=1}^{\\infty}\\frac{\\lambda^{k}}{(k-1)!}\\\\\n= & \\lambda e^{-\\lambda}\\sum_{k=1}^{\\infty}\\frac{\\lambda^{k-1}}{(k-1)!}\\\\\n= & \\lambda e^{-\\lambda}e^{\\lambda}=\\lambda.\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\n\n\nExample 27.1 Continued with the website visiting example, there are one million people visiting the site every day, each with probability \\(p=2\\times10^{-6}\\). Give an approximation for the probability of getting at least three visitors on a particular day.\nLet \\(X\\) be the number of visitors. Since \\(n\\) is large, \\(p\\) is small, \\(np=2\\) is fixed, \\(X\\) is well approximated by \\(\\text{Pois}(2)\\). Therefore, \\[\\begin{aligned}\nP(X\\geq3)=1-P(X&lt;3) & =1-P(X=0)-P(X=1)-P(X=2)\\\\\n& =1-e^{-2}-2e^{-2}-\\frac{2^{2}}{2!}e^{-2}\\\\\n& =1-5e^{-2}\\approx0.32.\\end{aligned}\\]\n\nThe Poisson distribution is often used in situations where we are counting the number of successes in a particular region or interval of time, where there are a large number of trials, each with a small probability of success. The Poisson paradigm says in situations like this, we can approximate the number of successes by a Poisson distribution. It is more general than Theorem 27.1, as we relax the assumption of independence and identical events.\n\nProposition 27.1 (Poisson paradigm) Let \\(A_{1},\\dots,A_{n}\\) be events with \\(p_{j}=P(A_{j})\\), where \\(n\\) is large, the \\(p_{j}\\) are small, and the \\(A_{j}\\) are independent or weakly dependent. Then \\(X=\\sum_{j=1}^{n}I(A_{j})\\), that is how many of the \\(A_{j}\\) occur, is approximately distributed as \\(\\text{Pois}(\\lambda)\\) with \\(\\lambda=\\sum_{j=1}^{n}p_{j}\\).\n\nThe Poisson paradigm is also called the law of rare events. The interpretation of “rare” is that the \\(p_{j}\\) are small, but \\(\\lambda\\) is relatively stable. The number of events that occur may not be exactly Poisson, but the Poisson distribution often gives good approximations. Note that the conditions for the Poisson paradigm to hold are fairly flexible: the \\(n\\) trials can have different success probabilities, and the trials don’t have to be independent, though they should not be very dependent. So there are a wide variety of situations that can be cast in terms of the Poisson paradigm. This makes the Poisson a very popular model.\nPoisson distribution is also used to model the number of events occurring randomly over time with constant rate, such as the number of customers visiting a store, the number of phone calls to a call center, and so on.\nWhy the random occurrence of events has anything to do with the Poisson distribution? Consider in this way: one can divide the time line into infinitely small intervals (e.g. milliseconds). In each interval, an event either happens or not. The chance that an event occurs in a millisecond is very small. While there are infinitely many trials. So counting events occurring randomly at a fixed average rate over time is mathematically equivalent to counting rare events in many trials.\n\nDefinition 27.2 (Poisson process) A sequence of arrivals in continuous time is a Poisson process with rate \\(\\lambda\\) per unit of time if\n\nThe number of arrivals in an interval of length \\(t\\) is distributed \\(\\text{Pois}(\\lambda t)\\);\nThe numbers of arrivals in disjoint time intervals are independent.",
    "crumbs": [
      "Discrete Distributions",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Poisson distribution</span>"
    ]
  },
  {
    "objectID": "chapters/26_birthday2.html",
    "href": "chapters/26_birthday2.html",
    "title": "28  Birthday problem revisited",
    "section": "",
    "text": "The beauty if approximating discrete problems by continuous function is that it makes calculation easier. Now we revisit the birthday problem with Poisson distribution.\n\nExample 28.1 If we have \\(m\\) people and \\(\\binom{m}{2}\\) pairs. Each pair of people has probability \\(p=1/365\\) of having the same birthday. Find the probability of at least one match.\n\n\nSolution. The probability of match is small, and the number of pairs is large. We consider using the Poisson paradigm to approximate the number \\(X\\) of birthday matches. \\(X\\approx Pois(\\lambda)\\) where \\(\\lambda=\\binom{m}{2}\\frac{1}{365}\\). Then the probability of at least one match is \\[P(X\\geq1)=1-P(X=0)\\approx1-e^{-\\lambda}.\\] For \\(m=23\\), \\(\\lambda=253/365\\) and \\(1-e^{-\\lambda}\\approx0.5\\), which agrees with our previous finding that we need 23 people to have 50% chance of a birthday match.\n\n\nExample 28.2 Continued with the assumption above. What’s the probability of two people who were born not only on the same day, but also at the same hour and the same minute?\n\n\nSolution. This is the birthday problem with \\(c=365\\cdot24\\cdot60=525600\\) categories rather than \\(365\\) categories. By Poisson approximation, the probability of at least one match is approximately \\(1-e^{-\\lambda_{1}}\\) where \\(\\lambda_{1}=\\binom{m}{2}\\frac{1}{525600}\\). This would require \\(m=854\\) to reach the break even point, 50% chance of getting a match.\n\nYou may wonder how good the Poisson approximation is. We can compare it with the true values.\n\n# compute the probability of coincidences for 1,2...100 people\nn &lt;- 1:100\np &lt;- sapply(n, pbirthday)\n\n# approximate the probability by Poisson paradigm\nlambda &lt;- choose(n, 2)/365\nq &lt;- 1 - exp(-lambda)\n\n# black line is the true probability\n# red line is the Poisson approximation\nplot(n, p, type = \"s\")\nlines(n, q, col = 2, type=\"s\")",
    "crumbs": [
      "Discrete Distributions",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Birthday problem revisited</span>"
    ]
  },
  {
    "objectID": "chapters/27_convolution.html",
    "href": "chapters/27_convolution.html",
    "title": "29  Convolution",
    "section": "",
    "text": "A convolution is a sum of independent random variables. The main task in this section is to determine the distribution of \\(T = X + Y\\), where \\(X\\) and \\(Y\\) are independent random variables whose distributions are known.\n\nTheorem 29.1 (Convolution) If \\(X\\) and \\(Y\\) are independent discrete random variables, then the PMF of their sum \\(T=X+Y\\) is \\[\\begin{aligned}P(T=t)\n&=\\sum_x P(Y=t-x)P(X=x)\\\\\n&=\\sum_y P(X=t-y)P(Y=y)\n\\end{aligned}.\\] If \\(X\\) and \\(Y\\) are independent continuous random variables, then the PDF of their sum \\(T=X+Y\\) is \\[\\begin{aligned}\nf_T(t) &= \\int_{-\\infty}^{\\infty} f_Y(t-x)f_X(x) dx  \\\\\n&=  \\int_{-\\infty}^{\\infty} f_X(t-y)f_Y(y) dy.\n\\end{aligned}\\]\n\n\nTheorem 29.2 (Sum of Binomial random variables) Let \\(X\\sim \\text{Bin}(n,p)\\) and \\(Y\\sim \\text{Bin}(m,p)\\) be two independent Binomial random variables. Then \\(X+Y\\sim \\text{Bin}(n+m,p)\\).\n\n\nProof. We have proved the theorem in Theorem 19.1. Here is another way to prove it using convolution.\n\\[\\begin{aligned}\nP(X+Y=k) & =\\sum_{i=0}^k P(X=i)P(Y=k-i)\\\\\n& =\\sum_{i=0}^k\\binom{n}{i}p^{i}(1-p)^{n-i}\\binom{m}{k-i}p^{k-i}(1-p)^{m-k+i}\\\\\n& =\\sum_{i=0}^k\\binom{n}{i}\\binom{m}{k-i}p^{k}(1-p)^{m+n-k}\\\\\n& =p^{k}(1-p)^{m+n-k}\\sum_{i=0}^{k}\\binom{n}{i}\\binom{m}{k-i}\\\\\n& =p^{k}(1-p)^{m+n-k}\\binom{n+m}{k}.\\end{aligned}\\]\nThe last step: \\(\\binom{n+m}{k}=\\sum_{i=0}^{k}\\binom{n}{i}\\binom{m}{k-i}\\)\nis known as the Vandermonde’s identity.\n\n\nExample 29.1 (Sum of Poisson random variables) If \\(X\\sim \\text{Pois}(\\lambda_{1})\\), \\(Y\\sim \\text{Pois}(\\lambda_{2})\\), and \\(X,Y\\) are independent, then \\(X+Y\\sim \\text{Pois}(\\lambda_{1}+\\lambda_{2})\\).\n\n\nProof. Intuitively, \\(X\\) is the number of events occurring at rate \\(\\lambda_1\\); \\(Y\\) is the number of events occurring at rate \\(\\lambda_2\\). Therefore, \\(X+Y\\) should be events occurring at rate \\(\\lambda_1+\\lambda_2\\).\nTo get the PMF of \\(X+Y\\), condition on \\(X\\) and use the law of total probability: \\[\\begin{aligned}\nP(X+Y=k)\n& =\\sum_{j=0}^{k}P(Y=k-j)P(X=j)\\\\\n& =\\sum_{j=0}^{k}\\frac{e^{-\\lambda_{2}}\\lambda_{2}^{k-j}}{(k-j)!}\\cdot\\frac{e^{-\\lambda_{1}}\\lambda_{1}^{j}}{j!}\\\\\n& =\\frac{e^{-(\\lambda_{1}+\\lambda_{2})}}{k!}\\sum_{j=0}^{k}\\binom{k}{j}\\lambda_{1}^{j}\\lambda_{2}^{k-j}\\\\\n& =\\frac{e^{-(\\lambda_{1}+\\lambda_{2})}}{k!}(\\lambda_{1}+\\lambda_{2})^{k}.\\end{aligned}\\] We thus arrive at the PMF for \\(\\text{Pois}(\\lambda_{1}+\\lambda_{2})\\). Intuitively, if there are two different types of events occurring at rates \\(\\lambda_{1}\\) and \\(\\lambda_{2}\\), independently, then the overall event rate is \\(\\lambda_{1}+\\lambda_{2}\\).",
    "crumbs": [
      "Discrete Distributions",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Convolution</span>"
    ]
  },
  {
    "objectID": "chapters/19_dice.html",
    "href": "chapters/19_dice.html",
    "title": "30  Dice rolling formula*",
    "section": "",
    "text": "The Binomial distribution gives the formula for the probability of observing \\(k\\) heads when flipping \\(n\\) coins. Can we find a formula for the probability of getting a total of \\(p\\) points when rolling \\(n\\) dice?\nThe probability of obtaining \\(p\\) points on \\(n\\) \\(s\\)-sided dice can be computed as the coefficient of \\(x^p\\) in\n\\[f(x)=(x+x^2+...+x^s)^n\\]\nsince each possible arrangement contributes one term.\n\\[f(x) = x^n(1+x+\\dots+x^{s-1})^n = x^n\\left(\\frac{1-x^s}{1-x}\\right)^n\\]\nTo obtain the coefficient of \\(x^p\\), expand the binomial power:\n\\[x^n(1-x^s)^n(1-x)^{-n} = x^n \\sum_{k=0}^n (-1)^k \\binom{n}{k} x^{sk}\n\\sum_{l=0}^\\infty\\binom{n+l-1}{l}x^l\\]\nThe coefficient of \\(x^p\\) include all terms with \\(p=n+sk+l\\). Therefore,\n\\[c_p = \\sum_{k=0}^n (-1)^k \\binom{n}{k}\\binom{p-sk-1}{p-sk-n}\\]\nBut \\(p-sk-n&gt;0\\) only when \\(k&lt;(p-n)/s\\), so the other terms do not contribute. Furthermore, applying the symmetric property of the binomial formula, we have\n\\[\\binom{p-sk-1}{p-sk-n}= \\binom{p-sk-1}{n-1}\\]\nTherefore, the probability of getting \\(p\\) points when rolling \\(n\\) \\(s\\)-sided dice is given by\n\\[f(p,n,s) = \\sum_{k=0}^{\\lfloor{(p-n)/s}\\rfloor} (-1)^k \\binom{n}{k} \\binom{p-sk-1}{n-1}.\\]\n\n\n\n\n\n\nBinomial formula for negative \\(n\\)\n\n\n\n\\[\\begin{aligned}\n\\binom{-n}{k} &= \\prod_{i=0}^{k-1} \\frac{-n-i}{k-i}\n=(-1)^k \\prod_{i=0}^{k-1} \\frac{n+i}{k-i} \\\\\n&=(-1)^k\\frac{n(n+1)\\dots(n+k-1)}{k!}\\\\\n&=(-1)^k\\frac{(n+k-1)!}{k!(n-1)!}\\\\\n&=(-1)^k\\binom{n+k-1}{k}\n\\end{aligned}\\]\n\n\nWe can verify our formula by simulating the dice rolling game.\n\nset.seed(0)\n\n# simulates rolling n dice and returns the sum\nroll_dice &lt;- function(n, s=6) {\n  sum(sample(seq(1,s), n, replace = T))\n}\n\n# rolling 10 dice 1000 times and collect the results\npoints &lt;- replicate(1000, roll_dice(10))\n\n# distribution of the sum of points\nhist(points, freq = F)\n\n\n\n\n\n\n\n\n\n# formula for computing probability of dice points\ndice_formula &lt;- function(p, n, s=6) {\n  prob &lt;- 1/s^n*sum(\n    sapply(seq(0, floor((p-n)/s)), \n           function(k) (-1)^k*choose(n,k)*choose(p-s*k-1,n-1)))\n}\n\n# computing the probability of getting 20-50 when rolling 10 dice\nx &lt;- 20:50; \ny &lt;- sapply(x, function(p) dice_formula(p,n=10))\n\n# overlay the formula on the histogram\n# it turns out the formula does a nice job!\nhist(points, ylim = c(0, 0.07), freq = F)\nlines(x, y, col = 2, lwd=2)",
    "crumbs": [
      "Discrete Distributions",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Dice rolling formula\\*</span>"
    ]
  },
  {
    "objectID": "chapters/27_ratings.html",
    "href": "chapters/27_ratings.html",
    "title": "31  Application: seller ratings*",
    "section": "",
    "text": "This example involves multiple types of discrete distributions. The technique used to solve this problem aligns with Bayesian inference, which is beyond the scope of this course. However, it remains an interesting case. The procedure illustrates the process of statistical modeling: we begin with an assumption and a proposed statistical model, then update it with new data. Finally, we draw inferences based on the model, typically addressing the question we aim to answer. You are not required to understand everything in this example. Nonetheless, it helps to develop a mindset of statistical inference early in the study.\nSuppose you are shopping a product online. There are three sellers with the following ratings:\n\nSeller 1: 100% positive out of 10 reviews\nSeller 2: 96% positive out of 50 reviews\nSeller 3: 93% positive out of 200 reviews\n\nWhich seller is likely to give the best service?\nThe problem is intriguing because it is obvious that higher ratings do not necessarily means higher satisfaction. We have to weight in the number of reviews. The more reviews, the more trustworthy the ratings are. Let \\(X_{j}^{(i)}\\) be a random variable that means consumer \\(j\\) is satisfied with seller \\(i\\), where \\(i\\in\\left\\{ 1,2,3\\right\\}\\). Assume \\(X_{j}^{(i)}\\) follows a Bernoulli distribution:\n\\[X_{j}^{(i)}=\\begin{cases}\n1 & \\textrm{satisfied with probability } \\theta_{i}\\\\\n0 & \\textrm{otherwise}\n\\end{cases}\\]\nwhere \\(\\theta_{i}\\) is an unknown parameter of seller \\(i\\) that captures their “genuine” satisfaction rate. We assume the consumers independently write their ratings. The overall positive rate of seller \\(i\\) is therefore \\(R_{i}=\\frac{1}{n_{i}}\\sum_{j}X_{j}^{(i)}\\) where \\(n_{i}\\) is the total number of reviews. We want to infer the value of \\(\\theta_{i}\\) from their observed positive rate \\(R_{i}\\). From now on we drop the seller index \\(i\\) to simply the notation since it is symmetric for all sellers.\nBecause we have no prior knowledge about \\(\\theta\\). We assume that \\(\\theta\\) takes any value from \\([0,1]\\) equally likely, i.e. \\(\\theta\\sim\\textrm{Unif}(0,1)\\). Assuming each \\(X_{j}\\) is independent and identical, then\n\\[S=X_{1}+X_{2}+\\dots+X_{n}\\]\nfollows the Binomial distribution with PMF:\n\\[p(k|\\theta)=\\binom{n}{k}\\theta^{k}(1-\\theta)^{n-k}\\]\nOur goal is to find: \\(p(\\theta|k)\\). Recall that the Bayes’ rule allows us to invert the conditional probability:\n\\[\\begin{aligned}p(\\theta|k) & =\\frac{p(k|\\theta)p(\\theta)}{p(k)}=\\frac{p(k|\\theta)p(\\theta)}{\\int_{-\\infty}^{\\infty}p(k|\\theta)p(\\theta)d\\theta}\\\\[1em]\\end{aligned}\\]\nSince \\(\\theta\\sim\\textrm{Unif}(0,1)\\), we have \\[p(\\theta)=\\begin{cases}\n1 & \\textrm{if }\\theta\\in[0,1]\\\\\n0 & \\textrm{otherwise}\n\\end{cases}\\]\nWe now focus on \\(\\theta\\in[0,1]\\), since the probability is \\(0\\) otherwise. Substitute in the PMF of the Binomial distribution,\n\\[p(\\theta|k)=\\frac{\\binom{n}{k}\\theta^{k}(1-\\theta)^{n-k}}{\\int_{0}^{1}\\binom{n}{k}\\theta^{k}(1-\\theta)^{n-k}d\\theta}\\]\nThe hard part is to evaluate the integral. We state without proof (this is known as the Beta function, which we will prove in later chapters):\n\\[\\int_{0}^{1}\\theta^{k}(1-\\theta)^{n-k}=\\frac{k!(n-k)!}{(n+1)!}\\]\nTherefore,\n\\[p(\\theta|k)=\\frac{(n+1)!}{k!(n-k)!}\\theta^{k}(1-\\theta)^{n-k}\\]\nNow suppose you are the next customer. The probability that you would be satisfied is\n\\[\\begin{aligned}P(X_{n+1}=1|S=k)= & \\int_{0}^{1}P(x_{n+1}=1|\\theta)p(\\theta|k)d\\theta\\\\\n= & \\int_{0}^{1}\\theta\\times\\frac{(n+1)!}{k!(n-k)!}\\theta^{k}(1-\\theta)^{n-k}d\\theta\\\\\n= & \\frac{(n+1)!}{k!(n-k)!}\\int_{0}^{1}\\theta^{k+1}(1-\\theta)^{(n+1)-(k+1)}d\\theta\\\\\n= & \\frac{(n+1)!}{k!(n-k)!}\\times\\frac{(k+1)!(n-k)!}{(n+2)!}\\\\\n= & \\frac{k+1}{n+2}.\n\\end{aligned}\\]\nNow we substitute the ratings for the three sellers:\n\nSeller 1: \\(n=10,k=10\\)\nSeller 2: \\(n=50,k=48\\)\nSeller 3: \\(n=200,k=186\\)\n\nThe probabilities that you would be satisfied with each seller are: 92%, 94%, 93%. The result is known as the Laplace’s rule of succession. The rule of thumb is, pretending we have too more reviews: one is positive, the other is negative. Compute the satisfaction rate as \\(\\frac{k+1}{n+2}\\).",
    "crumbs": [
      "Discrete Distributions",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Application: seller ratings\\*</span>"
    ]
  },
  {
    "objectID": "chapters/28_exp.html",
    "href": "chapters/28_exp.html",
    "title": "32  Expectation revisited",
    "section": "",
    "text": "Definition 32.1 For discrete random variable \\(X\\), the expectation of \\(X\\) is defined as \\[E(X)=\\sum_{\\textrm{all }x}xP(X=x);\\]For continuous random variable \\(X\\) with density function \\(f(x)\\), the expectation is defined as \\[E(X)=\\int_{-\\infty}^{\\infty}x\\ f(x)\\ dx.\\]\n\n\nProposition 32.1 (Linearity) For random variables \\(X_1,X_2,\\dots,X_n\\), regardless of their dependencies, it holds that\n\\[E(X_{1}+\\cdots+X_{n})=E(X_{1})+\\cdots+E(X_{n}).\\]\n\n\nProof. We prove the simplest case \\(E(X+Y)=E(X)+E(Y)\\). \\[\\begin{aligned}\nE(X+Y) & =\\sum_{z=x+y}zP(X+Y=z)\\\\\n& =\\sum_{x}\\sum_{y}(x+y)P(X=x,Y=y)\\\\\n& =\\sum_{x}\\sum_{y}xP(X=x,Y=y)+\\sum_{x}\\sum_{y}yP(X=x,Y=y)\\\\\n& =\\sum_{x}x\\sum_{y}P(X=x,Y=y)+\\sum_{y}y\\sum_{x}P(X=x,Y=y)\\\\\n& =\\sum_{x}xP((X=x)\\cap\\bigcup_{\\textrm{all }y}(Y=y))+\\sum_{y}yP(\\bigcup_{\\textrm{all }x}(X=x)\\cap(Y=y))\\\\\n& =\\sum_{x}xP(X=x)+\\sum_{y}yP(Y=y)\\\\\n& =E(X)+E(Y).\n\\end{aligned}\\]\n\n\nProposition 32.2 Further properties on the linearity of expectations:\n\nIf \\(Y=aX+b\\), then \\(E(Y)=aE(X)+b\\).\n\\(E(a_{1}X_{1}+\\cdots+a_{n}X_{n}+b)=a_{1}E(X_{1})+\\cdots+a_{n}E(X_{n})+b\\)\n\n\n\nProposition 32.3 (Multiplication) If \\(X\\) and \\(Y\\) are independent, we have \\[E(XY)=E(X)E(Y).\\] In general, if \\(X_{1},\\ldots,X_{n}\\) are independent, we have \\[E(X_{1}X_{2}\\cdots X_{n})=E(X_{1})E(X_{2})\\cdots E(X_{n}).\\]\n\n\nProof. For discrete and independent \\(X,Y\\), \\[\\begin{aligned}\nE(XY) & =\\sum_{x}\\sum_{y}xyP(X=x,Y=y)\\\\\n& =\\sum_{x}\\sum_{y}xyP(X=x)P(Y=y)\\quad\\textrm{if independent}\\\\\n& =\\sum_{x}xP(X=x)\\sum_{y}yP(Y=y)\\\\\n& =E(X)E(Y).\\end{aligned}\\]\n\n\n\n\n\n\n\nMultiplication does not hold without independence\n\n\n\nIt is misleadingly natural to extend the generality of the addition rule to multiplication. But the multiplication rule of expectation is very restrictive. Always remember to check independence before applying the multiplication rule.\n\n\n\n\n\n\n\n\nSufficient but not necessary condition\n\n\n\nIf \\(X,Y\\) are independent, it follows that \\(E(XY)=E(X)E(Y)\\). However, the latter does not imply independence. Consider a counter-example, \\[X=\\begin{cases}\n    1 & \\textrm{with prob. }1/2\\\\\n    0 & \\textrm{with prob. }1/2\n    \\end{cases},\\quad Z=\\begin{cases}\n    1 & \\textrm{with prob. }1/2\\\\\n    -1 & \\textrm{with prob. }1/2\n    \\end{cases};\\] Then \\[Y=XZ=\\begin{cases}\n    -1 & \\textrm{with prob. }1/4\\\\\n    0 & \\textrm{with prob. }1/2\\\\\n    1 & \\textrm{with prob. }1/4\n    \\end{cases}.\\] We have \\(E(X)=1/2\\), \\(E(Y)=0\\), \\(E(XY)=0\\). So \\(E(XY)=E(X)E(Y)\\). But clearly \\(X,Y\\) are not independent.\n\n\n\nProposition 32.4 (Law of total expectation) Let \\(\\{A_i\\}\\) be a finite (or countable) partition of the sample space, then \\[E(X) = \\sum_i E(X|A_i)P(A_i).\\]\n\n\nTheorem 32.1 (Law of the unconscious statistician (LOTUS)) Let \\(X\\) be a random variable, and \\(g\\) be a real-valued function of a real variable. If \\(X\\) has a discrete distribution, then \\[E[g(X)]=\\sum_{\\textrm{all }x}g(x)P(X=x).\\]\n\nLOTUS says we can compute the expectation of \\(g(X)\\) without knowing the PMF of \\(g(X)\\).\n\nExample 32.1 Compute \\(E(X)\\) and \\(E(X^{2})\\) given the following distribution.\n\n\n\n\\(X\\)\n0\n1\n2\n\n\n\\(X^{2}\\)\n0\n1\n4\n\n\n\\(P\\)\n1/4\n1/2\n1/4\n\n\n\n\n\nSolution. According to the distribution table, we compute the expectations as \\[\\begin{aligned}\nE(X) & =0\\times1/4+1\\times1/2+2\\times1/4=1;\\\\\nE(X^{2}) & =0\\times1/4+1\\times1/2+4\\times1/4=3/2.\\end{aligned}\\] Note that \\(E(X^{2})\\neq[E(X)]^{2}\\).\n\n\n\n\n\n\n\nDon’t pull non-linear functions out of expectation\n\n\n\nIn general, \\(E[g(X)]\\neq g(E(X))\\). Linearity implies \\(E[g(X)]=g(E(X))\\) if \\(g\\) is a linear function. For a nonlinear function \\(g\\), you can’t pull function \\(g\\) out of expectation \\(E\\). The right way to find \\(E[g(X)]\\) is with LOTUS.\n\n\n\nExample 32.2 (St. Petersburg Paradox) Flip a fair coin over and over again until the head lands the first time. You will win \\(2^{k}\\) dollars if the head lands in the \\(k\\)-th trial (including the successful trial). What is the expected payoff of this game?\n\n\nSolution. Let \\(X=2^{k}\\). We want to find \\(E(X)\\). The probability of the first head showing up in the \\(k\\)-th trial is \\(\\frac{1}{2^{k}}\\). Therefore, \\[E(X)=\\sum_{k=1}^{\\infty}2^{k}\\cdot\\frac{1}{2^{k}}=\\sum_{k=1}^{\\infty}1=\\infty\\]\nThe expected payoff is infinitely high! This is against most people’s intuition, which is likely to be a small number. This is because we mistakenly go through the calculation \\(E(X)=E(2^{k})=2^{E(k)}\\) in our mind. \\(E(k)\\) the expected number of flips before a head is 2. Thus, \\(2^{E(k)}=4\\).\nAnother way to resolve the paradox is that we don’t typically reason about infinity. No one would play this game infinitely many times. For finite number of plays, the probability of getting very large payoff, say \\(2^{100}\\), is none. We can demonstrate this with a simulation.\n\n\n# Function to simulate the St. Petersburg Paradox\nst_petersburg_game &lt;- function(n_simulations) {\n  # Initialize a vector to store the outcomes\n  outcomes &lt;- numeric(n_simulations)\n  \n  for (i in 1:n_simulations) {\n    # Start with the initial reward\n    reward &lt;- 2\n    # Flip a coin until it lands tails\n    while (runif(1) &lt; 0.5) {\n      reward &lt;- reward * 2\n    }\n    # Store the reward for this simulation\n    outcomes[i] &lt;- reward\n  }\n  \n  # Return the outcomes\n  return(outcomes)\n}\n\n# Set the number of simulations\nn_simulations &lt;- 1000\n\n# Run the simulation\nresults &lt;- st_petersburg_game(n_simulations)\n\n# Calculate the average outcome (expected value)\nexpected_value &lt;- mean(results)\n\n# Print the results\ncat(\"Simulated Expected Value:\", expected_value)\n\nSimulated Expected Value: 12.644\n\n# However, as the number of simulations increases\n# We would see higher and higher maximum reward\ncat(\"Maximum Reward Observed:\", max(results))\n\nMaximum Reward Observed: 2048",
    "crumbs": [
      "Expectation and Variance",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Expectation revisited</span>"
    ]
  },
  {
    "objectID": "chapters/29_lifexp.html",
    "href": "chapters/29_lifexp.html",
    "title": "33  Life expectancy",
    "section": "",
    "text": "Life expectancy is the average number of years a person is expected to live. It is a crucial indicator of the quality of living and one of the three components of the Human Development Index (HDI) (the other two components are education and per capita GDP). Here is a toy example to compute life expectancy with hypothetical data.1\nTo simplify our analysis, we will assume there are only five possible ages: 0, 20, 40, 60, and 80. A baby is born at age 0, and can either die at that age or survive to age 20. We intentionally exclude intermediate ages such as 5 and 10 for the sake of computational simplicity.\nIt’s important to note that life expectancy is not the same as the average age of the population. For instance, based on the hypothetical data presented, the average age can be calculated as: \\[\\overline{\\textrm{Age}}=(0\\times200+20\\times300+40\\times250+60\\times150+80\\times100)/1000=33.\\]\nHowever, the expected age, denoted as \\(E(\\textrm{Age})\\), is defined as: \\[E(\\textrm{Age})=\\sum\\textrm{Age}\\times P(\\textrm{Age}).\\]\nTo compute this expected value, we need to determine \\(P(\\textrm{Age})\\), the probability of living to a specific age or dying at that age. This requires consideration of the mortality rate at each age, which is given in Column 3.\nAssuming 1000 babies are born at age 0, with a mortality rate of 1% at that age, we find that 99% of the babies survive to age 20. Thus, the number of babies that survive to age 20 is: \\(1000\\times(1-1\\%)=990\\). We can apply similar calculations to determine the number of survivors at each subsequent age.\nThe number of individuals who die at a specific age (Column 5) is the difference between the number of survivors at that age and the next (Column 4). To find the probability of living to a specific age, we compute: \\(P(\\textrm{Age})=\\textrm{Column 4}/1000\\).\nFinally, we compute the expected value of age (or life expectancy) as follows: \\[E(Age)=0\\times1\\%+20\\times2\\%+40\\times10\\%+60\\times17\\%+80\\times70\\%=70.6.\\] This figure differs from the average age. Since the mortality rate is low at younger ages, the probabilities \\(P(\\textrm{Age})\\) for these ages are also low, while they are higher for older ages. This example illustrates the distinction between average and expected values. In everyday conversation, we may use these terms interchangeably, but in certain contexts, expected values can significantly differ from averages.",
    "crumbs": [
      "Expectation and Variance",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Life expectancy</span>"
    ]
  },
  {
    "objectID": "chapters/29_lifexp.html#footnotes",
    "href": "chapters/29_lifexp.html#footnotes",
    "title": "33  Life expectancy",
    "section": "",
    "text": "This is an overly simplified example that only serves to clarify the definition of expectation. See this tutorial from MEASURE Evaluation for the actual computation of life expectancy.↩︎",
    "crumbs": [
      "Expectation and Variance",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Life expectancy</span>"
    ]
  },
  {
    "objectID": "chapters/30_twoenvp.html",
    "href": "chapters/30_twoenvp.html",
    "title": "34  Two envelope paradox",
    "section": "",
    "text": "Example 34.1 (Two-envelope paradox) Imagine you are given two identical envelopes, each containing money. One contains twice as much as the other. You may pick one envelope and keep the money it contains. Having chosen an envelope at will, but before inspecting it, you are given the chance to switch envelopes. Should you switch?\n\nThe paradox arises when you try to solve the expectation. Let \\(A\\) denote the amount of money in the envelope you have chosen, and \\(B\\) denote the amount of money in the other envelope.\nWe know \\(B\\) is either twice as much as \\(A\\), or half as much as \\(A\\). Each with probability \\(1/2\\). So\n\\[E(B) = \\frac{1}{2}(2A) + \\frac{1}{2}(A/2) = \\frac{5}{4}A\\]\nSince \\(E(B)&gt;A\\), you should always switch! However, after you switch to \\(B\\), by the same argument, you should switch back to \\(A\\). You you switch back and forth indefinitely!\nWhere do things go wrong? The error in this calculation lies in a subtle misunderstanding: the two \\(A\\)s in the calculation actually represent different values, that are incorrectly equated. In particular, the \\(2A\\) represents the expected value in the other envelope given that it is the larger one, and the \\(A/2\\) represents the expected value in the other envelope given that it is the smaller one.\n\\[E(B) = E(B|B&lt;A)P(B&lt;A) + E(B|B&gt;A)P(B&gt;A)\\]\nSuppose the amount of money in the two envelopes are \\(a\\) and \\(2a\\) respectively. \\(E(B|B&lt;A)=a\\) and \\(E(B|B&gt;A)=2a\\). Therefore,\n\\[E(B)=\\frac{1}{2}a + \\frac{1}{2}2a = \\frac{3}{2}a.\\]\nThe same calculation applies to \\(E(A)\\). Thus, \\(E(A)=E(B)\\).\n\nExample 34.2 (HH vs HT) Flip a coin indefinite times. Let \\(X\\) denote the number of flips until you see HH. Let \\(Y\\) denote the number of flips until you see HT. Find \\(E(X)\\) and \\(E(Y)\\).\n\nIt is tempting to think they are the same, since either H or T happens with probability 1/2. But the answer is extremely counter-intuitive: \\(E(X)&gt;E(Y)\\)!\nHH case. Let \\(E_0\\) = E(X|No H observed), and \\(E_1\\) = E(X|One H observed). Then \\[E_0 = 1 + \\frac{1}{2}E_1 + \\frac{1}{2}E_0\\]\nThe first term is we need to flip once. If the first flip is H, the additional expected number of flips is \\(E_1\\). If the first flip is T, we have to start over again (\\(E_0\\)).\n\\[E_1 = 1 + \\frac{1}{2}(0) + \\frac{1}{2}E_0\\]\nOnce we have observed an H, we do another flip. If it is another H, we are done. If it is a T, we have to start over again (\\(E_0\\)).\nSolve the two equations, we have \\(E_0=6\\), \\(E_1=4\\). Thus, \\(E(X)=6\\).\nHT case. Let \\(E_0\\) = E(Y|No H observed), and \\(E_1\\) = E(Y|One H observed). Then\n\\[E_0 = 1 + \\frac{1}{2}E_1 + \\frac{1}{2}E_0\\]\nIf the first flip is H, we need \\(E_1\\). If the first flip is T, we have wasted the flip, so it is \\(E_0\\) again.\n\\[E_1 = 1 + \\frac{1}{2}(0) + \\frac{1}{2}E_1\\]\nIf we have a T by 1/2 chance, we are done (the first term). If it is an H, we get another \\(E_1\\).\nIn this case, we have \\(E_0=4\\), \\(E_1=2\\). Thus, \\(E(Y)=4\\).\n\n# Function to simulate the number of flips until a specific pattern appears\nsimulate_pattern &lt;- function(pattern, n_simulations) {\n  results &lt;- numeric(n_simulations)\n  \n  for (i in 1:n_simulations) {\n    flips &lt;- character(0)  # Initialize an empty vector to store flips\n    count &lt;- 0  # Initialize the flip counter\n    \n    while (TRUE) {\n      # Simulate a coin flip (H or T)\n      flip &lt;- sample(c(\"H\", \"T\"), 1)\n      flips &lt;- c(flips, flip)  # Append the flip to the sequence\n      count &lt;- count + 1  # Increment the flip counter\n      \n      # Check if the last two flips match the pattern\n      if (length(flips) &gt;= 2 && all(tail(flips, 2) == pattern)) {\n        results[i] &lt;- count  # Record the number of flips\n        break  # Exit the loop\n      }\n    }\n  }\n  \n  return(results)\n}\n\n# Set the number of simulations\nn_simulations &lt;- 2000\n\n# Simulate for HH\nresults_HH &lt;- simulate_pattern(c(\"H\", \"H\"), n_simulations)\nexpected_HH &lt;- mean(results_HH)\n\n# Simulate for HT\nresults_HT &lt;- simulate_pattern(c(\"H\", \"T\"), n_simulations)\nexpected_HT &lt;- mean(results_HT)\n\n# Create a table of results\nresults_table &lt;- data.frame(\n  Pattern = c(\"HH\", \"HT\"),\n  Expected_Flips = c(expected_HH, expected_HT)\n)\n\n# Print the table\nknitr::kable(results_table)\n\n\n\n\nPattern\nExpected_Flips\n\n\n\n\nHH\n6.0105\n\n\nHT\n3.9650",
    "crumbs": [
      "Expectation and Variance",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Two envelope paradox</span>"
    ]
  },
  {
    "objectID": "chapters/31_linearity.html",
    "href": "chapters/31_linearity.html",
    "title": "35  Linearity and indicators",
    "section": "",
    "text": "Definition 35.1 (Indicator variable) An indicator variable \\(\\mathbb{I}_A\\) for an event \\(A\\) is a random variable defined as: \\[\\mathbb{I}_A =\n\\begin{cases}\n1 & \\text{if event } A \\text{ occurs}, \\\\\n0 & \\text{if event } A \\text{ does not occur}.\n\\end{cases}\n\\]\nThe indicator variable \\(\\mathbb{I}_A\\) “indicates” whether the event \\(A\\) happens (1) or not (0).\n\nThe expected value of an indicator variable is equal to the probability of the event \\(A\\): \\[E[\\mathbb{I}_A] = 1 \\cdot P(A) + 0 \\cdot P(A^c) = P(A)\\] This is known as the fundamental bridge, as it allows us to convert between probability and expectation.\nIndicator variables are often used in linearity of expectation calculations. This allows us to break down a problem into easy-to-solve small problems. For example, if \\(X = \\sum_{i=1}^n \\mathbb{I}_{A_i}\\), then: \\[E[X] = \\sum_{i=1}^n E[\\mathbb{I}_{A_i}] = \\sum_{i=1}^n P(A_i)\\]\n\nExample 35.1 In a group of \\(n\\) people, what is the expected number of distinct birthdays among the \\(n\\) people (the expected number of days on which at least one of the people was born)? What is the expected number of people sharing a birthday (any day)?\n\n\nSolution. Let \\(X\\) be the number of distinct birthdays, and write \\(X=I_{1}+\\cdots+I_{365}\\), where \\[I_{j}=\\begin{cases}\n1 & \\textrm{if someone was born on day }j\\\\\n0 & \\textrm{otherwise}\n\\end{cases}.\\] Then \\[\\begin{aligned}\nE(I_{j}) & =P(\\textrm{someone was born on day }j)\\\\\n& =1-P(\\textrm{no one was born on day }j)\\\\\n& =1-\\left(\\frac{364}{365}\\right)^{n}.\\end{aligned}\\] Then by linearity, \\[E(X)=365\\left(1-\\left(\\frac{364}{365}\\right)^{n}\\right).\\] Let \\(Y\\) be the number of people sharing a birthday, and \\(Y=J_{1}+\\cdots+J_{n}\\) where \\(J_{k}\\) is an indicator that the \\(j\\)-th person shares his birthday with somebody else. \\[\\begin{aligned}\nE(J_{k}) & =P(\\textrm{someone shares birthday with }k)\\\\\n& =1-P(\\textrm{no one shares birthday with }k)\\\\\n& =1-\\left(\\frac{364}{365}\\right)^{n-1}.\\end{aligned}\\] Therefore, \\[E(Y)=\\sum_{k=1}^{n}E(J_{k})=n\\left(1-\\left(\\frac{364}{365}\\right)^{n-1}\\right).\\]\nFor some numeric values, \\(E(Y)=2.3\\) if \\(n=30\\); \\(E(Y)=6.3\\) if \\(n=50\\).\n\n\nExample 35.2 Suppose that there are \\(n\\) people sitting in a classroom with exactly \\(n\\) seats. At some point, everyone got up, ran around the room, and sat back down randomly (i.e., all seating arrangements are equally likely). What is the expected value of the number of people sitting in their original seat?\n\n\nSolution. Number the people from 1 to \\(n\\). Let \\(X_{i}\\) be the Bernoulli random variable with value \\(1\\) if person \\(i\\) returns to their original seat and value \\(0\\) otherwise. Since person \\(i\\) is equally likely to sit back down in any of the \\(n\\) seats, the probability that person \\(i\\) returns to their original seat is \\(1/n\\). Therefore \\(E[X_{i}]=1/n\\). Now, let \\(X\\) be the number of people sitting in their original seat following the rearrangement. Then \\(X=X_{1}+X_{2}+\\cdots+X_{n}\\). By linearity of expected values, we have \\(E[X]=\\sum E[X_{i}]=\\sum1/n=1.\\)\n\n\nExample 35.3 Let \\(\\Pi\\) be a permutation over \\(\\{1,2,\\dots,n\\}\\). That is a reordering of the numbers. A fixed point of a permutation are the points not moved by the permutation. For example, in the permutation below \\[\\begin{array}{ccccc}\n& 1 & 2 & 3 & 4\\\\\n\\Pi & 2 & 4 & 3 & 1\n\\end{array}\\]\nThe fixed point is 3. Find the expected number of fixed points of a random permutation.\n\n\nSolution. Let \\(X\\) be the number of fixed points of a random permutation. Then \\(X=\\sum_{k=1}^{n}\\boldsymbol{1}_{\\Pi(k)=k}\\) where \\(\\boldsymbol{1}_{\\Pi(k)=k}\\) indicates the \\(k\\)-th number stays the same after the permutation. By linearity, \\[E(X)=E\\left(\\sum_{k=1}^{n}\\boldsymbol{1}_{\\Pi(k)=k}\\right)=\\sum_{k=1}^{n}E\\left(\\boldsymbol{1}_{\\Pi(k)=k}\\right)=\\sum_{k=1}^{n}\\frac{1}{n}=1.\\]\n\n\nExample 35.4 (Buffon’s needle). Rule a surface with parallel lines a distance \\(d\\) apart. What is the probability that a randomly dropped needle of length \\(l\\leq d\\) crosses a line?\n\n\nSolution. Consider dropping any (continuous) curve of length \\(l\\) onto the surface. Imagine dividing up the curve into \\(N\\) straight line segments, each of length \\(\\frac{l}{N}\\). Let \\(X_{i}\\) be the indicator for the \\(i\\)-th segment crossing a line. Let \\(X\\) be the total number of times the curve crosses a line. Then, \\[E(X)=E(\\sum X_{i})=\\sum E(X_{i})=N\\cdot E(X_{i}).\\] There could be infinitely many segments. It is hard to compute this expectation directly. But here we arrive an important Lemma: the expected number of crossings is proportional to the length of the curve, regardless of the shape of the curve. If we can compute \\(E(X)\\) for some curve, the we can compute \\(E(X)\\) for any length by scaling the value proportional to the length.\nConsider a circle of diameter \\(d\\). The circle always crosses the lines twice for sure. That is, \\(E(X_{\\textrm{circle}})=2\\). The length of the circle is \\(\\pi d\\). Therefore, the value of \\(E(X)\\) for any curve of length \\(l\\) is given by \\[E(X)=\\frac{2l}{\\pi d}.\\]\nNow a needle can cross a line either \\(1\\) or \\(0\\) times. Thus, \\(E(X)=1\\cdot P(X=1)+0\\cdot P(X=0)\\) is exactly the probability of a needle crossing a line.\nThis amazing example can be used to approximate the value of \\(\\pi\\). Let \\(q\\) be the probability of a needle crossing a line. \\(q\\) can be approximated by large number of simulations. Then \\(\\pi\\approx\\frac{2l}{qd}\\).\n\n\n# Buffon's Needle Simulation\nbuffon_needle &lt;- function(N, L, D) {\n  # Initialize the number of crossings\n  crossings &lt;- 0\n  \n  for (i in 1:N) {\n    # Randomly generate the position of the needle's midpoint\n    y &lt;- runif(1, min = 0, max = D / 2)  # Distance from the nearest line\n    # Randomly generate the angle of the needle (in radians)\n    theta &lt;- runif(1, min = 0, max = pi / 2)  # Angle with respect to the lines\n    \n    # Check if the needle crosses a line\n    if (y &lt;= (L / 2) * sin(theta)) {\n      crossings &lt;- crossings + 1\n    }\n  }\n  \n  # Estimate pi\n  pi_estimate &lt;- (2 * L * N) / (D * crossings)\n  \n  return(pi_estimate)\n}\n\n# Parameters\nN &lt;- 100000  # Number of trials\nL &lt;- 1       # Length of the needle\nD &lt;- 2       # Distance between the lines (D &gt;= L)\n\n# Run the simulation\npi_estimate &lt;- buffon_needle(N, L, D)\n\n# Print the result\ncat(\"Estimated value of pi:\", pi_estimate)\n\nEstimated value of pi: 3.161555",
    "crumbs": [
      "Expectation and Variance",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Linearity and indicators</span>"
    ]
  },
  {
    "objectID": "chapters/32_median.html",
    "href": "chapters/32_median.html",
    "title": "36  Median and mode",
    "section": "",
    "text": "The mean is called a measure of central tendency because it tells us something about the center of a distribution, specifically its center of mass. Other measures of central tendency that are commonly used in statistics are the median and the mode, which we now define.\n\nDefinition 36.1 (Median) We say that \\(c\\) is a median of a random variable \\(X\\) if \\[P(X\\leq c)\\geq1/2 \\text{ and } P(X\\geq c)\\geq1/2.\\]\n\nIntuitively, the median is a value \\(c\\) such that half the mass of the distribution falls on either side of \\(c\\) (or as close to half as possible, for discrete random variables). Note that the condition given above is more general than \\[P(X\\leq c)=P(X\\geq c)=\\frac{1}{2}\\] Consider a discrete distribution as follows: \\[P(X=k)=\\begin{cases}\n\\frac{1}{3},\\quad k=1 \\\\\n\\frac{1}{2},\\quad k=2 \\\\\n\\frac{1}{6},\\quad k=3\n\\end{cases}\\] In this case, 2 is a median since \\(P(X\\leq 2)=5/6 \\geq 1/2\\) and \\(P(X\\geq 2)=2/3 \\geq 1/2\\). However, \\(P(X\\leq 2)\\neq P(X\\geq 2)\\). For strictly continuous random variable \\(X\\), Definition 36.1 does imply \\[P(X\\leq c)=P(X\\geq c)=\\frac{1}{2}\\] Since the CDF of \\(X\\) satisfies \\(F(c)\\geq1/2\\) and \\(1-F(c)\\geq 1/2\\), which implies \\(F(c)=1/2\\). Moreover, if the CDF of \\(X\\) is strictly increasing, \\(F^{-1}(1/2)\\) is the unique median.\n\nDefinition 36.2 (Mode) For a discrete random variable \\(X\\), we say that \\(c\\) is a mode of \\(X\\) if it maximizes the PMF: \\[P(X=c)\\geq P(X=x)\\quad\\text{ for all }x.\\] For a continuous random variable \\(X\\) with PDF \\(f\\), we say that \\(c\\) is a mode if it maximizes the PDF: \\[f(c)\\geq f(x)\\quad\\text{ for all }x.\\]\n\nIntuitively, the mode is a value that has the greatest mass or density out of all values in the support of \\(X\\).\n\n\n\n\n\n\nNote\n\n\n\nA distribution can have multiple medians and multiple modes. Medians have to occur side by side; modes can occur all over the distribution.\n\n\n\nExample 36.1 The main reason why the median is sometimes preferred over the mean is that the median is more robust to extreme values. A typical example is the income distribution. Higher incomes are rare, but their absolute values are high. Thus, the mean income tends be higher than what the mass of the population would earn. But the median is more robust to extreme values and is closer to the earnings of an “average” person. For example, the average monthly income in China is \\(\\yen 2,500\\) in 2019, but the median is only \\(\\yen 1,000\\).\n\n\n\n\n\n\n\nTheorem 36.1 Let \\(X\\) be an random variable with mean \\(\\mu\\) , and let \\(m\\) be a median of \\(X\\).\n\nA value of \\(c\\) that minimizes the mean squared error \\(E\\left(X-c\\right)^{2}\\) is \\(c=\\mu\\).\nA value of \\(c\\) that minimizes the mean absolute error \\(E\\left|X-c\\right|\\) is \\(c=m\\).\n\n\n\nProof. Part 1 - Minimizing the mean squared error \\(E[(X - c)^2]\\). Expand the mean squared error: \\[E[(X - c)^2] = E[X^2 - 2cX + c^2] = E[X^2] - 2cE[X] + c^2.\\]\nTo find the value of \\(c\\) that minimizes this expression, take the derivative with respect to \\(c\\) and set it to zero: \\[\\frac{d}{dc} E[(X - c)^2] = -2E[X] + 2c=0\\]\nThis implies \\(c=\\mu\\). We can confirm with second-order condition that \\(c = \\mu\\) is indeed a minimizer.\nPart 2 - Minimizing the mean absolute error \\(E\\left|X-c\\right|\\).\n\\[E|X-c|=\\int_{-\\infty}^{c}(c-x)f(x)dx+\\int_c^{\\infty}(x-c)f(x)dx\\]\nTake derivative with respect to \\(c\\), applying the Leibniz’s rule:\n\\[(c-x)f(x)\\frac{d}{dc}c+\\int_{-\\infty}^c f(x)dx -(x-c)f(x)\\frac{d}{dc}c+\\int_c^\\infty (-f(x))dx=0\\]\nThe first-order condition resolves to \\[\\int_{-\\infty}^c f(x) dx = \\int_c^{\\infty}f(x)dx\\] which is exactly the definition of a median.",
    "crumbs": [
      "Expectation and Variance",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Median and mode</span>"
    ]
  },
  {
    "objectID": "chapters/33_var.html",
    "href": "chapters/33_var.html",
    "title": "37  Variance",
    "section": "",
    "text": "Expectation is the most commonly used summary of a distribution, as it indicates where values are likely centered. However, it provides limited insight into the distribution’s overall shape. For example, two random variables might have the same mean, yet one could have values spread far from the mean while the other has values tightly clustered around it. Variance, on the other hand, describes how far values in a distribution typically deviate from the mean, offering a measure of the distribution’s dispersion.\n\nDefinition 37.1 The variance of a random variable \\(X\\) is defined as \\[Var(X)=E\\left[X-E(X)\\right]^{2}.\\] By convention, variance is also denoted by Greek letter \\(\\sigma^2\\), where \\(\\sigma = \\sqrt{Var(X)}\\) is called the standard deviation.\n\nVariance measures how far \\(X\\) typically deviates from its mean, but instead of averaging the differences, we average the squared differences to ensure both positive and negative deviations contribute. The expected deviation, \\(E(X-E(X))\\), is always zero, so squaring avoids this cancellation. Since variance is in squared units, we take the square root to get the standard deviation, restoring the original units.\n\n\n\n\n\n\nWhy squared deviation?\n\n\n\nWe can measure the dispersion of a distribution in different ways. For example, \\(E(|X-E(X)|)\\) is also a possible choice. But it is less common because the absolute value function isn’t differentiable. Besides, squaring connects to geometric concepts like the distance formula and Pythagorean theorem, which have useful statistical meanings.\n\n\n\n\n\n\n\n\nSample variance\n\n\n\nDefinition 37.1 gives the theoretical variance of a distribution. With finite sample from the distribution, we estimate the variance with sample observations: \\[\\hat\\sigma^2=\\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar x)^2\\] Why dividing by \\(n-1\\)? Because we want an unbiased estimator. We will discuss this in later chapters.\n\n\n\nTheorem 37.1 For any random variable \\(X\\), \\[Var(X)=E(X^{2})-(EX)^{2}.\\]\n\n\nProof. Let \\(\\mu=E(X)\\). By definition, \\[\\begin{aligned}\nVar(X) & =E(X-\\mu)^{2}=E(X^{2}-2\\mu X+\\mu^{2})\\\\\n& =E(X^{2})-2\\mu E(X)+\\mu^{2}=E(X^{2})-\\mu^{2}.\\end{aligned}\\]\n\n\nExample 37.1 Find the variance for \\(X\\sim\\textrm{Bern}(p).\\) \\[Var(X)=E(X^{2})-E^{2}(X)=p-p^{2}=p(1-p).\\]\n\n\nProposition 37.1 Variance has the following properties:\n\n\\(Var(X)\\geq0\\)\n\\(Var(X+c)=Var(X)\\)\n\\(Var(cX)=c^{2}Var(X)\\)\nIf \\(X,Y\\) are independent, \\(Var(X+Y)=Var(X)+Var(Y)\\).\nIf \\(X_{1},X_{2},\\dots,X_{n}\\) are independent, \\({\\displaystyle Var(\\sum_{i=1}^{n}X_{i})=\\sum_{i=1}^{n}Var(X_{i})}\\).\n\n\n\nExample 37.2 (Variance of Binomial distribution) Find the variance for \\(X\\sim\\textrm{Bin}(n,p).\\) \\(X=X_{1}+\\cdots+X_{n}\\) where \\(X_{i}\\) are \\(i.i.d\\) Bernoulli distributions \\[Var(X)\\overset{iid}{=}\\sum_{i=1}^{n}Var(X_{i})=np(1-p).\\]\n\n\nExample 37.3 (Variance of Poisson distribution) Let \\(X\\sim\\text{Pois}(\\lambda)\\). To find the variance, we first compute \\(E(X^{2})\\). By LOTUS,\n\\[\\begin{aligned}E(X^{2})= & \\sum_{k=0}^{\\infty}k^{2}\\cdot\\frac{e^{-\\lambda}\\lambda^{k}}{k!}=e^{-\\lambda}\\sum_{k=1}^{\\infty}k^{2}\\frac{\\lambda^{k}}{k!}\\end{aligned}\\]\nDifferentiate \\(\\sum_{k=0}^{\\infty}\\frac{\\lambda^{k}}{k!}=e^{\\lambda}\\) on both sides with respect to \\(\\lambda\\) and multiply (replenish) again by \\(\\lambda\\):\n\\[\\sum_{k-1}^{\\infty}k\\frac{\\lambda^{k}}{k!}=\\lambda e^{\\lambda}\\]\nRepeat:\n\\[\\sum_{k-1}^{\\infty}k^{2}\\frac{\\lambda^{k}}{k!}=\\lambda(e^{\\lambda}+\\lambda e^{\\lambda})\\]\nTherefore, we have\n\\[E(X^{2})=e^{-\\lambda}(\\lambda+\\lambda^{2})e^{\\lambda}=\\lambda+\\lambda^{2}\\]\nFinally,\n\\[Var(X)=E(X^{2})-(E(X))^{2}=\\lambda+\\lambda^{2}-\\lambda^{2}=\\lambda.\\]",
    "crumbs": [
      "Expectation and Variance",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Variance</span>"
    ]
  },
  {
    "objectID": "chapters/34_cov.html",
    "href": "chapters/34_cov.html",
    "title": "38  Covariance",
    "section": "",
    "text": "For more than one random variable, it is also of interest to know the relationship between them. Are they dependent? How strong is the dependence? Covariance and correlation are intended to measure that dependence. But they only capture a particular type of dependence, namely linear dependence.\n\nDefinition 38.1 The covariance between random variables \\(X\\) and \\(Y\\) is defined as \\[Cov(X,Y)=E[(X-EX)(Y-EY)].\\]\n\nThe covariance between \\(X\\) and \\(Y\\) reflects how much \\(X\\) and \\(Y\\) simultaneously deviate from their respective means.\n\nIf \\(X&gt;EX\\) & \\(Y&gt;EY\\) or \\(X&lt;EX\\) & \\(Y&lt;EY\\), then \\(Cov(X,Y)\\) is positive.\nIf \\(X&gt;EX\\) & \\(Y&lt;EY\\) or \\(X&lt;EX\\) & \\(Y&gt;EY\\), then \\(Cov(X,Y)\\) is negative.\n\n\nTheorem 38.1 For any random variables \\(X\\) and \\(Y\\), \\[Cov(X,Y)=E(XY)-E(X)E(Y).\\]\n\n\nProof. Let \\(\\mu_{X}=E(X)\\) and \\(\\mu_{Y}=E(Y)\\). By definition, \\[\\begin{aligned}\nCov(X,Y) & =E(XY-\\mu_{X}Y-\\mu_{Y}X+\\mu_{X}\\mu_{Y})\\\\\n& =E(XY)-\\mu_{X}E(Y)-\\mu_{Y}E(X)+\\mu_{X}\\mu_{Y}\\\\\n& =E(XY)-E(X)E(Y).\\end{aligned}\\]\n\n\nTheorem 38.2 If \\(X,Y\\) are independent, they are uncorrelated. But the converse is false.\n\n\nProof. \\(Cov(X,Y)=E(XY)-E(X)E(Y)\\). Independence implies \\(E(XY)=E(X)E(Y)\\). Thus, \\(Cov(X,Y)=0\\). But \\(Cov(X,Y)=0\\) does not necessarily imply independence. Consider the following counter example. Let \\(X\\) be a random variable that takes three values -1, 0, 1 with equal probability. And \\(Y=X^{2}\\). \\(X\\) and \\(Y\\) are clearly dependent. But they their correlation is 0. Since \\(E(X)=0\\), \\(E(Y)=2/3\\), \\(E(XY)=E(X^{3})=0\\), \\(Cov(X,Y)=0\\).\n\n\n\n\n\n\n\nLinear dependency\n\n\n\nCovariance and correlation provide measures of the extend to which two random variables are linearly related. It is possible that the covariance is \\(0\\) even when \\(X\\) and \\(Y\\) are dependent but the relationship is nonlinear.\n\n\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Number of observations\nn &lt;- 100\n\n# Linear relationship\nX1 &lt;- rnorm(n)\nY1 &lt;- 2 * X1 + rnorm(n, sd = 0.5)\n\n# Nonlinear relationship\nX2 &lt;- rnorm(n)\nY2 &lt;- X2^2 + rnorm(n, sd = 0.5)\n\n# Independent variables\nX3 &lt;- rnorm(n)\nY3 &lt;- rnorm(n)\n\n# Visualize the relationship\n# Set up a 1x3 grid for plots\npar(mfrow = c(1, 3))\n\n# Plot 1: Linear Relationship\nplot(X1, Y1, main = \"Linear Relationship\", pch = 16)\nabline(lm(Y1 ~ X1), col = \"red\", lwd = 2)\n\n# Plot 2: Nonlinear Relationship\nplot(X2, Y2, main = \"Nonlinear Relationship\", pch = 16)\ncurve(x^2, add = TRUE, col = \"red\", lwd = 2)\n\n# Plot 3: Independent Variables\nplot(X3, Y3, main = \"Independent Variables\", pch = 16)\nabline(h = mean(Y3), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n# Compute covariances\ncov_linear &lt;- cov(X1, Y1)\ncov_nonlinear &lt;- cov(X2, Y2)\ncov_independent &lt;- cov(X3, Y3)\n\n# Print the results\nknitr::kable(\n  data.frame(\n    Relationship = c(\"Linear\", \"Nonlinear\", \"Independent\"),\n    Covariance = c(cov_linear, cov_nonlinear, cov_independent)\n  ), digits = 3)\n\n\n\n\nRelationship\nCovariance\n\n\n\n\nLinear\n1.645\n\n\nNonlinear\n0.473\n\n\nIndependent\n0.192\n\n\n\n\n\n\nProposition 38.1 Covariance has the following properties:\n\n\\(Cov(X,X)=Var(X)\\)\n\\(Cov(X,Y)=Cov(Y,X)\\)\n\\(Cov(cX,Y)=Cov(X,cY)=c\\left[Cov(X,Y)\\right]\\)\n\\(Cov(X+Y,Z)=Cov(X,Z)+Cov(Y,Z)\\)\n\\(Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)\\)\n\n\n\nProof. We only prove the variance-covariance property: \\[\\begin{aligned}\nVar(X+Y) & =E[(X+Y-\\mu_{X}-\\mu_{Y})^{2}]\\\\\n& =E[(X-\\mu_{X})^{2}+(Y-\\mu_{Y})^{2}+2(X-\\mu_{X})(Y-\\mu_{Y})]\\\\\n& =Var(X)+Var(Y)+2Cov(X,Y).\\end{aligned}\\]\n\n\nTheorem 38.3 For random variables \\(X_1,X_2,\\dots,X_n\\), it holds that \\[Var\\left(\\sum_{i=1}^{n}X_{i}\\right)=\\sum_{i=1}^{n}Var(X_{i})+\n2\\sum_{i&lt;j}Cov(X_{i},X_{j}).\\]\nIf \\(X_1,X_2,\\dots,X_n\\) are identically distributed and have the same covariance relationships (symmetric), then \\[Var\\left(\\sum_{i=1}^{n}X_{i}\\right)=nVar(X_1)+2\\binom{n}{2}Cov(X_1,X_2).\\]\n\nWhile \\(\\text{Cov}(X,Y)\\) quantifies how \\(X\\) and \\(Y\\) vary together, its magnitude also depends on the absolute scales of \\(X\\) and \\(Y\\) (multiply \\(X\\) by a constant \\(c\\), the covariance will be different). To establish a measure of association between \\(X\\) and \\(Y\\) that is unaffected by arbitrary changes in the scales of either variable, we introduce a “standardized covariance” called correlation.\n\nDefinition 38.2 The correlation between random variables \\(X\\) and \\(Y\\) is defined as \\[Corr(X,Y)=\\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}.\\]\n\nWe also denote correlation by \\(\\rho\\equiv Corr(X,Y)\\).\nUnlike covariance, scaling \\(X\\) or \\(Y\\) has no effect on the correlation. We can verify this: \\[Corr(cX,Y)=\\frac{Cov(cX,Y)}{\\sqrt{Var(cX)Var(Y)}}=\\frac{cCov(X,Y)}{c\\sqrt{Var(X)Var(Y)}}=Corr(X,Y).\\]\n\nTheorem 38.4 For any random variable \\(X\\) and \\(Y\\), \\[-1\\leq Corr(X,Y)\\leq1.\\]\n\n\nProof. Without loss of generality, assume \\(X,Y\\) both have variance 1, since scaling does not change the correlation. Let \\(\\rho=Corr(X,Y)=Cov(X,Y)\\). Then \\[\\begin{aligned}\nVar(X+Y) & =Var(X)+Var(Y)+2Cov(X,Y)=2+2\\rho\\geq0,\\\\\nVar(X-Y) & =Var(X)+Var(Y)-2Cov(X,Y)=2-2\\rho\\ge0.\\end{aligned}\\] Thus \\(-1\\leq\\rho\\leq1\\).\n\n\n\\(X\\) and \\(Y\\) are positively correlated if \\(\\rho_{XY}&gt;0\\);\n\\(X\\) and \\(Y\\) are negatively correlated if \\(\\rho_{XY}&lt;0\\);\n\\(X\\) and \\(Y\\) are uncorrelated if \\(\\rho_{XY}=0\\).\n\n\nTheorem 38.5 Suppose that \\(X\\) is a random variable and \\(Y=aX+b\\) for some constants \\(a,b\\), where \\(a\\neq0\\). If \\(a&gt;0\\), then \\(\\rho_{XY}=1\\). If \\(a&lt;0\\), then \\(\\rho_{XY}=-1\\).\n\n\nProof. If \\(Y=aX+b\\), then \\(E(Y)=aE(X)+b\\). Thus, \\(Y-E(Y)=a(X-E(X))\\). Therefore, \\[Cov(X,Y)=aE[(X-EX)^{2}]=aVar(X).\\] Since \\(Var(Y)=a^{2}Var(X)\\), \\(\\rho_{XY}=\\frac{a}{|a|}\\). The theorem thus follows.\n\n\n\n\n\n\n\nCorrelation analysis\n\n\n\nCorrelation analysis is a very commonly used method in statistics to measure the strength of the linear relationship between two variables and compute their association. Simply put, correlation analysis tells us how one variable changes with other variables.\n\n\n\n# Select variables for covariance analysis\nvariables &lt;- mtcars[, c(\"mpg\", \"disp\", \"hp\", \"wt\")]\n\n# Compute the covariance matrix\ncov_matrix &lt;- cov(variables)\n\n# Compute the correlation matrix\ncor_matrix &lt;- cor(variables)\n\n# Plot the correlation pairs\nGGally::ggpairs(variables)\n\n\n\n\n\n\n\n\n\nExample 38.1 Let \\(X\\sim \\text{HGeom}(w,b,n)\\). Find \\(Var(X)\\).\n\n\nSolution. Interpret \\(X\\) as the number of white balls in a sample of size \\(n\\) from an box with \\(w\\) white and \\(b\\) black balls. We can represent \\(X\\) as the sum of indicator variables, \\(X=I_{1}+\\cdots+I_{n}\\) , where \\(I_{j}\\) is the indicator of the \\(j\\)-th ball in the sample being white. Each \\(I_{j}\\) has mean \\(p=w/(w+b)\\) and variance \\(p(1-p)\\), but because the \\(I_{j}\\) are dependent, we cannot simply add their variances. Instead, \\[\\begin{aligned}\nVar(X) & =Var\\left(\\sum_{j=1}^{n}I_{j}\\right)\\\\\n& =Var(I_{1})+\\cdots+Var(I_{n})+2\\sum_{i&lt;j}Cov(I_{i},I_{j})\\\\\n& =np(1-p)+2\\binom{n}{2}Cov(I_{i},I_{j})\\end{aligned}\\]\nIn the last step, because of symmetry, for every pair \\(i\\) and \\(j\\), \\(Cov(I_{i},I_{j})\\) are the same. \\[\\begin{aligned}\nCov(I_{i},I_{j}) & =E(I_{i}I_{j})-E(I_{i})E(I_{j})\\\\\n& =P(i\\textrm{ and }j\\textrm{ both white})-P(i\\textrm{ is white})P(j\\textrm{ is white})\\\\\n& =\\frac{w}{w+b}\\cdot\\frac{w-1}{w+b-1}-p^{2}\\\\\n& =p\\frac{Np-1}{N-1}-p^{2}\\\\\n& =\\frac{p(p-1)}{N-1}\\end{aligned}\\]\nwhere \\(N=w+b\\). Plugging this into the above formula and simplifying, we eventually obtain \\[Var(X)=np(1-p)+n(n-1)\\frac{p(p-1)}{N-1}=\\frac{N-n}{N-1}np(1-p).\\] This differs from the Binomial variance of \\(np(1-p)\\) by a factor of \\(\\frac{N-n}{N-1}\\). This discrepancy arises because the Hypergeometric story involves sampling without replacement. As \\(N\\to\\infty\\), it becomes extremely unlikely that we would draw the same ball more than once, so sampling with or without replacement essentially become the same.\n\n\nExample 38.2 (PG exam). Put \\(k\\) balls into \\(n\\) boxes. Let \\(X\\) be the number of empty boxes. Find \\(E(X)\\) and \\(Var(X)\\).\n\n\nSolution. Define an indicator variable \\[I_{j}=\\begin{cases}\n1 & j\\textrm{-th box is empty}\\\\\n0 & \\textrm{otherwise}\n\\end{cases}\\]\nThen \\(X=\\sum_{j=1}^{n}I_{j}\\). Unconditionally, the probability of one box being empty is \\(\\left(\\frac{n-1}{n}\\right)^{k}\\). Therefore, \\[E(I_{j})=P(j\\textrm{-th box is empty})=\\left(\\frac{n-1}{n}\\right)^{k}\\] for \\(j=1,2,\\dots,n\\). It follows that \\[E(X)=\\sum_{j=1}^{n}I_{j}=nE(I_{j})=n\\left(\\frac{n-1}{n}\\right)^{k}.\\]\nTo compute the variance, \\[\\begin{aligned}\nVar(X) & =Var(I_{1}+\\cdots+I_{n})=\\sum_{j=1}^{n}Var(I_{j})+2\\sum_{i&lt;j}Cov(I_{i},I_{j})\\\\\n& =nVar(I_{j})+2\\binom{n}{2}Cov(I_{i},I_{j}),\\end{aligned}\\]\nsince by symmetry, \\(Var(I_{j})\\) is the same for all \\(j\\) and \\(Cov(I_{i},I_{j})\\) is the same for all \\(i\\neq j\\). It suffices to compute \\(Var(I_{j})\\) and \\(Cov(I_{i},I_{j})\\) for any \\(j\\) and \\(i\\neq j\\). Since \\(I_{j}\\) only takes number 0 and 1, \\[E(I_{j}^{2})=\\left(\\frac{n-1}{n}\\right)^{k},\\] \\[\\begin{aligned}\nVar(I_{j})=E(I_{j}^{2})-(E(I_{j}))^{2} & =\\left(\\frac{n-1}{n}\\right)^{k}-\\left(\\frac{n-1}{n}\\right)^{2k}.\\end{aligned}\\]\nFor the covariance term, \\[E(I_{i}I_{j})=P(i,j \\textrm{ are both empty})=\\left(\\frac{n-2}{n}\\right)^{k},\\] \\[Cov(I_{i},I_{j})=E(I_{i}I_{j})-E(I_{i})E(I_{j})=\\left(\\frac{n-2}{n}\\right)^{k}-\\left(\\frac{n-1}{n}\\right)^{2k}.\\]\nTherefore, \\[Var(X)=n\\left[\\left(\\frac{n-1}{n}\\right)^{k}-\\left(\\frac{n-1}{n}\\right)^{2k}\\right]+2\\binom{n}{2}\\left[\\left(\\frac{n-2}{n}\\right)^{k}-\\left(\\frac{n-1}{n}\\right)^{2k}\\right].\\]",
    "crumbs": [
      "Expectation and Variance",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Covariance</span>"
    ]
  },
  {
    "objectID": "chapters/34_portfolio.html",
    "href": "chapters/34_portfolio.html",
    "title": "39  Portfolio allocation*",
    "section": "",
    "text": "In the world of finance, one of the most well-established principles is the idea of diversification. By combining assets with varying levels of risk and return, investors can reduce the overall risk of their portfolio. One of the most powerful tools for achieving this is correlation, a statistical measure that quantifies how two assets move in relation to each other. Understanding and applying the concept of correlation can be the key to constructing portfolios that not only aim for higher returns but also minimize risk. This section demonstrates how a lower correlation between assets can significantly reduce portfolio risk while maintaining the same expected return.\nThe Role of Correlation in Portfolio Risk\nA core principle in portfolio management is that the overall portfolio risk (or volatility) is not simply the weighted average of the individual asset risks. Instead, it also depends on how the assets in the portfolio are correlated with each other. When assets have lower correlation, the overall risk of the portfolio is reduced. This occurs because the assets will not all move in the same direction at the same time, thus smoothing out large fluctuations in the portfolio value.\nTo quantify this relationship, we use a formula that calculates the portfolio risk:\n\\[\\sigma_P =\n\\sqrt{w_A^2 \\sigma_A^2 + w_B^2 \\sigma_B^2 + 2w_A w_B \\sigma_A \\sigma_B \\rho_{AB}}\\]\nWhere: \\(\\sigma_P\\) is the overall portfolio risk. \\(w_A\\) and \\(w_B\\) are the weights of the assets in the portfolio. \\(\\sigma_A\\) and \\(\\sigma_B\\) are the individual standard deviations (risks) of assets A and B. \\(\\rho_{AB}\\) is the correlation between the two assets.\nConsider a portfolio of two assets, Asset A and Asset B. Both assets have the same expected return of 10% and individual risks (standard deviations) of 15%. The key difference between the two cases is the correlation between the assets.\nCase 1: High Correlation\nLet’s assume that the correlation between the two assets is 0.8, indicating that the two assets tend to move in the same direction. In this scenario, we will calculate the portfolio risk when we allocate 50% of the portfolio to Asset A and 50% to Asset B.\n\\[\\sigma_P =\n\\sqrt{(0.5)^2 (0.15)^2 + (0.5)^2 (0.15)^2 + 2(0.5)(0.5)(0.15)(0.15)(0.8)}\\approx 20.12\\%\\]\nCase 2: Low Correlation\nNow, let’s assume that the correlation between the two assets is 0.2, indicating a much weaker relationship between the two assets. Again, we allocate 50% to Asset A and 50% to Asset B. Using the same formula, we can compute the portfolio risk:\n\\[\\sigma_P =\n\\sqrt{(0.5)^2 (0.15)^2 + (0.5)^2 (0.15)^2 + 2(0.5)(0.5)(0.15)(0.15)(0.2)} \\approx 16.1\\%\\]\nIn this example, by reducing the correlation between the two assets, we reduced the portfolio risk by approximately 4%, even though the expected return remained the same (since the return of both assets was identical).\nThe principle of diversification\nDiversification is often referred to as a “free lunch” in finance because it allows investors to reduce portfolio risk without sacrificing expected returns. The reason lower correlation reduces risk is that when assets are highly correlated, they tend to move together, amplifying the portfolio’s overall volatility. In contrast, when assets are less correlated, they do not move in lockstep, and the portfolio experiences less overall volatility because the negative fluctuations of one asset can be offset by the positive fluctuations of the other. This is a key advantage of diversification: it allows you to spread risk across assets that behave differently.",
    "crumbs": [
      "Expectation and Variance",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Portfolio allocation\\*</span>"
    ]
  },
  {
    "objectID": "chapters/35_condexp.html",
    "href": "chapters/35_condexp.html",
    "title": "40  Conditional expectation",
    "section": "",
    "text": "We have introduced conditional expectation in Definition 26.2. Here we reiterate the definition with continuous random variables.\n\nDefinition 40.1 (Conditional expectation) Let \\(X\\) and \\(Y\\) be continuous random variables with joint density \\(f_{X,Y}(x,y)\\), \\(X\\)’s density \\(f_X(x)\\), and conditional density \\(f_{Y|X}(y|x)=\\frac{f_{X,Y}(x,y)}{f_X(x)}\\). The conditional expectation of \\(Y\\) given \\(X=x\\) is \\[\\begin{aligned}\nE(Y|X=x) &= \\int_{-\\infty}^{\\infty}y\\ f_{Y|X}(y|x)dy \\\\\n&= \\int_{-\\infty}^{\\infty}y\\ \\frac{f_{X,Y}(x,y)}{f_X(x)} dy\n\\end{aligned}\\] When the denominator is zero, the expression is undefined.\n\nNote that conditioning on a continuous random variable is not the same as conditioning on the event \\(\\{X=x\\}\\) as it was in the discrete case. The probability of the event is zero, but we define the conditional expectation in terms of the density function.\n\nTheorem 40.1 (Law of iterated expectation) For any random variable \\(X\\) and \\(Y\\), it holds that \\[E(E(Y|X))=E(Y).\\]\n\n\nProof. Note that \\(E(Y|X)=g(X)\\) is a function of \\(X\\). Apply LOTUS: \\[\\begin{aligned}\nE(E(Y|X)) & =\\int g(x)f(x)dx\\\\\n& =\\int\\left(\\int yf(y|x)dy\\right)f(x)dx\\\\\n& =\\int\\int yf(y|x)f(x)dydx\\\\\n& =\\int y\\int f(y,x)dx\\,dy\\\\\n& =\\int_{-\\infty}^{\\infty}yf(y)dy\\\\\n& =E(Y).\\end{aligned}\\]\n\n\nTheorem 40.2 For any random variable \\(X\\) and \\(Y\\), and any function \\(g\\), we have \\[E(g(X)Y|X)=g(X)E(Y|X).\\]\n\n\nProof. For any specific value of \\(X=x\\), \\(g(x)\\) is a constant. Thus, \\(E(g(x)Y|X=x)=g(x)E(Y|X=x)\\). This is true for all values of \\(x\\).\n\n\nTheorem 40.3 (Best predictor) Conditional expectation \\(E(Y|X)\\) is the best predictor for \\(Y\\) using \\(X\\) (minimized the square loss function).\n\n\nProof. Let \\(g(X)\\) be a predictor for \\(Y\\) using \\(X\\). We want to find the \\(g\\) such that minimizes \\(E(Y-g(X))^{2}\\). \\[\\begin{aligned}\nE(Y-g(X))^{2} & =E(Y-E(Y|X)+E(Y|X)-g(X))^{2}\\\\\n& =E(Y-E(Y|X))^{2}+2\\underbrace{E(Y-E(Y|X)}_{E(Y)=E(E(Y|X))}((E(Y|X)-g(X))\\\\ &\\quad+E(E(Y|X)-g(X))^{2}\\\\\n& =E(Y-E(Y|X))^{2}+E(E(Y|X)-g(X))^{2}\\\\\n& \\geq E(Y-E(Y|X))^{2}.\\end{aligned}\\] Therefore, \\(E(Y-g(X))^{2}\\) is minimized when \\(g(X)=E(Y|X)\\).\n\n\nDefinition 40.2 (Linear conditional expectation model) An extremely widely used method for data analysis in statistics is linear regression. In its most basic form, we want to predict the mean of \\(Y\\) using a single explanatory variable \\(X\\). A linear conditional expectation model assumes that \\(E(Y|X)\\) is linear in \\(X\\): \\[E(Y|X)=a+bX,\\] or equivalently, \\[Y=a+bX+\\epsilon,\\] with \\(E(\\epsilon|X)=0\\). The intercept and the slope is given by \\[b=\\frac{Cov(X,Y)}{Var(X)},a=E(Y)-bE(X).\\]\n\nWe first show the equivalence of the two expressions of the model. Let \\(Y=a+bX+\\epsilon\\), with \\(E(\\epsilon|X)=0\\). Then by linearity, \\[E(Y|X)=E(a|X)+E(bX|X)+E(\\epsilon|X)=a+bX.\\] Conversely, suppose that \\(E(Y|X)=a+bX\\), and define \\[\\epsilon=Y-(a+bX).\\] Then \\(Y=a+bX+\\epsilon\\), with \\[E(\\epsilon|X)=E(Y|X)-E(a+bX|X)=E(Y|X)-(a+bX)=0.\\] To derive the expression for \\(a\\) and \\(b\\), take covariance between \\(X\\) and \\(Y\\), \\[\\begin{aligned}\nCov(X,Y) & =Cov(X,a+bX+\\epsilon)\\\\\n& =Cov(X,a)+bCov(X,X)+Cov(X,\\epsilon)\\\\\n& =bVar(X)+Cov(X,\\epsilon)\\end{aligned}\\] Note that \\(Cov(X,\\epsilon)=0\\) because \\[\\begin{aligned}\nCov(X,\\epsilon) & =E(X\\epsilon)-E(X)E(\\epsilon)\\\\\n& =E(E(X\\epsilon|X))-E(X)E(E(\\epsilon|X))\\\\\n& =E(XE(\\epsilon|X))-E(X)E(E(\\epsilon|X))\\\\\n& =0\\end{aligned}\\] Therefore, \\[Cov(X,Y)=bVar(X)\\] Thus, \\[\\begin{aligned}\nb & =\\frac{Cov(X,Y)}{Var(X)},\\\\\na & =E(Y)-bE(X)=E(Y)-\\frac{Cov(X,Y)}{Var(X)}E(X).\\end{aligned}\\]\nIn practice, we don’t know the true value of \\(Cov(X,Y)\\) or \\(Var(X)\\). We have to estimate it with sample observations. Thus, we compute \\(\\hat b=\\frac{\\sum_{i=1}^n (x_i-\\bar x)(y_i -\\bar y)}{\\sum_{i=1}^n (x_i - \\bar x)^2}\\). By definition, \\(b\\) gives the marginal change of \\(E(Y|X)\\) with respect to \\(X\\).\n\n\n\n\n\nLinear regression is the simple yet powerful modeling tool in statistics. It is useful whenever we want to predict one variable with another. When the assumptions are met (though this is rare), the model gives the conditional expectation — the best predictor. If the assumptions are not met, regression does not give the\n\n# Predict the miles/gallon of a car using its horsepower\n# Use the built-in dataset `mtcars`\ndata(mtcars)\n\n# Fit the linear regression model\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\n\n# Display the model summary\ngtsummary::tbl_regression(model)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\nhp\n-0.07\n-0.09, -0.05\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n# Scatter plot of the data\nplot(mtcars$hp, mtcars$mpg, \n     main = \"Simple Linear Regression: MPG vs. Horsepower\", \n     xlab = \"Horsepower (hp)\", \n     ylab = \"Miles per Gallon (mpg)\", \n     pch = 16)  # Use solid circles for points\n\n# Add the regression line\nabline(model, col = \"red\", lwd = 2)",
    "crumbs": [
      "Expectation and Variance",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Conditional expectation</span>"
    ]
  },
  {
    "objectID": "chapters/36_mgf.html",
    "href": "chapters/36_mgf.html",
    "title": "41  Moments and MGF",
    "section": "",
    "text": "Definition 41.1 Let \\(X\\) be a random variable with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\) . For any positive integer \\(n\\), the \\(n\\)-th moment of \\(X\\) is \\(E(X^{n})\\), the \\(n\\)-th central moment is \\(E(X-\\mu)^{n}\\), and the \\(n\\)-th standardized moment is \\(E\\left(\\frac{X-\\mu}{\\sigma}\\right)^{n}\\).\n\nIn accordance with this terminology, \\(E(X)\\) is the first moment of \\(X\\), \\(Var(X)\\) is the second central moment of \\(X\\). It is natural to ask if there are higher order moments. The answer is yes.\n\nDefinition 41.2 Let \\(X\\) be a random variable with mean \\(\\mu\\), standard deviation \\(\\sigma\\), and finite third moment. The skewness of \\(X\\) is defined as \\[\\textrm{Skew}(X)=E\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^{3}\\right].\\]\n\n\nDefinition 41.3 The Kurtosis of \\(X\\) is defined as \\[\\textrm{Kurt}(X)=\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^{4}\\right].\\]\n\nSkewness is the measure of the lopsidedness of the distribution; any symmetric distribution will have a third central moment, if defined, of zero. A distribution that is skewed to the left (the tail of the distribution is longer on the left) will have a negative skewness. A distribution that is skewed to the right (the tail of the distribution is longer on the right), will have a positive skewness.\nKurtosis is a measure of the heaviness of the tail of the distribution. If a distribution has heavy tails, the kurtosis will be high; conversely, light-tailed distributions have low kurtosis.\n\n\n\n\n\nWe see that moments give information about the shape of a distribution. Different orders of moments captures different aspects of the distribution. As higher and higher moments are calculated, they reveal more and more aspects of the distribution. Loosely speaking, it is somewhat like the Taylor theorem in the probability theory. We can approximate a distribution by “expectation of polynomials”: \\(E(X),E(X^{2}),E(X^{3}),\\ldots\\)\n\nDefinition 41.4 Let \\(X\\) be a random variable. For each real number \\(t\\), define the moment generating function (MGF) as \\[M_{X}(t)=E\\left(e^{tX}\\right).\\]\n\nTo see why it is “generating” moments, take the Taylor expansion of the exponential function: \\[e^{tX}=1+tX+\\frac{t^{2}X^{2}}{2!}+\\frac{t^{3}X^{3}}{3!}+\\cdots\\] Hence, \\[M_{X}(t)=E\\left(e^{tX}\\right)=1+E(X)t+E(X^{2})\\frac{t^{2}}{2!}+\\cdots\\]\nA natural question at this point is: What is the interpretation of \\(t\\)? The answer is that \\(t\\) has no interpretation in particular; it’s just a bookkeeping device that we introduce in order to encode the sequence of moments in a differentiable function.\n\nTheorem 41.1 Let \\(M_{X}(t)\\) be the MGF of \\(X\\). Then the \\(n\\)-th moment of \\(X\\) is given by \\(E(X^{n})=M_{X}^{(n)}(0)\\), where \\(M_{X}^{(n)}\\) denotes the \\(n\\)-th derivative of the MGF.\n\n\nTheorem 41.2 The MGF (if it exists) uniquely determines the distribution. That is, if two random variables have the same MGF, then they must have the same distribution.\n\n\nTheorem 41.3 If \\(X\\) and \\(Y\\) are independent, then the MGF of \\(X+Y\\) is the product of the individual MGFs: \\[M_{X+Y}(t)=M_{X}(t)M_{Y}(t).\\]\n\n\nExample 41.1 For \\(X\\sim Bern(p)\\), \\(e^{tX}\\) takes on the value \\(e^{t}\\) with probability \\(p\\) and the value \\(1\\) with probability \\(q\\), so \\(M(t)=E\\left(e^{tX}\\right)=pe^{t}+q\\). Since this is finite for all values of \\(t\\), the MGF is defined on the entire real line.\n\n\nExample 41.2 The MGF of a \\(Bin(n,p)\\) random variable is \\(M(t)=(pe^{t}+q)^{n}\\), since it is the product of \\(n\\) independent Bernoulli MGFs.",
    "crumbs": [
      "Expectation and Variance",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Moments and MGF</span>"
    ]
  },
  {
    "objectID": "chapters/37_ineq.html",
    "href": "chapters/37_ineq.html",
    "title": "42  Inequalities",
    "section": "",
    "text": "This section introduces some of the most popular inequality in statistics and general mathematics. Interestingly, our probability theories can shed light on these inequalities that are otherwise hard to explain. We don’t show formal proofs here, but just point out how these inequalities can be useful in statistics.\n\nTheorem 42.1 (Cauchy-Schwarz inequality). \\[\\left|\\sum x_{i}y_{i}\\right|\\leq\\sqrt{\\sum x_{i}^{2}}\\sqrt{\\sum y_{i}^{2}}\\]\n\n\nProof. If \\(X,Y\\) have zero means, their correlation can be written as\n\\[\\rho_{XY}=\\frac{E(XY)}{\\sqrt{E(X^{2})E(Y^{2})}}\\] Since \\(|\\rho_{XY}|\\leq1\\), we always have \\[|E(XY)|\\leq\\sqrt{E(X^{2})E(Y^{2})}.\\]\nConsider \\(\\{x_{i}\\}\\) and \\(\\{y_{i}\\}\\) as realizations of \\(X\\) and \\(Y\\) with equal probabilities, such that \\(E(X)=\\frac{1}{n}\\sum x_{i}\\). The original inequality is thus proved.\n\n\nTheorem 42.2 (Jensen’s inequality). For a convex function \\(f\\), we have \\[\\frac{1}{n}\\sum f(x_{i})\\geq f\\left(\\frac{1}{n}\\sum x_{i}\\right);\\] If \\(f\\) is concave, then \\[\\frac{1}{n}\\sum f(x_{i})\\leq f\\left(\\frac{1}{n}\\sum x_{i}\\right).\\]\n\n\nWe do not intend to prove it, but offer a special case in statistics that helps to understand Jensen’s inequality. Consider\n\\[Var(X)=E(X^{2})-(E(X))^{2}\\geq0\\]\nWe have\n\\[E(X^{2})\\geq(E(X))^{2}.\\]\nNote that \\(f(X)=X^{2}\\) is a convex function, and \\(E(*)=\\frac{1}{n}\\sum*\\), we have shown the first inequality. The concave case is the opposite.\nIn general, if \\(g\\) is a convex function, then \\(E(g(X))\\geq g(E(X))\\). If \\(g\\) is a concave function, then \\(E(g(X))\\leq g(E(X))\\). In both cases, the only way that equality can hold is if there are constants \\(a\\) and \\(b\\) such that \\(g(X)=a+bX\\) with probability 1.\n\n\nTheorem 42.3 (Markov inequality). Let \\(X\\) be a random variable, then \\[P(|X|\\geq a)\\leq\\frac{E|X|}{a}\\] That is, the probability of \\(|X|\\) deviating from its mean by a multiple of \\(a\\) must be less than \\(1/a\\).\n\n\nProof. Define a random variable \\[I_{|X|\\geq a}=\\begin{cases}\n1 & \\textrm{if }|X|\\geq a\\\\\n0 & \\textrm{if }|X|&lt;a\n\\end{cases}\\]\nNote that \\(P(|X|\\geq a)=E(I_{|X|\\geq a})\\). It always holds that\n\\[a\\cdot I_{|X|\\geq a}\\leq|X|\\]\nTherefore,\n\\[E\\left[a\\cdot I_{|X|\\geq a}\\right]\\leq E|X|\\]\nHence,\n\\[P(|X|\\geq a)\\leq\\frac{E|X|}{a}.\\]\n\nFor an intuitive interpretation, let \\(X\\) be the income of a randomly selected individual from a population. Taking \\(a=2E(X)\\), Markov’s inequality says that \\(P(X\\geq2E(X))\\leq1/2\\), i.e., it is impossible for more than half the population to make at least twice the average income. This is clearly true, since if over half the population were earning at least twice the average income, the average income would be higher. Similarly, \\(P(X\\geq3E(X))\\leq1/3\\): you can’t have more than \\(1/3\\) of the population making at least three times the average income, since those people would already drive the average above what it is.\n\nTheorem 42.4 (Chebyshev inequality). Let \\(X\\) be a random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then \\[P\\left(\\left|X-\\mu\\right|&gt; c\\sigma\\right)\\leq\\frac{1}{c^{2}}\\] That is, the probability of \\(X\\) deviating from its mean by \\(a\\) times the standard deviation must be less than \\(1/a^{2}\\).\n\n\nProof. We first show \\[P(|X-\\mu|&gt;a)\\leq\\frac{\\sigma^{2}}{a^{2}}\\] This is true by taking squares and applying the Markov inequality,\n\\[P(|X-\\mu|&gt;a)=P((X-\\mu)^{2}&gt;a^{2})\\leq\\frac{E(X-\\mu)^{2}}{a^{2}}=\\frac{\\sigma^{2}}{a^{2}}.\\]\nSubstitute \\(c\\sigma\\) for \\(a\\), we have the original inequality.\n\nThis gives us an upper bound on the probability of a random variable being more than \\(c\\) standard deviations away from its mean, e.g., there can’t be more than a 25% chance of being 2 or more standard deviations from the mean. Given the mean and standard deviation of a random variable \\(X\\), we know that \\(\\mu\\pm2\\sigma\\) captures 75% of its possible values; \\(\\mu\\pm3\\sigma\\) captures 90% of the possible values.",
    "crumbs": [
      "Expectation and Variance",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Inequalities</span>"
    ]
  },
  {
    "objectID": "chapters/40_contd.html",
    "href": "chapters/40_contd.html",
    "title": "43  Continuous vs Discrete",
    "section": "",
    "text": "Continuous random variables, in many ways, are more versatile and useful than discrete distributions. One key reason is that many quantities in the physical world, such as temperature, height, weight, and time, are inherently continuous in nature. These variables can take on any value within a range, providing a more accurate representation of real-world phenomena compared to discrete variables, which are limited to distinct values. Additionally, the probability density functions (PDFs) of continuous distributions are often defined by smooth, differentiable functions. This mathematical structure allows us to apply calculus for analysis, enabling precise calculations of probabilities, expected values, and other statistical measures. The ability to integrate and differentiate these functions not only simplifies manipulation but also makes continuous distributions a powerful tool for solving complex problems in physics, engineering, and data analysis.\n\nDefinition 43.1 A random variable has a continuous distribution if its CDF is differentiable. A continuous random variable is a random variable with a continuous distribution.\n\n\nDefinition 43.2 For a continuous random variable \\(X\\) with CDF \\(F\\), the probability density function (PDF) of \\(X\\) is the derivative of the CDF, given by \\(f(x)=F'(x)\\). The support of \\(X\\) is the set of all \\(x\\) where \\(f(x)&gt;0\\).\n\n\nRemark. By the fundamental theorem of calculus, we integrate a PDF to get the CDF: \\[F(x)=\\int_{-\\infty}^{x}f(t)dt.\\] PDF differs from the discrete PMF in important ways:\n\nFor a continuous random variable, \\(P(X=x)=0\\) for all \\(x\\);\nThe quantity \\(f(x)\\) is not a probability. To get the probability, we integrate the PDF (probability is the area under the PDF): \\[P(a&lt;X\\leq b)=F(b)-F(a)=\\int_{a}^{b}f(x)dx.\\]\nSince any single value has probability 0, including or excluding endpoints does not matter. \\[P(a&lt;X&lt;b)=P(a&lt;X\\leq b)=P(a\\leq X&lt;b)=P(a\\leq X\\leq b).\\]\n\n\n\nTheorem 43.1 The PDF \\(f\\) of a continuous random variable must satisfy the following criteria:\n\nNonnegative: \\(f(x)\\geq0\\);\nIntegrates to 1: \\(\\int_{-\\infty}^{\\infty}f(x)dx=1\\).\n\n\n\nDefinition 43.3 The expectation of a continuous random variable \\(X\\) with PDF \\(f\\) is \\[E(X)=\\int_{-\\infty}^{\\infty}xf(x)dx.\\]\n\n\nTheorem 43.2 If \\(X\\) is a continuous random variable with PDF \\(f\\) and \\(g:\\mathbb{R}\\to\\mathbb{R}\\). The LOTUS applies \\[E[g(X)]=\\int_{-\\infty}^{\\infty}g(x)f(x)dx.\\]\n\n\n\n\n\n\n\n\n\n\nDiscrete\nContinuous\n\n\n\n\nPMF/PDF\n\\(P(X=x)=p(x)\\)\n\\(P(a\\leq X\\leq b)=\\int_{a}^{b}f(x)dx\\)\n\n\nCDF\n\\(F(x)=P(X\\leq x)=\\sum_{k\\leq x}p(k)\\)\n\\(F(x)=P(X\\leq x)=\\int_{-\\infty}^{x}f(t)dt\\)\n\n\nExpectation\n\\(E(x)=\\sum_{x}xP(X=x)\\)\n\\(E(X)=\\int_{-\\infty}^{+\\infty}xf(x)dx\\)\n\n\nLOTUS\n\\(E[g(x)]=\\sum_{x}g(x)P(X=x)\\)\n\\(E[g(x)]=\\int_{-\\infty}^{+\\infty}g(x)f(x)dx\\)",
    "crumbs": [
      "Continuous Distributions",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Continuous vs Discrete</span>"
    ]
  },
  {
    "objectID": "chapters/41_unif.html",
    "href": "chapters/41_unif.html",
    "title": "44  Uniform distribution",
    "section": "",
    "text": "Definition 44.1 Let \\(a\\) and \\(b\\) be two given real numbers such that \\(a&lt;b\\). Let \\(X\\) be a random variable such that it is known that \\(a\\leq X\\leq b\\) and, for every subinterval of \\([a,b]\\), the probability that \\(X\\) will belong to that subinterval is proportional to the length of that subinterval. We then say that the random variable \\(X\\) has the Uniform distribution on the interval \\([a,b]\\). The PDF of \\(X\\) is \\[f(x)=\\begin{cases}\n\\frac{1}{b-a} & \\textrm{for }a\\leq x\\leq b\\\\\n0 & \\textrm{otherwise}\n\\end{cases}\\]\n\nThis is a valid PDF since \\[\\int_{-\\infty}^{+\\infty}f(x)dx=\\int_{a}^{b}\\frac{1}{b-a}dx=\\frac{1}{b-a}\\int_{a}^{b}dx=1.\\]\nThe CDF of \\(X\\) is\n\\[F(x)=\\int_{-\\infty}^{x}f(t)dt=\\int_{a}^{x}f(t)dt=\\begin{cases}\n0 & x&lt;a\\\\\n\\frac{x-a}{b-a} & a\\leq x\\leq b\\\\\n1 & x&gt;b\n\\end{cases}.\\]\nThe expectation of \\(X\\):\n\\[E(X)=\\int_{a}^{b}x\\frac{1}{b-a}dx=\\frac{1}{b-a}\\left[\\frac{x^{2}}{2}\\right]_{a}^{b}=\\frac{a+b}{2}.\\]\nTo figure out the variance, first compute\n\\[E(X^{2})=\\int_{a}^{b}x^{2}\\frac{1}{b-a}dx=\\frac{1}{b-a}\\left[\\frac{x^{3}}{3}\\right]_{a}^{b}=\\frac{a^{2}+ab+b^{2}}{3}\\]\nThus,\n\\[Var(X)=E(X^{2})-E^{2}(X)=\\frac{a^{2}+ab+b^{2}}{3}-\\frac{(a+b)^{2}}{4}=\\frac{(b-a)^{2}}{12}.\\]\n\nExample 44.1 A stick of unit length is broken at a random point X. What is the expected length of the longer piece?\n\n\nSolution. The lengths of the two pieces are \\(X\\) and \\(1-X\\), with \\(X\\sim Unif(0,1)\\). The longer piece is \\(\\max(X,1-X)\\). For \\(X&lt;0.5\\), the longer piece is \\(1-X\\), and for \\(X\\geq0.5\\), it is \\(X\\). The expected value is: \\[E[\\max(X,1-X)]=\\int_{0}^{0.5}(1-X)\\,dx+\\int_{0.5}^{1}X\\,dx=\\frac{3}{4}.\\]\n\nIntuition might suggest that since the stick is broken at a random point, the longer piece should be “somewhat larger” than the shorter piece, but not as large as 3/4. However, the uniform distribution of the break point means that the longer piece can sometimes be much larger than the shorter piece, especially when the break point is close to one end.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Number of simulations\nn_simulations &lt;- 1000\n\n# Simulate breaking the stick\nX &lt;- runif(n_simulations, min = 0, max = 1)  # Random break points\nlonger_piece &lt;- pmax(X, 1 - X)  # Length of the longer piece\n\n# Compute the expected length\nexpected_length &lt;- mean(longer_piece)\n\n# Print the result\ncat(\"Simulated Expected Length of Longer Piece:\", expected_length)\n\nSimulated Expected Length of Longer Piece: 0.7488728",
    "crumbs": [
      "Continuous Distributions",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Uniform distribution</span>"
    ]
  },
  {
    "objectID": "chapters/42_gauss.html",
    "href": "chapters/42_gauss.html",
    "title": "45  Special integrals",
    "section": "",
    "text": "There are many reasons to learn integrals. But the most compelling reason is that math is no longer the same with integrals. We can have many amazing results with integrals that were otherwise not imaginable. This section introduces two integrals that are of special importance to continuous distributions.\n\nExample 45.1 Show that \\(\\int_{-\\infty}^{+\\infty}e^{-x^{2}}dx=\\sqrt{\\pi}\\).\n\n\nProof. This is known as Gaussian integral, which is the kernel of the PDF of the normal distribution. It also amazingly relates two of the most famous constants in mathematics. It is not integrable by normal integration techniques. But it can be solved by switching to the polar coordinate. \\[\\begin{aligned}\\left(\\int_{-\\infty}^{+\\infty}e^{-x^{2}}dx\\right)^{2}= & \\int_{-\\infty}^{+\\infty}e^{-x^{2}}dx\\int_{-\\infty}^{+\\infty}e^{-y^{2}}dy\\\\\n= & \\int_{-\\infty}^{+\\infty}\\int_{-\\infty}^{+\\infty}e^{-(x^{2}+y^{2})}dxdy\\\\\n= & \\int_{0}^{2\\pi}\\int_{0}^{\\infty}e^{-r^{2}}rdrd\\theta & dA=dxdy=rdrd\\theta\\\\\n= & \\int_{0}^{2\\pi}\\int_{0}^{\\infty}\\frac{1}{2}e^{-u}dud\\theta & \\textrm{let }u=r^{2}\\\\\n= & \\frac{1}{2}\\int_{0}^{2\\pi}d\\theta=\\pi.\n\\end{aligned}\\]\n\n\nExample 45.2 Show that \\(\\int_{0}^{\\infty}t^{n}e^{-t}dt=n!\\)\n\n\nProof. \\(\\Gamma(z)=\\int_{0}^{\\infty}t^{z-1}e^{-t}dt\\) is known as the Gamma function, which is definitely one of the most interesting functions in mathematics. It is the extension of factorials to real numbers or even complex numbers. It also has many interesting properties, such as \\(\\Gamma(n)=(n-1)!\\), \\(\\Gamma(1/2)=\\sqrt{\\pi}\\), \\(\\Gamma(3/2)=\\sqrt{\\pi}/2\\), \\(\\Gamma'(1)=-\\gamma\\) and so on. The \\((n-1)\\) in the Gamma function is due to historical reasons and does not matter in our case. We will prove the integral with \\(n\\) instead of \\((n-1)\\).\nThere are many ways to prove this. One is to discover the recursive relationship \\(\\Gamma(n+1)=n\\Gamma(n)\\). But it does not give a clue why we need this integral to approximate the factorial. We start with an elementary integral\n\\[\\int_{0}^{\\infty}e^{at}dt=-\\frac{1}{a}\\] where \\(a&lt;0\\). Differentiate both sides \\(n\\) times with respect to \\(a\\):\n\\[\\begin{aligned}\\int_{0}^{\\infty}e^{at}tdt & = & -(-1)a^{-2}\\\\\n\\int_{0}^{\\infty}e^{at}t^{2}dt & = & -(-1)(-2)a^{-3}\\\\\n\\int_{0}^{\\infty}e^{at}t^{3}dt & = & -(-1)(-2)(-3)a^{-4}\\\\\n& \\vdots\\\\\n\\int_{0}^{\\infty}e^{at}t^{n}dt & = & (-1)^{n+1}n!a^{-(n+1)}\n\\end{aligned}\\]\nLet \\(a=-1\\), we have \\[\\int_{0}^{\\infty}e^{t}t^{n}=n!\\]",
    "crumbs": [
      "Continuous Distributions",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Special integrals</span>"
    ]
  },
  {
    "objectID": "chapters/42_sumrvs.html",
    "href": "chapters/42_sumrvs.html",
    "title": "46  Sum of random variables",
    "section": "",
    "text": "We have seen the sum of coin heads and the sum of dice points follow a “bell-shaped” distribution. This is not a coincidence. The pattern does not exist when the number of coins or dice are small, but becomes apparent when the numbers get large.\n\n# Function to simulate the sum of rolling n fair dice\nsimulate_dice_sum &lt;- function(n, n_simulations) {\n  \n  # Simulate rolling n dice n_simulations times\n  dice_rolls &lt;- matrix(sample(1:6, n * n_simulations, replace = TRUE), \n                       nrow = n_simulations, ncol = n)\n  \n  # Compute the sum of each roll\n  sums &lt;- rowSums(dice_rolls)\n  \n  return(sums)\n}\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Rolling 2 dice\nsums &lt;- simulate_dice_sum(2, 1000)\n\n# Plot the distribution of sums\nhist(sums, prob = TRUE, col = \"lightblue\")\n\n\n\n\n\n\n\n# Rolling 10 dice\nsums &lt;- simulate_dice_sum(10, 1000)\n\n# Plot the distribution of sums\nhist(sums, prob = TRUE, ylim = c(0,.08), col = \"lightblue\")\n\n# Overlay the normal curve\ncurve(dnorm(x, mean = mean(sums), sd = sd(sums)), \n      col = \"red\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\nThis does not only hold for dice points. In fact, the sum of random variables from any distribution would reveal a similar pattern.\n\n# Sum of uniform random variables\nsimulate_uniform_sum &lt;- function(n, n_simulations) {\n  sums &lt;- numeric(n_simulations)\n  \n  # Simulate the experiment n_simulations times\n  for (i in 1:n_simulations) {\n    # Sum of n Uniform random variables\n    sums[i] &lt;- sum(runif(n))\n  }\n  \n  return(sums)\n} \n\n# Sum of 2 Uniform random variables\nsums &lt;- simulate_uniform_sum(2, 1000)\n\n# Plot the histogram\nhist(sums, prob = TRUE, col = \"lightblue\")\n\n\n\n\n\n\n\n# Sum of 10 Uniform random variables\nsums &lt;- simulate_uniform_sum(10, 1000)\n\n# Plot the histogram\nhist(sums, prob = TRUE, col = \"lightblue\")\n\n# Overlay the normal curve\ncurve(dnorm(x, mean = mean(sums), sd = sd(sums)), \n      col = \"red\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSum of random variables approaches Normal distribution\n\n\n\nLet \\(X_1, X_2, \\dots, X_n\\) be a sequence of i.i.d random variables with mean \\(\\mu=E(X_i)\\) and variance \\(\\sigma^2=Var(X_i)\\). Let \\[S_n = X_1 + X_2 + \\dots + X_n\\] Then, as \\(n\\to\\infty\\), \\(S_n\\) converge in distribution to a normal distribution. That is \\[S_n \\to^d N(n\\mu, n\\sigma^2).\\]",
    "crumbs": [
      "Continuous Distributions",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Sum of random variables</span>"
    ]
  },
  {
    "objectID": "chapters/42_normal.html",
    "href": "chapters/42_normal.html",
    "title": "47  Normal distribution",
    "section": "",
    "text": "The most widely used model for random variables with continuous distributions is the family of normal distributions. One reason is that many real world samples appears to be normally distributed (the mass centered around the mean). The other reason is because of the Central Limit Theorem (will be discussed in later chapters), which essentially says the sum (or mean) or any random samples are approximately normal.\n\nDefinition 47.1 A random variable \\(Z\\) has the standard Normal distribution with mean \\(0\\) and variance \\(1\\), denoted as \\(Z\\sim N(0,1)\\), if \\(Z\\) has a PDF that follows \\[f(z)=\\frac{1}{\\sqrt{2\\pi}}e^{-z^{2}/2}.\\]\n\nThis is a valid PDF because \\(\\int_{-\\infty}^{\\infty}f(z)dz=1\\), which directly follows from Example 45.1. We further verify its mean and variance:\n\\[E(Z)=\\int_{-\\infty}^{+\\infty}z\\cdot\\frac{1}{\\sqrt{2\\pi}}e^{-z^{2}/2}dz=0\\quad\\textrm{by symmetry.}\\]\n\\[\\begin{aligned}\nVar(Z) & =E(Z^{2})-(EZ)^{2}=E(Z^{2})\\\\\n& =\\int_{-\\infty}^{+\\infty}z^{2}\\cdot\\frac{1}{\\sqrt{2\\pi}}e^{-z^{2}/2}dz\\\\\n& =\\frac{2}{\\sqrt{2\\pi}}\\int_{0}^{\\infty}\\underbrace{z}_{u}\\cdot\\underbrace{ze^{-z^{2}/2}dz}_{dv}\\\\\n& =\\frac{2}{\\sqrt{2\\pi}}\\left\\{ \\left[z(-e^{-z^{2}/2})\\right]_{0}^{\\infty}+\\underbrace{\\int_{0}^{\\infty}e^{-z^{2}/2}dz}_{\\sqrt{2\\pi}/2}\\right\\} \\\\\n& =1.\\end{aligned}\\]\n\nDefinition 47.2 The CDF of standard normal distribution is usually denoted by \\(\\Phi\\). Therefore, \\[\\Phi(z)=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{z}e^{-t^{2}/2}dt.\\] By symmetry, we have \\(\\Phi(-z)=1-\\Phi(z)\\).\n\n\nDefinition 47.3 Let \\(X=\\mu+\\sigma Z\\) where \\(Z\\sim N(0,1)\\). Then we say \\(X\\) has the Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\), denoted as \\(X\\sim N(\\mu,\\sigma^{2})\\). The PDF of \\(X\\) is given by \\[f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right].\\]\n\nThe mean and variance of \\(X\\) can be easily verified by the properties of expectation and variance. \\[\\begin{aligned}\nE(X) & =E(\\mu+\\sigma Z)=\\mu+\\sigma E(Z)=\\mu,\\\\\nVar(X) & =Var(\\mu+\\sigma Z)=\\sigma^{2}Var(Z)=\\sigma^{2}.\\end{aligned}\\]\nTo verify the PDF, we utilize the standard normal CDF: \\[P(X\\leq x)=P\\left(\\frac{X-\\mu}{\\sigma}\\leq\\frac{x-\\mu}{\\sigma}\\right)=\\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)\\]\nThe PDF is the derivative of the CDF, \\[f(x)=\\frac{1}{\\sigma}\\Phi'\\left(\\frac{x-\\mu}{\\sigma}\\right)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right].\\]\nThe shape of the normal distribution is the famous bell-shaped curve.\n\n\n\n\n\n\n\n\n\n\n\nThree-sigma rule\n\n\n\nThe normal distribution has the “three-sigma rule”: \\[\\begin{aligned}\nP(|X-\\mu|\\leq\\sigma) & \\approx0.68\\\\\nP(|X-\\mu|\\leq2\\sigma) & \\approx0.95\\\\\nP(|X-\\mu|\\leq3\\sigma) & \\approx0.997\\end{aligned}\\] Critical values: \\(\\Phi(-1)\\approx0.16,\\Phi(-2)\\approx0.025,\\Phi(-3)\\approx0.0015\\).\n\n\n\nTheorem 47.1 Let \\(X\\) have the Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\). Let \\(F\\) be the CDF of \\(X\\). Then the standardization of \\(X\\) \\[Z=\\frac{X-\\mu}{\\sigma}\\] has the standard normal distribution, and, for all \\(x\\): \\[F(x)=\\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right).\\]\n\nTo find the value of \\(\\Phi(z)\\), we need to use the normal probability table or statistical software.\n\nExample 47.1 Suppose the test score of a class of 50 students is normally distributed with mean 80 and standard deviation 20 (the total mark is 100). A student has scored 90. What is his percentile in the class?\n\n\nSolution. \\(X\\sim N(80,20)\\). We want to find \\(P(X&lt;90)\\). Standardize the distribution: \\[P(X&lt;90)=P\\left(\\frac{X-80}{20}&lt;\\frac{90-80}{20}\\right)=\\Phi(0.5)\\approx0.69.\\]\n\n\n# Exam scores from past students\nscores &lt;- read.csv(\"../dataset/exam.csv\")$final\n\n# Histogram of the exam scores\nhist(scores, prob = TRUE, col = \"lightblue\")\n\n# Overlay the normal curve\ncurve(dnorm(x, mean(scores), sd(scores)), col = \"red\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\n\nTheorem 47.2 Suppose \\(X\\sim N(\\mu,\\sigma^{2})\\). If \\(Y=aX+b\\), then \\(Y\\) has the Normal distribution \\(Y\\sim N(a\\mu+b,a^{2}\\sigma^{2})\\).\n\n\nTheorem 47.3 If the random variables \\(X_{1},\\ldots,X_{k}\\) are independent and \\(X_{i}\\sim N(\\mu_{i},\\sigma_{i}^{2})\\). Then \\[X_{1}+\\cdots+X_{k}\\sim N(\\mu_{1}+\\cdots+\\mu_{k},\\sigma_{1}^{2}+\\cdots+\\sigma_{k}^{2}).\\]\n\n\nExample 47.2 Suppose the heights (in centimeters) of women and men independently follow the normal distribution, \\(X\\sim N(165,25)\\), \\(Y\\sim N(170,25)\\). Determine the probability that a randomly selected woman will be taller than a man.\n\n\nSolution. Let \\(W=Y-X\\sim N(170-165,25+25).\\) Then \\(W\\sim N(5,50)\\). Therefore, \\[P(W&lt;0)=P\\left(\\frac{W-5}{\\sqrt{50}}&lt;\\frac{-5}{\\sqrt{50}}\\right)=P\\left(Z&lt;-\\frac{1}{\\sqrt{2}}\\right)=\\Phi(-0.707)\\approx0.24.\\]",
    "crumbs": [
      "Continuous Distributions",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "chapters/42_mvn.html",
    "href": "chapters/42_mvn.html",
    "title": "48  Multivariate normal",
    "section": "",
    "text": "Definition 48.1 (Bivariate normal distribution) \\((X,Y)\\) is said to have a Bivariate Normal distribution if the joint PDF satisfies \\[f(x,y)=\\frac{1}{2\\pi\\sqrt{1-\\rho^{2}}}\\exp\\left(-\\frac{1}{2(1-\\rho^{2})}(x^{2}+y^{2}-2\\rho xy)\\right)\\] where \\(\\rho\\in(-1,1)\\) is the correlation between \\(X\\) and \\(Y\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate Normal (MVN) is an extension of the bivariate normal distribution to \\(n\\)-dimensional variables. We skip the joint PDF here since it is too complicated. But like the bivariate case, an MVN is fully specified by knowing the mean of each component, the variance of each component, and the covariance between any two components.\n\n\n\n\n\n\nMarginal normality does not imply joint normality\n\n\n\nIf \\((X_{1},...,X_{k})\\) is MVN, then the marginal distribution of every \\(X_{j}\\) is Normal. However, the converse is false: it is possible to have Normally distributed \\(X_{1},...,X_{k}\\) such that \\((X_{1},...,X_{k})\\) is not Multivariate Normal.\n\n\n\n# Load necessary library\nlibrary(MASS)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate bivariate normal data\nbvn_data &lt;- mvrnorm(n = 1000, \n                    mu = c(0, 0), \n                    Sigma = matrix(c(1, 0.5, 0.5, 1), nrow = 2))\n\n# Modify the joint distribution: apply a nonlinear transformation\nbvn_data[, 2] &lt;- bvn_data[, 2] + 2 * sin(bvn_data[, 1]) \n\n# The marginal distribution remains normal\npar(mfrow = c(1, 3))\nhist(bvn_data[, 1], main = \"Marginal X1\", col = \"lightblue\")\nhist(bvn_data[, 2], main = \"Marginal X2\", col = \"lightblue\")\n\n# But the joint distribution is not normal\nplot(bvn_data, main = \"Joint Distribution\", pch = 16, col = rgb(1,0,0,.2))\n\n\n\n\n\n\n\n\n\nTheorem 48.1 A random vector \\((X_{1},...,X_{k})\\) is Multivariate Normal if every linear combination of the \\(X_{j}\\) has a Normal distribution (\\(X_{j}\\) do not have to be independent). That is, we require \\(t_{1}X_{1}+\\cdot\\cdot\\cdot+t_{k}X_{k}\\) to have a Normal distribution for any choice of constants \\(t_{1},...,t_{k}\\).\n\n\nTheorem 48.2 In general, uncorrelated does not imply independent. But with an MVN random vector, uncorrelated implies independent. In particular, if \\((X,Y)\\) is Bivariate Normal and \\(\\rho_{XY}=0\\), then X and Y are independent.\n\n\nTheorem 48.3 If \\((X,Y)\\) is Bivariate Normal, then the conditional expectation satisfies \\[E(Y|X)=E(Y)+\\frac{Cov(X,Y)}{Var(X)}(X-E(X)).\\]\nIn other words, \\[E(Y|X)=a+bX\\]where \\(b=\\frac{Cov(X,Y)}{Var(X)}\\) and \\(a=E(Y)-bE(X)\\).\n\nThis is exactly the case in Definition 40.2, where we assume the conditional expectation \\(E(Y|X)\\) is a linear function of \\(X\\). This assumption is true when \\((X,Y)\\) are jointly normal. Otherwise, the assumption might not be reasonable. In practice, we don’t know precisely the joint distribution of variables. The linear model is just a simplified assumption.",
    "crumbs": [
      "Continuous Distributions",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Multivariate normal</span>"
    ]
  },
  {
    "objectID": "chapters/43_expo.html",
    "href": "chapters/43_expo.html",
    "title": "49  Exponential distribution",
    "section": "",
    "text": "Imagine you are a shop owner that waits for your next customer. The customers arrive randomly, with no preference for any specific time interval. What interests us is the waiting time until the next customer arrives. Since the customers arrives randomly, the likelihood of it coming in the next moment is the same whether you’ve been waiting for one minute or ten minutes. In other words, the waiting time between events that occur randomly and independently over time. The exponential distribution is the mathematical model that best describes such scenarios.\nTo model the waiting time, let \\(X\\) represent the time until the next event. A crucial feature of this process is that the waiting time has no “memory.” That is, no matter how long you’ve already waited, the probability of waiting an additional amount of time is the same. Mathematically, this memoryless property is expressed as: \\[P(X\\geq s+t\\mid X\\geq s)=P(X\\geq t),\\quad\\text{for all }s,t\\geq0.\\] The conditional probability can be rewritten using the definition of conditional probabilities: \\[P(X\\geq s+t\\mid X\\geq s)=\\frac{P(X\\geq s+t)}{P(X\\geq s)}.\\] Thus, the memoryless property implies: \\[\\frac{P(X\\geq s+t)}{P(X\\geq s)}=P(X\\geq t).\\] Let the survival function \\(S(x)\\) represent \\(P(X\\geq x)\\) . Substituting \\(S(x)\\) into the equation gives: \\[\\frac{S(s+t)}{S(s)}=S(t).\\] This reminds us of the exponential function. In fact, the only continuous and non-negative solution to this equation is: \\[S(x)=e^{-\\lambda x},\\quad\\lambda&gt;0,\\] where \\(\\lambda\\) is a positive constant. This solution represents the probability that the waiting time exceeds \\(x\\) , and \\(\\lambda\\) determines how quickly the probability decreases over time.\nThe CDF of \\(X\\) is exactly the opposite of \\(S(x)\\): \\[F(x)=1-S(x)=1-e^{-\\lambda x}.\\] Take derivative to get the PDF: \\[f(x)=F'(x)=\\lambda e^{-\\lambda x}.\\]\n\nDefinition 49.1 (Exponential distribution) A random variable \\(X\\) is said to have the Exponential distribution with parameter \\(\\lambda\\) if its PDF is \\[f(x)=\\lambda e^{-\\lambda x},\\qquad x&gt;0.\\]\nWe denote this as \\(X\\sim\\textrm{Expo}(\\lambda).\\)\\(\\lambda\\) is interpreted as the “rate”, i.e. number of events per unit of time.\n\nTo compute the expectation and variance, we first standardize the exponential distribution. Let \\(Y=\\lambda X\\), then \\(Y\\sim\\textrm{Expo}(1)\\), because \\[P(Y\\leq y)=P(X\\leq y/\\lambda)=1-e^{-y}.\\] It follows that, \\[\\begin{aligned}\nE(Y) & =\\int_{0}^{\\infty}ye^{-y}dy=\\left[-ye^{-y}\\right]_{0}^{\\infty}+\\int_{0}^{\\infty}e^{-y}dy=1;\\\\\nVar(Y) & =E(Y^{2})-(EY)^{2}=\\int_{0}^{\\infty}y^{2}e^{-y}dy-1=1.\\end{aligned}\\] For \\(X=Y/\\lambda\\), we have \\(E(X)=\\frac{1}{\\lambda}\\), \\(Var(X)=\\frac{1}{\\lambda^{2}}\\).\n\nTheorem 49.1 (Memoryless property) If \\(X\\) has the exponential distribution with parameter \\(\\lambda\\), and let \\(t&gt;0\\), \\(h&gt;0\\), then \\[P(X\\geq t+h|X\\geq t)=P(X\\geq h).\\]\n\n\nProof. For \\(t&gt;0\\) we have \\[P(X\\geq t)=\\int_{t}^{\\infty}\\lambda e^{-\\lambda x}dx=e^{-\\lambda t}.\\] Hence for each \\(t&gt;0\\) and each \\(h&gt;0\\), \\[P(X\\geq t+h|X\\geq t)=\\frac{P(X\\geq t+h)}{P(X\\geq t)}=\\frac{e^{-\\lambda(t+h)}}{e^{-\\lambda t}}=e^{-\\lambda h}=P(X\\geq h).\\] \n\nWhat are the implications of the memoryless property? If human lifetimes were Exponential, then conditional on having survived to the age of 80, your remaining lifetime would have the same distribution as that of a newborn baby! Clearly, the memoryless property is not an appropriate description for human lifetimes.\nThe memoryless property is a very special property of the Exponential distribution. In fact, the Exponential is the only memoryless continuous distribution (with support \\((0,\\infty)\\)); and Geometric distribution is the only memoryless discrete distribution (with support \\(0,1,\\dots\\)).\n\nExample 49.1 (Waiting time) We try to model the waiting time at a bus station. Suppose the bus arrives at random time but on average there will be one bus per 10 minutes. You arrive at the bus stop at a random time, not knowing how long ago the previous bus came. What is the distribution of your waiting time for the next bus? What is the mean waiting time? What is the median waiting time?\n\n\nSolution. Let \\(X\\) be the waiting time and we know it is an Exponential distribution. Since \\(E(X)=1/\\lambda=10\\), the parameter \\(\\lambda=1/10\\). Thus \\(X\\sim\\textrm{Expo}(0.1)\\). By the memoryless property, how much longer the next bus will take to arrive is independent of how long ago the previous bus arrived. The average waiting time is always 10 minutes.\nThe CDF of \\(X\\) is: \\(F(x)=1-e^{-\\lambda x}\\). The median \\(m\\) satisfies \\(F(m) = 1/2\\). Thus, \\(m= \\log(2)/\\lambda \\approx 6.9\\) minutes. So the typical waiting experienced by most passengers is less than 10 minutes.\n\n\nTheorem 49.2 (Poisson-Exponential connection) Let \\(T\\) be the time between two consecutive events in Poisson process \\(\\text{Pois}(\\lambda t)\\). Then \\(T\\) follows Exponential distribution \\(T\\sim\\text{Expo}(\\lambda)\\).\n\n\nProof. The waiting time \\(T&gt;t\\) is equivalent to no event occurred during period \\(t\\). Therefore, \\[P(T&gt;t)=P(N_{t}=0)=e^{-\\lambda t}\\frac{(\\lambda t)^{0}}{0!}=e^{-\\lambda t}\\] where \\(N_{t}\\) is the number of events occurred in \\([0,t]\\), which follows a Poisson distribution. The CDF of \\(T\\) is\n\\[F(t)=1-P(T&gt;t)=1-e^{-\\lambda t}\\]\nThe PDF of \\(T\\) is \\[f(t)=F'(t)=\\lambda e^{-\\lambda t}\\] This indicates \\(T\\sim \\text{Expo}(\\lambda)\\).\n\n\n#\n# Simulate random arrivals and inter-arrival time\n#\nT &lt;- 1000  # Total time horizon\nrate &lt;- 1  # rate of occurrence per unit time\n\n# Total number of arrivals\nn_arrivals &lt;- rpois(1, lambda = rate * T)\n\n# Time of each arrivals\nt_arrivals &lt;- sort(runif(n_arrivals, min = 0, max = T))\n\n# Plot the timeline of arrivals\nplot(t_arrivals[1:20], rep(1, 20), type = \"h\", col = \"red\", ann = F)\n\n\n\n\n\n\n\n# Compute inter-arrival time\ninter_arrival_time &lt;- diff(t_arrivals)\n\n# Plot the distribution of inter-arrival time\nhist(inter_arrival_time, prob = TRUE, breaks = 20)\n\n# Overlay the exponential function\ncurve(exp(-x), col = \"red\", add = TRUE)",
    "crumbs": [
      "Continuous Distributions",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Exponential distribution</span>"
    ]
  },
  {
    "objectID": "chapters/44_gamma.html",
    "href": "chapters/44_gamma.html",
    "title": "50  Gamma distribution",
    "section": "",
    "text": "The Gamma distribution is a continuous distribution on the positive real line; it is a generalization of the Exponential distribution. While an Exponential RV represents the waiting time for the first event to occur, we shall see that a Gamma RV represents the total waiting time for \\(n\\) events to occur.\nLet’s start with a simple case. Suppose we want to find out the total waiting until the 2nd event occurred. Let \\(Y=X_{1}+X_{2}\\) where \\(X_{1},X_{2}\\sim \\textrm{Expo}(\\lambda)\\) independently. If \\(Y\\) is discrete, we have \\(P(Y=y)=\\sum_{k=0}^{y}P(X_{1}=k,X_{2}=y-k)\\). For continuous \\(y\\), we have \\[\\begin{aligned}\nf_{Y}(y) & =\\int_{0}^{y}f_{X}(x)f_{X}(y-x)dx=\\int_{0}^{y}\\lambda e^{-\\lambda x}\\lambda e^{-\\lambda(y-x)}dx\\\\\n& =\\int_{0}^{y}\\lambda^{2}e^{-\\lambda y}dx=\\lambda^{2}e^{-\\lambda y}y.\\end{aligned}\\]\nIf there is a third variable, \\[\\begin{aligned}\nf_{Z}(z) & =\\int_{0}^{z}f_{X}(x)f_{Y}(z-x)dx=\\int_{0}^{z}\\lambda e^{-\\lambda x}\\lambda^{2}e^{-\\lambda(z-x)}(z-x)dx\\\\\n& =\\lambda^{3}e^{-\\lambda z}\\int_{0}^{z}(z-x)dx=\\lambda^{3}e^{-\\lambda z}z^{2}/2.\\end{aligned}\\]\nThe general pattern is the Gamma distribution.\n\nDefinition 50.1 (Exponential distribution) An random variable X is said to have the Gamma distribution with parameters \\(a\\) and \\(\\lambda\\), \\(a&gt;0\\) and \\(\\lambda&gt;0\\), if it has the PDF \\[f(x)=\\frac{\\lambda^{a}}{\\Gamma(a)}x^{a-1}e^{-\\lambda x},\\quad x&gt;0\\] We write \\(X\\sim\\textrm{Gamma}(a,\\lambda)\\).\n\nVerify this is a valid PDF:\n\\[\\int_{0}^{\\infty}\\frac{1}{\\Gamma(a)}(\\lambda x)^{a}e^{-\\lambda x}\\frac{dx}{x}\\overset{u=\\lambda x}{=}\\frac{1}{\\Gamma(a)}\\int_{0}^{\\infty}u^{a}e^{-u}\\frac{du}{u}=\\frac{\\Gamma(a)}{\\Gamma(a)}=1.\\]\nTaking \\(a=1\\), the \\(\\textrm{Gamma}(1,\\lambda)\\) PDF is \\(f(x)=\\lambda e^{-\\lambda x}\\), which is the same as \\(\\textrm{Expo}(\\lambda)\\). So Exponential distribution is a special case of Gamma distribution.\nLet’s find the expectation and variance of the Gamma distribution. Let \\(Y\\sim\\textrm{Gamma}(a,1)\\). Recall \\(\\Gamma\\) function has the property \\(\\Gamma(a+1)=a\\Gamma(a)\\).\n\\[E(Y)=\\int_{0}^{\\infty}y\\cdot\\frac{1}{\\Gamma(a)}y^{a-1}e^{-y}dy=\\frac{1}{\\Gamma(a)}\\int_{0}^{\\infty}y^{a}e^{-y}dy=\\frac{\\Gamma(a+1)}{\\Gamma(a)}=a.\\]\nApply LOTUS to evaluate the second moment:\n\\[E(Y^{2})=\\int_{0}^{\\infty}y^{2}\\cdot\\frac{1}{\\Gamma(a)}y^{a-1}e^{-y}dy=\\frac{1}{\\Gamma(a)}\\int_{0}^{\\infty}y^{a+1}e^{-y}dy=\\frac{\\Gamma(a+2)}{\\Gamma(a)}=(a+1)a.\\]\nTherefore, \\[Var(Y)=(a+1)a-a^{2}=a.\\]\nSo for \\(Y\\sim\\textrm{Gamma}(a,1)\\), \\(E(Y)=Var(Y)=a\\). For the general case \\(X\\sim\\textrm{Gamma}(a,\\lambda)\\), we now show that \\(X=\\frac{Y}{\\lambda}\\). Note that \\[\\begin{aligned}\nF_{X}(x)=P(X & \\leq x)=P(Y\\leq x/\\lambda)=F_{Y}(x/\\lambda)\\\\\nf_{X}(x)=\\frac{dF_{X}}{dx} & =\\frac{\\partial F_{Y}}{\\partial y}\\frac{dy}{dx}=f_{Y}(y)\\lambda\\end{aligned}\\]\nTherefore, \\[f_{X}(x)=\\frac{1}{\\Gamma(a)}y^{a-1}e^{-y}\\lambda=\\frac{\\lambda^{a}}{\\Gamma(a)}x^{a-1}e^{-\\lambda x}.\\]\nHence, we have \\(E(X)=\\frac{a}{\\lambda}\\), \\(Var(X)=\\frac{a}{\\lambda^{2}}\\).\n\nTheorem 50.1 (Exponential-Gamma connection) Let \\(X_{1},\\dots,X_{n}\\) be independent and identical \\(\\textrm{Expo}(\\lambda)\\). Then \\[X_{1}+\\cdots+X_{n}\\sim\\textrm{Gamma}(n,\\lambda).\\]\n\n\nProof. Let’s prove by showing the MGFs are equivalent. \\[M_{X}(t)=E(e^{tX})=\\int_{0}^{\\infty}e^{tx}\\lambda e^{-\\lambda x}dx=\\frac{\\lambda}{\\lambda-t}\\quad\\textrm{for }t&lt;\\lambda\\]\nThus, the MGF of \\(Y=X_{1}+\\cdots+X_{n}\\) is \\(M_{Y}(t)=\\left(M_{X}(t)\\right)^{n}=\\left(\\frac{\\lambda}{\\lambda-t}\\right)^{n}\\). We verify this is the MGF of a Gamma distribution. Suppose \\(Y\\sim\\textrm{Gamma}(n,\\lambda)\\), it has MGF:\n\\[\\begin{aligned}\nM_{Y}(t) & =E(e^{tY})=\\int_{0}^{\\infty}e^{ty}\\frac{\\lambda^{n}}{\\Gamma(a)}y^{n-1}e^{-\\lambda y}dy\\\\\n& =\\frac{\\lambda^{n}}{(\\lambda-t)^{n}}\\int_{0}^{\\infty}\\frac{1}{\\Gamma(a)}((\\lambda-t)y)^{n-1}e^{-(\\lambda-t)y}(\\lambda-t)dy\\\\\n& =\\frac{\\lambda^{n}}{(\\lambda-t)^{n}}\\int_{0}^{\\infty}\\frac{1}{\\Gamma(a)}u^{n-1}e^{-u}du\\qquad u=(\\lambda-t)y\\\\\n& =\\left(\\frac{\\lambda}{\\lambda-t}\\right)^{n}.\\end{aligned}\\]\n\nThus, if \\(X_{i}\\) represents the i.i.d inter-arrival time. \\(Y\\) has the interpretation of the arrival time until the \\(n\\)-th event. \\[Y=\\sum_{i=1}^{n}X_{i}=\\sum_{i=1}^{n}\\textrm{(time of the i-th arrival)}\\sim\\textrm{Gamma}(n,\\lambda).\\]\n\nExample 50.1 (Service time in a queue) Customer \\(i\\) must wait time \\(X_{i}\\) for service once reaching the head of the queue. The average service rate is 1 customer per 10 minutes. Assume the service for each customer is independent. If you are the 5th in the queue. What is the expected waiting to be served?\n\n\nSolution. \\(X_{i}\\sim\\textrm{Expo}(0.1)\\). Then \\(E(X_{i})=10\\). Let Y be the time until you are served. Then \\(Y\\sim\\textrm{Gamma}(5,0.1)\\). Thus, \\(E(Y)=\\frac{5}{0.1}=50\\) minutes. The probabilities of some selected values: \\[P(Y\\leq t)=\\begin{cases}\n5\\% & t=20\\\\\n18\\% & t=30\\\\\n71\\% & t=60\n\\end{cases}.\\]",
    "crumbs": [
      "Continuous Distributions",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Gamma distribution</span>"
    ]
  },
  {
    "objectID": "chapters/45_beta.html",
    "href": "chapters/45_beta.html",
    "title": "51  Beta distribution",
    "section": "",
    "text": "The Beta distribution is a continuous distribution on the interval \\((0,1)\\). It is a generalization of the \\(\\textrm{Unif}(0,1)\\) distribution, allowing the PDF to be non-constant on \\((0,1)\\).\n\nDefinition 51.1 (Beta distribution) A random variable \\(X\\) is said to have the Beta distribution with parameters \\(a\\) and \\(b\\), \\(a&gt;0\\) and \\(b&gt;0\\), if its PDF is \\[f(x)=\\frac{1}{\\beta(a,b)}x^{a-1}(1-x)^{b-1},\\quad0&lt;x&lt;1\\] where the constant \\(\\beta(a,b)\\) is chosen to make the PDF integrate to \\(1\\). We write this as \\(X\\sim\\textrm{Beta}(a,b)\\).\n\nThe Beta distribution takes different shapes for different \\(a\\) and \\(b\\) values. Here are some general patterns:\n\nIf \\(a=b=1\\), the \\(\\textrm{Beta}(1,1)\\) PDF is constant on \\((0,1)\\), equivalent to \\(\\textrm{Unif}(0,1)\\).\nIf \\(a&lt;1\\) and \\(b&lt;1\\), the PDF is U-shaped and opens upward. If \\(a&gt;1\\) and \\(b&gt;1\\), the PDF opens downward.\nIf \\(a=b\\), the PDF is symmetric about \\(1/2\\). If \\(a&gt;b\\), the PDF favors values larger than \\(1/2\\). If \\(a&lt;b\\), the PDF favors values smaller than \\(1/2\\).\n\nTo make the PDF integrates to 1, the constant \\(\\beta(a,b)\\) has to satisfy \\[\\beta(a,b)=\\int_{0}^{1}x^{a-1}(1-x)^{b-1}dx.\\]\nWe now try to find this integral:\n\\[\\begin{aligned}\n\\beta(a,b) & =\\int_{0}^{1}\\underbrace{x^{a-1}}_{f}\\underbrace{(1-x)^{b-1}}_{g'}dx\\\\\n& =\\left[-x^{a-1}\\frac{(1-x)^{b}}{b}\\right]_{0}^{1}+\\int_{0}^{1}(a-1)x^{a-2}\\frac{(1-x)^{b}}{b}dx\\\\\n& =\\frac{a-1}{b}\\beta(a-1,b+1)\\\\\n& =\\frac{a-1}{b}\\cdot\\frac{a-2}{b+1}\\beta(a-2,b+2)\\\\\n& =\\frac{a-1}{b}\\cdot\\frac{a-2}{b+1}\\cdot\\frac{a-3}{b+2}\\beta(a-3,b+3)\\\\\n& \\vdots\\\\\n& =\\frac{(a-1)!}{b(b+1)(b+2)\\cdots(b+a-2)}\\underbrace{\\beta(1,a+b-1)}_{\\frac{1}{a+b-1}}\\\\\n& =\\frac{(a-1)!}{\\frac{(b+a-2)!}{(b-1)!}}\\cdot\\frac{1}{a+b-1}\\\\\n& =\\frac{(a-1)!(b-1)!}{(a+b-1)!}\\\\\n& =\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}.\\end{aligned}\\]\n\nExample 51.1 Let \\(X_{1},\\ldots,X_{n}\\) be independent random variables with the uniform distribution on the interval \\([0,1]\\). Find the distribution of \\(Y=\\max(X_{1},\\ldots,X_{n})\\).\n\n\nSolution. Let’s find the CDF of \\(Y\\): \\[\\begin{aligned}\nP(Y\\leq y) & =P(X_{1}\\leq y\\cap X_{2}\\leq y\\cap\\cdots\\cap X_{n}\\leq y)\\\\\n& \\overset{iid}{=}P(X_{1}\\leq y)P(X_{2}\\leq y)\\cdots P(X_{n}\\leq y)\\\\\n& =y^{n}\\end{aligned}\\] for \\(y\\in[0,1]\\). Hence, \\[F_{Y}(y)=P(Y\\leq y)=\\begin{cases}\n0 & y&lt;0\\\\\ny^{n} & 0\\leq y\\leq1\\\\\n1 & y&gt;1\n\\end{cases}\\] The PDF of \\(Y\\) is \\[f_{Y}(y)=F'_{Y}(y)=\\begin{cases}\nny^{n-1} & 0\\leq y\\leq1\\\\\n0 & \\textrm{otherwise}\n\\end{cases}\\] Thus, \\(Y\\sim Beta(n,1)\\).\n\nBeta distributions are often used as priors for parameters in Bayesian inference. We do not cover Bayesian inference in this book. Nonetheless we illustrate this with an example.\n\nExample 51.2 (Beta-Binomial conjugacy) We have a coin that lands Heads with probability \\(p\\), but we don’t know what \\(p\\) is. Our goal is to infer the value of \\(p\\) after observing the outcomes of \\(n\\) tosses of the coin. The larger that \\(n\\) is, the more accurately we should be able to estimate \\(p\\).\n\n\nSolution. We model the unknown parameter \\(p\\) as a Beta distribution, \\(p\\sim\\textrm{Beta}(a,b)\\). Since we are completely ignorant about this \\(p\\), we can also model it as the uniform distribution. But we will see that using the Beta distribution is even simpler than the uniform distribution. Let \\(X\\) be the number of heads in \\(n\\) tosses of the coin. Then \\[X|p\\sim\\textrm{Bin}(n,p)\\] Apply the Bayes’ rule to inverse the conditioning: \\[\\begin{aligned}\nf(p|X=k) & =\\frac{P(X=k|p)f(p)}{P(X=k)}\\\\\n& =\\frac{\\binom{n}{k}p^{k}(1-p)^{n-k}\\cdot\\frac{1}{\\beta(a,b)}p^{a-1}(1-p)^{b-1}}{\\int_{0}^{1}\\binom{n}{k}p^{k}(1-p)^{n-k}f(p)dp}\\\\\n& \\propto p^{a+k-1}(1-p)^{b+n-k-1}\\end{aligned}\\]\nThis the kernel of \\(\\textrm{Beta}(a+k,b+n-k)\\). The rest is just a normalizing constant. Therefore, \\[p|X=k\\sim\\textrm{Beta}(a+k,b+n-k).\\]\nThe posterior distribution of \\(p\\) after observing \\(X=k\\) is still a Beta distribution! This is a special relationship between the Beta and Binomial distributions called conjugacy: if we have a Beta prior distribution on \\(p\\) and data that are conditionally Binomial given \\(p\\), then when going from prior to posterior, we don’t leave the family of Beta distributions. We say that the Beta is the conjugate prior of the Binomial.",
    "crumbs": [
      "Continuous Distributions",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Beta distribution</span>"
    ]
  },
  {
    "objectID": "chapters/46_2rvs.html",
    "href": "chapters/46_2rvs.html",
    "title": "52  Multivariate problems",
    "section": "",
    "text": "We extend the concepts of joint, marginal and conditional distribution to continuous random variables.\n\n\nExample 52.1 Suppose \\(X\\) and \\(Y\\) are uniformly distributed on a disk \\(\\{(x,y):x^{2}+y^{2}\\leq1\\}\\). Find the joint PDF, marginal distributions and conditional distributions. Are \\(X\\) and \\(Y\\) independent?\n\n\nSolution. The area of the disk is \\(\\pi\\), therefore\n\\[f(x,y)=\\begin{cases}\n\\frac{1}{\\pi} & x^{2}+y^{2}\\leq1\\\\\n0 & \\textrm{otherwise}\n\\end{cases}\\]\nThe marginal distributions are\n\\[\\begin{aligned}\nf_{X}(x) & =\\int_{-\\sqrt{1-x^{2}}}^{\\sqrt{1+x^{2}}}\\frac{1}{\\pi}dy=\\frac{2}{\\pi}\\sqrt{1-x^{2}},\\qquad-1\\leq x\\leq1\\\\\nf_{Y}(y) & =\\int_{-\\sqrt{1-y^{2}}}^{\\sqrt{1+y^{2}}}\\frac{1}{\\pi}dx=\\frac{2}{\\pi}\\sqrt{1-y^{2}},\\qquad-1\\leq y\\leq1\\end{aligned}\\]\nThe conditional distributions are\n\\[f_{Y|X}(y|x)=\\frac{f(x,y)}{f_{X}(x)}=\\frac{\\frac{1}{\\pi}}{\\frac{2}{\\pi}\\sqrt{1-x^{2}}}=\\frac{1}{2\\sqrt{1-x^{2}}}\\]\nTherefore, \\(Y|X\\sim\\textrm{Unif}(-\\sqrt{1-x^{2}},\\sqrt{1-x^{2}})\\).\nSince \\(f(x,y)\\neq f_{X}(x)f_{Y}(y)\\), \\(X\\) and \\(Y\\) are not independent. This is because knowing the value of \\(X\\) constrains the value of \\(Y\\).\n\n\nExample 52.2 Suppose \\(X,Y\\overset{iid}{\\sim}\\text{Unif}(0,1)\\). Find the probability \\(P\\left(Y\\leq\\frac{1}{2X}\\right)\\).\n\n\nSolution. The joint distribution is \\[f(x,y)=\\begin{cases}\n1 & 0\\leq x\\leq1,0\\leq y\\leq1\\\\\n0 & \\textrm{otherwise}\n\\end{cases}\\]\n\\[P\\left(Y\\leq\\frac{1}{2X}\\right)=\\int_{0}^{1/2}\\int_{0}^{1}1dydx+\\int_{1/2}^{1}\\int_{0}^{1/2x}1dydx=\\frac{1}{2}+\\int_{1/2}^{1}\\frac{1}{2x}dx=\\frac{1}{2}+\\ln\\sqrt{2}.\\]\n\n\nExample 52.3 For \\(X,Y\\overset{iid}{\\sim}\\textrm{Unif}(0,1)\\), find \\(E(|X-Y|)\\).\n\n\nSolution. Apply 2D LOTUS: \\[\\begin{aligned}E(|X-Y|)= & \\int_{0}^{1}\\int_{0}^{1}|x-y|dxdy\\\\\n= & \\int_{0}^{1}\\int_{y}^{1}(x-y)dxdy+\\int_{0}^{1}\\int_{0}^{y}(y-x)dxdy\\\\\n= & 2\\int_{0}^{1}\\int_{y}^{1}(x-y)dxdy\\\\\n= & \\frac{1}{3}.\n\\end{aligned}\\]\n\n\nExample 52.4 \\(X,Y\\overset{iid}{\\sim}N(0,1)\\), find \\(E(|X-Y|)\\).\n\n\nSolution. Since the sum or difference of independent Normal variables is also Normal, \\(X-Y\\sim N(0,2)\\). Let \\(Z=X-Y\\). Then \\(Z\\sim N(0,1)\\), and \\(E(|X-Y|)=\\sqrt{2}E(|Z|)\\). Apply LOTUS, \\[E(|Z|)=\\int_{-\\infty}^{\\infty}|z|\\frac{1}{\\sqrt{2\\pi}}e^{-z^{2}/2}\\,dz=2\\int_{0}^{\\infty}z\\frac{1}{\\sqrt{2\\pi}}e^{-z^{2}/2}\\,dz=\\sqrt{\\frac{2}{\\pi}},\\] Therefore, \\(\\mathbb{E}(|X-Y|)=\\frac{2}{\\sqrt{\\pi}}.\\)",
    "crumbs": [
      "Continuous Distributions",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Multivariate problems</span>"
    ]
  },
  {
    "objectID": "chapters/47_trans.html",
    "href": "chapters/47_trans.html",
    "title": "53  Transformation",
    "section": "",
    "text": "Example 53.1 (Min/max of random variables) Let \\(X_1, X_2, \\dots, X_n\\) be i.i.d random variables, each following a uniform distribution on the interval \\([0, 1]\\). Find the distribution of \\(\\max(X_1, X_2, \\dots, X_n)\\).\n\n\nSolution. Let \\(M = \\max(X_1, X_2, \\dots, X_n)\\). The CDF of \\(M\\), denoted \\(F_M(m)\\), is the probability that \\(M \\leq m\\). For \\(M \\leq m\\) to hold, all \\(X_i\\) must satisfy \\(X_i \\leq m\\). Since the \\(X_i\\) are independent and identically distributed: \\[\\begin{aligned}\nF_M(m) = P(M \\leq m) &= P(X_1 \\leq m, X_2 \\leq m, \\dots, X_n \\leq m) \\\\\n&= P(X_1 \\leq m) \\cdot P(X_2 \\leq m) \\cdots P(X_n \\leq m)\n\\end{aligned}\\] For a uniform distribution on \\([0, 1]\\), \\(P(X_i \\leq m) = m\\) for \\(0 \\leq m \\leq 1\\). Thus: \\[F_M(m) = m^n.\\] The PDF of \\(M\\), denoted \\(f_M(m)\\), is the derivative of the CDF: \\[f_M(m) = \\frac{d}{dm} F_M(m) = \\frac{d}{dm} (m^n) = n m^{n-1}.\\]\n\n\nExample 53.2 (Chi-square PDF) Let \\(X\\sim N(0,1)\\), \\(Y=X^{2}\\). The distribution of \\(Y\\) is an example of a Chi-Square distribution. Find the PDF of \\(Y\\).\n\n\nSolution. Again, we try to find the CDF first, and differentiate to the PDF. \\[F_{Y}(y)=P(X^{2}\\leq y)=P(-\\sqrt{y}\\leq X\\leq\\sqrt{y})=\\Phi(\\sqrt{y})-\\Phi(-\\sqrt{y})=2\\Phi(\\sqrt{y})-1\\] Therefore, \\[f_{Y}(y)=2\\varphi(\\sqrt{y})\\cdot\\frac{1}{2}y^{-1/2}=\\varphi(\\sqrt{y})y^{-1/2},\\quad y&gt;0.\\]\n\n\nTheorem 53.1 (Transformation) Let \\(X\\) be a continuous r.v. with PDF \\(f_{X}\\), and let \\(Y=g(X)\\), where \\(g\\) is differentiable and strictly increasing (or strictly decreasing). Then the PDF of \\(Y\\) is given by \\[f_{Y}(y)=f_{X}(x)\\left|\\frac{dx}{dy}\\right|,\\] where \\(x=g^{-1}(y)\\).\n\n\nProof. Let \\(g\\) be strictly increasing. The CDF of \\(Y\\) is \\[F_{Y}(y)=P(Y\\leq y)=P(g(X)\\leq y)=P(X\\leq g^{-1}(y))=F_{X}(g^{-1}(y))=F_{X}(x)\\] By the chain rule, the PDF of \\(Y\\) is \\[f_{Y}(y)=f_{X}(x)\\frac{dx}{dy}.\\] If \\(g\\) is strictly decreasing, \\[F_{Y}(y)=P(Y\\leq y)=P(g(X)\\leq y)=P(X\\geq g^{-1}(y))=1-F_{X}(g^{-1}(y))=1-F_{X}(x)\\] Then the PDF of \\(Y\\) is \\[f_{Y}(y)=-f_{X}(x)\\frac{dx}{dy}.\\] But in this case, \\(dx/dy&lt;0\\). So taking absolute value covers both cases.\n\n\nExample 53.3 (Log-Normal PDF) Let \\(X\\sim N(0,1)\\), \\(Y=e^{X}\\). Then the distribution of \\(Y\\) is called the Log-Normal distribution. Find the PDF of \\(Y\\).\n\n\nSolution. Since \\(g(x)=e^{x}\\) is strictly increasing. Let \\(y=e^{x}\\), so \\(x=\\log y\\) and \\(dy/dx=e^{x}\\). Then \\[f_{Y}(y)=f_{X}(x)\\left|\\frac{dx}{dy}\\right|=\\varphi(x)\\frac{1}{e^{x}}=\\varphi(\\log y)\\frac{1}{y},\\quad y&gt;0.\\] Note that after applying the change of variables formula, we write everything on the right-hand side in terms of \\(y\\), and we specify the support of the distribution. To determine the support, we just observe that as \\(x\\) ranges from \\(-\\infty\\) to\\(\\infty\\), \\(e^{x}\\) ranges from \\(0\\) to \\(\\infty\\).\n\n\nTheorem 53.2 (Transformation of multi-variables) Let \\(\\mathbf{X}=(X_{1},\\dots,X_{n})\\) be a continuous random vector with joint PDF \\(f_{\\mathbf{X}}\\), and let \\(\\mathbf{Y}=g(\\mathbf{X})\\) where \\(g\\) is an invertible function from \\(\\mathbb{R}^{n}\\) to \\(\\mathbb{R}^{n}\\). Let \\(\\mathbf{y}=g(\\mathbf{x})\\). Define the Jacobian matrix: \\[\\frac{\\partial\\mathbf{x}}{\\partial\\mathbf{y}}=\\begin{pmatrix}\\frac{\\partial x_{1}}{\\partial y_{1}} & \\frac{\\partial x_{1}}{\\partial y_{2}} & \\dots & \\frac{\\partial x_{1}}{\\partial y_{n}}\\\\\n\\vdots & \\vdots &  & \\vdots\\\\\n\\frac{\\partial x_{n}}{\\partial y_{1}} & \\frac{\\partial x_{n}}{\\partial y_{2}} & \\dots & \\frac{\\partial x_{n}}{\\partial y_{n}}\n\\end{pmatrix}.\\] Also assume that the determinant of the Jacobian matrix is never 0. Then the joint PDF of \\(\\mathbf{Y}\\) is \\[f_{\\mathbf{Y}}(\\mathbf{y})=f_{\\mathbf{X}}(\\mathbf{x})\\left|\\frac{\\partial\\mathbf{x}}{\\partial\\mathbf{y}}\\right|,\\] where \\(\\left|\\frac{\\partial\\mathbf{x}}{\\partial\\mathbf{y}}\\right|\\) is the absolute value of the determinant of the Jacobian matrix.\n\n\nExample 53.4 Suppose \\(X,Y\\overset{iid}{\\sim}Expo(1)\\). Find the distribution of \\(X/(X+Y)\\).\n\n\nSolution. Let \\(U=\\frac{X}{X+Y}\\), \\(V=X+Y\\). Then \\(X=UV\\), \\(Y=V-UV\\). The determinant of the Jacobian matrix is \\[\\left|\\frac{\\partial(x,y)}{\\partial(u,v)}\\right|=\\left|\\begin{array}{cc}\nv & u\\\\\n-v & 1-u\n\\end{array}\\right|=v\\] Thus, the joint distribution of \\((U,V)\\) is \\[f_{UV}(u,v)=f_{XY}(x,y)|v|=f_{X}(x)f_{Y}(y)v=e^{-(x+y)}v=e^{-v}v.\\] The distribution of \\(X/(X+Y)\\) is equivalent to the marginal distribution of \\(U\\): \\[f_{U}(u)=\\int_{0}^{\\infty}e^{-v}vdv=1\\] for \\(0\\leq u\\leq1\\). Hence \\(U\\) is a Uniform distribution over \\([0,1]\\).",
    "crumbs": [
      "Continuous Distributions",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Transformation</span>"
    ]
  },
  {
    "objectID": "chapters/50_lln.html",
    "href": "chapters/50_lln.html",
    "title": "54  Law of large numbers",
    "section": "",
    "text": "We now introduce two important theorems describing the behavior of the sample mean as the sample size grows. Throughout this section and the next, we assume \\(X_{1},X_{2},\\dots,X_{n}\\) are i.i.d RVs drawn from a population with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\). The sample mean is defined as\n\\[\\bar{X}_{n}=\\frac{X_{1}+\\cdots+X_{n}}{n}.\\]\nAs we have discussed previously, the sample mean is itself a random variable with mean and variance:\n\\[\\begin{aligned}\nE(\\bar{X}_{n}) & =\\frac{1}{n}E(X_{1}+\\cdots+X_{n})=\\frac{1}{n}(E(X_{1})+\\cdots+E(X_{n}))=\\mu,\\\\\nVar(\\bar{X}_{n}) & =\\frac{1}{n^{2}}Var(X_{1}+\\cdots+X_{n})\\overset{iid}{=}\\frac{1}{n^{2}}(Var(X_{1})+\\cdots+Var(X_{n}))=\\frac{\\sigma^{2}}{n}.\\end{aligned}\\]\nThe law of large numbers (LLN) says that as n grows, the sample mean \\(\\bar{X}_{n}\\) converges to the true mean \\(\\mu\\).\n\nTheorem 54.1 (Strong law of large numbers). The sample mean \\(\\bar{X}_{n}\\) converges to the true mean \\(\\mu\\) point-wise as \\(n\\to\\infty\\), with probability \\(1\\). In other words, the event \\(\\bar{X}_{n}\\to\\mu\\) has probability \\(1\\).\n\n\nTheorem 54.2 (Weak law of large numbers). For all \\(\\epsilon&gt;0\\), \\(P(|\\bar{X}_{n}-\\mu|&gt;\\epsilon)\\to0\\) as \\(n\\to\\infty\\). (this is known as converge in probability).\n\nWe don’t need a rigorous proof here. But an intuitive proof is obvious. As \\(n\\to\\infty\\), \\(Var(\\bar{X}_{n})=\\frac{\\sigma^{2}}{n}\\to0\\). The random variable \\(\\bar{X}_{n}\\) becomes fixed at \\(\\mu\\) as \\(n\\) becomes large. Thus, it converges to \\(\\mu\\) in a probabilistic sense.\nIt seems that the LLN just states the obvious. But it has wide applications in daily time that you might not even realize. What it says is essentially this: the uncertainty at the individual level becomes certain when aggregating together; the risks that are unmanageable at the individual level becomes manageable collectively. Think about a rare disease, it happens at 1 out of a million probability. For each individual, no one knows if they will get the disease or not. But as the sample size gets large, suppose we have one billion population, the LLN says the sample mean will be very close the true mean. That is, there will be almost surely 1000 people being infected by the disease. We provide two more examples.\n\nExample 54.1 (Lottery). A lottery company is designing a game with a 6-digit format. Each time someone buys a ticket, they receive a randomly generated 6-digit number. Only one number will win the grand prize of 10 million dollars. What should the company charge per ticket to break even?\n\n\nSolution. The probability of winning the game is \\(p=1/10^{6}\\). Suppose the company has sold \\(n\\) tickets. The price for each ticket is \\(x\\). The revenue for the company is therefore \\(xn\\). By the LLN, the cost of the company should be very close to \\(10^{7}np\\). The break even point is \\(xn=10^{7}np\\). So \\(x=10^{7}p=10\\). Therefore, if the company sells each ticket above 10 dollars. The business is surely profitable as long as \\(n\\) is large. If the company is a monopoly, it can reap as much profit as it desires as long as they know the basic probability theory! The same can be said about gambling companies.\n\n\nExample 54.2 (Insurance). Insurance is anther great application of the LLN. It is essentially the same as the the lottery game but most people do not realize it. Suppose there is a disease with infection rate of 1 out of 1 million. The medical expenditure to cure the disease is 10 million dollars. How much the insurance company should charge per customer to cover this disease?\n\n\nSolution. The solution is essentially the same as above. Suppose the premium for the insurance product is \\(x\\). The revenue of the company by selling the premium is \\(xn\\). The cost is — when one customer is infected, the company has to pay the medical cost —\\(10^{7}np\\). The break even price for the insurance premium is thus \\(10\\) dollars.\nWhat is the implication of this insurance? Without the insurance, each individual either chooses to set aside 10 million dollars pre-cautiously for the disease (if he is rich enough) or be exposed to the risk completely uncovered. The insurance product enables everyone to get covered at a cost of just 10 dollars. It is a typical example that the unmanageable risk at the individual level becomes manageable collectively.",
    "crumbs": [
      "Sampling distributions",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Law of large numbers</span>"
    ]
  },
  {
    "objectID": "chapters/51_clt.html",
    "href": "chapters/51_clt.html",
    "title": "55  Central limit theorem",
    "section": "",
    "text": "The LLN shows the convergence of the sample mean to the population mean. What about the entire sample distribution? This is addressed by the central limit theorem (CLT), which, as its name suggests, is a limit theorem of central importance in statistics.\nThe CLT states that for large \\(n\\), the distribution of \\(\\bar{X}_{n}\\) after standardization approaches a standard Normal distribution, regardless of the underlying distribution of \\(X_{i}\\). By standardization, we mean that we subtract \\(\\mu\\), the expected value of \\(\\bar{X}_{n}\\), and divide by \\(\\sigma/\\sqrt{n}\\), the standard deviation of \\(\\bar{X}_{n}\\).\n\nTheorem 55.1 (Central limit theorem). As \\(n\\to\\infty\\), \\[\\sqrt{n}\\left(\\frac{\\bar{X}_{n}-\\mu}{\\sigma}\\right)\\to N(0,1)\\textrm{ in distribution.}\\]\nIn other words, the CDF of the left-hand side approaches the CDF of the standard normal distribution.\n\n\nProof. We will prove the CLT assuming the MGF of the \\(X_{i}\\) exists, though the theorem holds under much weaker conditions. Without loss of generality let \\(\\mu=1,\\sigma^{2}=1\\) (since we standardize it anyway). We show that the MGF of \\(\\sqrt{n}\\bar{X}_{n}=(X_{1}+\\cdots+X_{n})/\\sqrt{n}\\) converges to the MGF of the \\(N(0,1)\\).\nThe MGF of \\(N(0,1)\\) is\n\\[\\begin{aligned}\nE(e^{tX}) & =\\int_{-\\infty}^{\\infty}e^{tx}\\cdot\\frac{1}{\\sqrt{2\\pi}}e^{-x^{2}/2}dx\\\\\n& =\\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}}e^{-x^{2}/2+tx}dx\\\\\n& =\\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(x-t)^{2}+\\frac{1}{2}t^{2}}dx\\\\\n& =e^{\\frac{t^{2}}{2}}\\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(x-t)^{2}}dx\\\\\n& =e^{t^{2}/2}\\end{aligned}\\]\nCompute the MGF of \\(\\sqrt{n}\\bar{X}_{n}\\):\n\\[\\begin{aligned}\nE(e^{\\sqrt{n}\\bar{X}_{n}}) & =E(e^{t(X_{1}+\\cdots+X_{n})/\\sqrt{n}})\\\\\n& =E(e^{tX_{1}/\\sqrt{n}})E(e^{tX_{2}/\\sqrt{n}})\\cdots E(e^{tX_{n}/\\sqrt{n}})\\\\\n& =\\left[E(e^{tX_{i}/\\sqrt{n}})\\right]^{n}\\qquad\\textrm{since }i.i.d\\\\\n& =\\left[E\\left(1+\\frac{tX_{i}}{\\sqrt{n}}+\\frac{t^{2}X_{i}^{2}}{2n}+o(n^{-1})\\right)\\right]^{n}\\\\\n& =\\left[1+\\frac{t}{\\sqrt{n}}E(X_{i})+\\frac{t^{2}}{2n}E(X_{i}^{2})+o(n^{-1})\\right]^{n}\\\\\n& =\\left[1+\\frac{t^{2}}{2n}+o(n^{-1})\\right]^{n}\\\\\n& =\\left[1+\\frac{t^{2}/2}{n}+o(n^{-1})\\right]^{n}\\\\\n& \\to e^{t^{2}/2}\\qquad\\textrm{as }n\\to\\infty\\end{aligned}\\]\nTherefore, the MGF of \\(\\sqrt{n}\\bar{X}_{n}\\) approaches the MGF of the standard normal. Since MGF determines the distribution, the distribution of \\(\\sqrt{n}\\bar{X}_{n}\\) also approaches the standard normal distribution.\n\nThe CLT tells us about the limiting distribution of \\(\\bar{X}_{n}\\) as \\(n\\to\\infty\\). That means, we can reasonably approximate the distribution \\(\\bar{X}_{n}\\) with normal distribution when \\(n\\) is a finite large number. \\[\\bar{X}_{n}\\approx N(\\mu,\\sigma^{2}/n)\\quad\\textrm{for large }n.\\]\nThe Central Limit Theorem was first proved by Pierre-Simon Laplace in 1810. Let’s take a moment to admire the generality of this result. The distribution of the individual \\(X_{i}\\) can be anything in the world, as long as the mean and variance are finite. This does mean the distribution of \\(X_{i}\\) is irrelevant, however. If the distribution is fairly close to normal, the result would hold for smaller \\(n\\). If the distribution is far away from normal, it would take larger \\(n\\) to converge.\nThe CLT gives the distribution of the sample mean regardless of the underlying distribution. This allows to assess the “quality” of the sample mean — how close it is to the true mean. The LLN tells us the larger the sample, the closer the sample mean to the population mean. The CLT tells us the distribution of the sample mean for sample size \\(n\\). For smaller \\(n\\), the distribution is more spread-out (a normal distribution with large \\(\\sigma^{2}\\)); hence the uncertainty is huge, other values are more likely. For larger \\(n\\), the uncertainty is reduced, most values would be centered around the true mean. We will delve deeper into this when we get to hypothesis testing.\n\nExample 55.1 Suppose that a fair coin is tossed 900 times. Approximate the probability of obtaining more than 395 heads.\n\n\nSolution. Let \\(H=\\sum_{i=1}^{900}X_{i}\\) be the number of heads, where \\(X_{i}\\sim\\textrm{Bern}(\\frac{1}{2})\\). We could compute the probability by\n\\[P(H&gt;495)=\\sum_{k=496}^{900}\\binom{900}{k}\\left(\\frac{1}{2}\\right)^{k}\\left(\\frac{1}{2}\\right)^{900-k}\\]\nBut this is quite tedious. Because \\(n=900\\) is reasonably large, we can apply the CLT:\n\\[\\begin{aligned}\n\\frac{1}{n}\\sum_{i=1}^{900}X_{i} & \\sim N(\\mu,\\sigma^{2}/n)\\quad\\textrm{or}\\\\\n\\sum_{i=1}^{900}X_{i} & \\sim N(n\\mu,n\\sigma^{2})\\end{aligned}\\]\nWe know \\(\\mu=E(X_{i})=\\frac{1}{2}\\), \\(\\sigma^{2}=Var(X_{i})=\\frac{1}{4}\\). Thus \\(H\\sim N(450,225)\\). Therefore,\n\\[P(H&gt;495)=1-P(H\\leq495)\\approx1-\\Phi\\left(\\frac{495-450}{15}\\right)=0.0013.\\]",
    "crumbs": [
      "Sampling distributions",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Central limit theorem</span>"
    ]
  },
  {
    "objectID": "chapters/52_samples.html",
    "href": "chapters/52_samples.html",
    "title": "56  Samples and statistics",
    "section": "",
    "text": "We model real-world uncertain events with random variables. We have also introduced various distributions suitable to model different kinds of events. However, we never observe the full distribution or the true parameters of the assumed distribution. Instead, we only observe a sample of that random variable. We can only infer the properties of the distribution from a limited sample. For example, suppose we model the height of an Asian women with a normal distribution. But we never know exactly what the mean and variance are. We can only observe a sample of the distribution.\nIn statistics, the conceptual distribution \\(F\\) is called the population distribution, or just the population.1 It is tempting to think of the population as all the observations (e.g. all the population on the planet), but this is not exactly correct. The population distribution is more of a mathematical abstraction or an assumption. Suppose we are modeling the height of human being, even if we have all the observations on the planet, that does not include the people that have died or yet to be born. Thus, it is still a sample of the assumed distribution.\nA collection of random variables \\(\\{X_{1},X_{2},\\dots,X_{n}\\}\\) is a random sample from the population \\(F\\) if \\(X_{i}\\) are independent and identically distributed (i.i.d) with distribution \\(F\\). What we mean by i.i.d is that \\(X_{1},\\dots,X_{n}\\) are mutually independent and have exactly the same distribution \\(X_{i}\\sim F\\). Survey sampling is an useful metaphor to understand random sampling, in which we randomly select a subset of the population with equal probability. The sample size \\(n\\) is the number of individuals in the sample.\nA data set is a collection of numbers, typically organized by observation. We sometimes call a data set also as a sample. But it should not be confused with the random sample defined above. As the former is a collection of random variables, whereas the latter is one realization of the random variables.\nTypically, we will use \\(X\\) without the subscript to denote a random variable or vector with distribution \\(F\\), \\(X_{i}\\) with a subscript to denote a random observation in the sample, and \\(x_{i}\\) or \\(x\\) to denote a specific or realized value.\nThe problem of statistical inference is to learn about the underlying process — the population distribution or data generating process — by examining the observations. In most cases, we assume the population distribution and want to learn about the its parameters (e.g. \\(\\mu\\) and \\(\\sigma^{2}\\) in the normal distribution). As a convention, we use greek letters to denote population parameters.\nA statistic is a function of the random sample \\(\\{X_{1},X_{2},\\dots,X_{n}\\}\\). Recall that there is a distinction between random variables and their realizations. Similarly there is a distinction between a statistic as a function of a random sample — and is therefore a random variable as well — and a statistic as a function of the realized sample, which is a realized value. When we treat a statistic as random we are viewing it is a function of a sample of random variables. When we treat it as a realized value we are viewing it as a function of a set of realized values. One way of viewing the distinction is to think of “before viewing the data” and “after viewing the data”. When we think about a statistic “before viewing” we do not know what value it will take. From our vantage point it is unknown and random. After viewing the data and specifically after computing and viewing the statistic the latter is a specific number and is therefore a realization. It is what it is and it is not changing. The randomness is the process by which the data was generated — and the understanding that if this process were repeated the sample would be different and the specific realization would be therefore different. The distribution of a statistic is called the sampling distribution, since it is the distribution induced by sampling.\nAn estimator \\(\\hat{\\theta}\\) for a population parameter \\(\\theta\\) is a statistic intended to infer \\(\\theta\\). It is conventional to use the hat notation \\(\\hat{\\theta}\\) to denote an estimator. Note that \\(\\hat{\\theta}\\) is a statistic and hence also a random variable. We call \\(\\hat{\\theta}\\) an estimate when it is a specific value (or realized value) calculated in a specific sample.\nA standard way to construct an estimator is by the analog principle. The idea is to express the parameter \\(\\theta\\) as a function of the population \\(F\\), and then express the estimator \\(\\hat{\\theta}\\) as the analog function in the sample.\nFor example, suppose we want to construct an estimator for the population mean \\(\\mu=E(X)\\). By definition, if each value of \\(X\\) is of equal probability, \\(\\mu\\) is simply the average. By analogy, we construct the sample mean as \\(\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\). It is conventional to denote a sample average by the notation “X bar”. Because it is an estimator for \\(\\mu\\), we also denote it as \\(\\hat{\\mu}=\\bar{X}_{n}\\). Note that from different samples we calculate different estimates. In one sample, \\(\\hat{\\mu}=6.5\\); in another sample, \\(\\hat{\\mu}=6.7\\). All of them are erroneous estimate of the true parameter \\(\\mu\\). The question is therefore how close they are to the true parameter. To answer this question, we need to study the distribution of the sample mean.",
    "crumbs": [
      "Sampling distributions",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Samples and statistics</span>"
    ]
  },
  {
    "objectID": "chapters/52_samples.html#footnotes",
    "href": "chapters/52_samples.html#footnotes",
    "title": "56  Samples and statistics",
    "section": "",
    "text": "This section is based on Bruce Hansen’s Probability and Statistics for Economists.↩︎",
    "crumbs": [
      "Sampling distributions",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Samples and statistics</span>"
    ]
  },
  {
    "objectID": "chapters/53_estimator.html",
    "href": "chapters/53_estimator.html",
    "title": "57  Estimator accuracy",
    "section": "",
    "text": "The central question of statistics is we want to learn about the population from a finite sample. We know sample mean is different from the population mean. But we also want to know how large the error could be, that is, how far or close the sample mean is from the true population mean. The question is exceedingly difficult to answer because the population mean is unknown. Fortunately, with the help of the CLT, we can say more about the distribution of the sample mean. This chapter bridges our probability theory with statistics. We use the theorems we have derived to infer the properties of a statistic.\nAs the purpose of statistics is to learn about the population, we want our sample estimator to be as good as possible. But what is a “good” estimator? This section we discuss two properties that we usually demand from a good estimator, namely, unbiased and consistency. Next section will tackle the more challenging concept of confidence interval.\n\nDefinition 57.1 The bias of an estimator \\(\\hat{\\theta}\\) of a parameter \\(\\theta\\) is \\[\\textrm{Bias}[\\hat{\\theta}]=E(\\hat{\\theta})-\\theta.\\] We say that an estimator is biased if its sampling is incorrectly centered. We say that an estimator is unbiased is the bias is zero.\n\n\nTheorem 57.1 \\(\\bar{X}_{n}\\) is unbiased for \\(\\mu=E(x)\\) if \\(E(X)&lt;\\infty\\).\n\n\nProof. \\[E(\\bar{X}_{n})=E\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right)=\\frac{1}{n}\\sum_{i=1}^{n}E(X_{i})=\\frac{1}{n}\\sum_{i=1}^{n}\\mu=\\mu.\\]\n\n\nTheorem 57.2 If \\(\\hat{\\theta}\\) is an unbiased estimator of \\(\\theta\\), then \\(\\hat{\\beta}=a\\hat{\\theta}+b\\) is an unbiased estimator of \\(\\beta=a\\theta+b\\).\n\nBut obtaining an unbiased estimator is not always as straightforward as it seems. Consider the sample variance as an estimator for the population variance. By the analog principle, the sample variance should be\n\\[\\begin{aligned}\n\\hat{\\sigma}^{2} & =\\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\bar{X_{n}})^{2}\\\\\n& =\\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\mu+\\mu-\\bar{X_{n}})^{2}\\\\\n& =\\frac{1}{n}\\sum(X_{i}-\\mu)^{2}+\\frac{2}{n}\\sum(X_{i}-\\mu)(\\mu-\\bar{X_{n}})+\\frac{1}{n}\\sum(\\mu-\\bar{X_{n}})^{2}\\\\\n& =\\frac{1}{n}\\sum(X_{i}-\\mu)^{2}+2(\\bar{X_{n}}-\\mu)(\\mu-\\bar{X_{n}})+(\\mu-\\bar{X_{n}})^{2}\\\\\n& =\\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\mu)^{2}-(\\bar{X}_{n}-\\mu)^{2}\\\\\n& =\\tilde{\\sigma}^{2}-(\\bar{X}_{n}-\\mu)^{2}\\end{aligned}\\]\nWe know that \\[E(\\tilde{\\sigma}^{2})=\\frac{1}{n}\\sum_{i=1}^{n}E(X_{i}-\\mu)^{2}=\\sigma^{2}\\] Thus, if we compute the bias of this estimator:\n\\[\\begin{aligned}\nE[\\hat{\\sigma}^{2}] & =\\sigma^{2}-\\frac{\\sigma^{2}}{n}=\\left(1-\\frac{1}{n}\\right)\\sigma^{2}\\\\\n\\textrm{Bias}[\\hat{\\sigma}^{2}] & =-\\frac{\\sigma^{2}}{n}\\neq0\\end{aligned}\\]\nTherefore, the estimator \\(\\hat{\\sigma}^{2}\\) is a biased estimator for \\(\\sigma^{2}\\)! To correct the bias, we divide the sample sum of squares by \\((n-1)\\).\n\\[s^{2}=\\frac{n}{n-1}\\hat{\\sigma}^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i}-\\bar{X_{n}})^{2}.\\]\nIt is straightforward to see that \\(s^{2}\\) is an unbiased estimator for \\(\\sigma^{2}\\). We call \\(s^{2}\\) the bias-corrected variance estimator.\n\nTheorem 57.3 \\(s^{2}\\) is an unbiased estimator for \\(\\sigma^{2}\\) if \\(E(X^{2})&lt;\\infty\\).\n\n\nDefinition 57.2 The mean square error of an estimator \\(\\hat{\\theta}\\) for \\(\\theta\\) is\n\\[\\textrm{MSE}[\\hat{\\theta}]=E\\left[(\\hat{\\theta}-\\theta)^{2}\\right].\\]\n\nBy expanding the square we find that\n\\[\\begin{aligned}\n\\textrm{MSE}[\\hat{\\theta}] & =E\\left[(\\hat{\\theta}-\\theta)^{2}\\right]\\\\\n& =E\\left[(\\hat{\\theta}-E[\\hat{\\theta}]+E[\\hat{\\theta}]-\\theta)^{2}\\right]\\\\\n& =E\\left[(\\hat{\\theta}-E[\\hat{\\theta}])^{2}\\right]+2E(\\hat{\\theta}-E[\\hat{\\theta}])(E[\\hat{\\theta}]-\\theta)+(E[\\hat{\\theta}]-\\theta)^{2}\\\\\n& =Var[\\hat{\\theta}]+(\\textrm{Bias}[\\hat{\\theta}])^{2}.\\end{aligned}\\]\nThus the MSE is the variance plus the squared bias. The MSE as a measure of accuracy combines the variance and bias.\n\nTheorem 57.4 For any estimator with a finite variance, we have \\[\\textrm{MSE}[\\hat{\\theta}]=Var[\\hat{\\theta}]+(\\textrm{Bias}[\\hat{\\theta}])^{2}.\\]\n\n\nDefinition 57.3 An estimator is consistent if \\(\\textrm{MSE}[\\hat{\\theta}]\\to0\\) as \\(n\\to\\infty\\).\n\nBias is the property of an estimator for finite samples. Consistency is the property of an estimator when the sample size gets large. It means that for any given data distribution, there is a sample size \\(n\\) sufficiently large such that the estimator \\(\\hat{\\theta}\\) will be arbitrarily close to the true value \\(\\theta\\) with high probability. In practice, we usually do not know how large this \\(n\\) has to be. But it is a desirable property for an estimator to be considered a “good” estimator.\nFor unbiased estimator, MSE is solely determined by the variance of the estimator. Recall that the variance for the sample mean is \\(Var(\\bar{X}_{n})=\\sigma^{2}/n\\). But this is not a very useful formula because the it depends on unknown parameter \\(\\sigma^{2}\\). We need to replace these unknown parameters by estimators. To put the latter in the same units as the parameter estimate we typically take the square root before reporting. We thus arrive at the following concept.\n\nDefinition 57.4 A standard error of an estimator \\(\\hat{\\theta}\\) is defined as \\[SE(\\hat{\\theta})=\\hat{V}^{1/2}\\] where \\(\\hat{V}\\) is the estimator for \\(Var[\\hat{\\theta}]\\).\n\n\nDefinition 57.5 The standard error for \\(\\bar{X}_{n}\\) is \\[SE(\\bar{X}_{n})=\\frac{s}{\\sqrt{n}}\\] where \\(s\\) is the bias-corrected estimator for \\(\\sigma\\).\n\nNote the difference between standard error and standard deviation. Standard deviation describes the dispersion of a distribution. Standard error is the standard deviation of an estimator. It indicates the “precision” of the estimator, thereby carrying a sense of “error”. The smaller the standard error, the more precise the estimator.",
    "crumbs": [
      "Sampling distributions",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Estimator accuracy</span>"
    ]
  },
  {
    "objectID": "chapters/54_ci.html",
    "href": "chapters/54_ci.html",
    "title": "58  Confidence intervals",
    "section": "",
    "text": "Confidence intervals provide a method of adding more information to an estimator \\(\\hat{\\theta}\\) when we wish to estimate an unknown parameter \\(\\theta\\). We can find an interval \\((A,B)\\) that we think has high probability of containing \\(\\theta\\). The length of such an interval gives us an idea of how closely we can estimate \\(\\theta\\).\n\nDefinition 58.1 A \\(100(1-\\alpha)\\%\\) confidence interval (CI) for \\(\\theta\\) is an interval \\([L(\\theta),U(\\theta)]\\) such that the probability that the interval contains the true \\(\\theta\\) is \\((1-\\alpha)\\).\n\nDue to randomness we rarely seek a confidence interval with 100% coverage as this would typically need to be the entire parameter space. Instead we seek an interval which includes the true value with reasonably high probability. Standard choices are \\(\\alpha=0.05\\) and \\(0.10\\), corresponding to 95% and 90% confidence.\nConfidence intervals are reported to indicate the degree of precision of our estimates. The narrower the confidence interval, the more precise the estimate. Because a small range of values contains the true parameter with high probability.\nWith the help of the CLT, it is not hard to find the CI for the sample mean \\(\\bar{X}_{n}\\). Let’s set \\(\\alpha=5\\%\\), that is, we are trying to find the CI that contains the true mean 95% of the times. Assume our sample size \\(n\\) is large enough to invoke the CLT, we thus have\n\\[\\begin{aligned}\n\\frac{\\bar{X}_{n}-\\mu}{\\sigma/\\sqrt{n}} & \\sim N(0,1)\\end{aligned}\\] Let’s find the interval \\([a,b]\\) such that \\[P\\left(a\\leq\\frac{\\bar{X}_{n}-\\mu}{\\sigma/\\sqrt{n}}\\leq b\\right)=1-2\\Phi(L)=0.95\\]\nsince the normal distribution is symmetric, \\(b=-a\\). By looking at the CDF of standard normal, we get \\(a=-1.96\\), \\(b=1.96\\). Thus,\n\\[P\\left(-1.96\\leq\\frac{\\bar{X}_{n}-\\mu}{\\sigma/\\sqrt{n}}\\leq1.96\\right)=0.95\\]\nWith a little rearrangement, we have\n\\[P\\left(\\bar{X}_{n}-1.96\\frac{\\sigma}{\\sqrt{n}}\\leq\\mu\\leq\\bar{X}_{n}+1.96\\frac{\\sigma}{\\sqrt{n}}\\right)=0.95\\] Therefore, the interval \\(\\left[\\bar{X}_{n}-1.96\\frac{\\sigma}{\\sqrt{n}},\\bar{X}_{n}+1.96\\frac{\\sigma}{\\sqrt{n}}\\right]\\) contains the true mean 95% of the times.\n\nTheorem 58.1 The \\(100(1-\\alpha)\\%\\) confidence interval for the sample mean \\(\\bar{X}_{n}\\) is \\(\\bar{X}_{n}\\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\), where \\(z_{\\alpha/2}\\) is the critical value such that \\(\\Phi(z_{\\alpha/2})=\\frac{\\alpha}{2}\\).\n\nIn practice, because we do not know \\(\\sigma/\\sqrt{n}\\), we replace it with the standard error \\(s/\\sqrt{n}\\). Thus, we compute the confidence interval as \\(\\bar{X}_{n}\\pm z_{\\alpha/2}SE\\). However, this replacement is not without risk. When the sample size is small, \\(s\\) is a very poor estimate of \\(\\sigma\\). For the approximation to be valid, we require either the sample size is large enough (\\(n\\geq30\\) at least) or the population distribution is nearly normal. Some commonly used confidence levels:\n\n90% CI: \\(\\alpha=0.1\\), \\(z_{0.05}=1.645\\)\n95% CI: \\(\\alpha=0.05\\), \\(z_{0.025}=1.96\\)\n99% CI: \\(\\alpha=0.01\\), \\(z_{0.005}=2.58\\)\n\nWe go through some common misunderstandings about confidence intervals through an example. Suppose we have a sample fo size 50 with mean 3.2 and standard deviation 1.74. We construct the 95% confidence interval as \\[\\bar{X}\\pm1.96\\times\\frac{1.74}{\\sqrt{50}}\\approx3.2\\pm0.5=(2.7,3.7).\\]\nNow check the following interpretations (true or false):\n\nWe are 95% confident that the sample mean is between 2.7 and 3.7.\nFalse. The CI definitely contains the sample mean \\(\\bar{X}\\).\n95% of the population observations are in 2.7 to 3.7.\nFalse. The CI is about covering the population mean, not for covering 95% of the entire population.\nThe true mean falls in the interval (2.7, 3.7) with probability 95%.\nFalse. The true mean \\(\\mu\\) is a fixed number, not a random one that happens with a probability.\nIf a new random sample is taken, we are 95% confident that the new sample mean will be between 2.7 and 3.7.\nFalse. The confidence interval is for covering the population mean, not for covering the mean of another sample.\nThis confidence interval is not valid if the population or sample is not normally distributed.\nFalse. The construction of the CI only uses the normality of the sampling distribution of the sample mean (by the CLT). Neither the population nor the sample is required to be normally distributed.\n\nSo what is exactly the thing that has a 95% change to happen? It is the procedure to construct the 95% interval. About 95% of the intervals constructed following the procedure will cover the true population mean \\(\\mu\\). After taking the sample and an interval is constructed, the constructed interval either covers \\(\\mu\\) or it doesn’t. But if we were able to take many such samples and reconstruct the interval many times, 95% of the intervals will contain the true mean.",
    "crumbs": [
      "Sampling distributions",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "chapters/55_ht.html",
    "href": "chapters/55_ht.html",
    "title": "59  Hypothesis testing",
    "section": "",
    "text": "Confidence interval allows us to construct an interval estimate of a population parameter. Hypothesis testing allows us to test specific hypothesis about a population parameter with the evidence obtained from a sample. The earliest use of statistical hypothesis testing is generally credited to the question of whether male and female births are equally likely (null hypothesis), which was addressed in the 1700s by John Arbuthnot and later by Pierre-Simon Laplace.\nLet \\(p\\) be the population ratio (defined as the ratio of boys to the total number of babies). We hypotheses that \\[H_{0}:p=0.5\\] This is called the null hypothesis, which is the hypothesis we want to test. If the null hypothesis is false, we have \\[H_{1}:p\\neq0.5\\] This is called the alternative hypothesis. How am I able to test which hypothesis is true? I can answer this question by collecting a small sample. Suppose I have collected a sample of \\(50\\) babies computed a sample ratio of \\(\\hat{p}=0.55\\). Does it prove or disprove the hypothesis?\nNote that the ratio \\(\\hat{p}\\) can be regarded as a sample mean. Let \\(X_{i}\\) be a random variable that equals \\(1\\) if the \\(i\\)-th baby is a boy and \\(0\\) otherwise. Then, \\(\\hat{p}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\). The variance of \\(\\hat{p}\\) is given by \\[Var(\\hat{p})=\\frac{1}{n^{2}}\\sum_{i=1}^{n}Var(X_{i})=\\frac{p(1-p)}{n}\\] since \\(X_{i}\\) is a Bernoulli random variable. By the Central Limit Theorem, we have \\[\\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{n}}}\\to N(0,1)\\] Suppose \\(H_{0}\\) is true, then we know the distribution of \\(\\hat{p}\\). In particular, there is 95% chance that \\(\\hat{p}\\) would be in the interval \\[p\\pm1.96\\sqrt{\\frac{p(1-p)}{n}}=0.5\\pm0.14\\] Our observed sample mean \\(\\hat{p}=0.55\\) is not outrageous. It is well within this interval. That means the evidence is not against the null hypothesis. It does not mean \\(H_{0}\\) is true. But it is reasonable given we have observed a sample mean \\(\\hat{p}=0.55\\).\nSuppose we have observed \\(\\hat{p}=0.65\\). This piece of evidence does not seem to be consistent with the null hypothesis. Because if \\(H_{0}\\) is true, we only have less than 5% chance of observing this sample mean. It is extremely unlikely. Based on this sample, we are more inclined to reject the \\(H_{0}\\). Rejecting the null hypothesis does not mean it is false, but it means our evidence does not support this hypothesis.\n\n\n\n\n\n\\(p\\)-value: the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct. A very small \\(p\\)-value means that such an extreme observed outcome would be very unlikely under the null hypothesis. Thus, The smaller the \\(p\\)-value, the stronger the evidence against the \\(H_{0}\\).\nIn some studies, we can simply report the \\(p\\)-value and let people judge whether the evidence is strong enough. In other studies, we prefer to select a cut-off value \\(\\alpha\\), call the significance level, and follow the rule:\n\nIf the \\(p\\textrm{-value}&lt;\\alpha\\), reject \\(H_{0}\\);\nIf the \\(p\\textrm{-value}&gt;\\alpha\\), do not reject \\(H_{0}\\).\n\nCommonly used significance levels: \\(0.05\\) and \\(0.01\\). And we like to use the word “significant” to describe the test result:\n\nA test with \\(p\\textrm{-value}&lt;0.05\\) is said to be (statistically) significant;\nA test with \\(p\\textrm{-value}&lt;0.01\\) is said to be highly significant.\n\nWhen we make a decision about accepting or rejecting a hypothesis, there are chances that we make a mistake. There are two types of mistakes: Type 1 error and Type 2 error.\n\n\n\n\n\nDecision\n\n\n\n\n\nReject \\(H_{0}\\)\nFail to reject \\(H_{0}\\)\n\n\n\n\\(H_{0}\\) is true\nType 1 error\n\\(\\checkmark\\)\n\n\n\n\\(H_{0}\\) is false\n\\(\\checkmark\\)\nType 2 error\n\n\n\nType 1 error is rejecting the \\(H_{0}\\) when it is true. Type 2 error is failing to reject the \\(H_{0}\\) when it is false. Usually, it is more important to control the Type 1 error than the the Type 2 error. That is, we want to minimize the chance of falsely rejecting the null hypothesis.\nIn the example above, we reject the null hypothesis on the ground that there is only 2.3% of the chance that we could observe this sample. Therefore, the probability of Type 1 error is only 2.3%.\nIf we make decisions based on a significance level, the significance level is the Type 1 error rate. In other words, when using a 5% significance level, there is 5% chance of making a Type 1 error.\n\\[P(\\textrm{Type 1 error}|H_{0}\\textrm{ is true})=\\alpha\\]\nThis is why we prefer small values of \\(\\alpha\\)—smaller \\(\\alpha\\) reduces the Type 1 error rate. However, significance level doesn’t control Type 2 error rate.\n\nHypothesis testing with \\(z\\)-statistics\nWe may have noticed that, in the above example, the assumption that the population \\(\\sigma\\) is known is unrealistic. In practice, we approximate it with the standard error \\(s/\\sqrt{n}\\). The approximate is valid if the the sample size is large enough or the underlying distribution is nearly normal. If this is not the case, we would opt for a \\(t\\)-test. Here we summarize the steps of testing for a population mean with \\(z\\)-statistics.\nWe notice that the two-sided hypothesis tests are very closed related to the concept of confidence intervals. A two-sided test means we are interested in rejection regions on both sides of the tail distribution. Typically, the alternative hypothesis is \\(H_{1}:\\mu\\neq\\mu_{0}\\).\n\n\n\n\n\nSuppose we are doing a hypothesis test under the significance level \\(\\alpha\\), the region of accepting the \\(H_{0}\\) is\n\\[-z_{\\alpha/2}\\leq\\frac{\\bar{X}-\\mu}{SE}\\leq z_{\\alpha/2}\\]\nsuch that the rejection region (\\(p\\)-value) has probability \\(\\alpha\\). This is equivalent to \\[\\bar{X}-z_{\\alpha/2}SE\\leq\\mu\\leq\\bar{X}+z_{\\alpha/2}SE\\]\nwhich is exactly the \\(100(1-\\alpha)\\%\\) confidence interval of \\(\\bar{X}\\). Therefore, for a two-sided test, we have the rule:\n\nReject \\(H_{0}\\) if \\(\\mu\\) is not in the \\(100(1-\\alpha)\\%\\) CI: \\(\\bar{X}\\pm z_{\\alpha/2}SE\\)\n\nWe conclude this chapter by reiterating a couple of critical points that could be easily misunderstood.\nRejecting \\(H_{0}\\) doesn’t means we are 100% sure that \\(H_{0}\\) is false. We might make Type 1 errors. Setting a significance level just guarantee we won’t make Type 1 error too often.\nFailing to reject \\(H_{0}\\) does not necessarily mean \\(H_{0}\\) is true. We could make a type 2 error when failing to reject \\(H_{0}\\). Moreover, unlike type 1 error rate is controlled at a low level, type 2 error rate is usually quite high. When we fail to reject \\(H_{0}\\), it just means the data are not able to distinguish between \\(H_{0}\\) and \\(H_{1}\\). That’s why we say fail to reject. \\(p\\)-value is not the probability that the \\(H_{0}\\) is true.\nSaying that results are statistically significant just informs the reader that the findings are unlikely due to chance alone. However, it says nothing about the practical importance of the finding. For example, rejecting the \\(H_{0}\\): \\(\\mu=\\mu_{0}\\) does not tell us how big the difference \\(|\\mu-\\mu_{0}|\\) is. Mostly in practice we care more about the magnitude of this difference, rather than the fact that they are indeed different. It is possible that the difference is too small to be relevant even if it is significant.\n\n\nHypothesis testing with \\(t\\)-statistics\nWhen the sample size is small, we opt for \\(t\\)-test for more reliable hypothsis testing. Define test statistics \\[T=\\frac{\\bar{X}-\\mu}{s/\\sqrt{n}}\\] where \\(s\\) is the sample standard deviation. For small samples, this test statistics follows a Student \\(t\\)-distribution with \\(n\\) degrees of freedom, \\(T\\sim t(n)\\).\nWhy Student-\\(t\\) distribution? Recall the definition of Student-\\(t\\) distribution: when the underlying distribution of \\(X_{1},X_{2},\\dots,X_{n}\\) is Normal, sample variance \\(s^{2}\\) follows a \\(\\chi^{2}\\) distribution. \\(T\\) follows \\(t\\) distribution by definition regardless of the sample size. However, if the underlying distribution is not normal, this argument loses ground. We use \\(t\\)-test mainly as a convention. But \\(t\\) distribution has heavier tails than standard normal, meaning that we are more likely to reject a hypothesis based on \\(t\\) distribution. In other words, \\(t\\)-test is a more conservative choice than \\(z\\)-test for small samples.\n\n\n\none-tail \\(\\alpha\\)\n0.05\n0.025\n0.005\n\n\ntwo-tail \\(\\alpha\\)\n0.10\n0.05\n0.01\n\n\nd.f.\n\n\n\n\n\n10\n1.812\n2.228\n3.169\n\n\n20\n1.725\n2.086\n2.845\n\n\n30\n1.697\n2.042\n2.750\n\n\n\\(z\\) value\n1.645\n1.960\n2.576\n\n\n\nThe table shows a few critical values for \\(t\\)-test with different degrees of freedom (d.f.). We can see as the sample size gets larger, \\(t\\) distribution converges to standard normal.",
    "crumbs": [
      "Sampling distributions",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  }
]