Chains with finitely many states Exercises [6.5.4]-[6.6.3]

fori # j, and where £ is a constant satisfying 0 < 6 < 1. The diagonal terms g;; are arranged so that
Q is a stochastic matrix. Show that Y is reversible in equilibrium, and find its stationary distribution.
Describe the situation in the limit as B | 0.

4. Cana reversible chain be periodic?
5. Ehrenfest dog-flea model. The dog—flea model of Example (6.5.5) is a Markov chain X on the
state space {0, 1,..., m} with transition probabilities

i i .
Pitt =1-—, P-L =—, for O<i<m.
m m

Show that, if Xo = i,

6. Which of the following (when stationary) are reversible Markov chains?
(a) The chain X = {X,} having transition matrix P = ( 1 3 a ; ° 5) where a + B > 0.

0 Pp 1-—p

(b) The chain Y = {Y,} having transition matrix P = | 1 — p 0 Pp where 0 < p < 1.
p 1l-—p 0

(C) Zn = (Xn, Yn), where X, and Y, are independent and satisfy (a) and (b).

7. Let Xn, Yn be independent simple random walks. Let Z, be (Xn, Y,) truncated to lie in the
region Xy, > 0, Yy = 0, Xn + Yn < a where a is integral. Find the stationary distribution of Zp.

8. Show that an irreducible Markov chain with a finite state space and transition matrix P is reversible
in equilibrium if and only if P = DS for some symmetric matrix S$ and diagonal matrix D with strictly
positive diagonal entries. Show further that for reversibility in equilibrium to hold, it is necessary but
not sufficient that P has real eigenvalues.

9. Random walk on a graph. Let G be a finite connected graph with neither loops nor multiple
edges, and let X be arandom walk on G as in Exercise (6.4.6). Show that X is reversible in equilibrium.

6.6 Exercises. Chains with finitely many states

The first two exercises provide proofs that a Markov chain with finitely many states has a stationary
distribution.
1. The Markov—Kakutani theorem asserts that, for any convex compact subset C of R” and any
linear continuous mapping T of C into C, T has a fixed point (in the sense that T(x) = x for some
x €C). Use this to prove that a finite stochastic matrix has a non-negative non-zero left eigenvector
corresponding to the eigenvalue 1.
2. LetT beam xn matrix and let v ¢ R”. Farkas’s theorem asserts that exactly one of the following
holds:

(i) there exists x € R™ such that x > 0 and xT =v,

(ii) there exists y € IR” such that yv’ < 0 and Ty’ > 0.
Use this to prove that a finite stochastic matrix has a non-negative non-zero left eigenvector corre-
sponding to the eigenvalue 1.

3. Arbitrage. Suppose you are betting on arace with m possible outcomes. There are n bookmakers,
and a unit stake with the ith bookmaker yields ;; if the jth outcome of the race occurs. A vector

69
