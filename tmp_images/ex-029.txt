[3.6.6]-[3.7.2] Exercises Discrete random variables

(a) Show that E(log fy(X)) > Edog fy(X)).
(b) Show that the mutual information

far \)
= l —_———
/ B (108 |

satisfies 7 > 0, with equality if and only if X and Y are independent.

6. Voter paradox. Let X, Y, Z be discrete random variables with the property that their values are
distinct with probability 1. Leta = P(X > Y),b=P(Y > Z),c =P(Z > X).

(a) Show that min{a, b, c} < , and give an example where this bound is attained.

(b) Show that, if X, Y, Z are independent and identically distributed, then a = b = c = 7:

(c) Find min{a, b, c} and SUP» min{a, b,c} when P(X = 0) = 1, and Y, Z are independent with
P(Z = 1) = P(Y = -1) = p, P(Z = —2) = P(YY = 2) = 1 — p. Here, SUP, denotes the
supremum as p varies over [0, 1].

[Part (a) is related to the observation that, in an election, it is possible for more than half of the voters

to prefer candidate A to candidate B, more than half B to C, and more than half C to A.]

7. Benford’s distribution, or the law of anomalous numbers. If one picks a numerical entry at
random from an almanac, or the annual accounts of a corporation, the first two significant digits, X,
Y, are found to have approximately the joint mass function

FCs, y) = logo (1+ ), 1<x<9,0<y<9.

10x + y

Find the mass function of X and an approximation to its mean. [A heuristic explanation for this
phenomenon may be found in the second of Feller’s volumes (1971).]

8. Let X and Y have joint mass function

i+ kai tk
FU. = Re k=O,
jie

where a is a constant. Find c, P(X = j), P(X + Y =r), and E(X).

3.7 Exercises. Conditional distributions and conditional expectation

1. Show the following:

(a) E(@Y +bZ |X) = aE(Y | X)+bE(Z | X) fora,be R,

(b) E(Y | X) > Oif Y > 0,

(c) EQ {| X)=1,

(d) if X and Y are independent then E(Y | X) = E(Y),

(e) (‘pull-through property’) E(Y 2(X) | X) = g(X)E(¥ | X) for any suitable function g,

(f) (‘tower property’) E{E(Y | X, Z) | X} = E(Y | X) = Ef{E(Y | X) | X, Z}.
2. Uniqueness of conditional expectation. Suppose that X and Y are discrete random variables,
and that #(X) and y(X) are two functions of X satisfying

E($(X)g(X)) = E(y(X)g(X)) = E(¥g(X))

for any function g for which all the expectations exist. Show that #(X) and y(X) are almost surely
equal, in that P(@(X) = w(X)) = 1.

20

