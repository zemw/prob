{
  "hash": "031d56ad8ec0310d920fea0aed3cb684",
  "result": {
    "engine": "knitr",
    "markdown": "# Expectation revisited\n\n::: {#def-exp}\nFor discrete random variable $X$, the expectation of $X$ is defined as\n$$E(X)=\\sum_{\\textrm{all }x}xP(X=x);$$For continuous random variable $X$\nwith density function $f(x)$, the expectation is defined as\n$$E(X)=\\int_{-\\infty}^{\\infty}x\\ f(x)\\ dx.$$\n:::\n\n::: {#prp-exp-lin}\n### Linearity\n\nFor random variables $X_1,X_2,\\dots,X_n$, regardless of their\ndependencies, it holds that\n\n$$E(X_{1}+\\cdots+X_{n})=E(X_{1})+\\cdots+E(X_{n}).$$\n:::\n\n::: proof\nWe prove the simplest case $E(X+Y)=E(X)+E(Y)$. $$\\begin{aligned}\nE(X+Y) & =\\sum_{z=x+y}zP(X+Y=z)\\\\\n & =\\sum_{x}\\sum_{y}(x+y)P(X=x,Y=y)\\\\\n & =\\sum_{x}\\sum_{y}xP(X=x,Y=y)+\\sum_{x}\\sum_{y}yP(X=x,Y=y)\\\\\n & =\\sum_{x}x\\sum_{y}P(X=x,Y=y)+\\sum_{y}y\\sum_{x}P(X=x,Y=y)\\\\\n & =\\sum_{x}xP((X=x)\\cap\\bigcup_{\\textrm{all }y}(Y=y))+\\sum_{y}yP(\\bigcup_{\\textrm{all }x}(X=x)\\cap(Y=y))\\\\\n & =\\sum_{x}xP(X=x)+\\sum_{y}yP(Y=y)\\\\\n & =E(X)+E(Y).\n \\end{aligned}$$\n:::\n\n::: {#prp-exp-lin2}\nFurther properties on the linearity of expectations:\n\n-   If $Y=aX+b$, then $E(Y)=aE(X)+b$.\n\n-   $E(a_{1}X_{1}+\\cdots+a_{n}X_{n}+b)=a_{1}E(X_{1})+\\cdots+a_{n}E(X_{n})+b$\n:::\n\n::: {#prp-exp-mult}\n### Multiplication\n\nIf $X$ and $Y$ are independent, we have $$E(XY)=E(X)E(Y).$$ In general,\nif $X_{1},\\ldots,X_{n}$ are independent, we have\n$$E(X_{1}X_{2}\\cdots X_{n})=E(X_{1})E(X_{2})\\cdots E(X_{n}).$$\n:::\n\n::: proof\nFor discrete and independent $X,Y$, $$\\begin{aligned}\nE(XY) & =\\sum_{x}\\sum_{y}xyP(X=x,Y=y)\\\\\n & =\\sum_{x}\\sum_{y}xyP(X=x)P(Y=y)\\quad\\textrm{if independent}\\\\\n & =\\sum_{x}xP(X=x)\\sum_{y}yP(Y=y)\\\\\n & =E(X)E(Y).\\end{aligned}$$\n:::\n\n::: callout-caution\n### Multiplication does not hold without independence\n\nIt is misleadingly natural to extend the generality of the addition rule\nto multiplication. But the multiplication rule of expectation is very\nrestrictive. Always remember to check independence before applying the\nmultiplication rule.\n:::\n\n::: callout-note\n### Sufficient but not necessary condition\n\nIf $X,Y$ are independent, it follows that $E(XY)=E(X)E(Y)$. However, the\nlatter does not imply independence. Consider a counter-example,\n$$X=\\begin{cases}\n    1 & \\textrm{with prob. }1/2\\\\\n    0 & \\textrm{with prob. }1/2\n    \\end{cases},\\quad Z=\\begin{cases}\n    1 & \\textrm{with prob. }1/2\\\\\n    -1 & \\textrm{with prob. }1/2\n    \\end{cases};$$ Then $$Y=XZ=\\begin{cases}\n    -1 & \\textrm{with prob. }1/4\\\\\n    0 & \\textrm{with prob. }1/2\\\\\n    1 & \\textrm{with prob. }1/4\n    \\end{cases}.$$ We have $E(X)=1/2$, $E(Y)=0$, $E(XY)=0$. So\n$E(XY)=E(X)E(Y)$. But clearly $X,Y$ are not independent.\n:::\n\n::: {#prp-lote}\n### Law of total expectation\nLet $\\{A_i\\}$ be a finite (or countable) partition of the sample space, then\n$$E(X) = \\sum_i E(X|A_i)P(A_i).$$\n:::\n\n::: {#thm-lotus}\n### Law of the unconscious statistician (LOTUS)\n\nLet $X$ be a random variable, and $g$ be a real-valued function of a\nreal variable. If $X$ has a discrete distribution, then\n$$E[g(X)]=\\sum_{\\textrm{all }x}g(x)P(X=x).$$\n:::\n\nLOTUS says we can compute the expectation of $g(X)$ without knowing the\nPMF of $g(X)$.\n\n::: {#exm-exp-sqx}\nCompute $E(X)$ and $E(X^{2})$ given the following distribution:\n$$f(x)=\\begin{cases}\n\\frac{1}{4}, \\quad x=0 \\\\\n\\frac{1}{2}, \\quad x=1 \\\\\n\\frac{1}{4}, \\quad x=2 \\\\\n\\end{cases}$$\n:::\n\n::: solution\nCompute the expectations of $X$ by definition:\n$$E(X) =0\\times\\frac{1}{4}+1\\times\\frac{1}{2}+2\\times\\frac{1}{4}=1$$\nCompute the expectations of $X^2$ by LOTUS:\n$$E(X^{2}) =0^2\\times\\frac{1}{4}+1^2\\times\\frac{1}{2}+2^2\\times\\frac{1}{4}=\\frac{3}{2}.$$ \n\nNote that $E(X^{2})\\neq[E(X)]^{2}$.\n:::\n\n::: callout-caution\n## Don't pull non-linear functions out of expectation\n\nIn general, $E[g(X)]\\neq g(E(X))$. Linearity implies $E[g(X)]=g(E(X))$\nif $g$ is a linear function. For a nonlinear function $g$, you can't\npull function $g$ out of expectation $E$. The right way to find\n$E[g(X)]$ is with LOTUS.\n:::\n\n::: {#exm-petersburg}\n### St. Petersburg Paradox\n\nFlip a fair coin over and over again until the head lands the first\ntime. You will win $2^{k}$ dollars if the head lands in the $k$-th trial\n(including the successful trial). What is the expected payoff of this\ngame?\n:::\n\n::: solution\nLet $X=2^{k}$. We want to find $E(X)$. The probability of the first head\nshowing up in the $k$-th trial is $\\frac{1}{2^{k}}$. Therefore,\n$$E(X)=\\sum_{k=1}^{\\infty}2^{k}\\cdot\\frac{1}{2^{k}}=\\sum_{k=1}^{\\infty}1=\\infty$$\n\nThe expected payoff is infinitely high! This is against most people's\nintuition, which is likely to be a small number. This is because we\nmistakenly go through the calculation $E(X)=E(2^{k})=2^{E(k)}$ in our\nmind. $E(k)$ the expected number of flips before a head is 2. Thus,\n$2^{E(k)}=4$.\n\nAnother way to resolve the paradox is that we don't typically reason\nabout infinity. No one would play this game infinitely many times. For\nfinite number of plays, the probability of getting very large payoff,\nsay $2^{100}$, is none. We can demonstrate this with a simulation.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# number of simulations\nN <- 1000\n\n# store simulated results\nX <- numeric(N)\n\nset.seed(0)\n\n# run simulation\nfor (i in 1:N) {\n  # start with the initial reward\n  x <- 2\n  # flip a coin until it lands tails\n  while (runif(1) < 0.5) {\n    x <- x * 2\n  }\n  # store the reward for this simulation\n  X[i] <- x\n}\n  \ncat(\"Expected Reward:\", mean(X))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected Reward: 12.938\n```\n\n\n:::\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}