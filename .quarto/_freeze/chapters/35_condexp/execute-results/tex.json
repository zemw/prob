{
  "hash": "5d35db8e41c17dcae4f1fb82857d5408",
  "result": {
    "engine": "knitr",
    "markdown": "## Conditional expectation\n\nWe have introduced conditional expectation in @def-cond-exp. Here we\nreiterate the definition with continuous random variables.\n\n::: {#def-cond-exp-2}\n## Conditional expectation\n\nLet $X$ and $Y$ be continuous random variables with joint density\n$f_{X,Y}(x,y)$, $X$'s density $f_X(x)$, and conditional density\n$f_{Y|X}(y|x)=\\frac{f_{X,Y}(x,y)}{f_X(x)}$. The conditional expectation\nof $Y$ given $X=x$ is $$\\begin{aligned}\nE(Y|X=x) &= \\int_{-\\infty}^{\\infty}y\\ f_{Y|X}(y|x)dy \\\\\n&= \\int_{-\\infty}^{\\infty}y\\ \\frac{f_{X,Y}(x,y)}{f_X(x)} dy\n\\end{aligned}$$ When the denominator is zero, the expression is\nundefined.\n:::\n\nNote that conditioning on a continuous random variable is not the same\nas conditioning on the event $\\{X=x\\}$ as it was in the discrete case.\nThe probability of the event is zero, but we define the conditional\nexpectation in terms of the density function.\n\n::: {#thm-lie}\n## Law of iterated expectation\n\nFor any random variable $X$ and $Y$, it holds that $$E(E(Y|X))=E(Y).$$\n:::\n\n::: proof\nNote that $E(Y|X)=g(X)$ is a function of $X$. Apply LOTUS:\n$$\\begin{aligned}\nE(E(Y|X)) & =\\int g(x)f(x)dx\\\\\n & =\\int\\left(\\int yf(y|x)dy\\right)f(x)dx\\\\\n & =\\int\\int yf(y|x)f(x)dydx\\\\\n & =\\int y\\int f(y,x)dx\\,dy\\\\\n & =\\int_{-\\infty}^{\\infty}yf(y)dy\\\\\n & =E(Y).\\end{aligned}$$\n:::\n\n::: {#thm-expfun}\nFor any random variable $X$ and $Y$, and any function $g$, we have\n$$E(g(X)Y|X)=g(X)E(Y|X).$$\n:::\n\n::: proof\nFor any specific value of $X=x$, $g(x)$ is a constant. Thus,\n$E(g(x)Y|X=x)=g(x)E(Y|X=x)$. This is true for all values of $x$.\n:::\n\n::: {#thm-bestpre}\n## Best predictor\n\nConditional expectation $E(Y|X)$ is the best predictor for $Y$ using $X$\n(minimized the square loss function).\n:::\n\n::: proof\nLet $g(X)$ be a predictor for $Y$ using $X$. We want to find the $g$\nsuch that minimizes $E(Y-g(X))^{2}$. $$\\begin{aligned}\nE(Y-g(X))^{2} & =E(Y-E(Y|X)+E(Y|X)-g(X))^{2}\\\\\n & =E(Y-E(Y|X))^{2}+2\\underbrace{E(Y-E(Y|X)}_{E(Y)=E(E(Y|X))}((E(Y|X)-g(X))\\\\ &\\quad+E(E(Y|X)-g(X))^{2}\\\\\n & =E(Y-E(Y|X))^{2}+E(E(Y|X)-g(X))^{2}\\\\\n & \\geq E(Y-E(Y|X))^{2}.\\end{aligned}$$ Therefore, $E(Y-g(X))^{2}$ is\nminimized when $g(X)=E(Y|X)$.\n:::\n\n::: {#def-lm}\n## Linear conditional expectation model\n\nAn extremely widely used method for data analysis in statistics is\nlinear regression. In its most basic form, we want to predict the mean\nof $Y$ using a single explanatory variable $X$. A linear conditional\nexpectation model assumes that $E(Y|X)$ is linear in $X$:\n$$E(Y|X)=a+bX,$$ or equivalently, $$Y=a+bX+\\epsilon,$$ with\n$E(\\epsilon|X)=0$. The intercept and the slope is given by\n$$b=\\frac{Cov(X,Y)}{Var(X)},a=E(Y)-bE(X).$$\n:::\n\nWe first show the equivalence of the two expressions of the model. Let\n$Y=a+bX+\\epsilon$, with $E(\\epsilon|X)=0$. Then by linearity,\n$$E(Y|X)=E(a|X)+E(bX|X)+E(\\epsilon|X)=a+bX.$$ Conversely, suppose that\n$E(Y|X)=a+bX$, and define $$\\epsilon=Y-(a+bX).$$ Then $Y=a+bX+\\epsilon$,\nwith $$E(\\epsilon|X)=E(Y|X)-E(a+bX|X)=E(Y|X)-(a+bX)=0.$$ To derive the\nexpression for $a$ and $b$, take covariance between $X$ and $Y$,\n$$\\begin{aligned}\nCov(X,Y) & =Cov(X,a+bX+\\epsilon)\\\\\n & =Cov(X,a)+bCov(X,X)+Cov(X,\\epsilon)\\\\\n & =bVar(X)+Cov(X,\\epsilon)\\end{aligned}$$ Note that $Cov(X,\\epsilon)=0$\nbecause $$\\begin{aligned}\nCov(X,\\epsilon) & =E(X\\epsilon)-E(X)E(\\epsilon)\\\\\n & =E(E(X\\epsilon|X))-E(X)E(E(\\epsilon|X))\\\\\n & =E(XE(\\epsilon|X))-E(X)E(E(\\epsilon|X))\\\\\n & =0\\end{aligned}$$ Therefore, $$Cov(X,Y)=bVar(X)$$ Thus,\n$$\\begin{aligned}\nb & =\\frac{Cov(X,Y)}{Var(X)},\\\\\na & =E(Y)-bE(X)=E(Y)-\\frac{Cov(X,Y)}{Var(X)}E(X).\\end{aligned}$$\n\nIn practice, we don't know the true value of $Cov(X,Y)$ or $Var(X)$. We\nhave to estimate it with sample observations. Thus, we compute\n$\\hat b=\\frac{\\sum_{i=1}^n (x_i-\\bar x)(y_i -\\bar y)}{\\sum_{i=1}^n (x_i - \\bar x)^2}$.\nBy definition, $b$ gives the marginal change of $E(Y|X)$ with respect to\n$X$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\nexam <- read.csv(\"../dataset/exam.csv\")\n\n# midterm score\nx <- exam$mid\n# final score\ny <- exam$final\n\n# regress y on x, compute coefficients\nb <- cov(x,y)/var(x)\na <- mean(y) - b*mean(x)\n\n# plot the data and the regression line\nplot(x,y)\nabline(a,b,col=\"red\")\n```\n\n::: {.cell-output-display}\n![](35_condexp_files/figure-pdf/unnamed-chunk-1-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nLinear regression is the simple yet powerful modeling tool in statistics. It is useful whenever we want to predict one variable with another. When the assumptions are met (though this is rare), the model gives the best predictor (conditional expectation). If the assumptions are not met,\nregression gives a linear approximation.\n",
    "supporting": [
      "35_condexp_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}