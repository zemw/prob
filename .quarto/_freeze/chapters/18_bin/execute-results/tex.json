{
  "hash": "3f24b1f94b2b9a0f84bf4fe3c4ab7da3",
  "result": {
    "engine": "knitr",
    "markdown": "# Binomial distribution\n\n::: {#def-bin}\n### Binomial distribution\n\nSuppose $X_{1},X_{2},\\dots,X_{n}$ are independent and identical\n$\\text{Bern}(p)$ distributions. Let $X$ be the total number of successes\nof the $n$ independent trials. That is, $X=X_{1}+X_{2}+\\cdots+X_{n}$.\nThen $X$ has the Binomial distribution, $X\\sim \\text{Bin}(n,p)$.\n:::\n\nThe PMF of $X$ directly follows from the combination theory:\n$$P(X=k)=\\binom{n}{k}p^{k}(1-p)^{n-k}.$$\n\nThis is a valid PMF because, by the Binomial theorem, we have\n$$\\sum_{k=0}^{n}\\binom{n}{k}p^{k}(1-p)^{n-k}=(p+(1-p))^{n}=1.$$\n\n::: callout-note\n### Binomial distribution and Binomial theorem\n\nYou may have noticed the connection between Binomial distribution and\nBinomial theorem. Consider using the polynomial $px+q$ to represent the\noutcome of a single Bernoulli trial, where $x$ is the indicator for a\nsuccess. Then $(px+q)^n$ is the outcome for $n$ independent trials. The\ncoefficient of $x^k$ gives the probability of there being exactly $k$\nsuccesses.\n:::\n\n::: {#thm-bin}\nLet $X\\sim \\text{Bin}(n,p)$ and $Y\\sim \\text{Bin}(m,p)$ be two\nindependent Binomial random variables. Then $X+Y\\sim \\text{Bin}(n+m,p)$.\n:::\n\n::: proof\nBy the definition of the Binomial distribution, $X=\\sum_{i=1}^{n}X_{i}$\nwhere $X_{i}\\sim \\text{Bern}(p)$; $Y=\\sum_{j=1}^{m}Y_{j}$ where\n$Y_{j}\\sim \\text{Bern}(p)$. Therefore,\n$$X+Y=\\sum_{i=1}^{n}X_{i}+\\sum_{j=1}^{m}Y_{j}=\\sum_{k=1}^{n+m}Z_{k}$$\nwhere $Z_{k}\\sim \\text{Bern}(p)$. Since $X_{i}$ and $Y_{j}$ are\nidentical Bernoulli random variables,\n$$Z_k = \\begin{cases}X_k,&\\text{ for }k=1,\\dots,n\\\\ \nY_{k-n}, &\\text{ for }k=n+1,\\dots,n+m\n\\end{cases}$$ By definition,\n$X+Y=\\sum_{k=1}^{n+m}Z_{k}\\sim \\text{Bin}(n+m,p)$.\n:::\n\n## Coin tossing problem {.unnumbered}\n\n::: {#exm-3.3}\nIn the previous example of tossing two coins, we compute the\ndistribution of $X$ by counting the equally likely outcomes in an event.\nWe can get the same result by realizing it is a Binomial distribution.\n$X\\sim\\textrm{Bin}(2,1/2)$. Since each coin tossing is an independent\nBernoulli trial. The probabilities come directly from the PMF.\n$$\\begin{aligned}\nP(X=0) & =\\binom{2}{0}\\left(\\frac{1}{2}\\right)^{0}\\left(\\frac{1}{2}\\right)^{2}=\\frac{1}{4};\\\\\nP(X=1) & =\\binom{2}{1}\\left(\\frac{1}{2}\\right)^{1}\\left(\\frac{1}{2}\\right)^{1}=\\frac{1}{2};\\\\\nP(X=2) & =\\binom{2}{2}\\left(\\frac{1}{2}\\right)^{2}\\left(\\frac{1}{2}\\right)^{0}=\\frac{1}{4}.\\end{aligned}$$\n\nUtilizing the Binomial distribution also allows us to generalize the\nproblem. Suppose we are tossing $n$ coins, we want to find the\nprobability of getting $k$ heads. It is almost impossible to count all\nthe possible outcomes, but the answer immediately follows from the\nBinomial PMF.\n:::\n\nThe following code simulates the number of heads if tossing $N$ coins:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Number of simulations\nk <- 1000 \n\n# Number of coins\nn <- 20\n\n# Store the results\nn_heads <- numeric(k)\n\n# Initialize random generator\nset.seed(100)\n\n# Run simulations\nfor (i in 1:k) { \n  toss <- sample(c('H','T'), n, replace = T)\n  n_heads[i] <- sum(toss == 'H')\n}\n\n# Plot distribution\nhist(n_heads, probability=TRUE)\n\n# Overlay the Binomial PMF\ncurve(choose(n,x)* 0.5^x * 0.5^(n-x), from=1, to=n, n=n, col=2, add=T)\n```\n\n::: {.cell-output-display}\n![](18_bin_files/figure-pdf/unnamed-chunk-1-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Binomial functions in R {.unnumbered}\n\nThere are built-in functions in R to work with Binomial distributions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# computes P(X=5) for Bin(10,0.5)\np <- dbinom(5, 10, 0.5)\n\npar(mfrow=c(1,2))\n\n# plot the PMF for Bin(10,0.5)\ncurve(dbinom(x, 10, 0.5), from=0, to=10, n=11, type=\"b\", ann=F)\n\n# `pbinom` computes the CDF \ncurve(pbinom(x, 10, 0.5), from=0, to=10, n=11, type=\"b\",ann=F)\n```\n\n::: {.cell-output-display}\n![](18_bin_files/figure-pdf/unnamed-chunk-2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# draw a random value from a given Binomial distribution\n# this allows us to simulate a random experiment\n# e.g. the number of heads when flipping 10 fair coins\noutcome <- rbinom(1, 10, 0.5)\n\n# Repeat the experiment 1000 times\nheads <- rbinom(1000, 10, 0.5)\n\n# the histogram will converge to the ideal Binomial distribution\n# if the experiment is repeated a large number of times\nhist(heads)\n```\n\n::: {.cell-output-display}\n![](18_bin_files/figure-pdf/unnamed-chunk-3-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Exam survival problem {.unnumbered}\n\n::: {#exm-bin}\nAn exam consists of 20 multiple-choice questions, each with four choices\nand exactly one correct answer. Suppose a student answers every question\nby guessing at random. What is the probability that the student passes\nthe exam, defined as answering more than 60% of the questions correctly?\n:::\n\n::: solution\nThe probability of correctly answering one question is $p=1/4$. Let $N$\nbe the total number of questions, $N=20$. Let $X$ be the number of\ncorrectly answered questions, $X\\leq N$. Then $X$ follows the Binomial\ndistribution $X \\sim B(N, 1/4)$. The probability of passing the exam is\ntherefore\n$$P(X \\geq 12) = 1-P(X\\leq 11)=1-\\text{CDF}^{\\text{Bin}}(11)\\approx 0.001.$$\n:::\n\nNow we compare the survival probability for different choice of $N$ and\n$p$ :\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Percentage of correct answers\nx <- seq(0, 1, .1)\n\n# Survival probabilities for different N\ny1 <- 1 - pbinom(10*x, 10, .25)\ny2 <- 1 - pbinom(20*x, 20, .25)\ny3 <- 1 - pbinom(30*x, 30, .25)\n\n# Compare the curves for different N\nplot(x, y1, type=\"b\", col=1, ann=F)\nlines(x, y2, type=\"b\", col=2)\nlines(x, y3, type=\"b\", col=3)\n\n# Indicating passing the exam\nabline(v=0.6, lty=2)\n\n# Add a legend at the top right corner of the plot\nlegend(\"topright\", c(\"N=10\", \"N=20\", \"N=30\"), lty=1, col=1:3)\n```\n\n::: {.cell-output-display}\n![](18_bin_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Percentage of correct answers\nx <- seq(0, 1, .1)\n\n# Survival probabilities for different p (number of choices)\ny1 <- 1 - pbinom(10*x, 10, .25)\ny2 <- 1 - pbinom(10*x, 10, .33)\ny3 <- 1 - pbinom(10*x, 10, .5)\n\n# Compare the curves for different p\nplot(x, y1, type=\"b\", col=1, ann=F)\nlines(x, y2, type=\"b\", col=2)\nlines(x, y3, type=\"b\", col=3)\n\n# Indicating passing the exam\nabline(v=0.6, lty=2)\n\n# Add a legend at the top right corner of the plot\nlegend(\"topright\", c(\"p=1/4\", \"p=1/3\", \"p=1/2\"), lty=1,col=1:3)\n```\n\n::: {.cell-output-display}\n![](18_bin_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n",
    "supporting": [
      "18_bin_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}