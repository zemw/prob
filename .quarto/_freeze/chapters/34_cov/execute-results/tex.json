{
  "hash": "343bc29434bb1727b86b7c7f4049294d",
  "result": {
    "engine": "knitr",
    "markdown": "## Covariance\n\nFor more than one random variable, it is also of interest to know the\nrelationship between them. Are they dependent? How strong is the\ndependence? Covariance and correlation are intended to measure that\ndependence. But they only capture a particular type of dependence,\nnamely linear dependence.\n\n::: {#def-cov}\n### Covariance\nThe covariance between random variables $X$ and $Y$ is defined as\n$$Cov(X,Y)=E[(X-EX)(Y-EY)].$$\n:::\n\nThe covariance between $X$ and $Y$ reflects how much $X$ and $Y$\n*simultaneously* deviate from their respective means. \n\n::: {#thm-cov}\nFor any random variables $X$ and $Y$, $$Cov(X,Y)=E(XY)-E(X)E(Y).$$\n:::\n\n::: proof\nLet $\\mu_{X}=E(X)$ and $\\mu_{Y}=E(Y)$. By definition, $$\\begin{aligned}\nCov(X,Y) & =E(XY-\\mu_{X}Y-\\mu_{Y}X+\\mu_{X}\\mu_{Y})\\\\\n & =E(XY)-\\mu_{X}E(Y)-\\mu_{Y}E(X)+\\mu_{X}\\mu_{Y}\\\\\n & =E(XY)-E(X)E(Y).\\end{aligned}$$\n:::\n\n::: {#thm-indep-cov}\nIf $X,Y$ are independent, they are uncorrelated. But the converse is\nfalse.\n:::\n\n::: proof\n1. $Cov(X,Y)=E(XY)-E(X)E(Y)$. Independence implies $E(XY)=E(X)E(Y)$. Thus,\n$Cov(X,Y)=0$. \n2. $Cov(X,Y)=0$ does not necessarily imply independence.\nConsider the following counter example. Let $X$ be a random variable\nthat takes three values -1, 0, 1 with equal probability. And $Y=X^{2}$.\n$X$ and $Y$ are clearly dependent. But they their covariance is 0.\nSince $E(X)=0$, $E(Y)=2/3$, $E(XY)=E(X^{3})=0$, $Cov(X,Y)=0$.\n:::\n\n::: callout-note\n## Linear dependency\nCovariance and correlation provide measures of the extend to which two\nrandom variables are linearly related. It is possible that the\ncovariance is $0$ even when $X$ and $Y$ are dependent but the relationship is\nnonlinear.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](34_cov_files/figure-pdf/unnamed-chunk-1-1.pdf)\n:::\n:::\n\n:::\n\n::: {#prp-cov}\nCovariance has the following properties:\n\n-   $Cov(X,X)=Var(X)$\n-   $Cov(X,Y)=Cov(Y,X)$\n-   $Cov(cX,Y)=Cov(X,cY)=c\\left[Cov(X,Y)\\right]$\n-   $Cov(X+Y,Z)=Cov(X,Z)+Cov(Y,Z)$\n-   $Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)$\n\n:::\n\n::: proof\nWe only prove the variance-covariance property: $$\\begin{aligned}\nVar(X+Y) & =E[(X+Y-\\mu_{X}-\\mu_{Y})^{2}]\\\\\n & =E[(X-\\mu_{X})^{2}+(Y-\\mu_{Y})^{2}+2(X-\\mu_{X})(Y-\\mu_{Y})]\\\\\n & =Var(X)+Var(Y)+2Cov(X,Y).\\end{aligned}$$\n:::\n\n::: {#thm-var-cov}\nFor random variables $X_1,X_2,\\dots,X_n$, it holds that\n$$Var\\left(\\sum_{i=1}^{n}X_{i}\\right)=\\sum_{i=1}^{n}Var(X_{i})+\n2\\sum_{i<j}Cov(X_{i},X_{j}).$$\n\nIf $X_1,X_2,\\dots,X_n$ are identically distributed and have the same\ncovariance relationships (symmetric), then\n$$Var\\left(\\sum_{i=1}^{n}X_{i}\\right)=nVar(X_1)+2\\binom{n}{2}Cov(X_1,X_2).$$\n:::\n\n\nWhile $\\text{Cov}(X,Y)$ quantifies how $X$ and $Y$ vary together, its\nmagnitude also depends on the absolute scales of $X$ and $Y$ (multiply\n$X$ by a constant $c$, the covariance will be different). To establish a\nmeasure of association between $X$ and $Y$ that is unaffected by\narbitrary changes in the scales of either variable, we introduce a\n\"standardized covariance\" called correlation.\n\n::: {#def-corr}\n### Correlation\nThe correlation between random variables $X$ and $Y$ is defined as\n$$Corr(X,Y)=\\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}.$$\n:::\n\nBy convention, we denote correlation by Greek letter $\\rho\\equiv Corr(X,Y)$.\n\nUnlike covariance, scaling $X$ or $Y$ has no effect on the correlation.\nWe can verify this:\n$$Corr(cX,Y)=\\frac{Cov(cX,Y)}{\\sqrt{Var(cX)Var(Y)}}=\\frac{cCov(X,Y)}{c\\sqrt{Var(X)Var(Y)}}=Corr(X,Y).$$\n\n::: {#thm-corr}\nFor any random variable $X$ and $Y$, $$-1\\leq Corr(X,Y)\\leq1.$$\n:::\n\n::: proof\nWithout loss of generality, assume $X,Y$ both have variance 1, since\nscaling does not change the correlation. Let $\\rho=Corr(X,Y)=Cov(X,Y)$.\nThen $$\\begin{aligned}\nVar(X+Y) & =Var(X)+Var(Y)+2Cov(X,Y)=2+2\\rho\\geq0,\\\\\nVar(X-Y) & =Var(X)+Var(Y)-2Cov(X,Y)=2-2\\rho\\ge0.\\end{aligned}$$ Thus\n$-1\\leq\\rho\\leq1$.\n:::\n\n-   $X$ and $Y$ are **positively correlated** if $\\rho_{XY}>0$;\n-   $X$ and $Y$ are **negatively correlated** if $\\rho_{XY}<0$;\n-   $X$ and $Y$ are **uncorrelated** if $\\rho_{XY}=0$.\n\n::: {#thm-corrlin}\nSuppose that $X$ is a random variable and $Y=aX+b$ for some constants\n$a,b$, where $a\\neq0$. If $a>0$, then $\\rho_{XY}=1$. If $a<0$, then\n$\\rho_{XY}=-1$.\n:::\n\n::: proof\nIf $Y=aX+b$, then $E(Y)=aE(X)+b$. Thus, $Y-E(Y)=a(X-E(X))$. Therefore,\n$$Cov(X,Y)=aE[(X-EX)^{2}]=aVar(X).$$ Since $Var(Y)=a^{2}Var(X)$,\n$\\rho_{XY}=\\frac{a}{|a|}$. The theorem thus follows.\n:::\n\n::: callout-note\n### Correlation analysis\nA **correlation matrix** shows the pairwise correlation coefficients between variables.\nItâ€™s one of the most common tools for exploring relationships in multivariate data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# variables for analysis\nvars <- mtcars[, 1:4]\n\n# compute the correlation matrix\nprint(cor(vars))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        mpg    cyl   disp     hp\nmpg   1.000 -0.852 -0.848 -0.776\ncyl  -0.852  1.000  0.902  0.832\ndisp -0.848  0.902  1.000  0.791\nhp   -0.776  0.832  0.791  1.000\n```\n\n\n:::\n:::\n\n:::\n\n::: {#exm-4.16}\nLet $X\\sim \\text{HGeom}(w,b,n)$. Find $Var(X)$.\n:::\n\n::: solution\nInterpret $X$ as the number of white balls in a sample of size $n$ from\nan box with $w$ white and $b$ black balls. We can represent $X$ as the\nsum of indicator variables, $X=I_{1}+\\cdots+I_{n}$ , where $I_{j}$ is\nthe indicator of the $j$-th ball in the sample being white. Each $I_{j}$\nhas mean $p=w/(w+b)$ and variance $p(1-p)$, but because the $I_{j}$ are\ndependent, we cannot simply add their variances. Instead,\n$$\\begin{aligned}\nVar(X) & =Var\\left(\\sum_{j=1}^{n}I_{j}\\right)\\\\\n & =Var(I_{1})+\\cdots+Var(I_{n})+2\\sum_{i<j}Cov(I_{i},I_{j})\\\\\n & =np(1-p)+2\\binom{n}{2}Cov(I_{i},I_{j})\\end{aligned}$$\n\nIn the last step, because of symmetry, for every pair $i$ and $j$,\n$Cov(I_{i},I_{j})$ are the same. $$\\begin{aligned}\nCov(I_{i},I_{j}) & =E(I_{i}I_{j})-E(I_{i})E(I_{j})\\\\\n & =P(i\\textrm{ and }j\\textrm{ both white})-P(i\\textrm{ is white})P(j\\textrm{ is white})\\\\\n & =\\frac{w}{w+b}\\cdot\\frac{w-1}{w+b-1}-p^{2}\\\\\n & =p\\frac{Np-1}{N-1}-p^{2}\\\\\n & =\\frac{p(p-1)}{N-1}\\end{aligned}$$\n\nwhere $N=w+b$. Plugging this into the above formula and simplifying, we\neventually obtain\n$$Var(X)=np(1-p)+n(n-1)\\frac{p(p-1)}{N-1}=\\frac{N-n}{N-1}np(1-p).$$ This\ndiffers from the Binomial variance of $np(1-p)$ by a factor of\n$\\frac{N-n}{N-1}$. This discrepancy arises because the Hypergeometric\nstory involves sampling without replacement. As $N\\to\\infty$, it becomes\nextremely unlikely that we would draw the same ball more than once, so\nsampling with or without replacement essentially become the same.\n:::\n",
    "supporting": [
      "34_cov_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}