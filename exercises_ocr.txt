One Thousand
Exercises in Probability

GEOFFREY R. GRIMMETT
Statistical Laboratory, University of Cambridge

and

DAVID R. STIRZAKER
Mathematical Institute, University of Oxford

OXFORD

UNIVERSITY PRESS

OXFORD

UNIVERSITY PRESS
Great Clarendon Street, Oxford 0x2 6pP

Oxford University Press is a department of the University of Oxford.
It furthers the University’s objective of excellence in research, scholarship,

and education by publishing worldwide in
Oxford New York

Athens Auckland Bangkok Bogoté Buenos Aires Cape Town

Chennai Dar es Salaam Delhi Florence Hong Kong Istanbul Karachi
Kolkata Kuala Lumpur Madrid Melbourne Mexico City Mumbai Nairobi
Paris Sao Paulo Shanghai Singapore Taipei Tokyo Toronto Warsaw

with associated companies in Berlin Ibadan

Oxford is a registered trade mark of Oxford University Press
in the UK and in certain other countries

Published in the United States
by Oxford University Press Inc., New York

© Geoffrey R. Grimmett and David R. Stirzaker 2001

The moral rights of the author have been asserted

Database tight Oxford University Press (maker)

First published 2001

All rights reserved. No part of this publication may be reproduced,
stored in a retrieval system, or transmitted, in any form or by any means,
without the prior permission in writing of Oxford University Press,

or as expressly permitted by law, or under terms agreed with the appropriate
reprographics rights organization. Enquiries concerning reproduction
outside the scope of the above should be sent to the Rights Department,
Oxford University Press, at the address above

You must not circulate this book in any other binding or cover
and you must impose this same condition on any acquirer

A catalogue record for this title is available from the British Library

Library of Congress Cataloging in Publication Data
Data available

ISBN 0 19 857221 2
10987654321
Typeset by the authors

Printed in Great Britain
on acid-free paper by Biddles Ltd, Guildford & King’s Lynn

Preface

This book contains more than 1000 exercises in probability and random processes, together
with their solutions. Apart from being a volume of worked exercises in its own right, it is
also a solutions manual for exercises and problems appearing in our textbook Probability and
Random Processes (3rd edn), Oxford University Press, 2001, henceforth referred to as PRP.
These exercises are not merely for drill, but complement and illustrate the text of PRP, or are
entertaining, or both. The current volume extends our earlier book Probability and Random
Processes: Problems and Solutions, and includes in addition around 400 new problems. Since
many exercises have multiple parts, the total number of interrogatives exceeds 3000.

Despite being intended in part as a companion to PRP, the present volume is as self-
contained as reasonably possible. Where knowledge of a substantial chunk of bookwork is
unavoidable, the reader is provided with a reference to the relevant passage in PRP. Expressions
such as ‘clearly’ appear frequently in the solutions. Although we do not use such terms in
their Laplacian sense to mean ‘with difficulty’, to call something ‘clear’ is not to imply that
explicit verification is necessarily free of tedium.

The table of contents reproduces that of PRP; the section and exercise numbers corre-
spond to those of PRP; there are occasional references to examples and equations in PRP.
The covered range of topics is broad, beginning with the elementary theory of probability
and random variables, and continuing, via chapters on Markov chains and convergence, to
extensive sections devoted to stationarity and ergodic theory, renewals, queues, martingales,
and diffusions, including an introduction to the pricing of options. Generally speaking, exer-
cises are questions which test knowledge of particular pieces of theory, while problems are
less specific in their requirements. There are questions of all standards, the great majority
being elementary or of intermediate difficulty. We ourselves have found some of the later
ones to be rather tricky, but have refrained from magnifying any difficulty by adding asterisks
or equivalent devices. If you are using this book for self-study, our advice would be not to
attempt more than a respectable fraction of these at a first read.

We pay tribute to all those anonymous pedagogues whose examination papers, work
assignments, and textbooks have been so influential in the shaping of this collection. To them
and to their successors we wish, in turn, much happy plundering. If you find errors, try to
keep them secret, except from us. If you know a better solution to any exercise, we will be
happy to substitute it in a later edition.

We acknowledge the expertise of Sarah Shea-Simonds in preparing the TEXscript of this
volume, and of Andy Burbanks in advising on the front cover design, which depicts a favourite
confluence of the authors.

Cambridge and Oxford G.R.G.
April 2001 D.R.S.

Life is good for only two things, discovering mathematics and teaching it.
Siméon Poisson
In mathematics you don’t understand things, you just get used to them.
John von Neumann

Probability is the bane of the age.
Anthony Powell
Casanova’s Chinese Restaurant

The traditional professor writes a, says b, and means c; but it should be d.
George Pélya
Contents

1 Events and their probabilities

1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8

Introduction

Events as sets

Probability

Conditional probability
Independence

Completeness and product spaces
Worked examples

Problems

2 Random variables and their distributions

2.1
2.2
2.3
2.4
2.5
2.6
2.7

Random variables

The law of averages

Discrete and continuous variables
Worked examples

Random vectors

Monte Carlo simulation
Problems

3. Discrete random variables

3.1
3.2
3.3
3.4
3.5
3.6
3.7
3.8
3.9
3.10
3.11

Probability mass functions
Independence

Expectation

Indicators and matching

Examples of discrete variables
Dependence

Conditional distributions and conditional expectation
Sums of random variables

Simple random walk

Random walk: counting sample paths
Problems

Vii

Questions

WN Ree

pp

10
10
11
11
12

12

16
16
17
18
19
19
20
21
22
23
23

Solutions

135
135
137
139

140
141

151
152
152
152
153

154

158
158
161
162
165
165
167
169
170
171
172

4 Continuous random variables

4.1 Probability density functions
4.2 Independence
4.3 Expectation

4.4 Examples of continuous variables

4.5 Dependence .
4.6 Conditional distributions and conditional expectation
4.7 Functions of random variables

4.8 Sums of random variables

4.9 Multivariate normal distribution

4.10 Distributions arising from the normal distribution
4.11. Sampling from a distribution

4.12 Coupling and Poisson approximation

4.13. Geometrical probability

4.14 Problems

5 Generating functions and their applications

5.1 Generating functions

5.2 Some applications

5.3 Random walk

5.4 Branching processes

5.5 Age-dependent branching processes
5.6 Expectation revisited

5.7 Characteristic functions

5.8 Examples of characteristic functions
5.9 Inversion and continuity theorems

5.10 Two limit theorems
5.11 Large deviations
5.12 Problems

6 Markov chains

6.1 Markov processes
6.2 Classification of states
6.3 Classification of chains

6.4 Stationary distributions and the limit theorem
6.5 Reversibility
6.6 Chains with finitely many states

6.7 Branching processes revisited
6.8 Birth processes and the Poisson process
6.9 Continuous-time Markov chains

6.10 Uniform semigroups

6.11 Birth—death processes and imbedding
6.12 Special processes

6.13 Spatial Poisson processes

6.14 | Markov chain Monte Carlo

6.15 Problems

viii

29
29
30
30
31
32
33
34
35
36
36
37
38
39

48
49
50
51
52
52
53
54
55
56
57
57

64
65
66
67
68
69
70
71
72

73
74
74
75
76

Contents

187
188
189
190
191
193
195
199
201
202
204
205
206
209

230
232
234
238
239
241
241
244
247
249
253
254

272
275
276
281
286
287
289
290
293

297
299
301
303
304

Contents

7 Convergence of random variables

7A Introduction
7.2 Modes of convergence
73 Some ancillary results
74 Laws of large numbers
75 The strong law
7.6 The law of the iterated logarithm
7.7 Martingales
78 Martingale convergence theorem
7.9 Prediction and conditional expectation
7.10 Uniform integrability
7.11 Problems
8 Random processes
8.1 Introduction
8.2 Stationary processes
8.3 Renewal processes
8.4 Queues
8.5 The Wiener process
8.6 Existence of processes
8.7 Problems
9 Stationary processes
9.1 Introduction
9.2 Linear prediction
9.3 Autocovariances and spectra
9.4 Stochastic integration and the spectral representation
9.5 The ergodic theorem
9.6 Gaussian processes
9.7 Problems

10 Renewals

10.1. The renewal equation
10.2 Limit theorems
10.3. Excess life
10.4 Applications
10.5 Renewal-reward processes
10.6 Problems
11 Queues
11.1 Single-server queues
11.2 M/M/1
11.3 M/G/l
11.4 G/M/
11.5 G/G/l
11.6 Heavy traffic
11.7. Networks of queues
11.8 Problems

ix

85
85
86
88
88
89
89
90
90
91
91

97
97
98
99

99

101
101
102
102
103
103
104

107
107
108
108
109
109

112
113
113
113
114
114
115

323
323
326
330
331
331
331
332
333
334
336

349
350
351
352

353

355
356
357
359
359
360
361

370
371
373
375
375
376

382
384
384
385
386
386
387

12 Martingales

12.1 Introduction
12.2 Martingale differences and Hoeffding’s inequality
12.3. Crossings and convergence
12.4 Stopping times
12.5 Optional stopping
12.6 The maximal inequality
12.7 Backward martingales and continuous-time martingales
12.8 Some examples
12.9 Problems
13 Diffusion processes
13.1 Introduction
13.2. Brownian motion
13.3 Diffusion processes
13.4 _ First passage times
13.5 Barriers
13.6 Excursions and the Brownian bridge
13.7 Stochastic calculus
13.8 The Ité integral
13.9 — It6’s formula
13.10 Option pricing
13.11 Passage probabilities and potentials
13.12 Problems
Bibliography
Index

118
119
119
120
120

121

121

126
127
127
127
127
128
129
129
130
130

429
430

Contents

396
398
398
399
400

403

403

411
413
413
413
415
416
417
418
420
420

1

Events and their probabilities

1.2 Exercises. Events as sets

1. Let {A; : i € 7} be a collection of sets. Prove ‘De Morgan’s Laws’t:
c c
(Uai) =() 4%, (ar) = 4.
i i i i

2. Let A and B belong to some o-field F. Show that ¥ contains the sets AN B, A\ B,and AAB.

3. A conventional knock-out tournament (such as that at Wimbledon) begins with 2” competitors
and has n rounds. There are no play-offs for the positions 2, 3, ..., 2” — 1, and the initial table of
draws is specified. Give a concise description of the sample space of all possible outcomes.

4. Let ¥beao-field of subsets of & and suppose that B ¢ ¥. Show that¢ = {AN B:A€e Fhisa
o-field of subsets of B.

5. Which of the following are identically true? For those that are not, say when they are true.

(a) AU(BNC) = (AU B)N (AUC);

(b) AN(BNC)=(ANB)NC;

() (AUB)NC=AU(BNOC);

(@) A\ (BNC) =(A\ B)U(A\C).

1.3 Exercises. Probability

1. Let A and B be events with probabilities P(A) = } and P(B) = 3. Show that 7 < P(ANB) < 3,
and give examples to show that both extremes are possible. Find corresponding bounds for P(A U B).

2. A fair coin is tossed repeatedly. Show that, with probability one, a head turns up sooner or later.
Show similarly that any given finite sequence of heads and tails occurs eventually with probability
one. Explain the connection with Murphy’s Law.

3. Six cups and saucers come in pairs: there are two cups and saucers which are red, two white, and
two with stars on. If the cups are placed randomly onto the saucers (one each), find the probability
that no cup is upon a saucer of the same pattern.

+Augustus De Morgan is well known for having given the first clear statement of the principle of mathematical
induction. He applauded probability theory with the words: “The tendency of our study is to substitute the
satisfaction of mental exercise for the pernicious enjoyment of an immoral stimulus”.

1

[1.3.4]-[1.4.5] Exercises Events and their probabilities

4, Let A,, Az,..., An be events where n > 2, and prove that
n
°(U Ai) = 5> P(A) — SO P(A; N Aj) + SS PAL Aj Ag)
i=1 i i<j i<j<k

=e + (-1)"t! P(A] MAD N- ++  An)-

In each packet of Corn Flakes may be found a plastic bust of one of the last five Vice-Chancellors
of Cambridge University, the probability that any given packet contains any specific Vice-Chancellor
being 7 independently of all other packets. Show that the probability that each of the last three

Vice-Chancellors is obtained in a bulk purchase of six packets is 1 — 3(4)° + 3(3)6 - (3).
5. Let A;,r > 1, be events such that P(A,) = 1 for all r. Show that P((\72, Ar) = 1.

6. You are given that at least one of the events Ay, 1 <r <n, is certain to occur, but certainly no
more than two occur. If P(A;) = p, and P(Ay 1 As) = q,r #5, show that p > 1/nandgq <2/n.

7. You are given that at least one, but no more than three, of the events A;, 1 <r <n, occur, where
n > 3. The probability of at least two occurring is 4. If P(A;) = p, P(Ar N As) = g,r #8, and
P(Ap N As MN At) = x,7r <5 < t, show that p > 3/(2n), andg <4/n.

1.4 Exercises. Conditional probability

1. Prove that P(A | B) = P(B | A)P(A)/P(B) whenever P(A)P(B) ¥ 0. Show that, if P(A | B) >
P(A), then P(B | A) > PCB).

2. For events Aj, A2,..., An Satisfying P(A, MN Az N---M Ap_1) > 0, prove that

P(A, 1 A2M-+-M An) = P(A1)P(A2 | A1)PCA3 | At 9 A2)---P(An | At A20--- 9 Ap—1).

3. A man possesses five coins, two of which are double-headed, one is double-tailed, and two are
normal. He shuts his eyes, picks a coin at random, and tosses it. What is the probability that the lower
face of the coin is a head?

He opens his eyes and sees that the coin is showing heads; what is the probability that the lower
face is a head?

He shuts his eyes again, and tosses the coin again. What is the probability that the lower face is
a head?

He opens his eyes and sees that the coin is showing heads; what is the probability that the lower
face is a head?

He discards this coin, picks another at random, and tosses it. What is the probability that it shows
heads?

4. What do you think of the following ‘proof’ by Lewis Carroll that an urn cannot contain two balls
of the same colour? Suppose that the urn contains two balls, each of which is either black or white;
thus, in the obvious notation, P(BB) = P(BW) = P(WB) = P(WW) = 1) We add a black ball, so
that PCBBB) = P(BBW) = P(BWB) = P(BWW) = i Next we pick a ball at random; the chance
that the ball is black is (using conditional probabilities) 1 - 4 + 3 . 4 + 5 . i + 3 . 4 = i. However, if
there is probability 3 that a ball, chosen randomly from three, is black, then there must be two black
and one white, which is to say that originally there was one black and one white ball in the urn.

5. The Monty Hall problem: goats and cars. (a) Cruel fate has made you a contestant in a game
show; you have to choose one of three doors. One conceals a new car, two conceal old goats. You

2

Independence Exercises [1.4.6]-[1.5.7]

choose, but your chosen door is not opened immediately. Instead, the presenter opens another door
to reveal a goat, and he offers you the opportunity to change your choice to the third door (unopened
and so far unchosen). Let p be the (conditional) probability that the third door conceals the car. The
value of p depends on the presenter’s protocol. Devise protocols to yield the values p = 5: p= Z.

Show that, for a € (5, 3, there exists a protocol such that p = a. Are you well advised to change
your choice to the third door?

(b) In a variant of this question, the presenter is permitted to open the first door chosen, and to reward
you with whatever lies behind. If he chooses to open another door, then this door invariably conceals
a goat. Let p be the probability that the unopened door conceals the car, conditional on the presenter
having chosen to open a second door. Devise protocols to yield the values p = 0, p = 1, and deduce
that, for any a € [0, 1], there exists a protocol with p = a.

6. The prosecutor’s fallacy}. Let G be the event that an accused is guilty, and T the event that
some testimony is true. Some lawyers have argued on the assumption that P(G | T) = P(T | G).
Show that this holds if and only if P(G) = P(T).

7. Urns. There are n urns of which the rth contains r — 1 red balls and n — r magenta balls. You
pick an urn at random and remove two balls at random without replacement. Find the probability that:
(a) the second ball is magenta;

(b) the second ball is magenta, given that the first is magenta.

1.5 Exercises. Independence

1, Let A and B be independent events; show that A°, B are independent, and deduce that A°, B®
are independent.

2. We roll a die n times. Let Aj; be the event that the ith and jth rolls produce the same number.

Show that the events {A;; : 1 <i < j <x} are pairwise independent but not independent.

3. A fair coin is tossed repeatedly. Show that the following two statements are equivalent:

(a) the outcomes of different tosses are independent,

(b) for any given finite sequence of heads and tails, the chance of this sequence occurring in the first
m tosses is 2~”, where m is the length of the sequence.

4, LetQ={1,2,..., p} where p is prime, ¥ be the set of all subsets of Q, and P(A) = |A|/p for
all A € ¥. Show that, if A and B are independent events, then at least one of A and B is either @ or
Q.

5. Show that the conditional independence of A and B given C neither implies, nor is implied by,
the independence of A and B. For which events C is it the case that, for all A and B, the events A and
B are independent if and only if they are conditionally independent given C?

6. Safe or sorry? Some form of prophylaxis is said to be 90 per cent effective at prevention during
one year’s treatment. If the degrees of effectiveness in different years are independent, show that the
treatment is more likely than not to fail within 7 years.

7. Families, Jane has three children, each of which is equally likely to be a boy or a girl independently
of the others. Define the events:

A = {all the children are of the same sex},
B = {there is at most one boy},
C = {the family includes a boy and a girl}.

+The prosecution made this error in the famous Dreyfus case of 1894.

3

[1.5.8]-[1.8.3] Exercises Events and their probabilities

(a) Show that A is independent of B, and that B is independent of C.
(b) Is A independent of C?

(c) Do these results hold if boys and girls are not equally likely?

(d) Do these results hold if Jane has four children?

8. Galton’s paradox. You flip three fair coins. At least two are alike, and it is an evens chance that
the third is a head or a tail. Therefore P(all alike) = }. Do you agree?

9. Two fair dice are rolled. Show that the event that their sum is 7 is independent of the score shown
by the first die.

1.7 Exercises. Worked examples

1. There are two roads from A to B and two roads from B to C. Each of the four roads is blocked by
snow with probability p, independently of the others. Find the probability that there is an open road
from A to B given that there is no open route from A to C.

If, in addition, there is a direct road from A to C, this road being blocked with probability p
independently of the others, find the required conditional probability.

2. Calculate the probability that a hand of 13 cards dealt from a normal shuffled pack of 52 contains
exactly two kings and one ace. What is the probability that it contains exactly one ace given that it
contains exactly two kings?

3. Asymmetric random walk takes place on the integers 0, 1, 2, ..., N with absorbing barriers at 0
and N, starting at k. Show that the probability that the walk is never absorbed is zero.

4. The so-called ‘sure thing principle’ asserts that if you prefer x to y given C, and also prefer x to
y given C°, then you surely prefer x to y. Agreed?

5. A pack contains m cards, labelled 1,2,...,m. The cards are dealt out in a random order, one
by one. Given that the label of the kth card dealt is the largest of the first k cards dealt, what is the
probability that it is also the largest in the pack?

1.8 Problems

1. A traditional fair die is thrown twice. What is the probability that:
(a) a six tums up exactly once?

(b) both numbers are odd?

(c) the sum of the scores is 4?

(d) the sum of the scores is divisible by 3?

2. A fair coin is thrown repeatedly. What is the probability that on the nth throw:
(a) ahead appears for the first time?

(b) the numbers of heads and tails to date are equal?

(c) exactly two heads have appeared altogether to date?

(d) at least two heads have appeared to date?

3. Let Fand ¢ be o-fields of subsets of Q.

(a) Use elementary set operations to show that ¥ is closed under countable intersections; that is, if
Aj, A2,... are in F, then so is (}; A;.-

(b) Let H = FN G be the collection of subsets of Q lying in both F and 3. Show that # is a o-field.

(c) Show that FU G, the collection of subsets of Q lying in either F or G, is not necessarily a o-field.

4

Problems Exercises [1.8.4]-[1.8.14]

4. Describe the underlying probability spaces for the following experiments:

(a) a biased coin is tossed three times;

(b) two balls are drawn without replacement from an urn which originally contained two ultramarine
and two vermilion balls;

(c) a biased coin is tossed repeatedly until a head turns up.

5. Show that the probability that exactly one of the events A and B occurs is

P(A) + P(B) — 2P(AN B).

6. Prove that P(AU BUC) =1—P(AS | BSN C°)P(BS | CPCS).

7. (a)If A is independent of itself, show that P(A) is 0 or 1.
(b) If P(A) is 0 or 1, show that A is independent of all events B.

8. Let Fbeac-field of subsets of Q, and suppose P : ¥ -> [0, 1] satisfies: (4) P(&2) = 1, and Gi) P
is additive, in that P(A U B) = P(A) + P(B) whenever AM B = @. Show that P(@) = 0.

9, Suppose (Q, ¥, P) is a probability space and B € F satisfies P(B) > 0. Let Q: F — [0, 1] be
defined by Q(A) = P(A | B). Show that (Q, F, Q) is a probability space. If C € Fand Q(C) > 0,
show that Q(A | C) = P(A | BNC); discuss.

10. Let By, Bo,... be a partition of the sample space Q, each B; having positive probability, and
show that

le ¢)
P(A) = P(A | Bj)P(B;).
j=l

11. Prove Boole’s inequalities:
n n A n
»(U Ai) < > P(Ai), (1 Ai) > 1-5 > P(AP).
i=1 i=l i=l i=1
12. Prove that

P(A) = So P(A) — OPA; U Aj) +S P(A; UA; U Ag)
1 i

i<j i<j<k

~ +++ —(=1)"P(Ay U Az U--*U Ag).

13. Let Aj, Az,..., An be events, and let Ny be the event that exactly k of the A; occur. Prove the
result sometimes referred to as Waring’s theorem:

n-k :
,f{k+i
P(N) = > (-1)' ( k ) see where S; = » P(A;, N Ai Ai):
i=0 i <ig<-<ij

Use this result to find an expression for the probability that a purchase of six packets of Corn Flakes
yields exactly three distinct busts (see Exercise (1.3.4)).
14. Prove Bayes’s formula: if A;, Az, ..., An isa partition of Q, each A; having positive probability,
then

P(B | Aj)P(A;)
Dj PCB | Aj)PCAi)

P(A; | B) =

5
[1.8.15]-[1.8.22] Exercises Events and their probabilities

15, A random number N of dice is thrown. Let A; be the event that N = i, and assume that
P(A;) = 27',i = 1. The sum of the scores is S. Find the probability that:

(a) N =2 given S = 4;

(b) S =4 given N is even;

(c) N = 2, given that S = 4 and the first die showed 1;

(d) the largest number shown by any die is r, where S is unknown.

16. Let A;, Az,... be a sequence of events. Define

ioe) io, 0)
Bn= (J Am, Cn =] Am.
=n =n

Clearly Cp © An © By. The sequences {B,} and {C,,} are decreasing and increasing respectively
with limits

lim Bn = B=(\Bn=(|) UJ Am. limp =C =| J Cn =U) Am.
n n

nm>n nomen

The events B and C are denoted lim sup,_,4. An and lim infy—oo An respectively. Show that

(a) B = {wm € Q: w € Ay for infinitely many values of 7},

(b) C = {wm € 2: w € Ap for all but finitely many values of 7}.

We say that the sequence { A, } converges to a limit A = lim A, if B and C are the same set A. Suppose
that A, — A and show that

(c) A is an event, in that A € F,

(d) P(An) > P(A).

17. In Problem (1.8.16) above, show that B and C are independent whenever By, and Cy are inde-
pendent for all n. Deduce that if this holds and furthermore A, — A, then P(A) equals either zero or
one.

18. Show that the assumption that P is countably additive is equivalent to the assumption that P is
continuous. That is to say, show that if a function P : ¥ — [0, 1] satisfies P(@) = 0, P(Q) = 1, and
P(A U B) = P(A) + P(B) whenever A, B € Fand AN B = @, then P is countably additive (in the
sense of satisfying Definition (1.3.1b)) if and only if P is continuous (in the sense of Lemma (1.3.5)).

19. Anne, Betty, Chloé, and Daisy were all friends at school. Subsequently each of the G) =6
subpairs meet up; at each of the six meetings the pair involved quarrel with some fixed probability
p, or become firm friends with probability 1 — p. Quarrels take place independently of each other.
In future, if any of the four hears a rumour, then she tells it to her firm friends only. If Anne hears a
rumour, what is the probability that:

(a) Daisy hears it?

(b) Daisy hears it if Anne and Betty have quarrelled?

(c) Daisy hears it if Betty and Chloé have quarrelled?

(d) Daisy hears it if she has quarrelled with Anne?

20. A biased coin is tossed repeatedly. Each time there is a probability p of a head turning up. Let py
be the probability that an even number of heads has occurred after n tosses (zero is an even number).
Show that pp = 1 and that pp = p(1— pp—-1) + C1 — p) pn_1 ifn = 1. Solve this difference equation.

21. A biased coin is tossed repeatedly. Find the probability that there is a run of r heads in a row
before there is a run of s tails, where r and s are positive integers.

22. A bowl contains twenty cherries, exactly fifteen of which have had their stones removed. A
greedy pig eats five whole cherries, picked at random, without remarking on the presence or absence
of stones. Subsequently, a cherry is picked randomly from the remaining fifteen.

6

Problems Exercises [1.8.23]-[1.8.31]

(a) What is the probability that this cherry contains a stone?
(b) Given that this cherry contains a stone, what is the probability that the pig consumed at least one
stone?

23. The ‘ménages’ problem poses the following question. Some consider it to be desirable that men
and women alternate when seated at a circular table. If n couples are seated randomly according to
this rule, show that the probability that nobody sits next to his or her partner is

1< p 2n {2n-k
mi aD sel k Jaw

You may find it useful to show first that the number of ways of selecting k non-overlapping pairs of
adjacent seats is ors i) 2n(Qn —k)7!.

24. Anurn contains D blue balls andr red balls. They are removed at random and not replaced. Show
that the probability that the first red ball drawn is the (k + 1)th ball drawn equals ("*?—*-') / (?°).
Find the probability that the last ball drawn is red.

25. An urn contains a azure balls and c carmine balls, where ac 4 0. Balls are removed at random
and discarded until the first time that a ball (B, say) is removed having a different colour from its
predecessor. The ball B is now replaced and the procedure restarted. This process continues until the
last ball is drawn from the urn. Show that this last ball is equally likely to be azure or carmine.

26. Protocols. A pack of four cards contains one spade, one club, and the two red aces. You deal
two cards faces downwards at random in front of a truthful friend. She inspects them and tells you
that one of them is the ace of hearts. What is the chance that the other card is the ace of diamonds?
Perhaps 5?

Suppose that your friend’s protocol was:
(a) with no red ace, say “no red ace”,
(b) with the ace of hearts, say “ace of hearts”,
(c) with the ace of diamonds but not the ace of hearts, say “ace of diamonds”.
Show that the probability in question is 5.

Devise a possible protocol for your friend such that the probability in question is zero.

27. Eddington’s controversy. Four witnesses, A, B, C, and D, at a trial each speak the truth with
probability 4 independently of each other. In their testimonies, A claimed that B denied that C declared
that D lied. What is the (conditional) probability that D told the truth? [This problem seems to have
appeared first as a parody in a university magazine of the ‘typical’ Cambridge Philosophy Tripos
question.]

28. The probabilistic method. 10 per cent of the surface of a sphere is coloured blue, the rest is red.
Show that, irrespective of the manner in which the colours are distributed, it is possible to inscribe a
cube in S with all its vertices red.

29. Repulsion. The event A is said to be repelled by the event B if P(A | B) < P(A), and to be
attracted by B if P(A | B) > P(A). Show that if B attracts A, then A attracts B, and B® repels A.

If A attracts B, and B attracts C, does A attract C?
30. Birthdays. If m students born on independent days in 1991 are attending a lecture, show that the
probability that at least two of them share a birthday is p = 1 — (365)!/{(365 — m)! 365}. Show
that p > 5 when m = 23.
31. Lottery. You choose r of the first n positive integers, and a lottery chooses a random subset L of
the same size. What is the probability that:
(a) L includes no consecutive integers?
[1.8.32]-[1.8.36] Exercises Events and their probabilities

(b) L includes exactly one pair of consecutive integers?

(c) the numbers in L are drawn in increasing order?

(d) your choice of numbers is the same as L?

(e) there are exactly k of your numbers matching members of L?

32. Bridge. During a game of bridge, you are dealt at random a hand of thirteen cards. With an
obvious notation, show that P(4S, 3H, 3D, 3C) ~ 0.026 and P(4S, 4H, 3D, 2C) ~ 0.018. However

if suits are not specified, so numbers denote the shape of your hand, show that P(4, 3,3, 3) ~ 0.11
and P(4, 4, 3, 2) ~ 0.22.

33. Poker. During a game of poker, you are dealt a five-card hand at random. With the convention
that aces may count high or low, show that:

PC pair) ~ 0.423, P(2 pairs) ~ 0.0475, P@ of a kind) ~ 0.021,
P(straight) ~ 0.0039, P(flush) ~ 0.0020, P(full house) ~ 0.0014,
P(4 of a kind) ~ 0.00024, P(straight flush) ~ 0.000015.

34. Poker dice. There are five dice each displaying 9, 10, J, Q, K, A. Show that, when rolled:

P(1 pair) ~ 0.46, P(2 pairs) ~ 0.23, P(3 of a kind) ~ 0.15,
P(no 2 alike) ~ 0.093, P(full house) ~ 0.039, P(4 of a kind) ~ 0.019,
P(5 of a kind) ~ 0.0008.

35. You are lost in the National Park of Bandrikat. Tourists comprise two-thirds of the visitors to

the park, and give a correct answer to requests for directions with probability 3. (Answers to repeated

questions are independent, even if the question and the person are the same.) If you ask a Bandrikan

for directions, the answer is always false.

(a) You ask a passer-by whether the exit from the Park is East or West. The answer is East. What is
the probability this is correct?

(b) You ask the same person again, and receive the same reply. Show the probability that it is correct
is 3

(c) You ask the same person again, and receive the same reply. What is the probability that it is
correct?

(d) You ask for the fourth time, and receive the answer East. Show that the probability it is correct
27

is $y.
70
(e) Show that, had the fourth answer been West instead, the probability that East is nevertheless
correct is a
36. Mr Bayes goes to Bandrika. Tom is in the same position as you were in the previous problem,
but he has reason to believe that, with probability «, East is the correct answer. Show that:
(a) whatever answer first received, Tom continues to believe that East is correct with probability «,

(b) if the first two replies are the same (that is, either WW or EE), Tom continues to believe that East
is correct with probability €,
(c) after three like answers, Tom will calculate as follows, in the obvious notation:

lle
9+ 2e°

9
P(East correct | EEE) = a

, P(East correct | WWW) =
1—2e

Evaluate these when € = ra

+A fictional country made famous in the Hitchcock film ‘The Lady Vanishes’.

8

Problems Exercises [1.8.37]-[1.8.39]

37. Bonferroni’s inequality. Show that

»(U Ar) > S>P(Ar) — $>P(Ar 9 Ax).
r=] r=1

r<k

38. Kounias’s inequality. Show that

P(U A,) < min {7 P(Ar) — D7 P(Ar 9 Ag)
r=l r=l

rirZk

39. Then passengers for a Bell-Air flight in an airplane with n seats have been told their seat numbers.
They get on the plane one by one. The first person sits in the wrong seat. Subsequent passengers sit
in their assigned seats whenever they find them available, or otherwise in a randomly chosen empty
seat. What is the probability that the last passenger finds his seat free?

2

Random variables and their distributions

2.1 Exercises. Random variables

1. Let X be arandom variable on a given probability space, and let a € R. Show that

(i) aX is a random variable,

(ii) X — X = 0, the random variable taking the value 0 always, and X + X = 2X.
2. Arandom variable X has distribution function F. What is the distribution function of Y = aX +b,
where a and b are real constants?
3. A fair coin is tossed n times. Show that, under reasonable assumptions, the probability of exactly
k heads is (RG). What is the corresponding quantity when heads appears with probability p on
each toss?

4. Show that if F and G are distribution functions andO < 4 < 1 thenA F+(1 —A)G isa distribution
function. Is the product FG a distribution function?

5. Let F be a distribution function and r a positive integer. Show that the following are distribution
functions:

(a) F(x)’,

(b) 1-f1- FQ),

(ce) F(x) + {1 — F(x)} log{1 — F@)},
(d) {F(x) — 1}e + exp{1 — F(x)}.

2.2 Exercises. The law of averages

1. You wish to ask each of a large number of people a question to which the answer “yes” is
embarrassing. The following procedure is proposed in order to determine the embarrassed fraction of
the population. As the question is asked, a coin is tossed out of sight of the questioner. If the answer
would have been “no” and the coin shows heads, then the answer “yes” is given. Otherwise people
respond truthfully. What do you think of this procedure?

2. Acoinis tossed repeatedly and heads turns up on each toss with probability p. Let Hy, and Ty be
the numbers of heads and tails in n tosses. Show that, for « > 0,

1
P(2p—1-€ < “(Hin ~ Ty) <2p—-1 46) +1 asin —> oo.
n

3. Let {X;, :r > 1} be observations which are independent and identically distributed with unknown
distribution function F. Describe and justify a method for estimating F(x).

10
Worked examples Exercises [2.3.1]-[2.4.2]
2.3 Exercises. Discrete and continuous variables

1. Let X be a random variable with distribution function F, and let a = (am : —-oO < m < cw)
be a strictly increasing sequence of real numbers satisfying a_m™ — —oo and am — co asm —> ov.
Define G(x) = P(X < am) when a,,-| <x < am, So that G is the distribution function of a discrete
random variable. How does the function G behave as the sequence a is chosen in such a way that
SUP,, |dm — Am—1| becomes smaller and smaller?

2. Let X be a random variable and let g : R > R be continuous and strictly increasing. Show that
Y = g(X) is arandom variable.

3. Let X be arandom variable with distribution function

0 ifx <0,
P(X <x}=4 x if0<x <1,
1 ifx>t.

Let F be a distribution function which is continuous and strictly increasing. Show that Y = F -1(X)
is arandom variable having distribution function F’. Is itnecessary that F be continuous and/or strictly
increasing?

4. Show that, if f and g are density functions, and 0 < A < 1, thenAf + (1 —A)g is a density. Is
the product fg a density function?

5. Which of the following are density functions? Find c and the corresponding distribution function
F for those that are.

—d
cx
@) f@)= { 0 otherwise.

(b) f@®)=ce*(1+e%)7-2, xER.

x>l,

2.4 Exercises. Worked examples

1. Let X be a random variable with a continuous distribution function F. Find expressions for the
distribution functions of the following random variables:

(a) X?, (b) VX,
(c) sin X, (d) G-1(X),
(e) F(X), (f) G-!(F(X)),

where G is a continuous and strictly increasing function.

2. Truncation. Let X be a random variable with distribution function F,, and let a < b. Sketch the
distribution functions of the ‘truncated’ random variables Y and Z given by

a if X <a,
Y= < X ifa<X <b,

{x tik s
Z=
b ifX>b,

0 if|X|>b.

Indicate how these distribution functions behave as a > —oo, b > oo.

11

[2.5.1]-{2.7.4] Exercises Random variables and their distributions

2.5 Exercises. Random vectors
1. A fair coin is tossed twice. Let X be the number of heads, and let W be the indicator function of
the event {X = 2}. Find P(X = x, W = w) for all appropriate values of x and w.

2. Let X bea Bernoulli random variable, so that PX = 0) = 1— p, P(X = 1) = p. LetY = 1-X
and Z = XY. Find P(X =x, Y = y) and P(X =x, Z =z) forx, y,z € {0, 1}.

3. The random variables X and Y have joint distribution function
0 ifx <0,

Fy,y(, y) = 11
x,y (x y) (1 —e7*) (5 + ~ tan! ) ifx > 0.

Show that X and Y are (jointly) continuously distributed.
4. Let X and Y have joint distribution function F. Show that

Pla < X <b,ce<Y<d)=F(b,d) — F(a,d) — F(b,c)+ Flac)

whenever a < bandc <d.
5. Let X, Y be discrete random variables taking values in the integers, with joint mass function f.
Show that, for integers x, y,

f@, y= P(X>x,¥ <y)-PX2>x4+1,¥ <y)
—-PX>x,¥ <y-)4+P(X2x4+1,¥ <y-D.
Hence find the joint mass function of the smallest and largest numbers shown in r rolls of a fair die.

6. Is the function F(x, y) = 1 — e-*7, 0 < x, y < on, the joint distribution function of some pair
of random variables?

2.7 Problems

1. Each toss of a coin results in a head with probability p. The coin is tossed until the first head
appears. Let X be the total number of tosses. What is P(X > m)? Find the distribution function of
the random variable X.

2. (a) Show that any discrete random variable may be written as a linear combination of indicator
variables.

(b) Show that any random variable may be expressed as the limit of an increasing sequence of discrete
random variables.

(c) Show that the limit of any increasing convergent sequence of random variables is a random
variable.

3. (a) Show that, if X and Y are random variables on a probability space (Q, ¥, P), then so are
X+Y, XY, and min{X, Y}.

(b) Show that the set of all random variables on a given probability space (Q, F, P) constitutes a
vector space over the reals. If Q is finite, write down a basis for this space.

4. Let X have distribution function

0 ifx <0,
F(x)=4 5x if0<x <2,
1 if x > 2,

12

Problems Exercises [2.7.5]-{2.7.13]

and let Y = X2. Find

@P(4<X<3), QOPU<X <2),
(c) PY < X), (d) P(X < 2Y),
(ec) P(X +¥ <3), _ (f) the distribution function of Z = VX.
5. Let X have distribution function
0 ifx < —l,
1-p if —l<x <0,
l—-p+yxp if0<x <2,
1 ifx > 2.

Sketch this function, and find: (a) PX = —1), (b) P(X =0), (c) P(X > 1).

6. Buses arrive at ten minute intervals starting at noon. A man arrives at the bus stop a random
number X minutes after noon, where X has distribution function

0 ifx <0,
P(X <x) = x/60 if0 <x < 60,
1 if x > 60.

What is the probability that he waits less than five minutes for a bus?

7. Airlines find that each passenger who reserves a seat fails to turn up with probability b inde-
pendently of the other passengers. So Teeny Weeny Airlines always sell 10 tickets for their 9 seat
aeroplane while Blockbuster Airways always sell 20 tickets for their 18 seat aeroplane. Which is more
often over-booked?

8. A fairground performer claims the power of telekinesis. The crowd throws coins and he wills
them to fall heads up. He succeeds five times out of six. What chance would he have of doing at least
as well if he had no supernatural powers?

9, Express the distribution functions of
Xt = max{0, X}, X~ =~min{0O,X}, |X|=X7t+4X7, —-X,

in terms of the distribution function F of the random variable X.
10. Show that Fy (x) is continuous at x = xo if and only if P(X = xp) = 0.

11. The real number m is called a median of the distribution function F whenever limysm F(y) <

5 < F(m). Show that every distribution function F has at least one median, and that the set of

medians of F is a closed interval of R.

12. Show that it is not possible to weight two dice in such a way that the sum of the two numbers
shown by these loaded dice is equally likely to take any value between 2 and 12 (inclusive).

13. A function d : S x S — Ris called a metric on S if:
(@) d(s,t) =d(t,s) > Oforalls,t € S,
Gi) d(s, t) = Oif and only if s = t, and
Giii) d(s,t) < d(s,u) + d(u,t) foralls,t,ue¢ S.
(a) Lévy metric. Let F and G be distribution functions and define the Lévy metric

a (F, G) = inf {¢ >0:Ga@—«)-e< F@)<Gat+et+e for all x}.
Show that dy, is indeed a metric on the space of distribution functions.

13
[2.7.14]-[2.7.18] Exercises Random variables and their distributions

(b) Total variation distance. Let X and Y be integer-valued random variables, and let

dpy(X,Y) = So|P(X =k) - PY =).
k

Show that dry satisfies (i) and (iii) with S the space of integer-valued random variables, and that
dyy(X, Y) = 0 if and only if P(X = Y) = 1. Thus dry is a metric on the space of equivalence
classes of S with equivalence relation given by X ~ Y if P(X = Y) = 1. We call dtvy the total
variation distance.

Show that
dry(X, Y) =2 sup |P(X € A) — P(Y € A).
ACZ

14, Ascertain in the following cases whether or not F is the joint distribution function of some pair
(X, Y) of random variables. If your conclusion is affirmative, find the distribution functions of X and
Y separately.

1—e7*-¥ ifx,y > 0,
a Fa,y= _
(a) (y) { 0 otherwise.
l-e*—xe if0<x<y,
(b) Fa,y)=4 1l-e%-ye if0<y<x,
0 otherwise.
15. Itis required to place in order n books By, Bo, ..., By on alibrary shelf in such a way that readers

searching from left to right waste as little time as possible on average. Assuming that each reader
requires book B; with probability p,, find the ordering of the books which minimizes P(T = k) for
all k, where T is the (random) number of titles examined by a reader before discovery of the required
book.

16. Transitive coins. Three coins each show heads with probability 3 and tails otherwise. The first
counts 10 points for a head and 2 for a tail, the second counts 4 points for both head and tail, and the
third counts 3 points for a head and 20 for a tail.

You and your opponent each choose a coin; you cannot choose the same coin. Each of you tosses
your coin and the person with the larger score wins £ 10!°, Would you prefer to be the first to pick a
coin or the second?

17. Before the development of radar and inertial navigation, flying to isolated islands (for example,
from Los Angeles to Hawaii) was somewhat ‘hit or miss’. In heavy cloud or at night it was necessary
to fly by dead reckoning, and then to search the surface. With the aid of a radio, the pilot had a good
idea of the correct great circle along which to search, but could not be sure which of the two directions
along this great circle was correct (since a strong tailwind could have carried the plane over its target).
When you are the pilot, you calculate that you can make n searches before your plane will run out of
fuel. On each search you will discover the island with probability p (if it is indeed in the direction of
the search) independently of the results of other searches; you estimate initially that there is probability
a that the island is ahead of you. What policy should you adopt in deciding the directions of your
various searches in order to maximize the probability of locating the island?

18. Eight pawns are placed randomly on a chessboard, no more than one to a square. What is the
probability that:

(a) they are in a straight line (do not forget the diagonals)?

(b) no two are in the same row or column?

14

Problems Exercises [2.7.19]-[2.7.20]

19. Which of the following are distribution functions? For those that are, give the corresponding

density function f.
x > 0,

1-e~*
(a) F(x) =

0 otherwise.

—1/x 0

e x>
b) FQ) = ,
(6) F@) { 0 otherwise.
(c) F(x) = e*/(e* +e-*), x ER.
(dd) F@)= eo? +e*/(e* +e7*), x ER.
20. (a) If U and V are jointly continuous, show that POU = V) = 0.
(b) Let X be uniformly distributed on (0,1), and let Y = X. Then X and Y are continuous, and
P(X = Y) = 1. Is there a contradiction here?

15

3

Discrete random variables

3.1 Exercises. Probability mass functions

1. For what values of the constant C do the following define mass functions on the positive integers
1,2,...?
(a) Geometric: f(x) = C27*.
(b) Logarithmic: f(x) = C27*/x.
(c) Inverse square: f(x) = Cx.
(d) ‘Modified’ Poisson: f (x) = C2*/x!.
2. For arandom variable X having (in turn) each of the four mass functions of Exercise (1), find:
Gi) P(X > 1),
(ii) the most probable value of X,
(iii) the probability that X is even.
3. We toss n coins, and each one shows heads with probability p, independently of each of the

others. Each coin which shows heads is tossed again. What is the mass function of the number of
heads resulting from the second round of tosses?

4. Let S, be the set of positive integers whose base-10 expansion contains exactly k elements (so
that, for example, 1024 € S4). A fair coin is tossed until the first head appears, and we write T for
the number of tosses required. We pick a random element, N say, from S7, each such element having
equal probability. What is the mass function of N?

5. Log-convexity. (a) Show that, if X is a binomial or Poisson random variable, then the mass
function f (k) = P(X =k) has the property that f(k —D f(k + 1 < f(k)”.

(b) Show that, if f(k) = 90/(k)*, k > 1, then f(kK —Df(k+1) = f(b”.

(c) Find a mass function f such that f(k)? = f(k —Df(k+1), k>1.

3.2 Exercises. Independence

1. Let X and Y be independent random variables, each taking the values —1 or 1 with probability
5. and let Z = XY. Show that X, Y, and Z are pairwise independent. Are they independent?
2.

Let X and Y be independent random variables taking values in the positive integers and having

the same mass function f(x) = 2~* for x =1,2,.... Find:
(a) Pamin{X, Y} <x), (b) P(Y > X),
(c) P(X = Y), (d) P(X = kY), for a given positive integer k,

(e) P(X divides Y), (f) P(X =rY), for a given positive rational r.

16

Expectation Exercises [3.2.3]-[3.3.5]

3. Let X1, X2, X3 be independent random variables taking values in the positive integers and having
mass functions given by P(X; = x) = (1 — pip forx = 1,2,..., andi = 1, 2,3.
(a) Show that
(1 — p1)(1 — pa) pap5
(1 — pop3)(l — pip2p3)

P(X, < X2 < X3) =

(b) Find P(X) < Xz < X3).
4. Three players, A, B, and C, take turns to roll a die; they do this in the order ABCABCA....
(a) Show that the probability that, of the three players, A is the first to throw a 6, B the second, and
C the third, is 216/1001.
(b) Show that the probability that the first 6 to appear is thrown by A, the second 6 to appear is thrown
by B, and the third 6 to appear is thrown by C, is 46656/753571.
5. Let X,,1 <r <n, be independent random variables which are symmetric about 0; that is,
X, and —X, have the same distributions. Show that, for all x, P(S, > x) = P(S, < —x) where
Sn = or Xr.
Is the conclusion necessarily true without the assumption of independence?

3.3 Exercises. Expectation

1. Is it generally true that E(1/X) = 1/E(X)? Is it ever true that E(1/X) = 1/E(X)?

2. Coupons. Every package of some intrinsically dull commodity includes a small and exciting

plastic object. There are c different types of object, and each package is equally likely to contain any

given type. You buy one package each day.

(a) Find the mean number of days which elapse between the acquisitions of the jth new type of object
and the (j + 1)th new type.

(b) Find the mean number of days which elapse before you have a full set of objects.

3. Each member of a group of n players rolls a die.

(a) For any pair of players who throw the same number, the group scores | point. Find the mean and
variance of the total score of the group.

(b) Find the mean and variance of the total score if any pair of players who throw the same number
scores that number.

4, St Petersburg paradox. A fair coin is tossed repeatedly. Let T be the number of tosses until

the first head. You are offered the following prospect, which you may accept on payment of a fee. If

T =k, say, then you will receive £2". What would be a ‘fair’ fee to ask of you?

5. Let X have mass function

{x(a+D}-! ifx =1,2,...,
0 otherwise,

fay={
and let a € R. For what values of « is it the caset that E(X%) < 00?

+This problem was mentioned by Nicholas Bernoulli in 1713, and Daniel Bernoulli wrote about the question
for the Academy of St Petersburg.

If @ is not integral, than E(X%) is called the fractional moment of order a of X. A point concerning
notation: for real a and complex x = re!?, x” should be interpreted as r%e’?”, so that |x| = r®. In particular,
E(|X*|) = E(|X|®).

17

[3.3.6]-[3.4.8] Exercises Discrete random variables

6. Show that var(a + X) = var(X) for any random variable X and constant a.

7. Arbitrage. Suppose you find a warm-hearted bookmaker offering payoff odds of (k) against
the kth horse in an n-horse race where rel {(k) + 1}-! < 1. Show that you can distribute your
bets in such a way as to ensure you win.

8. You roll a conventional fair die repeatedly. If it shows 1, you must stop, but you may choose to
stop at any prior time. Your score is the number shown by the die on the final roll. What stopping
strategy yields the greatest expected score? What strategy would you use if your score were the square
of the final roll?

9. Continuing with Exercise (8), suppose now that you lose c points from your score each time you
roll the die. What strategy maximizes the expected final score if ¢ = 5? What is the best strategy if
c=1?

3.4 Exercises. Indicators and matching

1. A biased coin is tossed ” times, and heads shows with probability p on each toss. A run isa
sequence of throws which result in the same outcome, so that, for example, the sequence HHTHTTH
contains five runs. Show that the expected number of runs is 1 + 2(n — 1) p(1 — p). Find the variance
of the number of runs.

2. Anurn contains 7 balls numbered 1, 2, ...,. Weremove k balls at random (without replacement)
and add up their numbers. Find the mean and variance of the total.

3. Of the 2n people in a given collection of n couples, exactly m die. Assuming that the m have
been picked at random, find the mean number of surviving couples. This problem was formulated by
Daniel Bernoulli in 1768.

4. Urn R contains n red balls and urn B contains n blue balls. At each stage, a ball is selected at
random from each urn, and they are swapped. Show that the mean number of red balls in urn R after

stage k is zn{l + (1 -2/ nyk }. This ‘diffusion model’ was described by Daniel Bernoulli in 1769.

5. Consider a square with diagonals, with distinct source and sink. Each edge represents a component
which is working correctly with probability p, independently of all other components. Write down an
expression for the Boolean function which equals 1 if and only if there is a working path from source
to sink, in terms of the indicator functions X; of the events {edge i is working} as i runs over the set
of edges. Hence calculate the reliability of the network.

6. A system is called a ‘k out of n’ system if it contains n components and it works whenever k or
more of these components are working. Suppose that each component is working with probability
p, independently of the other components, and let X, be the indicator function of the event that
component c is working. Find, in terms of the X,, the indicator function of the event that the system
works, and deduce the reliability of the system.

7. The probabilistic method. Let G = (V, EZ) bea finite graph. For any set W of vertices and any
edge e € E, define the indicator function

1 if e connects W and W°,

0 otherwise.

Iw(e) = {

Set Nw = Yicce Iw(e). Show that there exists W C V such that Nw > sIEl.

8. A total of n bar magnets are placed end to end in a line with random independent orientations.
Adjacent like poles repel, ends with opposite polarities join to form blocks. Let X be the number of
blocks of joined magnets. Find E(X) and var(X).

18

Dependence Exercises [3.4.9]-[3.6.5]

9. Matching. (a) Use the inclusion—exclusion formula (3.4.2) to derive the result of Example
(3.4.3), namely: in a random permutation of the first » integers, the probability that exactly r retain

their original positions is
1 (5 1 feed (-1)"*
ri\2! 3! (n—r)t J

(b) Let dy, be the number of derangements of the first n integers (that is, rearrangements with no
integers in their original positions). Show that d,41 = ndn +nd,_, forn > 2. Deduce the result of
part (a).

3.5 Exercises. Examples of discrete variables

1. De Moivre trials. Each trial may result in any of ¢ given outcomes, the ith outcome having
probability p;. Let Nj; be the number of occurrences of the ith outcome in n independent trials. Show

that
n!

P(N; = nj for 1 <i <t) = ————— p'! py? -- py"
nying!---ny!
for any collection 11,2, ...,n; of non-negative integers with sum n. The vector N is said to have

the multinomial distribution.

2. In your pocket is a random number N of coins, where N has the Poisson distribution with
parameter A. You toss each coin once, with heads showing with probability p each time. Show that
the total number of heads has the Poisson distribution with parameter Ap.

3. Let X be Poisson distributed where P(X = 2) = pa(A) = Ate /n! for n > 0. Show that
P(X <n) =1- fo Pn (x) dx.

4. Capture-recapture. A population of b animals has had a number a of its members captured,

marked, and released. Let X be the number of animals it is necessary to recapture (without re-release)
in order to obtain m marked animals. Show that

vam 5 (roa) / (2)

and find EX. This distribution has been called negative hypergeometric.

3.6 Exercises. Dependence
1, Show that the collection of random variables on a given probability space and having finite
variance forms a vector space over the reals.
2. Find the marginal mass functions of the multinomial distribution of Exercise (3.5.1).
3. Let X and Y be discrete random variables with joint mass function

Cc
ty-)atyatyt+l’

f@.y)= x,y =1,2,3,....

Find the marginal mass functions of X and Y, calculate C, and also the covariance of X and Y.

4. Let X and Y be discrete random variables with mean 0, variance 1, and covariance p. Show that
E(max{X?, ¥7}) <14+ V1 — p?.

5. Mutual information. Let X and Y be discrete random variables with joint mass function /.

19
[3.6.6]-[3.7.2] Exercises Discrete random variables

(a) Show that E(log fy(X)) > Edog fy(X)).
(b) Show that the mutual information

far \)
= l —_———
/ B (108 |

satisfies 7 > 0, with equality if and only if X and Y are independent.

6. Voter paradox. Let X, Y, Z be discrete random variables with the property that their values are
distinct with probability 1. Leta = P(X > Y),b=P(Y > Z),c =P(Z > X).

(a) Show that min{a, b, c} < , and give an example where this bound is attained.

(b) Show that, if X, Y, Z are independent and identically distributed, then a = b = c = 7:

(c) Find min{a, b, c} and SUP» min{a, b,c} when P(X = 0) = 1, and Y, Z are independent with
P(Z = 1) = P(Y = -1) = p, P(Z = —2) = P(YY = 2) = 1 — p. Here, SUP, denotes the
supremum as p varies over [0, 1].

[Part (a) is related to the observation that, in an election, it is possible for more than half of the voters

to prefer candidate A to candidate B, more than half B to C, and more than half C to A.]

7. Benford’s distribution, or the law of anomalous numbers. If one picks a numerical entry at
random from an almanac, or the annual accounts of a corporation, the first two significant digits, X,
Y, are found to have approximately the joint mass function

FCs, y) = logo (1+ ), 1<x<9,0<y<9.

10x + y

Find the mass function of X and an approximation to its mean. [A heuristic explanation for this
phenomenon may be found in the second of Feller’s volumes (1971).]

8. Let X and Y have joint mass function

i+ kai tk
FU. = Re k=O,
jie

where a is a constant. Find c, P(X = j), P(X + Y =r), and E(X).

3.7 Exercises. Conditional distributions and conditional expectation

1. Show the following:

(a) E(@Y +bZ |X) = aE(Y | X)+bE(Z | X) fora,be R,

(b) E(Y | X) > Oif Y > 0,

(c) EQ {| X)=1,

(d) if X and Y are independent then E(Y | X) = E(Y),

(e) (‘pull-through property’) E(Y 2(X) | X) = g(X)E(¥ | X) for any suitable function g,

(f) (‘tower property’) E{E(Y | X, Z) | X} = E(Y | X) = Ef{E(Y | X) | X, Z}.
2. Uniqueness of conditional expectation. Suppose that X and Y are discrete random variables,
and that #(X) and y(X) are two functions of X satisfying

E($(X)g(X)) = E(y(X)g(X)) = E(¥g(X))

for any function g for which all the expectations exist. Show that #(X) and y(X) are almost surely
equal, in that P(@(X) = w(X)) = 1.

20

Sums of random variables Exercises [3.7.3]-[3.8.3]

3. Suppose that the conditional expectation of Y given X is defined as the (almost surely) unique
function y(X) such that E(w(X)g(X)) = E(¥g(X)) for all functions g for which the expectations
exist. Show (a)-(f) of Exercise (1) above (with the occasional addition of the expression ‘with
probability 1°).

4. How should we define var(Y | X), the conditional variance of Y given X? Show that var(Y) =
E(var(Y | X)) + var(E(Y | X)).

5. The lifetime of a machine (in days) is a random variable T with mass function f. Given that the
machine is working after t days, what is the mean subsequent lifetime of the machine when:

(a) f(x) =(N+1)7! forx € {0,1,..., N},

(b) f(x) =2~~ forx = 1,2,....

(The first part of Problem (3.11.13) may be useful.)

6 Let X1, X2,... be identically distributed random variables with mean yz, and let N be a random

variable taking values in the non-negative integers and independent of the X;. Let S = X, + X2+
-+» + Xy. Show that E(S | N) = wN, and deduce that E(S) = pE(N).

7. A factory has produced n robots, each of which is faulty with probability ¢. To each robot a test
is applied which detects the fault (if present) with probability 8. Let X be the number of faulty robots,
and Y the number detected as faulty. Assuming the usual independence, show that

E(X | Y) = {ng(i — 8) + 1 — 6) ¥}/(1 — 8).

8. Families. Each child is equally likely to be male or female, independently of all other children.

(a) Show that, in a family of predetermined size, the expected number of boys equals the expected
number of girls. Was the assumption of independence necessary?

(b) A randomly selected child is male; does the expected number of his brothers equal the expected
number of his sisters? What happens if you do not require independence?

9. Let X and Y be independent with mean jz. Explain the error in the following equation:

E(X | X+Y =z) =E(X|X=z-Y)=E(e—-Y) =z-w’.

10. A coin shows heads with probability p. Let X, be the number of flips required to obtain a run of
n consecutive heads. Show that E(Xn) = S_, p-*.

3.8 Exercises. Sums of random variables

1. Let X and Y be independent variables, X being equally likely to take any value in {0, 1,..., m},
and Y similarly in {0, 1,...,}. Find the mass function of Z = X + Y. The random variable Z is
said to have the trapezoidal distribution.

2. Let X and Y have the joint mass function

Cc
(x+y—-Daetye+tyt+l’

faywe= x,y=1,2,3,....

Find the mass functions of VU = X + Y and V = X -— Y.

3. Let X and Y be independent geometric random variables with respective parameters a and f.
Show that B
at _
P(X +¥ =2)= reerase — pe! —-a@)*"}.

21

[3.8.4]-[3.9.6] Exercises Discrete random variables

4. Let {X;:1 <r <n} be independent geometric random variables with parameter p. Show that
Z= pan X, has a negative binomial distribution. [Hint: No calculations are necessary. ]

5. Pepys’s problem}. Sam rolls 6n dice once; he needs at least n sixes. Isaac rolls 6(n + 1) dice;
he needs at least n + 1 sixes. Who is more likely to obtain the number of sixes he needs?

6. Let N be Poisson distributed with parameter 4. Show that, for any function g such that the
expectations exist, E(Ng(NV)) = AEg(N + 1). More generally, if 5 = ys X,, where {X; : r > 0}
are independent identically distributed non-negative integer-valued random variables, show that

E(Sg(S)) = AE(g(S + Xo)Xo).

3.9 Exercises. Simple random walk

1. Let T be the time which elapses before a simple random walk is absorbed at either of the absorbing
barriers at 0 and N, having started at k where 0 < k < N. Show that P(T < oo) = 1 and E(T*) < 00
for all k > 1.

2. For simple random walk S with absorbing barriers at 0 and N, let W be the event that the particle
is absorbed at 0 rather than at NV, and let py, = P(W | So = k). Show that, if the particle starts at
k where 0 < k < N, the conditional probability that the first step is rightwards, given W, equals
PPr+1/ Pr. Deduce that the mean duration J; of the walk, conditional on W, satisfies the equation

PPk+1Sk+1 — PeIk + (Pk — PPk+i)Jk-1 = —Pk, forO<k <N.
Show that we may take as boundary condition Jo = 0. Find J; in the symmetric case, when p = 4.

3. With the notation of Exercise (2), suppose further that at any step the particle may remain where
it is with probability r where p + g +r = 1. Show that J; satisfies

PPkAA S41 — U — 1) pe Jk + 9Pk—-1Sk-1 = —Pk
and that, when p = q/p # 1,

1
Ji W {rt +p) —

1—p%

_ 2NpN (1 — p*)
p-@q pk—-p

4. Problem of the points. A coin is tossed repeatedly, heads turning up with probability p on each
toss. Player A wins the game if m heads appear before n tails have appeared, and player B wins
otherwise. Let pmn be the probability that A wins the game. Set up a difference equation for the pmn.
What are the boundary conditions?

5. Consider a simple random walk on the set {0, 1,2, ..., N} in which each step is to the right with
probability p or to the left with probability g = 1 — p. Absorbing barriers are placed at 0 and N.
Show that the number X of positive steps of the walk before absorption satisfies

E(X) = 4{Dg—k +N — pe}
where D, is the mean number of steps until absorption and p, is the probability of absorption at 0.

6. (a) “Millionaires should always gamble, poor men never” [J. M. Keynes].
(b) “If I wanted to gamble, I would buy a casino” [P. Getty].

(c) “That the chance of gain is naturally overvalued, we may learn from the universal success of
lotteries” [Adam Smith, 1776].

Discuss.

+Pepys put a simple version of this problem to Newton in 1693, but was reluctant to accept the correct reply
he received.

22
Problems Exercises [3.10.1]-[3.11.6]

3.10 Exercises. Random walk: counting sample paths

1. Consider a symmetric simple random walk S with Sg = 0. Let T = min{n > 1: S, = 0} be the
time of the first return of the walk to its starting point. Show that

F(T = 2n) = ao (*") 2-2n
n—

l\n

and deduce that E(T“) < oo if and only ifa < 5. You may need Stirling’s formula: n! ~
1
nto" Son,

2. For asymmetric simple random walk starting at 0, show that the mass function of the maximum
satisfies P(M, = r) = P(S, =r) +P(Sy, =r +1) forr > 0.

3. For asymmetric simple random walk starting at 0, show that the probability that the first visit to
Son takes place at time 2k equals the product P(S2, = 0)P(S2,-2, = 0), forO < k <n.

3.11 Problems

1. (a) Let X and Y be independent discrete random variables, and let g, h : R + R. Show that 9(X)
and h(Y) are independent.

(b) Show that two discrete random variables X and Y are independent if and only if fy, y(x, y) =
Ix (x) fy (y) for all x, y ER.

(c) More generally, show that X and Y are independent if and only if fy y(x, y) can be factorized
as the product 9(x)h(y) of a function of x alone and a function of y alone.

2. Show that if var(X) = 0 then X is almost surely constant; that is, there exists a € R such that
P(X = a) = 1. (First show that if E(X2) = 0 then P(X = 0) = 1.)

3. (a) Let X be a discrete random variable and let g : R > R. Show that, when the sum is absolutely
convergent,

E(g(X)) = D> g(x)P(X = x).

(b) If X and Y are independent and g,h : R — R, show that E(g(X)A(Y)) = E(g(X))E(A(Y))
whenever these expectations exist.

4, Let Q = {«1, @2, w3}, with P(w)) = P(@2) = P(w3) = 4. Define X, ¥, Z: Q > R by

X(@1)=1, X(@2)=2, X(w3) =3,

Y(m1)=2, Y(@2)=3, Y(@3)=1,

Z(@|)=2, Z(@2)=2, Z{w3) = 1.
Show that X and Y have the same mass functions. Find the mass functions of X + Y, XY, and X/Y.
Find the conditional mass functions fy|z and fzyy.

5. For what values of k and a is f a mass function, where:

(a) fy) =k/{nvt+Djn=1,2,...,

(b) f(m) = kn®,n =1,2,... (zeta or Zipf distribution)?

6. Let X and Y be independent Poisson variables with respective parameters 1 and jz. Show that:
(a) X + Y is Poisson, parameter 1. + 2,

(b) the conditional distribution of X, given X + Y =n, is binomial, and find its parameters.

23
(3.11.7]-[3.11.14] Exercises Discrete random variables

7. If X is geometric, show that P(X =n+k |X > n) = P(X =&) fork, n > 1. Why do you think
that this is called the ‘lack of memory’ property? Does any other distribution on the positive integers
have this property?

8. Show that the sum of two independent binomial variables, bin(m, p) and bin(n, p) respectively,
is bin(m +n, p).

9. Let N be the number of heads occurring inn tosses of a biased coin. Write down the mass function
of N in terms of the probability p of heads turning up on each toss. Prove and utilize the identity

» (3) =3{@ ty" +0-2)"}

in order to calculate the probability p, that N is even. Compare with Problem (1.8.20).

10. Anurn contains N balls, b of which are blue and r (= N — b) of which are red. A random sample
of n balls is withdrawn without replacement from the urn. Show that the number B of blue balls in
this sample has the mass function

men ()(09/()

This is called the hypergeometric distribution with parameters N, b, and n. Show further that if N, b,
and r approach oo in such a way that b/N — p andr/N — 1 — p, then

P(B =k > (7) pea — pyr.

You have shown that, for small n and large N, the distribution of B barely depends on whether or not
the balls are replaced in the urn immediately after their withdrawal.

11, Let X and Y be independent bin(n, p) variables, and let Z = X + Y. Show that the conditional
distribution of X given Z = N is the hypergeometric distribution of Problem (3.11.10).

12. Suppose X and Y take values in {0, 1}, with joint mass function f(x, y). Write f(0,0) = a,
fO, 1) = b, fA, 0) =c, fU, ) = d, and find necessary and sufficient conditions for X and Y to
be: (a) uncorrelated, (b) independent.

13. (a) If X takes non-negative integer values show that
io.¢]
E(X) = 5) P(X > n).
=0

(b) An urn contains b blue and r red balls. Balls are removed at random until the first blue ball is
drawn. Show that the expected number drawn is (b +r 4+ 1)/(b4+ 1).

(c) The balls are replaced and then removed at random until all the remaining balls are of the same
colour. Find the expected number remaining in the urn.

14. Let X,, X2,..., Xn be independent random variables, and suppose that X; is Bernoulli with
parameter py. Show that Y = X,; + X2 +--+-+ Xn has mean and variance given by

E(Y)= >> pe, var(¥) = >> pe(1 — pr).
1 1

24
Problems Exercises [3.11.15]-[3.11.21]

Show that, for E(Y) fixed, var(Y) is a maximum when p; = p2 = -:- = pn. That is to say, the
variation in the sum is greatest when individuals are most alike. Is this contrary to intuition?

15. Let X = (Xj, X2,..., Xn) bea vector of random variables. The covariance matrix V(X) of X is
defined to be the symmetric n by n matrix with entries (vj; : 1 <i, 7 <n) givenby uj; = cov(X;, X;).
Show that |[VCX)| = 0 if and only if the X; are linearly dependent with probability one, in that
P(a,X1 +a2X2 +---+anXpn = b) = 1 for some a and b. (|V| denotes the determinant of V.)

16. Let X and Y be independent Bernoulli random variables with parameter 5. Show that X + Y and
|X — Y| are dependent though uncorrelated.

17. A secretary drops n matching pairs of letters and envelopes down the stairs, and then places the
letters into the envelopes in a random order. Use indicators to show that the number X of correctly
matched pairs has mean and variance 1 for all n > 2. Show that the mass function of X converges to
a Poisson mass function as n —> oo.

18. LetX = (Xj, X2,..., Xn) bea vector of independent random variables each having the Bernoulli

distribution with parameter p. Let f : {0, 1}” — R be increasing, which is to say that f(x) < f(y)

whenever x; < y; for eachi.

(a) Let e(p) = E( f(X)). Show that e(p1) < e(p2) if pi < po.

(b) FKG inequality}. Let f and g be increasing functions from {0, 1}” into R. Show by induction
on n that cov( f (X), g(K)) > 0.

19. Let R(p) be the reliability function of a network G with a given source and sink, each edge of
which is working with probability p, and let A be the event that there exists a working connection
from source to sink. Show that

R(p) = S T4(@) pN@ (1 — pym-N@)

where w is a typical realization (i.e., outcome) of the network, N(w) is the number of working edges
of w, and m is the total number of edges of G.
Deduce that R’(p) = cov(J4, N)/{p(1 — p)}, and hence that

R(p)(1 — R(p)) _ oy mR(p)(1 — R(p))
———__—_——— <R ———
pd-p) sy pil —p)

20. Let R(:p) be the reliability function of a network G, each edge of which is working with probability

p.

(a) Show that R(pi p2) < R(p1)R(p2) if0 < pi, p2 <1.

(b) Show that R(p”) < R(p)” for all0 < p< landy > 1.

21. DNA fingerprinting. In a certain style of detective fiction, the sleuth is required to declare “‘the

criminal has the unusual characteristics ...; find this person and you have your man”. Assume that

any given individual has these unusual characteristics with probability 10—7 independently of all other

individuals, and that the city in question contains 107 inhabitants. Calculate the expected number of

such people in the city.

(a) Given that the police inspector finds such a person, what is the probability that there is at least
one other?

(b) If the inspector finds two such people, what is the probability that there is at least one more?

(c) How many such people need be found before the inspector can be reasonably confident that he

has found them all?

+Named after C. Fortuin, P. Kasteleyn, and J. Ginibre (1971), but due in this form to T. E. Harris (1960).

25

[3.11.22]-{3.11.30] Exercises Discrete random variables

(d) For the given population, how improbable should the characteristics of the criminal be, in order
that he (or she) be specified uniquely?

22. In 1710, J. Arbuthnot observed that male births had exceeded female births in London for 82
successive years. Arguing that the two sexes are equally likely, and 2~82 is very small, he attributed
this run of masculinity to Divine Providence. Let us assume that each birth results in a girl with
probability p = 0.485, and that the outcomes of different confinements are independent of each other.
Ignoring the possibility of twins (and so on), show that the probability that girls outnumber boys in 2n
live births is no greater than 2") P"a"{q/(q — p)}, where q = 1 — p. Suppose that 20,000 children
are born in each of 82 successive years. Show that the probability that boys outnumber girls every
year is at least 0.99. You may need Stirling’s formula.

23. Consider a symmetric random walk with an absorbing barrier at N and a reflecting barrier at 0
(so that, when the particle is at 0, it moves to 1 at the next step). Let a, (j) be the probability that
the particle, having started at k, visits 0 exactly j times before being absorbed at N. We make the
convention that, if k = 0, then the starting point counts as one visit. Show that

N-k 1\J7!

24. Problem of the points (3.9.4). A coin is tossed repeatedly, heads turning up with probability p
on each toss. Player A wins the game if heads appears at least m times before tails has appeared n
times; otherwise player B wins the game. Find the probability that A wins the game.

25. A coin is tossed repeatedly, heads appearing on each toss with probability p. A gambler starts
with initial fortune k (where 0 < k < N); he wins one point for each head and loses one point for
each tail. If his fortune is ever 0 he is bankrupted, whilst if it ever reaches N he stops gambling to buy

a Jaguar. Suppose that p < 4 Show that the gambler can increase his chance of winning by doubling
the stakes. You may assume that k and N are even.
What is the corresponding strategy if p > 5?

26. A compulsive gambler is never satisfied. At each stage he wins £1 with probability p and loses
£1 otherwise. Find the probability that he is ultimately bankrupted, having started with an initial
fortune of £k.

27. Range of random walk. Let {X, : n > 1} be independent, identically distributed random
variables taking integer values. Let Sp = 0, Sy = S07, X;. The range Ry of So, S1,..., Sn is the
number of distinct values taken by the sequence. Show that P(Ry = R,-1 +1) = P(S1S2--- Sp #9),
and deduce that, asn —> ov,

1
—E(Rn) > PCS, # 0 for all k > 1).
n

Hence show that, for the simple random walk, n—B(Ry) —> |p—gq|asn > oo.

28. Arc sine law for maxima. Consider a symmetric random walk S starting from the origin, and
let M, = max{S; :0 <i <n}. Show that, for i = 2k, 2k + 1, the probability that the walk reaches
Moy, for the first time at time i equals A P(Soy = 0)P(S2,_2,% = 0).

29. Let S be a symmetric random walk with Sg = 0, and let N, be the number of points that have
been visited by S exactly once up to time n. Show that E(V,) = 2.

30. Family planning. Consider the following fragment of verse entitled ‘Note for the scientist’.

People who have three daughters try for more,
And then its fifty—fifty they'll have four,
Those with a son or sons will let things be,

26

Problems Exercises [3.11.31]-{3.11.37]

Hence all these surplus women, QED.

(a) What do you think of the argument?

(b) Show that the mean number of children of either sex in a family whose fertile parents have
followed this policy equals 1. (You should assume that each delivery yields exactly one child
whose sex is equally likely to be male or female.) Discuss.

31. Let B > 1, let pj, po,... denote the prime numbers, and let N(1), N(2), ... be independent

random variables, N(i) having mass function P(N (i) = k) = (A — vive for k > 0, where y; = P; B
for all i. Show that M = [[S°, p)" is a random integer with mass function P(M = m) = Cm~8
for m > 1 (this may be called the Dirichlet distribution), where C is a constant satisfying

m=1

32. N+ 1 plates are laid out around a circular dining table, and a hot cake is passed between them in
the manner of a symmetric random walk: each time it arrives on a plate, it is tossed to one of the two
neighbouring plates, each possibility having probability S. The game stops at the moment when the
cake has visited every plate at least once. Show that, with the exception of the plate where the cake
began, each plate has probability 1/N of being the last plate visited by the cake.

33. Simplex algorithm. There are (7) points ranked in order of merit with no matches. You seek to
reach the best, B. If you are at the jth best, you step to any one of the j — 1 better points, with equal
probability of stepping to each. Let rj be the expected number of steps to reach B from the jth best

vertex. Show that r; = ia k—!, Give an asymptotic expression for the expected time to reach B
from the worst vertex, for large m,n.

34, Dimer problem. There are n unstable molecules in a row, m,,m2,... , mn. One of then — 1
pairs of neighbours, chosen at random, combines to form a stable dimer; this process continues until
there remain U;, isolated molecules no two of which are adjacent. Show that the probability that m
remains isolated is (-))"/ris e—! asn — oo. Deduce that limy—+oo n— EU, =e ?,

35. Poisson approximation. Let {7, : 1 <r <n} be independent Bernoulli random variables with
respective parameters {py : 1 <r <n} satisfying py <c < 1forallr andsomec. LetA = 7?_, pr
and X = 5°?_, Xr. Show that

rea kK
PX =hH= i 1+0 A max Pr + —- max Pr .

36. Sampling. The length of the tail of the rth member of a troop of N chimeras is xy. A random
sample of n chimeras is taken (without replacement) and their tails measured. Let J; be the indicator
of the event that the rth chimera is in the sample. Set

~ 1< ix ix
Xyp=xrly, Y= n > Xp, X= > Xr, a? = WV Dr — x).
r=

Show that E(Y) = y, and var(Y) = (N —n)o2/{n(N — 1}.

37. Berkson’s fallacy. Any individual in a group G contracts a certain disease C with probability
y; such individuals are hospitalized with probability c. Independently of this, anyone in G may be
in hospital with probability a, for some other reason. Let X be the number in hospital, and Y the

27

[3.11.38]-[3.11.40] Exercises Discrete random variables

number in hospital who have C (including those with C admitted for any other reason). Show that the
correlation between X and Y is

d —a)(1 — ye)
pan = |e ye
—-yp at+yce-aye

where p = a +c —ac.
It has been stated erroneously that, when p(X, Y) is near unity, this is evidence for a causal
relation between being in G and contracting C.

38. A telephone sales company attempts repeatedly to sell new kitchens to each of the N families

in a village. Family i agrees to buy a new kitchen after it has been solicited K; times, where the K;

are independent identically distributed random variables with mass function f(n) = P(K; =n). The

value oo is allowed, so that f(co) > 0. Let X, be the number of kitchens sold at the nth round of
solicitations, so that X, = we I{K;=n}- Suppose that N is a random variable with the Poisson

distribution with parameter v.

(a) Show that the X, are independent random variables, X; having the Poisson distribution with
parameter vf (r).

(b) The company loses heart after the Tth round of calls, where T = inf{n : X, = O}. Let
S = X,+X2+---+ Xr be the number of solicitations made up to time T. Show further that
E(S) = vE(F(T)) where F(k) = f(1I) + f(QQ) +--- + Ff).

39. A particle performs a random walk on the non-negative integers as follows. When at the point n

(> 0) its next position is uniformly distributed on the set {0, 1, 2,..., + 1}. When it hits 0 for the

first time, it is absorbed. Suppose it starts at the point a.

(a) Find the probability that its position never exceeds a, and prove that, with probability 1, it is
absorbed ultimately.

(b) Find the probability that the final step of the walk is from 1 to0 when a = 1.

(c) Find the expected number of steps taken before absorption when a = 1.

40. Let G be a finite graph with neither loops nor multiple edges, and write dy for the degree of
the vertex v. An independent set is a set of vertices no pair of which is joined by an edge. Let
a(G) be the size of the largest independent set of G. Use the probabilistic method to show that
a(G) > >>, 1/(dy + 1). [This conclusion is sometimes referred to as Turdn’s theorem.]

28

4

Continuous random variables

4.1 Exercises. Probability density functions

1. For what values of the parameters are the following functions probability density functions?

(a) f@) =C{xd - x2, 0 <x < 1, the density function of the ‘arc sine law’.

(b) f(x) = Cexp(—x — e~*), x € R, the density function of the ‘extreme-value distribution’.

(c) f@®)=CO4+x2)-™, x ER.

2. Find the density function of Y = aX, where a > 0, in terms of the density function of X. Show

that the continuous random variables X and —X have the same distribution function if and only if
fx(x) = fx(—x) forall x eR.

3. If f and g are density functions of random variables X and Y, show thataf + (1 —a)g isa
density function for 0 < a < 1, and describe a random variable of which it is the density function.

4. Survival. Let X be a positive random variable with density function f and distribution function
F.. Define the hazard function H(x) = —log[1 — F(x)] and the hazard rate

1
r@) = Tim (P(X Sx +h |X > x), x >0.

Show that:

(a) r(x) = H'(x) = f(x)/{1- F@)},

(b) If r(x) increases with x then H(x)/x increases with x,

(c) H(x)/x increases with x if and only if [1 — F(x)]* < 1— F(ax) forall0 <a <1,
(d) If H(x)/x increases with x, then H(x + y) > H(x) + Hy) forall x, y > 0.

4.2 Exercises. Independence

1. [am selling my house, and have decided to accept the first offer exceeding £K. Assuming
that offers are independent random variables with common distribution function F’, find the expected
number of offers received before I sell the house.

2. Let X and Y be independent random variables with common distribution function F and density
function f. Show that V = max{X, Y} has distribution function P(V < x) = F (x)* and density
function fy (x) = 2f(x)F(), x € R. Find the density function of VU = min{X, Y}.

3. The annual rainfall figures in Bandrika are independent identically distributed continuous random
variables {X, : r > 1}. Find the probability that:

29
[4.2.4]-[4.4.5] Exercises Continuous random variables

(a) Xj < X2q < X3 < Xq,
(b) Xy > X2 < X3 < X4.

4. Let{X; :r > 1}be independent and identically distributed with distribution function F satisfying
F(y) < 1 for all y, and let Y(y) = min{k : X,; > y}. Show that

slim PY) < BY(y)) = 1 eT!

4.3 Exercises. Expectation

1. For what values of a is E(|X|*) finite, if the density function of X is:

(a) f(x) =e for x > 0,

(b) f(x) = CU +x2)~™ for x € R?

If a is not integral, then E({X|) is called the fractional moment of order a of X, whenever the
expectation is well defined; see Exercise (3.3.5).

2. Let X1, X2,..., Xn be independent identically distributed random variables for which E(X 1)
exists. Show that, if m < n, then E(S,;,/S,) = m/n, where Sy = Xj + Xz +---+Xm.

3. Let X be a non-negative random variable with density function f. Show that
foe)
E(X") = I rx’ P(X > x) dx
0
for any r > 1 for which the expectation is finite.

4, Show that the mean yz, median m, and variance o” of the continuous random variable X satisfy

(u —m)* <o?.

5. Let X bea random variable with mean yz and continuous distribution function F. Show that
a OO
/ Fo)dx = [ [1 — F(@)] dx,
—OO a

if and only if a = pw.

4.4 Exercises. Examples of continuous variables

1. Prove that the gamma function satisfies P(t) = (¢ — 1) C@¢ — 1) fort > 1, and deduce that
P@) = (a —1)! forn = 1,2,.... Show that T(4) = ./n and deduce a closed form for P(n + 5)
forn =0,1,2,....

2. Show, as claimed in (4.4.8), that the beta function satisfies B(a, b) =T(a)P(b)/T(a@+b).

3. Let X have the uniform distribution on [0, 1]. For what function g does Y = g9(X) have the
exponential distribution with parameter 1?

4. Find the distribution function of a random variable X with the Cauchy distribution. For what
values of a does |X| have a finite (possibly fractional) moment of order a?

5. Log-normal distribution. Let Y = e* where X has the N(0, 1) distribution. Find the density
function of Y.

30

Dependence Exercises [4.4.6]-[4.5.5]

6. Let X be N(u, o”), Show that E{(X — w)g(X)} = o7E(g’(X)) when both sides exist.

7. With the terminology of Exercise (4.1.4), find the hazard rate when:

(a) X has the Weibull distribution, P(X > x) = exp(—ax8-1), x > 0,

(b) X has the exponential distribution with parameter i,

(c) X has density function af + (1 — w)g, where 0 < a < 1 and f and g are the densities of
exponential variables with respective parameters A and jz. What happens to this last hazard rate
r(x) in the limit as x > 00?

8. Mills’s ratio. For the standard normal density ¢(x), show that ¢’(x) + x@(x) = 0. Hence show

that
1 1 1-@ 1 =#«1~— 3
x x

4.5 Exercises. Dependence

[|
f@ y= Ta exp{—|x|—5x7y?}, x,y ER.

Show that f is a continuous joint density function, but that the (first) marginal density function
gix)= ion i, y) dy is not continuous. Let Q = {qn : n > 1} bea set of real numbers, and define

le ¢)
folx.y) = ¥°G)" F@ — an. y).

n=1

Show that fg is a continuous joint density function whose first marginal density function is discon-
tinuous at the points in Q. Can you construct a continuous joint density function whose first marginal
density function is continuous nowhere?

2. Buffon’s needle revisited. Two grids of parallel lines are superimposed: the first grid contains
lines distance a apart, and the second contains lines distance b apart which are perpendicular to those
of the first set. A needle of length r (< min{a, b}) is dropped at random. Show that the probability it
intersects a line equals r(2a + 2b — r)/(2rab).

3. Buffon’s cross. The plane is ruled by the lines y = n, for n = 0, +1,..., and on to this plane
we drop a cross formed by welding together two unit needles perpendicularly at their midpoints. Let
Z be the number of intersections of the cross with the grid of parallel lines. Show that E(Z/2) = 2/x

and that B
3-2 4
var(Z/2) = =

-=.
a

If you had the choice of using either a needle of unit length, or the cross, in estimating 2/7, which
would you use?

4. Let X and Y be independent random variables each having the uniform distribution on [0, 1]. Let
U = min{X, Y} and V = max{X, Y}. Find E(V), and hence calculate cov(U, V).

5. Let X and Y be independent continuous random variables. Show that
E(g(X)A(Y)) = E(g(X))E(A(Y)),

whenever these expectations exist. If X and Y have the exponential distribution with parameter 1, find
Efexp(4(X + Y))}.

31

[4.5.6]-[4.6.9] Exercises Continuous random variables

6. Three points A, B, C are chosen independently at random on the circumference of a circle. Let
b(x) be the probability that at least one of the angles of the triangle ABC exceeds x. Show that

Hence find the density and expectation of the largest angle in the triangle.

7. Let {X,: 1 <r <n} be independent and identically distributed with finite variance, and define
X =n! 57"_, X,. Show that cov(X, X, — X) =0.

8. Let X and Y be independent random variables with finite variances, and let U = X + Y and
V = XY. Under what condition are U and V uncorrelated?

9. Let X and Y be independent continuous random variables, and let U be independent of X and Y
taking the values +1 with probability 7. Define S = UX and T = UY. Show that S and 7 are in
general dependent, but S? and T? are independent.

4.6 Exercises. Conditional distributions and conditional expectation

1. A point is picked uniformly at random on the surface of a unit sphere. Writing © and ® for its
longitude and latitude, find the conditional density functions of © given ®, and of ® given ©.

2. Show that the conditional expectation w(X) = E(Y | X) satisfies E(W(X)g(X)) = E(Vg(X)),
for any function g for which both expectations exist.

3. Construct an example of two random variables X and Y for which E(Y) = oo but such that
E(Y | X) < oo almost surely.

4. Find the conditional density function and expectation of Y given X when they have joint density
function:

(a) fa y= 42e7*Y for 0 <x<y<o,
(b) f(x,y) = xe*OF for x, y > 0.

5. Let Y be distributed as bin(n, X), where X is a random variable having a beta distribution on
[O, 1] with parameters a and b. Describe the distribution of Y, and find its mean and variance. What
is the distribution of Y in the special case when X is uniform?

6. Let {X, :r > 1} be independent and uniformly distributed on [0, 1]. Let 0 < x < 1 and define
N =min{n >1:X,+X2+-+-+ Xp > x}.

Show that P(N > n) = x”"/n!, and hence find the mean and variance of N.
7. Let X and Y be random variables with correlation p. Show that E(var(Y | X)) < d—- p*) var Y.

8. Let X, Y, Z be independent and exponential random variables with respective parameters A, (1, v.
Find P(X < Y < Z).

9, Let X and Y have the joint density f(x, y) = cx(y-—x)e?, O<x<y<oo.,
(a) Find c.
(b) Show that:

frye ly) =6x(y—x)y9, Ox <y,

fyix(Qy |x) = (y—x)e*7?, O<x<y<ovo.

32

Functions of random variables Exercises [4.6.10]-[4.7.12]

(c) Deduce that E(X | ¥Y) = 4Y and E(Y | X) = X +2.

10. Let {X; : r > 0} be independent and identically distributed random variables with density
function f and distribution function F. Let N = min{n > 1: Xy, > Xo} and M = min{n > 1:
Xo =X, S++ = Xpn-1 < Xn}. Show that Xy has distribution function F + (1 — F)log(1 — F),
and find P(M = m).

4.7 Exercises. Functions of random variables

1. Let X, Y, and Z be independent and uniformly distributed on [0, 1]. Find the joint density function
of XY and Z?, and show that P(XY¥ < Z7) = 3.

2. Let X and Y be independent exponential random variables with parameter 1. Find the joint density
function of U = X + Y and V = X/(X + Y), and deduce that V is uniformly distributed on [0, 1].

3. Let X be uniformly distributed on [0, hx]. Find the density function of Y = sin X.

4. Find the density function of Y = sin—! X when:
(a) X is uniformly distributed on [0, 1],
(b) X is uniformly distributed on [—1, 1].

5. Let X and Y have the bivariate normal density function

- | ft ge *)}
fe.) = so { x0 - ph" 2pxyty)e-

e

Show that X and Z = (Y — pX)/+/1 — 2 are independent N(O, 1) variables, and deduce that

1 1
P(X >0,¥>0)=-+—sin'! p.
4  2n

6. Let X and Y have the standard bivariate normal density function of Exercise (5), and define
Z = max{X, Y}. Show that E(Z) = /(1 — p)/m, and E(Z?) = 1.

7. Let X and Y be independent exponential random variables with parameters A and wz. Show that
Z = min{X, Y} is independent of the event {X < Y}. Find:

(a) P(X = Z),

(b) the distributions of U = max{X — Y, 0}, denoted (X — Y)*+, and V = max{X, Y} — min{X, Y},
(c) P(X <t < X+Y) wheret > 0.

8. A point (X, Y) is picked at random uniformly in the unit circle. Find the joint density of R and
X, where R?2 = X24 Y2.

9. A point (X, Y, Z) is picked uniformly at random inside the unit ball of R?. Find the joint density
of Z and R, where R? = X* + Y2 4 2.

10. Let X and Y be independent and exponentially distributed with parameters 4 and wz. Find the
joint distribution of S = X + Y and R = X/(X + Y). What is the density of R?

11. Find the density of ¥Y = a/(1 + X*), where X has the Cauchy distribution.
12. Let (X, Y) have the bivariate normal density of Exercise (5) with 0 < p < 1. Show that

pe) — 0@))

[i-@@]1 -Oe)] < PX >a, Y>b) <[1-O@)]1- Oo] + la)

?

33
[4.7.13]-[4.8.8] Exercises Continuous random variables

where c = (b — pa)/+/1 — p2,d = (a— pb)/V/1 — 02, and @ and © are the density and distribution
function of the N(O, 1) distribution.

13. Let X have the Cauchy distribution. Show that Y = X~! has the Cauchy distribution also. Find
another non-trivial distribution with this property of invariance.

14. Let X and Y be independent and gamma distributed as (A, a), P(A, B) respectively. Show that
W = X+4+Y and Z = X/(X +Y) are independent, and that Z has the beta distribution with parameters

a, B.

4.8 Exercises. Sums of random variables

1. Let X and Y be independent variables having the exponential distribution with parameters A and
pe respectively. Find the density function of X + Y.

2. Let X and Y be independent variables with the Cauchy distribution. Find the density function of
aX + BY where a8 # 0. (Do you know about contour integration?)

3. Find the density function of Z = X + Y when X and Y have joint density function f(x, y) =
Lox + ye Ot”), x,y >0.

4. Hypoexponential distribution. Let {X; : r => 1} be independent exponential random variables

with respective parameters {A, : r > 1} no two of which are equal. Find the density function of

Sn = >0Y_, Xr. [Hint: Use induction.]

5. (a) Let X, Y, Z be independent and uniformly distributed on [0, 1]. Find the density function of
X+Y+Z.

(b) If {X; : r > 1} are independent and uniformly distributed on [0, 1], show that the density of
yo Xp at any point x € (0, n) is a polynomial in x of degree n — 1.

6. For independent identically distributed random variables X and Y, show that U = X + Y and
V = X — ¥ are uncorrelated but not necessarily independent. Show that U and V are independent if
X and Y are N(0, 1).

7. Let X and Y have a bivariate normal density with zero means, variances o”, t”, and correlation
p. Show that:

(a) E(X | Y) = mY,

(b) var(X | Y) = 07(1 — p?),
(o2 + pot)z
o2+2p0t +12’
o212(1 — p2)
t24+2p0t +02

8. Let X and Y be independent N(0, 1) random variables, and let Z = X + Y. Find the distribution
and density of Z given that X > 0 and Y > 0. Show that

(©) EX|X+Y¥=z2)=

(d) va(X |X +¥=H=

E(Z | X > 0, Y > 0) =2V2/z.

34

Multivariate normal distribution Exercises [4.9.1]-[4.9.9]

4.9 Exercises. Multivariate normal distribution

1. A symmetric matrix is called non-negative (respectively positive) definite if its eigenvalues are
non-negative (respectively strictly positive). Show that a non-negative definite symmetric matrix V
has a square root, in that there exists a symmetric matrix W satisfying W* = V. Show further that W
is non-singular if and only if V is positive definite.

2. If X is a random vector with the N(, V) distribution where V is non-singular, show that Y =
(X - pw! has the N(0, I) distribution, where I is the identity matrix and W is a symmetric matrix
satisfying W2 = V. Therandom vector Y is said to have the standard multivariate normal distribution.

3. Let X = (X1, X2,..., Xn) have the N(, V) distribution, and show that Y = a, X, + a.Xo +
.++ ++ ayXy has the (univariate) N(w, o2) distribution where

n i
w= > ajE(X;), = Da? var(X;) +2) > ajajcov(X;, Xj).

i=1 i=1 i<j

4, Let X and Y have the bivariate normal distribution with zero means, unit variances, and correlation
p. Find the joint density function of X + Y and X — Y, and their marginal density functions.

5. Let X have the N(0, 1) distribution and let a > 0. Show that the random variable Y given by
, ye { X if |X| <a
~ (=x if |X|>a
has the N(0, 1) distribution, and find an expression for p(a) = cov(X, Y) in terms of the density
function ¢ of X. Does the pair (X, Y) have a bivariate normal distribution?

6. Let {¥, : 1 <r <n} be independent N(O, 1) random variables, and define X; = aan cir Yr,
1 <r <1, for constants cjy. Show that

E(X; | Xx) = (ae) Xp.
Dor Ckr
What is var(X; | X;)?

7. Let the vector (X; : 1 <r <n) have a multivariate normal distribution with covariance matrix
V = (v;;). Show that, conditional on the event Hi Xy = x, X1 has the N(a, b) distribution where

a = (ps/t)x, b =s*(1 — p*), ands? = v1, 1? = Oj; uj, 0 = Dy MA /(t).

8. Let X, Y, and Z have a standard trivariate normal distribution centred at the origin, with zero
means, unit variances, and correlation coefficients p1, 92, and p3. Show that

1 1
P(X >0,¥ >0,Z>0)= 5+ asin p, + sin—! pz + sin~! p3}.
Tt

9. Let X, Y, Z have the standard trivariate normal density of Exercise (8), with pj = o(X, Y). Show
that

E(Z | X,Y) = {(03 — pip2)X + (o2 — p103)¥ }/(1 — pj),
var(Z | X, ¥) = {1 — pf — 63 — 93 + 210203}/(1 — pj).

35

[4.10.1]-[4.11.7] Exercises Continuous random variables

4.10 Exercises. Distributions arising from the normal distribution

1, Let X, and X4 be independent variables with the x2(m) and x2(n) distributions respectively.
Show that X, + X9 has the x2(m +n) distribution.

2. Show that the mean of the t(r) distribution is 0, and that the mean of the F(r, s) distribution is
s/(s — 2) if s > 2. What happens if s < 2?

3. Show that the ¢(1) distribution and the Cauchy distribution are the same.

4. Let X and Y be independent variables having the exponential distribution with parameter 1. Show
that X/Y has an F distribution. Which?

5. Use the result of Exercise (4.5.7) to show the independence of the sample mean and sample
variance of an independent sample from the N (i, o2) distribution.

6. Let {X,; : 1 < r < n} be independent N(0, 1) variables. Let Y e€ [0,2] be the angle
between the vector (X,, X2,..., Xn) and some fixed vector in R”. Show that W has density
fh) = Gin wy"? BA, In - 3). 0 < y < x, where B is the beta function.

4.11 Exercises. Sampling from a distribution

1. Uniform distribution. If U is uniformly distributed on [0, 1], what is the distribution of X =
[nU}4+1?

2. Random permutation. Given the first n integers in any sequence Sg, proceed thus:

(a) pick any position Pp from {1, 2,... ,m} at random, and swap the integer in that place of Sg with
the integer in the nth place of So, yielding Sj.
(b) pick any position P, from {1,2,...,— 1} at random, and swap the integer in that place of Sj

with the integer in the (n ~— 1)th place of S1, yielding S»,

(c) at the (r — 1)th stage the integer in position P,_ , chosen randomly from {1,2,...,2 —r+ 1},
is swapped with the integer at the (x — r + 1)th place of the sequence S,_ .

Show that S, 1 is equally likely to be any of the n! permutations of {1, 2,..., n}.

3. Gamma distribution. Use the rejection method to sample from the gamma density ['(A, t) where
t (> 1) may not be assumed integral. [Hint: You might want to start with an exponential random
variable with parameter 1/t.]

4. Beta distribution. Show how to sample from the beta density B(a, 8) where a, 8 > 1. [Hint:
Use Exercise (3).]

5. Describe three distinct methods of sampling from the density f(x) = 6x(1 —x), O<x <1.

6. Aliasing method. A finite real vector is called a probability vector if it has non-negative entries
with sum 1. Show that a probability vector p of length n may be written in the form

1 n
Pro] du

where each v;, is a probability vector with at most two non-zero entries. Describe a method, based on
this observation, for sampling from p viewed as a probability mass function.

7. Box-Muller normals. Let U; and U2 be independent and uniformly distributed on [0, 1], and
let T; = 2U; — 1. Show that, conditional on the event that R = \/T? + T? <1,

qT 2 Ta 2
X= RV ~2logR , Y= BV —2logR ,

36

>

Coupling and Poisson approximation Exercises [4.11.8]-[4.12.3]

are independent standard normal random variables.

8. Let U be uniform on [0, 1] and 0 < g < 1. Show that X = 1 4 [log U/log q| has a geometric
distribution.

9. A point (X, Y) is picked uniformly at random in the semicircle xe y2 <1, x > 0. What is the
distribution of Z = Y/X?

10. Hazard-rate technique. Let X be a non-negative integer-valued random variable with h(r) =
P(X =r|X =r). If {U; : i = 0} are independent and uniform on [0, 1], show that Z = min{n :
Un < h(n)} has the same distribution as X.

li. Antithetic variables. Let g(x, x2,..., ,) be an increasing function in all its variables, and
let {U; : r > 1} be independent and identically distributed random variables having the uniform
distribution on [0, 1]. Show that

cov{g(U;, U2, ...,Un), 8 — Uj, 1— U2, ...,1—Un)} <0.
[Hint: Use the FKG inequality of Problem (3.10.18).] Explain how this can help in the efficient
estimation of J = fo g(x) dx.

12. Importance sampling. We wish to estimate J = [ g(x) fx(x) dx = E(g(X)), where either it
is difficult to sample from the density fy, or g(X) has a very large variance. Let fy be equivalent
to fy, which is to say that, for all x, fy(x) = 0 if and only if fy(~) = 0. Let {Y¥; :0 <i <n}be
independent random variables with density function fy, and define

(Yr) fx Yr)
= Ly sa) .
fy)
r=1
Show that:
OR]
E(/) = 1 =E | —————],
@) EY) | AW)
i g(¥)* fx (¥)* 2
(b) var(J) = = fe (ae —I*|,

(c) J +5, Tasn — 00. (See Chapter 7 for an account of convergence.)
The idea here is that fy should be easy to sample from, and chosen if possible so that var J is
much smaller than n—[E(g (X )?) -—T 21, The function fy is called the importance density.

13. Construct two distinct methods of sampling from the are sin density
2
f@) = —————, O84 K1L.

aV1—x2 ~

4.12 Exercises. Coupling and Poisson approximation
1. Show that X is stochastically larger than Y if and only if E(@(X)) > E(u(Y)) for any non-
decreasing function u for which the expectations exist.

2. Let X and Y be Poisson distributed with respective parameters A and yz. Show that X is stochas-
tically larger than Y if A > yp.

3. Show that the total variation distance between two discrete variables X, Y satisfies

dry(X, Y) = 2 sup |P(X € A) — PY € A)|.
ACR

37

[4.12.4]-[4.13.6] Exercises Continuous random variables

4. Maximal coupling. Show for discrete random variables X, Y that P(X = Y) < 1—- sary (X,Y),
where dry denotes total variation distance.

5. Maximal coupling continued. Show that equality is possible in the inequality of Exercise
(4.12.4) in the following sense. For any pair X, Y of discrete random variables, there exists a pair X’,
Y’ having the same marginal distributions as X, Y such that P(X’ = Y’) = 1 — ary (X, Y).

6. Let X and Y be indicator variables with EX = p, EY = q. What is the maximum possible value
of P(X = Y), as a function of p, g? Explain how X, Y need to be distributed in order that P(X = Y)
be: (a) maximized, (b) minimized.

4.13 Exercises. Geometrical probability

With apologies to those who prefer their exercises better posed ...

1. Pick two points A and B independently at random on the circumference of a circle C with centre
O and unit radius. Let I be the length of the perpendicular from O to the line AB, and let © be the
angle AB makes with the horizontal. Show that (IT, @) has joint density

1
f(p, 0) = ———, 0< p <1, 0<6 «<2zn.
we/1

— p2

2. Let Sy; and Sz be disjoint convex shapes with boundaries of length b(S,), b(S2), as illustrated
in the figure beneath. Let b(H) be the length of the boundary of the convex hull of S; and 5S,
incorporating their exterior tangents, and b(X) the length of the crossing curve using the interior
tangents to loop round S$; and Sj. Show that the probability that a random line crossing Sj also
crosses S2 is {b(X) — b(H)}/b(S1). (See Example (4.13.2) for an explanation of the term ‘random
line’.) How is this altered if S; and $5 are not disjoint?

The circles are the shapes S, and S$). The shaded regions are denoted A and B, and b(X) is

the sum of the perimeter lengths of A and B.
3. Let S, and Sz be convex figures such that Sy C S;. Show that the probability that two independent
random lines 4, and Az, crossing S;, meet within S> is 27r|S>| /b(S1)?, where |S2| is the area of Sz
and b(S}) is the length of the boundary of S;. (See Example (4.13.2) for an explanation of the term
‘random line’.)

4. Let Z be the distance between two points picked independently at random in a disk of radius a.
Show that E(Z) = 128a/(45s), and E(Z”) = a?.

5. Pick two points A and B independently at random in a ball with centre O. Show that the probability
that the angle AOB is obtuse is 3. Compare this with the corresponding result for two points picked
at random in a circle.

6. A triangle is formed by A, B, and a point P picked at random in a set S with centre of gravity G.
Show that E/ABP| = |ABG].

38
Problems Exercises [4.13.7]-[4.14.1]

7. Appoint D is fixed on the side BC of the triangle ABC. Two points P and Q are picked independently
at random in ABD and ADC respectively. Show that EJAPQ| = |AG,Go| = lABCl, where G, and
Gp are the centres of gravity of ABD and ADC.

8. From the set of all triangles that are similar to the triangle ABC, similarly oriented, and inside
ABC, one is selected uniformly at random. Show that its mean area is tb |ABC].

9. Two points X and Y are picked independently at random in the interval (0, a). By varying a,
show that F(z, a) = P(|X — Y| < z) satisfies

and hence find F(z, a). Let r > 1, and show that m,(a) = E(|X — Y{") satisfies
dm, a’
=2 _ .
“ da {> +1 mr}

10. Lines are laid down independently at random on the plane, dividing it into polygons. Show that
the average number of sides of this set of polygons is 4. [Hint: Consider n random great circles of a
sphere of radius R; then let R and n increase.)

11. A point P is picked at random in the triangle ABC. The lines AP, BP, CP, produced, meet BC,
AC, AB respectively at L, M, N. Show that E|LMN| = (10 — m*)|ABC].

12. Sylvester’s problem. If four points are picked independently at random inside the triangle ABC,

show that the probability that no one of them lies inside the triangle formed by the other three is z.

Hence find m;(a).

13. Ifthree points P, Q, R are picked independently at random ina disk of radius a, show that E|PQR| =
35a”/(487). [You may find it useful that fj’ [> sin? x sin? y sin |x — y|dx dy = 357 /128.]

14. Two points A and B are picked independently at random inside a disk C’.. Show that the probability
that the circle having centre A and radius |AB| lies inside C is é.

15. Two points A and B are picked independently at random inside a ball S. Show that the probability
that the sphere having centre A and radius |AB| lies inside S is y-

4.14 Problems

1. (a) Show that [°° e~*” dx = ./7, and deduce that

2
fQ@)= v0 | Sook —00 <x < 00,

202

1
ov2n
is a density function if o > 0.

(b) Calculate the mean and variance of a standard normal variable.
(c) Show that the V0, 1) distribution function ® satisfies

1,2 1,2
(xh axe < V/2n[1 — O(x)] < xe? x>0.

These bounds are of interest because ® has no closed form.
(d) Let X be N(O, 1), and a > 0. Show that P(X > x +a/x |X >x)7 e 4% asx > 0.

39

[4.14.2]-[4.14.11] Exercises Continuous random variables

2. Let X be continuous with density function f(x) = C(x — x”), where wa < x < BandC >0.,
(a) What are the possible values of a and 6?

(b) What is C?

3. Let X be arandom variable which takes non-negative values only. Show that

ioe)

le ¢)
SoG = Dla, < X < Oily,

i=1 i=1

where A; = {i —1 < X < i}. Deduce that

OO ie,¢]
> P(X > i) S$ E(X) < 1+ DOP(X > 3).

i=l i=1

4. (a) Let X have a continuous distribution function F. Show that
G) F(X) is uniformly distributed on [0, 1],
Gi) — log F(X) is exponentially distributed.

(b) A straight line / touches a circle with unit diameter at the point P which is diametrically opposed
on the circle to another point Q. A straight line QR joins Q to some point R on J. If the angle POR
between the lines PQ and QR is a random variable with the uniform distribution on [-57, 57],
show that the length of PR has the Cauchy distribution (this length is measured positive or negative
depending upon which side of P the point R lies).

5. Let X have an exponential distribution. Show that P(X > s+.x |X > s) = P(X > x), for
x,s > 0. This is the ‘lack of memory’ property again. Show that the exponential distribution is the
only continuous distribution with this property. You may need to use the fact that the only non-negative
monotonic solutions of the functional equation g(s + t) = g(s)g(t) for s,t > 0, with g(0) = 1, are
of the form g(s) = e“*. Can you prove this?

6. Show that X and Y are independent continuous variables if and only if their joint density function
J factorizes as the product f(x, y) = g(x)h(y) of functions of the single variables x and y alone.

7. Let X and Y have joint density function f(x, y) = 2e°-*-7, 0 < x < y < oo. Are they
independent? Find their marginal density functions and their covariance.

8. Bertrand’s paradox extended. A chord of the unit circle is picked at random. What is the

probability that an equilateral triangle with the chord as base can fit inside the circle if:

(a) the chord passes through a point P picked uniformly in the disk, and the angle it makes with a
fixed direction is uniformly distributed on [0, 27),

(b) the chord passes through a point P picked uniformly at random on a randomly chosen radius, and
the angle it makes with the radius is uniformly distributed on [0, 27r).

9. Monte Carlo. It is required to estimate J = So g(x) dx where 0 < g(x) < 1 for all x, as
in Example (2.6.3). Let X and Y be independent random variables with common density function
f(x) = 1if 0 <x <1, f(x) = 0 otherwise. Let U = Ipy<g(x)}, the indicator function of the event
that Y < 9(X), andlet V = g(X),W = 5{g(X)+e(1—X)}. Show that E(U) = E(V) = E(W) = J,
and that var(W) < var(V) < var(U), so that, of the three, W is the most ‘efficient’ estimator of J.

10. Let X1, X2,..., Xn be independent exponential variables, parameter 1. Show by induction that
S=X,+Xo.+---+ Xy has the P(A, n) distribution.

11. Let X and Y be independent variables, [(A, m) and (A, n) respectively.

(a) Use the result of Problem (4.14.10) to show that X + Y is [(A, m+n) when m and n are integral
(the same conclusion is actually valid for non-integral m and 7).

40

Problems Exercises [4.14.12]-[4.14.19]

(b) Find the joint density function of X + Y and X/(X + Y), and deduce that they are independent.

(c) If Z is Poisson with parameter At, and m is integral, show that P(Z < m) = P(X > f).

(d) If0 <m <n and B is independent of Y with the beta distribution with parameters m and n — m,
show that Y B has the same distribution as X.

12. Let X1, X2,..., Xn be independent N(0, 1) variables.

(a) Show that Xf is x7(1).

(b) Show that X : + xs is x7(2) by expressing its distribution function as an integral and changing
to polar coordinates.

(c) More generally, show that Xt + X3 +--+» +X? is x(n).

13. Let X and Y have the bivariate normal distribution with means «41, 442, variances a7, a4, and

correlation p. Show that

(a) E(X | Y) = wy + poy (¥ — u2)/o2,

(b) the variance of the conditional density function fy|y is var(X | Y) = of (1 _ p).

14. Let X and Y have joint density function f. Find the density function of Y/X.

15. Let X and Y be independent variables with common density function f. Show that tan (Y /X)
has the uniform distribution on (— $7, 577) if and only if

/ FOO Fay)leldx = yeR,
oo

m(1+y?)’
Verify that this is valid if either f is the N(0, 1) density function or f(x) = a(1 + x*)~! for some
constant a.

16. Let X and Y be independent N(0, 1) variables, and think of (X, Y) as arandom point in the plane.
Change to polar coordinates (R, ©) given by R? = X24 Y? tn@=Y /X; show that R? is x7 (2),
tan © has the Cauchy distribution, and R and © are independent. Find the density of R.

Find E(X2/R) and
EY min{|X|, iY |} \
max{(X/, |Y |}

17. If X and Y are independent random variables, show that U = min{X, Y} and V = max{X, Y}
have distribution functions

Fyu@) =1—-{1—-Fx@}{1-Fy@}, Fy) = Fx(v)Fy().

Let X and Y be independent exponential variables, parameter 1. Show that
(a) U is exponential, parameter 2,
(b) V has the same distribution as X + SY. Hence find the mean and variance of V.
18. Let X and Y be independent variables having the exponential distribution with parameters 1 and
pt respectively. Let U = min{X, Y}, V = max{X, Y}, and W = V —U.
(a) Find POU = X) = P(X < Y).
(b) Show that U and W are independent.
19. Let X and Y be independent non-negative random variables with continuous density functions
on (0, 00). ;
(a) If, given X + Y = u, X is uniformly distributed on [0, u] whatever the value of u, show that X
and Y have the exponential distribution.

41

[4.14.20]-[4.14.27] Exercises Continuous random variables

(b) If, given that X + Y = u, X/u has a given beta distribution (parameters a and f, say) whatever
the value of u, show that X and Y have gamma distributions.

You may need the fact that the only non-negative continuous solutions of the functional equation
g(s +t) = g(s)g(t) for s,t > 0, with g(0) = 1, are of the form g(s) = e#*. Remember Problem
(4.14.5).

20. Show that it cannot be the case that U = X + Y where U is uniformly distributed on [0, 1] and X
and Y are independent and identically distributed. You should not assume that X and Y are continuous
variables.

21. Order statistics. Let X|, X2,..., Xn be independent identically distributed variables with acom-
mon density function f. Suchacollectionis calleda random sample. Foreachw € Q, arrange the sam-
ple values X1(@),..., Xn() in non-decreasing order X(j)(@) < X(2)(@) < «++ < Xn) (@), where
(1), (2),..., (@) is a (random) permutation of 1,2,...,. The new variables Xy, X(2).-- +» Xm)
are called the order statistics. Show, by a symmetry argument, that the joint distribution function of
the order statistics satisfies

P(X) < Y1.--+, Xn) S Yn) = I PCK S y1,--., Xn S Yn, X1 < X2 << +++ < Xn)
= [fos L(x,.--,%n)n! fx) +++ fn) dx, +++ dxn
x

292

Xn <n

where L is given by
L(x) = { 1 if xy <*2 <s++ < Xp,
0 otherwise,

and X = (%1,%2,...,%n). Deduce that the joint density function of X(1),..-,Xq) is g(y) =
aLiy)fO)--- £On).
22, Find the marginal density function of the kth order statistic X(,) of a sample with size n:
(a) by integrating the result of Problem (4.14.21),
(b) directly.

23. Find the joint density function of the order statistics of n independent uniform variables on [0, 7].

24. Let X1, X2,..., Xn be independent and uniformly distributed on [0, 1], with order statistics

X 1), X(2)s ++ Xen):

(a) Show that, for fixed k, the density function of n.X(,) converges as n — oo, and find and identify
the limit function.

(b) Show that log Xj) has the same distribution as — )77_, i-1Y;, where the Y; are independent
random variables having the exponential distribution with parameter 1.

(c) Show that Z1, Z2,..., Zn, defined by Z, = (Xay/X eer fork < nand Zy = (X(qy)", are
independent random variables with the uniform distribution on [0, 1].

25. Let X,, X2, X3 be independent variables with the uniform distribution on [0,1]. What is the
probability that rods of lengths X,, X2, and X3 may be used to make a triangle? Generalize your
answer to n rods used to form a polygon.

26. Let X, and X be independent variables with the uniform distribution on [0, 1]. A stick of unit
length is broken at points distance X; and X 7 from one of the ends. What is the probability that the
three pieces may be used to make a triangle? Generalize your answer to a stick broken in n places.

27. Let X, Y be a pair of jointly continuous variables.
(a) Hélder’s inequality. Show that if p,q > 1 and p~! +q7! = 1 then

E|XY| < {E|x?|}!/? {eva} '/4.

42

Problems Exercises [4.14.28]-[4.14.34]

Set p = q = 2 to deduce the Cauchy-Schwarz inequality E(XY)? < E(X*)E(Y?).
(b) Minkowski’s inequality. Show that, if p > 1, then

{EX + Y|P)}1/? < {e)xP}l/? 4 {RlyP i} /?.

Note that in both cases your proof need not depend on the continuity of X and Y; deduce that the same
inequalities hold for discrete variables.

28. Let Z be arandom variable. Choose X and Y appropriately in the Cauchy—Schwarz (or Holder)
inequality to show that g(p) = log E|Z? | is a convex function of p on the interval of values of p such
that E]Z?| < co. Deduce Lyapunov’s inequality:

{E|Z" |}!/" > (Bi Z5]}!/5 whenever r > s > 0.

You have shown in particular that, if Z has finite rth moment, then Z has finite sth moment for all
positive s <r.

29. Show that, using the obvious notation, E{E(X | Y, Z)| Y¥} = E(X | Y).

30. Motor cars of unit length park randomly in a street in such a way that the centre of each car, in
turn, is positioned uniformly at random in the space available to it. Let m(x) be the expected number
of cars which are able to park in a street of length x. Show that

1 Xx
ma«tl)= <i {m(y) +m(x — y) +1} dy.

It is possible to deduce that m(x) is about as big as 3x when x is large.

31. Buffon’s needle revisited: Buffon’s noodle.

(a) A plane is ruled by the lines y = nd (n = 0,41,...). A needle with length LZ (< d) is cast
randomly onto the plane. Show that the probability that the needle intersects a line is 2L/(wd).

(b) Now fix the needle and let C be a circle diameter d centred at the midpoint of the needle. Let
4 be a line whose direction and distance from the centre of C are independent and uniformly
distributed on [0, 277] and [0, 5d] respectively. This is equivalent to ‘casting the ruled plane at
random’. Show that the probability of an intersection between the needle and A is 2L/(1d).

(c) Let S be a curve within C having finite length L(S). Use indicators to show that the expected
number of intersections between § and 1 is 2L(S)/(7d).

This type of result is used in stereology, which seeks knowledge of the contents of a cell by studying

its cross sections.

32. Buffon’s needle ingested. In the excitement of calculating 2, Mr Buffon (no relation) inadver-
tently swallows the needle and is X-rayed. If the needle exhibits no preference for direction in the
gut, what is the distribution of the length of its image on the X-ray plate? If he swallowed Buffon’s
cross (see Exercise (4.5.3)) also, what would be the joint distribution of the lengths of the images of
the two arms of the cross?

33. Let X1, X2,...,Xn be independent exponential variables with parameter A, and let Xq1) <
X(2) S +++ S X(n) be their order statistics. Show that

Y=nXqy, Yr=+1—-r)(Xw — Xor-1), l<r<n

are also independent and have the same joint distribution as the X;.

34, Let X(1), X(2), ..., Xn) be the order statistics of a family of independent variables with common
continuous distribution function F’. Show that

F(X) r
Y, = {F(Xq))}", t= {pee », Ll<re<n,
n { (n) } r F(X(-41))

43
[4.14.35]-[4.14.42] Exercises Continuous random variables

are independent and uniformly distributed on {0, 1]. This is equivalent to Problem (4.14.33). Why?

35. Secretary/marriage problem. You are permitted to inspect the n prizes at a féte in a given order,
at each stage either rejecting or accepting the prize under consideration. There is no recall, in the sense
that no rejected prize may be accepted later. It may be assumed that, given complete information, the
prizes may be ranked in a strict order of preference, and that the order of presentation is independent
of this ranking. Find the strategy which maximizes the probability of accepting the best prize, and
describe its behaviour when n is large.

36. Fisher’s spherical distribution. Let R? = X24 Y¥2+4 Z2 where X , Y, Z are independent normal
random variables with means i, jz, v, and common variance a”, where (A, #,v”) 4 (0, 0,0). Show
that the conditional density of the point (X, Y, Z) given R = r, when expressed in spherical polar
coordinates relative to an axis in the direction e = (A, yz, v), is of the form

£69) = aetone sin, 0<60<2,0<¢<2z,
where a = rje|.
37. Let @ be the N(O, 1) density function, and define the functions H,, n > 0, by Ayo = 1, and
(-1)" Hyd = o™, the nth derivative of ¢. Show that:
(a) Hy(x) is a polynomial of degree n having leading term x”, and

ifm An,
ifm =n.

[. Am (x) An (xox) dx = { 0
-o% n!

le ¢) th
(b) y Hn (x) = exp(tx — 31).

38. Lancaster’s theorem. Let X and Y have a standard bivariate normal distribution with zero
means, unit variances, and correlation coefficient o, and suppose U = u(X) and V = v(Y) have finite
variances. Show that |o(U, V)| < |o|. [Hint: Use Problem (4.14.37) to expand the functions u and
v. You may assume that u and v lie in the linear span of the Ay.)

39. Let X (1)> X(2).-+- » X(n) be the order statistics of n independent random variables, uniform on
[0, 1]. Show that:
ra—st+]1)
(n+1)2(n +2)
40. (a) Let X, Y, Z be independent N(O, 1) variables, and set R = VX 24 y¥24 72. Show that
x? / R? has a beta distribution with parameters 4 and 1, and is independent of R?,

(b) Let X, Y, Z be independent and uniform on [—1, 1] and set R = X24 ¥2+4 Z2. Find the
density of X 2/R2 given that R? <1.

r
(a) E(Xq@)) = maT’ (b) cov(X(7), X(s)) = forr <s.

41. Let ¢ and ® be the standard normal density and distribution functions. Show that:

(a) P@&) =1— O(—x),

(b) f() = 26(x) Px), —00 < x < ov, is the density function of some random variable (denoted
by Y), and that |Y| has density function 2¢.

(c) Let X be astandard normal random variable independent of Y, anddefine Z = (X-+A|Y|)/V/1 +22.
Write down the joint density of Z and |Y|, and deduce that Z has density function /.

42. The six coordinates (X;, Y;), 1 <i < 3, of three points A, B, C in the plane are independent
N(O, 1). Show that the the probability that C lies inside the circle with diameter AB is i

44
Problems Exercises [4.14.43]-[4.14.49]

43. The coordinates (X;, Y;, Zj), 1 <i <3, of three points A, B, C are independent N(0, 1). Show

that the probability that C lies inside the sphere with diameter AB is : - aes

44, Skewness. Let X have variance o” and write m, = E(X*). Define the skewness of X by

skw(X) = E[(X — m1)*]/o3. Show that:

(a) skw(X) = (m3 — 3mym2 + 2m})/o3,

(b) skw(S,) = skw(X1)/./n, where S;, = ey X; is a sum of independent identically distributed
random variables,

(c) skw(X) = (1 —2p)/,/npq, when X is bin(n, p) where p+ q = 1,

(d) skw(X) = 1/ Ji, when X is Poisson with parameter A,

(e) skw(X) = 2/./t, when X is gamma I'(A, t), and ¢ is integral.

45. Kurtosis. Let X have variance o” and E(X*) = mx. Define the kurtosis of X by kur(X) =

E[(X — m,)*]/o*+. Show that:

(a) kur(X) = 3, when X is N(, 7),

(b) kur(X) = 9, when X is exponential with parameter A,

(c) kur(X) = 3+ 271, when X is Poisson with parameter A,

(d) kur(S,) = 3 + {kur(X1) — 3}/n, where Sy, = ye X, is a sum of independent identically
distributed random variables.

46. Extreme value. Fisher-Gumbel-Tippett distribution. Let X,, 1 <r <n, be independent and
exponentially distributed with parameter 1. Show that X(,) = max{X, :1 <r <n} satisfies

. _ _ a
im, PX a) logn < x) = exp(—e“).

Hence show that Io” {1 — exp(—e~*)} dx = y where y is Euler’s constant.

47. Squeezing. Let S and X have density functions satisfying b(x) < fs(x) < a(x) and fs(x) <
fx(). Let U be uniformly distributed on [0, 1] and independent of X. Given the value X, we
implement the following algorithm:

if Ufx(X) > a(X), reject X;
otherwise: if Ufy(X) < b(X), — accept X;
otherwise: if Ufy(X) < fs(X), accept X;
otherwise: reject X.

Show that, conditional on ultimate acceptance, X is distributed as $. Explain when you might use this
method of sampling.

48. Let X, Y, and {U; : r = 1} be independent random variables, where:

P(X =x) =(e-le*, PY = y= yore yah,

1
(e-1)
and the U; are uniform on [0,1]. Let 4 = max{U,, U2,..., Uy}, and show that Z = X — M is
exponentially distributed.

49. Let U and V be independent and uniform on [0, 1]. Set X = —a7! log U and Y = —logV
where a > 0.

we 1 2 . : _ 5 x2
(a) show the conditional on the event Y > 5(X —a)*, X has density function f(x) = /2/me
orx > 0.

45

[4.14.50]-[4.14.56] Exercises Continuous random variables

(b) In sampling from the density function f, it is decided to use a rejection method: for given a > 0,
we sample U and V repeatedly, and we accept X the first time that Y > 3(X — a). What is the
optimal value of a?

(c) Describe how to use these facts in sampling from the N (0, 1) distribution.

50. Let S be a semicircle of unit radius on a diameter D.
(a) A point P is picked at random on D. If X is the distance from P to S along the perpendicular to
D, show E(X) = 1/4.
(b) A point Q is picked at random on S. If Y is the perpendicular distance from Q to D, show
E(Y) = 2/zx.
51. (Set for the Fellowship examination of St John’s College, Cambridge in 1858.) ‘A large quantity
of pebbles lies scattered uniformly over a circular field; compare the labour of collecting them one by
one:
(i) at the centre O of the field,
(ii) at a point A on the circumference.’
To be precise, if Lo and La are the respective labours per stone, show that E(Lo) = fa and
E(La) = 32a/(97) for some constant a.
Gii) Suppose you take each pebble to the nearer of two points A or B at the ends of a diameter. Show
in this case that the labour per stone satisfies

4a (16 17 1 2
E(L =f — log(l 2exzldl =a.
(Lap) {5 5 V2 + 5 log +v5h 3x 3a

(iv) Finally suppose you take each pebble to the nearest vertex of an equilateral triangle ABC inscribed
in the circle. Why is it obvious that the labour per stone now satisfies E(Lagc) < E(Lo)?
Enthusiasts are invited to calculate E(L apc).

52. The lines L, M, and N are parallel, and P lies on L. A line picked at random through P meets M
at Q. A line picked at random through Q meets N at R. What is the density function of the angle ©
that RP makes with L? [Hint: Recall Exercise (4.8.2) and Problem (4.14.4).]

53. Let A denote the event that you can form a triangle with three given parts of a rod R.

(a) R is broken at two points chosen independently and uniformly. Show that P(A) = i.

(b) R is broken in two uniformly at random, the longer part is broken in two uniformly at random.
Show that P(A) = log(4/e).

(c) R is broken in two uniformly at random, a randomly chosen part is broken into two equal parts.
Show that P(A) = 4.

(d) In case (c) show that, given A, the triangle is obtuse with probability 3 — 2/2.

54. You break a rod at random into two pieces. Let R be the ratio of the lengths of the shorter to the
longer piece. Find the density function fr, together with the mean and variance of R.

55. Let R be the distance between two points picked at random inside a square of side a. Show that
E(R2) = za’, and that R*/a? has density function

fr) r-4/r4n if0<r <1,
rioj=
4/r —-1—-2—r42sin7! Vr-!—2sin7! V1 —r-! if t<r <2.

56. Show that a sheet of paper of area A cm? can be placed on the square lattice with period 1 cm in
such a way that at least [A] points are covered.

46

Problems Exercises [4.14.57]-[4.14.63]

57. Show that it is possible to position a convex rock of surface area S in sunlight in such a way that
its shadow has area at least iS .

58. Dirichlet distribution. Let {X; : 1 <r < k + 1} be independent ['(,, f,) random variables
(respectively).

(a) Show that ¥, = X,/(X, +---+ Xr), 2 <r <k +1, are independent random variables.

(b) Show that Z; = X,/(X] +-+++ Xg41), 1 <r <k, have the joint Dirichlet density

P(By +++: + Beri) By-1_Bo-1 = By-1 Brri-l
zy ors (l= zy — 22 — + — ey)PRHA™,
PB) TB | k 2 ‘
59. Hotelling’s theorem. Let X; = (X1,;, X27,..., Xmr), 1 <7 <n, be independent multivariate

normal random vectors having zero means and the same covariance matrix V = (v;;). Show that the
two random variables

n n n n—-l
1
si => XirXjr — = Ss Xir D Xie Ti = D> Xir Xj.
r=1 r=1 r=1

r=1

are identically distributed.

60. Choose P, Q, and R independently at random in the square S(a) of side a. Show that E/PQR| =
11a? /144. Deduce that four points picked at random in a parallelogram form a convex quadrilateral
with probability (3).

61. Choose P, Q, and R uniformly at random within the convex region C illustrated beneath. By

considering the event that four randomly chosen points form a triangle, or otherwise, show that the
mean area of the shaded region is three times the mean area of the triangle PQR.

62. Multivariate normal sampling. Let V be a positive-definite symmetric n x n matrix, and L
a lower-triangular matrix such that V = L’L,; this is called the Cholesky decomposition of V. Let
X = (X, X2,..., Xn) be a vector of independent random variables distributed as N(0, 1). Show that
the vector Z = ft + XL has the multivariate normal distribution with mean vector yz and covariance
matrix V.

63. Verifying matrix multiplications. We need to decide whether or not AB = C where A, B, C are
given n x n matrices, and we adopt the following random algorithm. Let x be a random {0, 1}”-valued
vector, each of the 2” possibilities being equally likely. If (AB — C)x = 0, we decide that AB = C,
and otherwise we decide that AB 4 C. Show that

1
P(the decision is comect){ >1 ABYC
23 L .

Describe a similar procedure which results in an error probability which may be made as small as
desired.

47
5

Generating functions and their applications

5.1 Exercises. Generating functions

1. Find the generating functions of the following mass functions, and state where they converge.
Hence calculate their means and variances.

(a) f(m) = ("*™ 1) p" (1 — py", for m > 0.

(b) f(m) = {mm + Dy}, for m > 1.

(c) f(m) = (1 — p)p'"!/(1 + p), form =...,-1,0,1,....

The constant p satisfies 0 < p < 1.

2. Let X (= 0) have probability generating function G and write t(n) = P(X > n) for the ‘tail’
probabilities of X. Show that the generating function of the sequence {t(n) : n > O} is T(s) =
(1 — G(s))/(1 — s). Show that E(X) = T(1) and var(X) = 27’(1) + T(1) — T()?.

3. Let Gy y(s, 1) be the joint probability generating function of X and Y. Show that Gy(s) =
Gx,y(s, 1) and Gy (t) = Gy y(1, f). Show that

2

0
Y) = —G Jt
E(XY) 9s dt xy(s a

4. Find the joint generating functions of the following joint mass functions, and state for what values
of the variables the series converge.

(a) fU,k) = (1 —a)(B — aja p*-J—!) for 0 < k < j, where0 <a < 1a <fB.

(b) fG,k) = (e — Dew PH DI 71, for j,k = 0.

(c) fG, b= (5) pitka — p)¥-J /[klog{1/(1 — p)}], for O < j <k, k > 1, where 0 < p <1.
Deduce the marginal probability generating functions and the covariances.

5. Acoin is tossed n times, and heads turns up with probability p on each toss. Assuming the usual
independence, show that the joint probability generating function of the numbers H and T of heads
and tails is Gy,7 (x, y) = {px + (1 — p)y}". Generalize this conclusion to find the joint probability
generating function of the multinomial distribution of Exercise (3.5.1).

6. Let X have the binomial distribution bin(n, U), where U is uniform on (0, 1). Show that X is
uniformly distributed on {0, 1,2, ..., n}.

7. Show that
G(x, y,z,w) = dayzw txy + yz tow text yw+xz41)

48

Some applications Exercises [5.1.8]-[5.2.5]

is the joint generating function of four variables that are pairwise and triplewise independent, but are
nevertheless not independent.

8. Let pp > Oand a, € R for 1 <r <n. Which of the following is a moment generating function,
and for what random variable?

n n
(a) MW) =14+>0 prt”, (0) M@) = DP pre".
r=1 r=]
9. Let G, and G2 be probability generating functions, and suppose that 0 < a < 1. Show that
G G2, and aG, + (1 — @)Gz are probability generating functions. Is G(as)/G(a@) necessarily a
probability generating function?

5.2, Exercises. Some applications

1. Let X be the number of events in the sequence A 1, A2,..., An which occur. Let Sy = E(*),
the mean value of the random binomial coefficient (x ), and show that

n
_fj—-l
P(X >i) = bs (;- ')s forl <i <n,

j=i
n

j—1
where Sy, = Ss; (; ee > jf), for! <m<n.
m—

J=m

2. Each person in a group of n people chooses another at random. Find the probability:
(a) that exactly k people are chosen by nobody,
(b) that at least k people are chosen by nobody.

3. Compounding.

(a) Let X have the Poisson distribution with parameter Y, where Y has the Poisson distribution with
parameter 4. Show that Gy ,y(x) = exp{y(xe*—! —1)}.

(b) Let X1, X2,... be independent identically distributed random variables with the logarithmic
mass function k

(1 — p) >

klog(1/p)

where 0 < p < 1. If N is independent of the X; and has the Poisson distribution with parameter

1, Show that Y = )~®_, X; has a negative binomial distribution.

>

fk) =

4. Let X have the binomial distribution with parameters n and p, and show that

B( 1 jee
14X/ = (nt+lp ~

Find the limit of this expression as n — oo and p — 0, the limit being taken in such a way that
np — dA where 0 < A < oo. Comment.

5. A coin is tossed repeatedly, and heads turns up with probability p on each toss. Let hy be
the probability of an even number of heads in the first n tosses, with the convention that 0 is an
even number. Find a difference equation for the hy and deduce that they have generating function

5{(+2ps—s)1+(1—s)7}}.

49

[5.2.6]-[5.3.6] Exercises Generating functions and their applications

6. An unfair coin is flipped repeatedly, where P(H) = p = 1 — q. Let X be the number of flips
until HTH first appears, and Y the number of flips until either HTH or THT appears. Show that
E(s*) = (p2qs3)/(1 — s + pqs” — pqs?) and find E(s’).

7. Matching again. The pile of (by now dog-eared) letters is dropped again and enveloped at
random, yielding X, matches. Show that P(Xn = j) = Gi + DP(Xn41 = j + 1). Deduce that the
derivatives of the Gr;(s) = E(s*") satisfy Gi, 41= Gn, and hence derive the conclusion of Example

(3.4.3), namely:
rte == 2 (dd yg GV)
neO= sor 3I @—ni)

8. Let X have a Poisson distribution with parameter A, where A is exponential with parameter pw.
Show that X has a geometric distribution.

9. Coupons. Recall from Exercise (3.3.2) that each packet of an overpriced commodity contains a
worthless plastic object. There are four types of object, and each packet is equally likely to contain
any of the four. Let T be the number of packets you open until you first have the complete set. Find
E(s?) and P(T =k).

5.3 Exercises. Random walk
1. For a simple random walk S$ with Sg = 0 and p = 1-—gq < i, show that the maximum
M = max{Sy : n > 0} satisfies P(M > r) = (p/q)’ forr > 0.

2. Use generating functions to show that, for a symmetric random walk,
(a) 2kfo(2k) = P(S2%~-2 = 0) for k > 1, and
(b) P(S,S2--- Son 4 0) = P(S2, = 0) forn > 1.

3. Avparticle performs arandom walk on the corners of the square ABCD. At each step, the probability
of moving from corner c to corner d equals p-g, where

PAB = PBA = PCD = PDC = &, PAD = PDA = PBC = PcB = B,

anda, B > 0,a+ 6 = 1. Let Ga(s) be the generating function of the sequence (paa(n) : n > 0),
where paa(7) is the probability that the particle is at A after n steps, having started at A. Show that

1f 4 1
Gx =3 {3+ too}

Hence find the probability generating function of the time of the first return to A.

4. A particle performs a symmetric random walk in two dimensions starting at the origin: each
step is of unit length and has equal probability i of being northwards, southwards, eastwards, or
westwards. The particle first reaches the line x + y = m at the point (X, Y) and at the time T. Find
the probability generating functions of T and X — Y, and state where they converge.

5. Derive the arc sine law for sojourn times, Theorem (3.10.21), using generating functions. That
is to say, let Lz, be the length of time spent (up to time 27) by a simple symmetric random walk to
the right of its starting point. Show that

P(Lon = 2k) = P(Sox = O)P(Son—2g = 0) = forO SK <n.

6. Let {S, : 2 > 0} be a simple symmetric random walk with Sg = 0, and let T = min{n > 0:
Sn = O}. Show that

E(min{T, 2m}) = 2E|S2m| = 4mP(Som =0) for m > 0.

50

Branching processes Exercises [5.3.7]-[5.4.6]

7. Let Sp = Soreo Xr be a left-continuous random walk on the integers with a retaining barrier
at zero. More specifically, we assume that the X; are identically distributed integer-valued random
variables with X; > —1, P(X; = 0) 4 0, and
Sn t+ Xn41 if Sy > 0,
Sn4l = :

Show that the distribution of Sg may be chosen in such a way that E(z5") = E(z50) for all n, if and
only if E(X,) < 0, and in this case
(1 — )E(X)EG*!)

1 — E(z*1)

E(z%*) =

8. Consider a simple random walk starting at 0 in which each step is to the right with probability
p (= 1-4). Let Ty be the number of steps until the walk first reaches b where b > 0. Show that
E(Tp | Th < 00) = b/|p — ql.

5.4 Exercises. Branching processes

1. Let Z, be the size of the nth generation in an ordinary branching process with Zg = 1, E(Z1) = pu,
and var(Z,) > 0. Show that E(Z,Zm) = u—"R(Z2) for m <n. Hence find the correlation
coefficient o(Zm, Zn) in terms of 2.

2. Consider a branching process with generation sizes Zy, satisfying Zo = 1 and P(Z, = 0) = 0.
Pick two individuals at random (with replacement) from the nth generation and let L be the index of
the generation which contains their most recent common ancestor. Show that P(L = r) = E(Z, 1y _

E(Z,,1) for 0 <r <n. What can be said if P(Z, = 0) > 0?

3. Consider a branching process whose family sizes have the geometric mass function f(k) = gp*,
k > 0, where p + q = 1, and let Zp be the size of the nth generation. Let T = min{n : Zp = 0} be
the extinction time, and suppose that Zg = 1. Find P(T = n). For what values of p is it the case that
E(T) < ~?

4. Let Z, be the size of the nth generation of a branching process, and assume Zp = 1. Find an

expression for the generating function Gy of Z,, in the cases when Z, has generating function given

by:

(a) G(s) =1-—a(1—s)?, 0<a,B <1.

(b) Gs) = f —ltp( f (s))}, where P is a probability generating function, and f is a suitable function
satisfying f(1) = 1.

(c) Suppose in the latter case that f(x) = x” and P(s) = s{y-—(v- 1s}7! where y > 1. Calculate
the answer explicitly.

5. Branching with immigration. Each generation of a branching process (with a single progenitor)
is augmented by a random number of immigrants who are indistinguishable from the other members
of the population. Suppose that the numbers of immigrants in different generations are independent
of each other and of the past history of the branching process, each such number having probability
generating function H(s). Show that the probability generating function G, of the size of the nth
generation satisfies G,i1(s) = Gn(G(s))A(s), where G is the probability generating function of a
typical family of offspring.

6. Let Z, be the size of the nth generation in a branching process with E(s41) = (2—s)7!
and Zp = 1. Let V, be the total number of generations of size r. Show that E(V|) = in?, and
E(2V2 — V3) = ¢2? — gy’.

51
[5.5.1]-[5.6.5] Exercises Generating functions and their applications

5.5 Exercises. Age-dependent branching processes
1. Let Z, be the size of the nth generation in an age-dependent branching process Z(t), the lifetime

distribution of which is exponential with parameter A. If Z(0) = 1, show that the probability generating
function G;(s) of Z(t) satisfies

0
9 Ors) = A{G(Gi(s)) — Gr(s)}.

Show in the case of ‘exponential binary fission’, when G(s) = s”, that

—At

sé
OS) = Te)

and hence derive the probability mass function of the population size Z(r) at time tf.

2. Solve the differential equation of Exercise (1) when A = 1 and G(s) = (1 +s), to obtain

2s +t(1—s)

CS) = Saas)”

Hence find P(Z(t) => k), and deduce that

P(Z(t)/t >x|Z(t) > 0) ae ast > ow.

5.6 Exercises. Expectation revisited

1. Jensen’s inequality. A function u : R — R is called convex if for all real a there exists A,
depending ona, such that u(x) > u(a)+A(x—a) for all x. (Draw adiagram to illustrate this definition.)
Show that, if u is convex and X is a random variable with finite mean, then E(u(X)) > u(E(X)).

2. Let X,,X2,... be random variables satisfying E(} +72) |X;|) < 00. Show that
[oe] CO
B(> x) = SO E(X).
i=1 i=1

3. Let {X,} be a sequence of random variables satisfying X, < Y a.s. for some Y with E/Y| < o.
Show that

E (iim sup Xn) > lim sup E(Xy).
n> CO

A> CO
4. Suppose that E]X"| < oo where r > 0. Deduce that x” P(|X| = x) > 0as x — oo. Conversely,
suppose that x” P(|X| => x) — Oas x — oo where r > 0, and show that E|X*| < cofor0<s <r.

5. Show that E|X| < oo if and only if the following holds: for all « > 0, there exists 6 > 0, such
that E(|X|J4) < € for all A such that P(A) < 6.

52

Characteristic functions Exercises [5.7.1]-[5.7.9]

5.7 Exercises. Characteristic functions

1. Find two dependent random variables X and Y such that dy+y(t) = $x (t)dy (1) for all ¢.

2. If @ is a characteristic function, show that Re{1 — @(f)} > 4Re{1 — $(2t)}, and deduce that
1 — |@(2t)| < 8{1 — |¢@)]}-
3. The cumulant generating function Ky (@) of the random variable X is defined by Ky (9) =

log E(e°*), the logarithm of the moment generating function of X. If the latter is finite in a neigh-
bourhood of the origin, then Kx has a convergent Taylor expansion:

oO

1
Kx@) = | kn (X)0"

n=1

and ky, (X) is called the nth cumulant (or semi-invariant) of X.
(a) Express ky (X), k2(X), and k3(X) in terms of the moments of X.
(b) If X and Y are independent random variables, show that k,(X + Y) = ky(X) + kn (Y¥).

4. Let X be N(O, 1), and show that the cumulants of X are kp(X) = 1, km(X) = 0 for m £2.

5. Therandom variable X is said to have a lattice distribution if there exist a and b such that X takes
values in the set L(a, b) = {a +bm:m =0,+1,...}. The span of such a variable X is the maximal
value of b for which there exists a such that X takes values in L(a, b).

(a) Suppose that X has a lattice distribution with span b. Show that |@x(27/b)| = 1, and that
ldx(t)| < 1 forO <t < 2n/b.

(b) Suppose that |¢x(@)| = 1 for some 6 + 0. Show that X has a lattice distribution with span
27k/@ for some integer k.

6. Let X bea random variable with density function f. Show that |¢x(t)| > 0 ast > oo.

7. LetX, X2,..., X, be independent variables, X; being N(j4;, 1), andlet Y = X7+X3+---+X2.
Show that the characteristic function of Y is

_ 1 ( it )
OY) = Ga oipalt OP Tai

where 0 = ut + ws feet we. The random variables Y is said to have the non-central chi-squared
distribution with n degrees of freedom and non-centrality parameter 0, written x2(n; 6).

8 Let X be N(u, 1) and let Y be x2(n), and suppose that X and Y are independent. The random
variable T = X/./Y/n is said to have the non-central t-distribution with n degrees of freedom and
non-centrality parameter jz. If U and V are independent, U being x*(m; @) and V being x7(n), then
F = (U/m)/(V/n) is said to have the non-central F-distribution with m and n degrees of freedom
and non-centrality parameter 6, written F (m,n; 0).

(a) Show that T? is F(1, n; 27).

(b) Show that

n(m + 6)
m(n — 2)

E(F) = ifn > 2.

9, Let X be arandom variable with density function f and characteristic function ¢. Show, subject
to an appropriate condition on /, that

lore) 3 1 fora) 9
[1@ dx = on [wo dt.

53

[5.7.10]-{5.8.11] Exercises Generating functions and their applications

10. If X and Y are continuous random variables, show that
Toe) - ro.)
/ ox(y) fy (ye? dy = / gy (x —t)fx@)dx.
—w —0CO

11. Tilted distributions. (a) Let X have distribution function F and let t be such that M(t) =
E(e**) < oo. Show that Fy(x) = M(r)7! f*,, e* dF(y) is a distribution function, called a ‘tilted
distribution’ of X, and find its moment generating function.

(b) Suppose X and Y are independent and E(e™* ), E(e™”) < oo. Find the moment generating function
of the tilted distribution of X + Y in terms of those of X and Y.

5.8 Exercises. Examples of characteristic functions

1. If ¢ is a characteristic function, show that ¢, 6”, |@|2, Re(@) are characteristic functions. Show
that || is not necessarily a characteristic function.

2. Show that
P(X >x)< inf {e" Mx()},
rt

where My is the moment generating function of X.

3. Let X have the F(A, m) distribution and let Y be independent of X with the beta distribution
with parameters n and m — n, where m and n are non-negative integers satisfying n < m. Show that
Z = XY has the ['(A, n) distribution.

4. Find the characteristic function of X2 when X has the N (LL, o) distribution.

5. Let X1, X2,... be independent N(0, 1) variables. Use characteristic functions to find the distri-
bution of: (a) xf, (b) Oy X?, (c) X1/X2, (d) X1 Xo, (©) XyX2 + X3Xq.

6. Let Xj, X2,..., Xn be such that, for all aj, az,...,a, € R, the linear combination a,X 1 +
a7 X7+---+ay,Xp has anormal distribution. Show that the joint characteristic function of the Xj is
exp(itye’ — ztve’ ), for an appropriate vector yz and matrix V. Deduce that the vector (X1, X2,..-, Xn)
has a multivariate normal density function so long as V is invertible.

7. Let X and Y be independent N (0, 1) variables, and let U and V be independent of X and Y. Show

that Z = (UX + VY)/VU2 + V7 has the N(0, 1) distribution. Formulate an extension of this result
to cover the case when X and Y have a bivariate normal distribution with zero means, unit variances,
and correlation p.

8. Let X be exponentially distributed with parameter 4. Show by elementary integration that
E(e#*) = a/(a — it).

9. Find the characteristic functions of the following density functions:

(a) f= ze for x ER,

(b) f(x) = 4|axle7Fl for x eR.

10. Is it possible for X, Y, and Z to have the same distribution and satisfy X = U(Y + Z), where
U is uniform on [0, 1], and Y, Z are independent of U and of one another? (This question arises in
modelling energy redistribution among physical particles.)

11. Find the joint characteristic function of two random variables having a bivariate normal distribution
with zero means. (No integration is needed.)

54
Inversion and continuity theorems Exercises [5.9.1]-[5.9.8]

5.9 Exercises. Inversion and continuity theorems

1. Let X, be a discrete random variable taking values in {1,2, ...,n}, each possible value having
probability n—!. Show that, asn — oo, Pan lx, <y)—> y,fo0<y<1.

2. Let X, have distribution function

sin(2n7x)

FAi@=x , O<x<l.

food

Qn

(a) Show that F,, is indeed a distribution function, and that X,, has a density function.
(b) Show that, as n — oo, F, converges to the uniform distribution function, but that the density
function of F, does not converge to the uniform density function.

3. Accoin is tossed repeatedly, with heads turning up with probability p on each toss. Let N be the
minimum number of tosses required to obtain & heads. Show that, as p | 0, the distribution function
of 2Np converges to that of a gamma distribution.

4, If X is an integer-valued random variable with characteristic function ¢, show that
1 fF .;
P(X =k = = | ek gr) dt.
2n J—x

What is the corresponding result for a random variable whose distribution is arithmetic with span
(that is, there is probability one that X is a multiple of A, and A is the largest positive number with this
property)?

5. Use the inversion theorem to show that

[. sin(at) sin(bt) dt

7 =m min{a, b}.

—C
6. Stirling’s formula. Let f,(~) be a differentiable function on R with a a global maximum at

a > 0, and such that Sor exp{ fn(x)}dx < oo. Laplace’s method of steepest descent (related to
Watson’s lemma and saddlepoint methods) asserts under mild conditions that

[ exp{ fn(x)} dx ~ i exp{ fn(@) + d(x - a)? f7'(a)} dx asn>o.
0

By setting f,(x) = nlogx — x, prove Stirling’s formula: n! ~ n"e"" JV 2700.

7. Let X = (X1, X2,...,Xn) have the multivariate normal distribution with zero means, and
covariance matrix V = (v;;) satisfying |V| > 0 and vj; > 0 for all i, 7. Show that
az
L iti¢i,
of Ox; 0X;
duj ) 102
4 ios ifi = j,
2 dx;

and deduce that P(maxg<p, Xx <u) > [Ty P(X, Sw).

8. Let X,, X2 have a bivariate normal distribution with zero means, unit variances, and correlation
p. Use the inversion theorem to show that

1

InV/1 — p2

a
—P(X, > 0, X2 > 0 =
dp

Hence find P(X; > 0, X2 > 0).

55
{5.10.1]-[5.10.9] Exercises Generating functions and their applications

5.10 Exercises. Two limit theorems

1. Prove that, for x > 0, asn — oo,

k:
KkK—5al<px/n
k x
n 1 12
b — ~e | e 2" du.
(®) 2 k! -x V2
|kK—n|<x./n

2. It is well known that infants born to mothers who smoke tend to be small and prone to a range of
ailments. It is conjectured that also they look abnormal. Nurses were shown selections of photographs
of babies, one half of whom had smokers as mothers; the nurses were asked to judge from a baby’s
appearance whether or not the mother smoked. In 1500 trials the correct answer was given 910 times.
Is the conjecture plausible? If so, why?

3. Let X have the (1, 5) distribution; given that X = x, let Y have the Poisson distribution with
parameter x. Find the characteristic function of Y, and show that

Y-E(Y) D

———_ > N(0, 1 ass > Oo.

/var(Y) (1)
Explain the connection with the central limit theorem. :
4. Let X1, X2,... be independent random variables taking values in the positive integers, whose

common distribution is non-arithmetic, in that gcd{n : P(X, = n) > 0} = 1. Prove that, for all
integers x, there exist non-negative integers r = r(x), s = s(x), such that

P(X, +--+ +X, — Xp4y —- ++ — Xr4s =x) > 0.

5. Prove the local central limit theorem for sums of random variables taking integer values. You
may assume for simplicity that the summands have span 1, in that gcd{ |x|: P(X = x) > 0} =1.

6. Let X1, X2,... be independent random variables having common density function f(x) =
1/{2|x|(log |x[)2} for |x| < e—!. Show that the Xj; have zero mean and finite variance, and that the
density function f, of Xj + X2 +---+ Xp satisfies fy(~) + oo as x — 0. Deduce that the X; do
not satisfy the local limit theorem.

7, First-passage density. Let X have the density function f(x) = /27x—3 exp(—{2x}7!), x>Q0.
Show that @(is) = E(e**) = e—V¥25, 5 > 0, and deduce that X has characteristic function
$(t) = { exp{-(1—-i)./t} ifr >0,
exp{-(1 +1)./lt]} ift <0.
(Hint: Use the result of Problem (5.12.18).]

8. Let {X; : r > 1} be independent with the distribution of the preceding Exercise (7). Let
U, =n} nr, Xr, and Ty = n—1U,,. Show that:
(a) P(U, <c) > Ofor any c < ~,

(b) T, has the same distribution as X1.
9. A-sequence of biased coins is flipped; the chance that the rth coin shows a head is ©;, where ©,

is arandom variable taking values in (0, 1). Let X, be the number of heads after n flips. Does Xp,
obey the central limit theorem when:

(a) the ©, are independent and identically distributed?
(b) ©, = @ for all r, where © is a random variable taking values in (0, 1)?

56
Problems Exercises [5.11.1]-[5.12.4]

5.11 Exercises. Large deviations

1. A fair coin is tossed n times, showing heads H,, times and tails T, times. Let S, = Hy — Thy.
Show that 1

v l/n :
P(Sp > anyi™ > Varad paica if0<a<1.

What happens if a > 1?

2. Show that
1/n 4

Th
~ Vata — ara

lk-4n|>gan

asn — oo, where 0 < a < 1 and

Find the asymptotic behaviour of T) /” where

k

Th = > ae where a > 0.

k:
k>n(1+a)

3. Show that the moment generating function of X is finite in a neighbourhood of the origin if and
only if X has exponentially decaying tails, in the sense that there exist positive constants A and yz such
that P(|X| > a) < ye for a > 0. [Seen in the light of this observation, the condition of the large
deviation theorem (5.11.4) is very natural].

4. Let X1, X2,... be independent random variables having the Cauchy distribution, and let S$, =
XytXot+---+ Xp. Find P(S, > an).

5.12 Problems

1. A die is thrown ten times. What is the probability that the sum of the scores is 27?

2. Acoin is tossed repeatedly, heads appearing with probability p on each toss.

(a) Let X be the number of tosses until the first occasion by which three heads have appeared
successively. Write down a difference equation for f(k) = P(X = k) and solve it. Now write
down an equation for E(X) using conditional expectation. (Try the same thing for the first
occurrence of HTH).

(b) Let N be the number of heads in n tosses of the coin. Write down Gy(s). Hence find the
probability that: (i) N is divisible by 2, (ii) N is divisible by 3.

3. Accoin is tossed repeatedly, heads occurring on each toss with probability p. Find the probability

generating function of the number T of tosses before a run of n heads has appeared for the first time.

4. Find the generating function of the negative binomial mass function

f®= (Fai Jora—me k=rnr4,...,

57
[5.12.5]-[5.12.13] Exercises Generating functions and their applications

where 0 < p < 1 andr is a positive integer. Deduce the mean and variance.

5. For the simple random walk, show that the probability po (2n) that the particle returns to the origin
at the (2n)th step satisfies po(2n) ~ (4pq)" /./mn, and use this to prove that the walk is persistent if

1
and only if p = 5. You will need Stirling’s formula: n! ~ n't de" / 2x,
6. A symmetric random walk in two dimensions is defined to be a sequence of points {(Xn, Yn) :
n > 0} which evolves in the following way: if (Xn, Yn) = (x, y) then (X41, Yn+41) is one of the
four points (x + 1, y), (x, y + 1), each being picked with equal probability i. If (Xo, Yo) = (0, 0):
(a) show that E(X?2 + ¥2) =n,
(b) find the probability pp (2n) that the particle is at the origin after the (2n)th step, and deduce that

the probability of ever returning to the origin is 1.

7. Consider the one-dimensional random walk {S,} given by

5 _ { Sn +2 with probability p,
n+l Sn — 1 with probability g = 1 — p,

where 0 < p < 1. What is the probability of ever reaching the origin starting from Sg = a where
a>0Q?

8. Let X and Y be independent variables taking values in the positive integers such that

PX =k|X+¥=n)= ({) oa — py*

for some p and all 0 < k <n. Show that X and Y have Poisson distributions.

9. In abranching process whose family sizes have mean jz and variance o”, find the variance of Zn,
the size of the nth generation, given that Zp = 1.

10. Waldegrave’s problem. A group {A), Az, ..., Ar} of r (= 2) people play the following game.
A, and A» wager on the toss of a fair coin. The loser puts £1 in the pool, the winner goes on to play
A3. In the next wager, the loser puts £1 in the pool, the winner goes on to play Ay, and so on. The
winner of the (r — 1)th wager goes on to play A, and the cycle recommences. The first person to
beat all the others in sequence takes the pool.

(a) Find the probability generating function of the duration of the game.

(b) Find an expression for the probability that A, wins.

(c) Find an expression for the expected size of the pool at the end of the game, given that A, wins.
(d) Find an expression for the probability that the pool is intact after the nth spin of the coin.

This problem was discussed by Montmort, Bernoulli, de Moivre, Laplace, and others.

11. Show that the generating function H, of the total number of individuals in the first n generations
of a branching process satisfies Hy, (s) = sG(H,_1(s)).

12. Show that the number Z, of individuals in the nth generation of a branching process satisfies
P(Zn > N | Zm =0) < Gm(O)% for n < m.

13. (a) A hen lays N eggs where N is Poisson with parameter 2. The weight of the nth egg is
Wn, where W,, W2,... are independent identically distributed variables with common probability
generating function G(s). Show that the generating function Gy of the total weight W = yy Wi
is given by Gw(s) = exp{—A + AG(s)}. W is said to have a compound Poisson distribution. Show
further that, for any positive integral value of n, Gw(s)!/" is the probability generating function of
some random variable; W (or its distribution) is said to be infinitely divisible in this regard.

58

Problems Exercises [5.12.14]-[5.12.19]

(b) Show that if H(s) is the probability generating function of some infinitely divisible distribution
on the non-negative integers then H(s) = exp{—A + AG(s)} for some A (> 0) and some probability
generating function G(s).

14. The distribution of a random variable X is called infinitely divisible if, for all positive integers n,

” ys... YK”

there exists a sequence Y t of independent identically distributed random variables

such that X and yi” + yi” +...4¥ have the same distribution.

(a) Show that the normal, Poisson, and gamma distributions are infinitely divisible.

(b) Show that the characteristic function ¢ of an infinitely divisible distribution has no real zeros, in
that @(t) ¥ 0 for all real t.

15. Let X,, X2,... be independent variables each taking the values 0 or 1 with probabilities 1— p and
Pp, where 0 < p < 1. Let N be a random variable taking values in the positive integers, independent
of the X;, and write S = X, + Xo. +---+ Xy. Write down the conditional generating function of N
given that § = N, in terms of the probability generating function G of N. Show that N has a Poisson
distribution if and only if EQN \P = EQN | S = N) forall pand x.

16. If X and Y have joint probability generating function

{1 — (pi + p2)}”

— Wedel) —
Cx.) = EST) = et py

where p, + p2 < 1,

find the marginal mass functions of X and Y, and the mass function of X + Y. Find also the conditional
probability generating function G x\y(s | y) = E(s* | ¥ = y) of X given that Y = y. The pair X,Y
is said to have the bivariate negative binomial distribution.

17. If X and Y have joint probability generating function
Gx,y(s,t) = exp{a(s —) + 6@-D+yv6r—-D}

find the marginal distributions of X, Y, and the distribution of X + Y, showing that X and Y have the
Poisson distribution, but that X + Y does not unless y = 0.

18. Define 60
I(a,b)= [ exp(—a?u? - b*u-?) du
0

for a, b > 0. Show that

(a) I(a,b) = a71(1, ab), (b) 91/86 = —2I (1, ab),
(©) Ia, b) = Jme~? /(2a).

(d) If X has density function (d/./x)e—°/*—8* for x > 0, then

E(e*) =d 7 exp(-2,/e(g +0), t>~g.

+t

1
(e) If X has density function (20x37) Ze—!/2) for x > O, then X has moment generating function
given by E(e'*) = exp{—V2r}, t > 0. [Note that E(X") = co forn > 1.]

19. Let X, Y, Z be independent N(0, 1) variables. Use characteristic functions and moment gener-
ating functions (Laplace transforms) to find the distributions of

(a) UV=X/Y,

(b) V=x~,

(c) W= XYZ/VX2Y¥2 + ¥2Z2 + Z2 x2.

59

[5.12.20]-[5.12.30] Exercises Generating functions and their applications

20. Let X have density function f and characteristic function ¢, and suppose that feo |P(t)| dt < ow.
Deduce that

F(x) = = [ aco dt.

21. Conditioned branching process. Consider a branching process whose family sizes have the
geometric mass function f(k) = qp* ,k > 0, where uw = p/q > 1. Let Z, be the size of the nth
generation, and assume Zp = 1. Show that the conditional distribution of Z,/j”, given that Z, > 0,
converges as n —> oo to the exponential distribution with parameter 1 — po!

22. A random variable X is called symmetric if X and —X are identically distributed. Show that X
is symmetric if and only if the imaginary part of its characteristic function is identically zero.

23. Let X and Y be independent identically distributed variables with means 0 and variances 1. Let

(t) be their common characteristic function, and suppose that X + Y and X — Y are independent.
Show that @(2t) = (t)?(—1), and deduce that X and Y are N(0, 1) variables.

More generally, suppose that X and Y are independent and identically distributed with means 0
and variances 1, and furthermore that E(X — Y | X + Y) = Oand var(X — Y | X + Y) = 2. Deduce
that @(s)? = $’(s)? — 6(s)¢”(s), and hence that X and Y are independent N(0, 1) variables.

24. Show that the average Z = n—! >, X; of n independent Cauchy variables has the Cauchy
distribution too. Why does this not violate the law of large numbers?

25. Let X and Y be independent random variables each having the Cauchy density function f(x) =
{r(1 +. x?)}—1, and let Z = $(X +).

(a) Show by using characteristic functions that Z has the Cauchy distribution also.

(b) Show by the convolution formula that Z has the Cauchy density function. You may find it helpful

to check first that
f@)+ fO-+*)
x -—x) =O H+ xf(x)+(y-—x —x
f@)F(y — x) n(44 yd sod{af(x) + — x) f(y —x)}

where g(y) = 2/{xy(4 + y*)}.
26. Let X1, X2,..., X» be independent variables with characteristic functions $1, $2, ...,¢n,. De-
scribe random variables which have the following characteristic functions:

(a) $1 @)d2(@) --- bn), (b) |i 1°,

(©) Of pj where pj > Oand Di pj =1, M2-HO)7,
(e) fo di (ute du.
27. Find the characteristic functions corresponding to the following density functions on (—0oo, 00):
(a) 1/cosh(zx), (b) (1 — cosx)/(x?),
(c) exp(—x — e*), (d) de,
Show that the mean of the ‘extreme-value distribution’ in part (c) is Euler’s constant y.

28. Which of the following are characteristic functions:
(a) o(t) = 1 — |t| if |t| < 1, 6(t) = 0 otherwise,
)o@)=A+t4) 1, ©e@ =exp-r*,
(d) o(t) = cost, (e) (t) = 2(1 — cost) /t”.
29. Show that the characteristic function @ of a random variable X satisfies |1 — @(t)| < E|rX|.

30. Suppose X and Y have joint characteristic function @(s, t). Show that, subject to the appropriate
conditions of differentiability,

as™ Ot” |, ¢<9

60

Problems Exercises [5.12.31]-[5.12.39]

for any positive integers m and n.

31. If X has distribution function F and characteristic function ¢, show that for t > 0

() [og yo2aF < Ft -Reo@,
[1h] t

1

7 t
(b) P (x > *) < : if [1 —Re b(v)] dv.

32. Let X,, X2,... be independent variables which are uniformly distributed on [0, 1]. Let My, =

max{X 1, X2,..., Xn} and show that n(1 — M,) ae X where X is exponentially distributed with
parameter 1. You need not use characteristic functions.

33. If X is either (a) Poisson with parameter A, or (b) F'(1, A), show that the distribution of Y,; =
(X — EX)//var X approaches the N(0, 1) distribution as 1 > oo.
(c) Show that

nt

2 1
e"(1ine7 4...45) = as n > 00.
2! n! 2

34. Coupon collecting. Recall that you regularly buy quantities of some ineffably dull commodity.
To attract your attention, the manufacturers add to each packet a small object which is also dull, and in
addition useless, but there are n different types. Assume that each packet is equally likely to contain
any one of the different types, as usual. Let 7, be the number of packets bought before you acquire

a complete set of n objects. Show that n—!(T, — nlogn) 3 T, where T is arandom variable with
distribution function P(T < x) = exp(—e™*), —0o < x < ow.

35. Find a sequence (¢,,) of characteristic functions with the property that the limit given by ¢(t) =
limy— oo Gn (t) exists for all t, but such that @ is not itself a characteristic function.

36. Use generating functions to show that it is not possible to load two dice in such a way that the
sum of the values which they show is equally likely to take any value between 2 and 12. Compare
with your method for Problem (2.7.12).

37. A biased coin is tossed N times, where N is a random variable which is Poisson distributed
with parameter A. Prove that the total number of heads shown is independent of the total number of
tails. Show conversely that if the numbers of heads and tails are independent, then N has the Poisson
distribution.

38. A binary tree is a tree (as in the section on branching processes) in which each node has exactly
two descendants. Suppose that each node of the tree is coloured black with probability p, and white
otherwise, independently of all other nodes. For any path 7 containing n nodes beginning at the root
of the tree, let B(zr) be the number of black nodes in 7, and let X,(k) be the number of such paths z
for which B(zr) > k. Show that there exists 6, such that

0 ifB> Be,
B(Xa(n)) -> { in > Be
co if B < Be,
and show how to determine the value Be.
Prove that 0 if
P(Xn (Bn) > 1) = { 1B > Bes
1 if B < Be.

39. Use the continuity theorem (5.9.5) to show that, as n > ov,
(a) if Xp is bin(n, 4/n) then the distribution of X, converges to a Poisson distribution,

61
[5.12.40]-[5.12.46] Exercises Generating functions and their applications

(b) if Y, is geometric with parameter p = )/n then the distribution of Y, /n converges to an expo-
nential distribution.

40. Let X1, Xo,... be independent random variables with zero means and such that ELX} | < oo for

all 7. Show that S, = X; + Xo +---4+ Xz» satisfies Sn//var(Sn) 5. N(O, 1) asn > oo if

ST EIX3| = o( tvar(S,)}~2).
j=l

The following steps may be useful. Let oF = var(X;), a(n) = var(Sy), 07 = E|X}|, and $j
and Wm be the characteristic functions of X; and Sp /o(n) respectively.
) Use Taylor’s theorem to show that |g; (t) — 1] < 28? and |pj(t) — 1 + 507271 < ep; for
j21.

Gi) Show that | log(1 + z) — z| < Iz|? if |z| < i, where the logarithm has its principal value.

(iii) Show that o3 < p;, and deduce from the hypothesis that max1<j<,0;/0(n) > Oasn — oo,
implying that max) <;<» |b; (¢/o(#)) — 1| > 0.

(iv) Deduce an upper bound for |log o;(t/o(n))— 500? /o (ny | , and sum to obtain that log y(t) >

—41?.

41. Let X1, Xo, ... be independent variables each taking values +1 or —1 with probabilities 5 and

4. Show that

3 nt
az EXE NOT) asn ow.
n
k=1

42. Normal sample. Let X1, X2,..., Xn be independent N(u, a) random variables. Define X =
n—! 5“? X; and Z; = X; — X. Find the joint characteristic function of X, Z,, Zz, ..., Zn, and hence
prove that X and S 2 (n— y7! I (Xj -— X)* are independent.

43. Log-normal distribution. Let X be N(0, 1), and let Y = e*: Y is said to have the log-normal
distribution. Show that the density function of Y is

f@ = exp{—4(logx)}, x>0.

1
xV/ 20
For |a| < 1, define fa(x) = {1 +a sin(2z log x)} F(x). Show that fo is adensity function with finite
moments of all (positive) orders, none of which depends on the value of a. The family { fa : |a| < 1}
contains density functions which are not specified by their moments.

44. Consider a random walk whose steps are independent and identically distributed integer-valued
random variables with non-zero mean. Prove that the walk is transient.

45. Recurrent events. Let {X; : r > 1} be the integer-valued identically distributed intervals
between the times of a recurrent event process. Let L be the earliest time by which there has been an
interval of length a containing no occurrence time. Show that, for integral a,

s?P(X1 > a)
1-4 P(X = 1)

E(s¥) =

46. A biased coin shows heads with probability p (= 1 — q). It is flipped repeatedly until the first
time W, by which it has shown n consecutive heads. Let E(s Wn) = Gy(s). Show that Gy =

62

Problems Exercises [5.12.47]-[5.12.52]

psGy_-1/( — gsGn_1), and deduce that

(1 — ps)p"s”
Gr(s) = : - n+1°
1l—s+qp"s

47. Inn flips of a biased coin which shows heads with probability p (= 1 — q), let Ly be the length
of the longest run of heads. Show that, for r > 1,

CO
1+ $0 s"PLn <r) = _

n=1

48. The random process {X, : n > 1} decays geometrically fast in that, in the absence of external
input, Xp41 = 5X n- However, at any time n the process is also increased by Y, with probability
4, where {Y;, : n > 1} is a sequence of independent exponential random variables with parameter 1.
Find the limiting distribution of X;, as n > oo.

49. Let G(s) = E(s*) where X > 0. Show that E{(X + 1)7}} = fo G(s) ds, and evaluate this
when X is (a) Poisson with parameter A, (b) geometric with parameter p, (c) binomial bin(n, p), (d)
logarithmic with parameter p (see Exercise (5.2.3)). Is there a non-trivial choice for the distribution
of X such that E{(X + 1)7!} = (R(X +. yy}?

50. Find the density function of yy Xy, where {X, : r > 1} are independent and exponentially
distributed with parameter 1, and N is geometric with parameter p and independent of the X;.

51. Let X have finite non-zero variance and characteristic function ¢(t). Show that

1 do

¥@) = ~ R(X) dt2

is a characteristic function, and find the corresponding distribution.

52. Let X and Y have joint density function

fay =H{ttrxy@?-y%)}, el <1, ly <1.

Show that ¢y (t)@y (t) = @x+y (t), and that X and Y are dependent.

63

6

Markov chains

6.1 Exercises. Markov processes

1. Show that any sequence of independent random variables taking values in the countable set S is
a Markov chain. Under what condition is this chain homogeneous?

2. Adie is rolled repeatedly. Which of the following are Markov chains? For those that are, supply
the transition matrix.

(a) The largest number X, shown up to the mth roll.

(b) The number N,, of sixes in n rolls.

(c) At time r, the time C, since the most recent six.

(d) At time r, the time B, until the next six.

3. Let {S, : 2 > 0} be a simple random walk with Sg = 0, and show that X, = |S,| defines a

Markov chain; find the transition probabilities of this chain. Let M, = max{S, : 0 < k <n}, and
show that Y, = My — Sy, defines a Markov chain. What happens if Sg 4 0?

4. Let X be a Markov chain and let {n; : r > 0} be an unbounded increasing sequence of positive
integers. Show that ¥, = X»y, constitutes a (possibly inhomogeneous) Markov chain. Find the
transition matrix of Y when ny = 2r and X is: (a) simple random walk, and (b) a branching process.

5. Let X be a Markov chain on S, and let J : S" — {0,1}. Show that the distribution of
Xn, Xn41,---, conditional on {7(X1,...,Xn) = 1} (Xn = 1}, is identical to the distribution
of Xn, Xn41,... conditional on {X, = i}.

6. Strong Markov property. Let X be a Markov chain on S, and let T be a random variable taking
values in {0, 1, 2, ... } with the property that the indicator function I;7—n}, of the event that T = n, is
a function of the variables X;, X2,..., Xn. Such a random variable T is called a stopping time, and
the above definition requires that it is decidable whether or not T = n with a knowledge only of the
past and present, Xg, X1,..., Xn, and with no further information about the future.

Show that
P(Xrim = j|X~ =x forO<k <T, Xp =i) =P(Xtim = Jj | X7 =i)

for m > 0, i, j € S, and all sequences (x;) of states.

7. Let X be a Markov chain with state space S, and suppose that h : § — T is one-one. Show that
Yn = h(Xn) defines a Markov chain on T. Must this be so if h is not one-one?

8. Let X and Y be Markov chains on the set Z of integers. Is the sequence Zy = Xn + Yn necessarily
a Markov chain?

9. Let X be a Markov chain. Which of the following are Markov chains?

64

Classification of states Exercises [6.1.10]-[6.2.5]

(a) Xm+r forr > 0.
(b) X, for m = 0.
(c) The sequence of pairs (Xp, Xn41) forn > 0.

10. Let X be a Markov chain. Show that, for 1 <r <n,

P(X, =k | X; =x; fori =1,2,...,r-—1,r+1...,n)
= P(X, =k | Xp_-1 = Xp-1, Xp = Ar 41)-

11. Let {X, : n > 1} be independent identically distributed integer-valued random variables. Let
Sp = OP Xr, with So = 0, Yn = Xn + Xp—1 with Xp = 0, and Zy = D-_o S;. Which of the
following constitute Markov chains: (a) Sp, (b) Yn, (¢) Zn, (d) the sequence of pairs (Sy, Zn)?

12. Astochastic matrix P is called doubly stochastic if 7; pij = 1 forall j. Itis called sub-stochastic
if 7; pi ij <1 for all j. Show that, if P is stochastic (respectively, doubly stochastic, sub-stochastic),
then P” is stochastic (respectively, doubly stochastic, sub-stochastic) for all n.

6.2 Exercises. Classification of states

1. Last exits, Let Jj;(2) = P(Xn = j, X_ # ifor1 < k <n | Xo =i), the probability that the
chain passes from i to j in n steps without revisiting i. Writing

Li(s) = >> sli),
n=1

show that P;;(s) = Pi (s)Lij(s) if i # j. Deduce that the first passage times and last exit times
have the same distribution for any Markov chain for which Pjj(s) = P;;(s) for all i and 7. Give an
example of such a chain.

2. Let X bea Markov chain containing an absorbing state s with which all other states i communicate,
in the sense that p;;(n) > 0 for some n = n(i). Show that all states other than s are transient.

3. Show that a state i is persistent if and only if the mean number of visits of the chain to 7, having
started at i, is infinite.

4. Visits. Let V; = |{n => 1: Xn = j}| be the number of visits of the Markov chain X to j, and
define nj; = P(V; = 00 | Xp =i). Show that:
1 ifi is persistent,
ans =
@) i { 0. if is transient,
P(T; <co| Xo =i) if j is persistent,
(b) nj = pas .
0 if j is transient,

5. Symmetry. The distinct pair i, j of states of a Markov chain is called symmetric if

where 7; = min{n > 1: X_ = j}.

PT) <T | Xo =) =P < T | X0= J),

where 7; = min{n > 1: X, =i}. Show that, if Xo =i andi, j is symmetric, the expected number
of visits to j before the chain revisits i is 1.

65

[6.3.1]-[6.3.8] Exercises Markov chains

6.3 Exercises. Classification of chains

1. Let X be a Markov chain on {0, 1, 2,...} with transition matrix given by po j = aj for j > 0,
Pii =r and p;;-) = 1 —r fori > 1. Classify the states of the chain, and find their mean recurrence
times.

2. Determine whether or not the random walk on the integers having transition probabilities p; ;42 =
P, Pii-1 = 1 — p, for all i, is persistent.

3. Classify the states of the Markov chains with transition matrices

1-—2p 2p 0
(a) p  1—2p~ p ,
0 2p 1-—2p
0 p 0 1-p
1-—p 0 Pp 0
(b) 0 1l-p 0 Pp
Pp 0 1l-—p 0

In each case, calculate p;;(n) and the mean recurrence times of the states.

4. A particle performs a random walk on the vertices of a cube. At each step it remains where it is
with probability i, or moves to one of its neighbouring vertices each having probability i. Let v and
w be two diametrically opposite vertices. If the walk starts at v, find:

(a) the mean number of steps until its first return to v,

(b) the mean number of steps until its first visit to w,

(c) the mean number of visits to w before its first return to v.

5. Visits. With the notation of Exercise (6.2.4), show that

(a) ifi > j and i is persistent, then yj; = nj; = 1,

(b) nj; = 1 ifand only if PZ; < co | X9 =I) =P <wo|Xo=f=l.

6. First passages. Let T4 = min{n > 0: Xy, € A}, where X is a Markov chain and A is a subset
of the state space S, and let nj = P(T4 < co | Xo = j). Show that

1 ifjeA,
=) S” pjem iff ¢ A.
keS

Show further that if x = (x; : j € S) is any non-negative solution of these equations then x; > nj; for
all j.

7, Mean first passage. In the notation of Exercise (6), let 0; = E(T4 | Xo = j). Show that

0 ifjeA,
PI=YV1+>0 Djeoe if i ¢ A,
keS
and that if x = (x; : j € S) is any non-negative solution of these equations then x; > 0; for all j.

8. Let X be an irreducible Markov chain and let A be a subset of the state space. Let S, and T,
be the successive times at which the chain enters A and visits A respectively. Are the sequences
{Xs 17 = 1}, {Xz 2 r => 1} Markov chains? What can be said about the times at which the chain
exits A?

66

Stationary distributions and the limit theorem Exercises [6.3.9]-[6.4.6]

9. (a) Show that for each pairi, j of states of an irreducible aperiodic chain, there exists N = N(i, j)

such that pj; (r) > 0 for allr > N.

(b) Show that there exists afunction f such that, if P is the transition matrix of an irreducible aperiodic
Markov chain with n states, then p;;(r) > 0 for all states i, j, and allr > f(m).

(c) Show further that f(4) > 6 and f(n) > (a — 1)(m — 2).
[Hint: The postage stamp lemma asserts that, for a, b coprime, the smallest n such that all integers
strictly exceeding n have the form wa + £b for some integers a, B > Ois (a — 1)(6 — 1).]

10. An urn initially contains n green balls and n + 2 red balls. A ball is picked at random: if it is
green then a red ball is also removed and both are discarded; if it is red then it is replaced together
with an extra red and an extra green ball. This is repeated until there are no green balls in the urn.
Show that the probability the process terminates is 1/(m + 1).

Now reverse the rules: if the ball is green, it is replaced together with an extra green and an extra
red ball; if it is red it is discarded along with a green ball. Show that the expected number of iterations
until no green balls remain is va 2 j +1) = n(n + 2). [Thus, a minor perturbation of a simple
symmetric random walk can be non-null persistent, whereas the original is null persistent]

6.4 Exercises. Stationary distributions and the limit theorem

1. The proof copy of a book is read by an infinite sequence of editors checking for mistakes. Each
mistake is detected with probability p at each reading; between readings the printer corrects the
detected mistakes but introduces a random number of new errors (errors may be introduced even if no
mistakes were detected). Assuming as much independence as usual, and that the numbers of new errors
after different readings are identically distributed, find an expression for the probability generating
function of the stationary distribution of the number X, of errors after the nth editor—printer cycle,
whenever this exists. Find it explicitly when the printer introduces a Poisson-distributed number of
errors at each stage.

2. Do the appropriate parts of Exercises (6.3.1)-(6.3.4) again, making use of the new techniques at
your disposal.

3. Dams. Let X,, be the amount of water in a reservoir at noon on day n. During the 24 hour period
beginning at this time, a quantity Y, of water flows into the reservoir, and just before noon on each
day exactly one unit of water is removed (if this amount can be found). The maximum capacity of
the reservoir is K, and excessive inflows are spilled and lost. Assume that the Y, are independent
and identically distributed random variables and that, by rounding off to some laughably small unit of
volume, all numbers in this exercise are non-negative integers. Show that (X;,) is a Markov chain, and
find its transition matrix and an expression for its stationary distribution in terms of the probability
generating function G of the Yy.

Find the stationary distribution when Y has probability generating function G(s) = p(1—qs)7!.

4. Show by example that chains which are not irreducible may have many different stationary
distributions.

5. Diagonal selection. Let (x;(n) : i,m > 1) be a bounded collection of real numbers. Show that
there exists an increasing sequence nj, ”2,... of positive integers such that limy—, 90 xj (",-) exists for
all i. Use this result to prove that, for an irreducible Markov chain, if it is not the case that p;j (1) > 0
as n — oo for alli and j, then there exists a sequence (n; : r > 1) and a vector « (< 9) such that
Pij Mr) > aj asr — oo for all i and j.

6. Random walk ona graph. A particle performs a random walk on the vertex set of a connected
graph G, which for simplicity we assume to have neither loops nor multiple edges. At each stage it
moves to a neighbour of its current position, each such neighbour being chosen with equal probability.

67
[6.4.7]-[6.5.3] Exercises Markov chains

If G has 7 (< oo) edges, show that the stationary distribution is given by ty = dy/(2n), where dy is
the degree of vertex v.

7. Show that a random walk on the infinite binary tree is transient.

8. At each time n = 0,1,2,... a number Y, of particles enters a chamber, where {Y, : n > 0}
are independent and Poisson distributed with parameter 1. Lifetimes of particles are independent and
geometrically distributed with parameter p. Let X, be the number of particles in the chamber at time
n. Show that X is a Markov chain, and find its stationary distribution.

9. Arandom sequence of convex polygons is generated by picking two edges of the current polygon
at random, joining their midpoints, and picking one of the two resulting smaller polygons at random
to be the next in the sequence. Let X, +3 be the number of edges of the nth polygon thus constructed.
Find E(X;,) in terms of Xo, and find the stationary distribution of the Markov chain X.

10. Lets be a state of an irreducible Markov chain on the non-negative integers. Show that the chain
is persistent if there exists a solution y to the equations y; > }> jrjgts BipVjpt FS, satisfying yj —> oo.

11. Bow ties. A particle performs a random walk on a bow tie ABCDE drawn beneath on the left,
where C is the knot. From any vertex its next step is equally likely to be to any neighbouring vertex.
Initially it is at A. Find the expected value of:

(a) the time of first return to A,

(b) the number of visits to D before returning to A,

(c) the number of visits to C before returning to A,

(d) the time of first return to A, given no prior visit by the particle to E,

(e) the number of visits to D before returning to A, given no prior visit by the particle to E.

A D

A B

12. A particle starts at A and executes a symmetric random walk on the graph drawn above on the
right. Find the expected number of visits to B before it returns to A.

6.5 Exercises. Reversibility

1. A random walk on the set {0, 1,2, ..., b} has transition matrix given by pop = 1 — Ag. Pop =
1 — pp, Pig. = AG and pj41G = Mi4, forO <i < b, where 0 < Aj, 4; < 1 for all i, and
Ai + wy = 1 for 1 <i < b. Show that this process is reversible in equilibrium.

2. Kolmogorov’s criterion for reversibility. Let X be an irreducible non-null persistent aperiodic
Markov chain. Show that X is reversible in equilibrium if and only if

Pjy jo Pinj3°*° Pin-tinPiniy = PivinPinin-1*° Pini
for all n and all finite sequences j1, j2,..., jn of states.

3. Let X be a reversible Markov chain, and let C be a non-empty subset of the state space S. Define
the Markov chain Y on S by the transition matrix Q = (q;;) where

Bp ifie Candj ¢C,
ij = :
pi; — otherwise,

68

Chains with finitely many states Exercises [6.5.4]-[6.6.3]

fori # j, and where £ is a constant satisfying 0 < 6 < 1. The diagonal terms g;; are arranged so that
Q is a stochastic matrix. Show that Y is reversible in equilibrium, and find its stationary distribution.
Describe the situation in the limit as B | 0.

4. Cana reversible chain be periodic?
5. Ehrenfest dog-flea model. The dog—flea model of Example (6.5.5) is a Markov chain X on the
state space {0, 1,..., m} with transition probabilities

i i .
Pitt =1-—, P-L =—, for O<i<m.
m m

Show that, if Xo = i,

6. Which of the following (when stationary) are reversible Markov chains?
(a) The chain X = {X,} having transition matrix P = ( 1 3 a ; ° 5) where a + B > 0.

0 Pp 1-—p

(b) The chain Y = {Y,} having transition matrix P = | 1 — p 0 Pp where 0 < p < 1.
p 1l-—p 0

(C) Zn = (Xn, Yn), where X, and Y, are independent and satisfy (a) and (b).

7. Let Xn, Yn be independent simple random walks. Let Z, be (Xn, Y,) truncated to lie in the
region Xy, > 0, Yy = 0, Xn + Yn < a where a is integral. Find the stationary distribution of Zp.

8. Show that an irreducible Markov chain with a finite state space and transition matrix P is reversible
in equilibrium if and only if P = DS for some symmetric matrix S$ and diagonal matrix D with strictly
positive diagonal entries. Show further that for reversibility in equilibrium to hold, it is necessary but
not sufficient that P has real eigenvalues.

9. Random walk on a graph. Let G be a finite connected graph with neither loops nor multiple
edges, and let X be arandom walk on G as in Exercise (6.4.6). Show that X is reversible in equilibrium.

6.6 Exercises. Chains with finitely many states

The first two exercises provide proofs that a Markov chain with finitely many states has a stationary
distribution.
1. The Markov—Kakutani theorem asserts that, for any convex compact subset C of R” and any
linear continuous mapping T of C into C, T has a fixed point (in the sense that T(x) = x for some
x €C). Use this to prove that a finite stochastic matrix has a non-negative non-zero left eigenvector
corresponding to the eigenvalue 1.
2. LetT beam xn matrix and let v ¢ R”. Farkas’s theorem asserts that exactly one of the following
holds:

(i) there exists x € R™ such that x > 0 and xT =v,

(ii) there exists y € IR” such that yv’ < 0 and Ty’ > 0.
Use this to prove that a finite stochastic matrix has a non-negative non-zero left eigenvector corre-
sponding to the eigenvalue 1.

3. Arbitrage. Suppose you are betting on arace with m possible outcomes. There are n bookmakers,
and a unit stake with the ith bookmaker yields ;; if the jth outcome of the race occurs. A vector

69
[6.6.4]-[6.7.3] Exercises Markov chains

K = (x1, x2,--- , Xn), where xy € (—00, 00) is your stake with the rth bookmaker, is called a betting

scheme. Show that exactly one of (a) and (b) holds:

(a) there exists a probability mass function p = (p1, p2,... , Pm) such that j=l tij pj = O for all
values of 7,

(b) there exists a betting scheme x for which you surely win, that is, 3“, x;t;; > 0 for all j.
i=l J

4. Let X bea Markov chain with state space S = {1, 2, 3} and transition matrix

l-p P 0
P= 0 l-—p Dp

Pp 0 1-p

where 0 < p < 1. Prove that
h Gin 42n 43y
PY =| a3, an 42
42n 43n 41n

where aj, + wd2, + Ww a3y, = (1 — p+ pw)", w being a complex cube root of 1.

5. Let P be the transition matrix of a Markov chain with finite state space. Let I be the identity
matrix, U the |S| x |S| matrix with all entries unity, and 1 the row |S|-vector with all entries unity.
Let # be a non-negative vector with )>; 7; = 1. Show that xP = z if and only ifa(@—P+U) =1.

Deduce that if P is irreducible then 7 = 10 —P + U7,
6. Chess. A chess piece performs a random walk on a chessboard; at each step it is equally likely

to make any one of the available moves. What is the mean recurrence time of a corner square if the
piece is a: (a) king? (b) queen? (c) bishop? (d) knight? (e) rook?

7, Chess continued. A rook and a bishop perform independent symmetric random walks with
synchronous steps on a 4 x 4 chessboard (16 squares). If they start together at a corner, show that the
expected number of steps until they meet again at the same corner is 448/3.

8. Find the 1-step transition probabilities p;; (7) for the chain X having transition matrix

WIN WE ©
Ale Ale NIE
Nir niu Nie

6.7 Exercises. Branching processes revisited

1. Let Z, be the size of the nth generation of a branching process with Zp = 1 and P(Z, = k) = q*
for k > 0. Show directly that, asn > 00, P(Zy, < 2yn | Z, > 0) > 1- ey, y > 0, in agreement
with Theorem (6.7.8).

2. Let Z be a supercritical branching process with Zg = 1 and family-size generating function G.
Assume that the probability 7 of extinction satisfies 0 < 7 < 1. Find a way of describing the process
Z, conditioned on its ultimate extinction.

3. Let Zy, be the size of the nth generation of a branching process with Zo = land P(Z,; =k) = gp*
fork > 0, where p+-g = land p > 5: Use your answer to Exercise (2) to show that, if we condition
on the ultimate extinction of Z, then the process grows in the manner of a branching process with
generation sizes Zn satisfying Zo = land P(Z; =k) = pa* for k > 0.

70

Birth processes and the Poisson process Exercises [6.7.4]-[6.8.7]

4. (a) Show that E(X | X > 0) < E(X2) /E(X) for any random variable X taking non-negative
values.

(b) Let Z,, be the size of the nth generation of a branching process with Zg = 1 and P(Z; =k) =q p*
for k > 0, where p > 7 Use part (a) to show that E(Z,/i” | Zn, > 0) < 2p/(p — q), where
B= p/q.

(c) Show that, in the notation of part (b), E(Zn/i” | Zn > 0) > p/(p — gq) asn > oo.

6.8 Exercises. Birth processes and the Poisson process

1. Superposition. Flies and wasps land on your dinner plate in the manner of independent Poisson
processes with respective intensities A and yz. Show that the arrivals of flying objects form a Poisson
process with intensity A + je.

2. Thinning. Insects land in the soup in the manner of a Poisson process with intensity 4, and each
such insect is green with probability p, independently of the colours of all other insects. Show that
the arrivals of green insects form a Poisson process with intensity Ap.

3. Let T,, be the time of the nth arrival in a Poisson process N with intensity A, and define the excess
lifetime process E(t) = Tyit)41 — t, being the time one must wait subsequent to t before the next
arrival. Show by conditioning on 7 that

t
P(E(t) > x) =e OF) 4 I P(E(@ —u) > x)se™ du.
0

Solve this integral equation in order to find the distribution function of E(t). Explain your conclusion.

4. Let B bea simple birth process (6.8.11b) with B(O) = /; the birth rates are A, = nA. Write
down the forward system of equations for the process and deduce that

P(B(t) =k) = ( : ier eel gay,

Show also that E(B(t)) = Te! and var(B(t)) = Te24f(1 — e*).

5. Let B be a process of simple birth with immigration (6.8.11c) with parameters 4 and v, and
with B(O) = 0; the birth rates are A, = nA + v. Write down the sequence of differential—-difference
equations for py (t) = P(B(t) = n). Without solving these equations, use them to show that m(t) =
E(B(t)) satisfies m!(t) = Am(t) + v, and solve for m(t).

6. Let N be a birth process with intensities 49,41,..., and let N(O) = 0. Show that p,(t) =

P(N(t) = n) is given by
—Ajt
pnt = 5, es Never

iA
provided that A; # A; whenever i # j.

7. Suppose that the general birth process of the previous exercise is such that a An 1 < 00.
Show that A, pn(t) > f(t) as n — oo where f is the density function of the random variable
T = sup{t : N(t) < oo}. Deduce that E(N(t) | N(t) < oo) is finite or infinite depending on the
convergence or divergence of 7, nAj, 1

Find the Laplace transform of f in closed form for the case when Ay, = (n + 3), and deduce an
expression for f.

71

[6.9.1]-[6.9.8] Exercises Markov chains

6.9 Exercises. Continuous-time Markov chains

1. Let Ay > Oand let X be a Markov chain on {1, 2} with generator

_{-h om
c=(5' 4).
(a) Write down the forward equations and solve them for the transition probabilities p;;(¢), i, 7 =
1,2.
(b) Calculate G" and hence find 57°29 (t” /n!)}G". Compare your answer with that to part (a).

(c) Solve the equation #G = 0 in order to find the stationary distribution. Verify that p; jt) > 1;
ast > oo.

2. Asa continuation of the previous exercise, find:
(a) P(X@) =2| XO) =1, XBt) = 1),
(b) P(X(t) =2| XO) = 1, XBN = 1, X40 = 1).

3. Jobs arrive in a computer queue in the manner of a Poisson process with intensity A. The central
processor handles them one by one in the order of their arrival, and each has an exponentially distributed
runtime with parameter jz, the runtimes of different jobs being independent of each other and of the
arrival process. Let X(t) be the number of jobs in the system (either running or waiting) at time t,
where X(0) = 0. Explain why X is a Markov chain, and write down its generator. Show that a
stationary distribution exists if and only if A < y, and find it in this case.

4. Pasta property. Let X = {X(t) : t > 0} be a Markov chain having stationary distribution
wz. We may sample X at the times of a Poisson process: let N be a Poisson process with intensity
A, independent of X, and define Y, = X(T,+), the value taken by X immediately after the epoch
Ty of the nth arrival of N. Show that Y = {¥, : n > 0} is a discrete-time Markov chain with the
same stationary distribution as X. (This exemplifies the ‘Pasta’ property: Poisson arrivals see time
averages.)

[The full assumption of the independence of N and X is not necessary for the conclusion. It suffices
that {N(s) : s > t} be independent of {X(s) : s < 1}, a property known as ‘lack of anticipation’. It is
not even necessary that X be Markov; the Pasta property holds for many suitable ergodic processes. ]

5. Let X be a continuous-time Markov chain with generator G satisfying gj = —g;; > 0 for all i.
Let H4 = inf{t > 0: X(t) € A} be the hitting time of the set A of states, and let nj = P(H4 < 00 |
X(0) = j) be the chance of ever reaching A from j. By using properties of the jump chain, which
you may assume to be well behaved, show that }°, gjxng =O for j ¢ A.

6. In continuation of the preceding exercise, let 4; = E(H, | X(0) = j). Show that the vector
is the minimal non-negative solution of the equations

wy =O iffFEA, 14+D° gem =O iff EA.
keS

7. Let X be a continuous-time Markov chain with transition probabilities p;j(t) and define Fj =
inf{t > T, : X(t) = i} where 7; is the time of the first jump of X. Show that, if g;; ~ 0, then
P(F; < co | X(0) =i) = 1 if and only if i is persistent.

8. Let X be the simple symmetric random walk on the integers in continuous time, so that

Piit1 (2) = pii-1 (A) = Zan + off).

Show that the walk is persistent. Let T be the time spent visiting m during an excursion from 0. Find
the distribution of T.

72

Birth-death processes and imbedding Exercises [6.9.9]-[6.11.4]

9. Let i be a transient state of a continuous-time Markov chain X with X(0) = i. Show that the
total time spent in state i has an exponential distribution.

10. Let X be an asymmetric simple random walk in continuous time on the non-negative integers
with retention at 0, so that

Ah+oth) iff =it+1,i2>0,
pij(h) = tps .
ph+oth) iff=i-1,i>1.
Suppose that X(0) = 0 and A > yw. Show that the total time V; spent in state r is exponentially
distributed with parameter A — je.
Assume now that X(0) has some general distribution with probability generating function G.
Find the expected amount of time spent at 0 in terms of G.

11. Let X = {X(t) : t > 0} beanon-explosive irreducible Markov chain with generator G and unique
stationary distribution 7. The mean recurrence time jzx is defined as follows. Suppose X(0) = k,
and let U = inf{s : X(s) #k}. Then py = E(inf{t > U : X(t) = k}). Let Z = {Zp : n > 0} be the
imbedded ‘jump chain’ given by Zp = X(0) and Z,, is the value of X just after its nth jump.

(a) Show that Z has stationary distribution # satisfying

~~ r
n= kK8&k ,
Di Bi
where g; = —g;;, provided >>, 2;g; < oo. When is it the case that # = 1?

(b) Show that 2; = 1/(;9;) if uz < oo, and that the mean recurrence time ji; of the state k in the
jump chain Z satisfies iz; = ux >>; 7:8; if the last sum is finite.

12. Let Z be an irreducible discrete-time Markov chain on a countably infinite state space S, having
transition matrix H = (h;;) satisfying hj; = 0 for all states i, and with stationary distribution v.
Construct a continuous-time process X on S for which Z is the imbedded chain, such that X has no
stationary distribution.

6.11 Exercises. Birth-death processes and imbedding

1. Describe the jump chain for a birth-death process with rates Ay, and fn.

2. Consider an immigration-death process X, being a birth-death process with birth rates A, = 2
and death rates x, = ny. Find the transition matrix of the jump chain Z, and show that it has as

stationary distribution
1 n
= ——/(14—) pte?
Tn rn!) ( + ") pe

where p = A/. Explain why this differs from the stationary distribution of X.

3. Consider the birth—death process X with A, = nA and py, = ny foralln > 0. Suppose X (0) = 1
and let n(¢) = P(X (t) = 0). Show that 7 satisfies the differential equation

WO+Q+u)n@ =Ht+an@’.

Hence find 7(t), and calculate P(X (¢) = 0 | X(u) = 0) forO <t <u.

4. For the birth-death process of the previous exercise with X < yu, show that the distribution of
X(t), conditional on the event {X(¢) > O}, converges as t — co to a geometric distribution.

73

[6.11.5]-[6.13.2] Exercises Markov chains

5. Let X bea birth—death process with A, = nd and zn = ny, and suppose X (0) = 1. Show that
the time T at which X(t) first takes the value 0 satisfies

1

5 08 (*,) ifr’ <p,
1 ry
— log (=) ifA > p.
a A- pb

R(T |T <o)=

What happens when A = 2?

6. Let X be the birth-death process of Exercise (5) with 4 4 wy, and let V;(t) be the total amount
of time the process has spent in state r > 0, up to time ¢. Find the distribution of Vj (oo) and the
generating function >°, s’E(V;(t)). Hence show in two ways that E(Vj(oo)) = [max{a, phy.
Show further that E(V,(00)) = 4°~!r7|[max{a, p}IT".

7. Repeat the calculations of Exercise (6) in the case A = pw.

6.12 Exercises. Special processes

1. Customers entering a shop are served in the order of their arrival by the single server. They
arrive in the manner of a Poisson process with intensity 4, and their service times are independent
exponentially distributed random variables with parameter 4. By considering the jump chain, show
that the expected duration of a busy period B of the server is (u — 4)! when A < yw. (The busy
period runs from the moment a customer arrives to find the server free until the earliest subsequent
time when the server is again free.)

2. Disasters. Immigrants arrive at the instants of a Poisson process of rate v, and each independently
founds a simple birth process of rate A. At the instants of an independent Poisson process of rate 6,
the population is annihilated. Find the probability generating function of the population X (ft), given
that X(0) = 0.

3. More disasters. In the framework of Exercise (2), suppose that each immigrant gives rise to a
simple birth—-death process of rates ) and yz. Show that the mean population size stays bounded if and
only if >A — pw.

4. The queue M/G/oo. (See Section 11.1.) An ftp server receives clients at the times of a Poisson
process with parameter A, beginning at time 0. The ith client remains connected for a length S;
of time, where the S; are independent identically distributed random variables, independent of the
process of arrivals. Assuming that the server has an infinite capacity, show that the number of clients
being serviced at time ¢ has the Poisson distribution with parameter A fo {1 — G(x)] dx, where G is
the common distribution function of the $;.

6.13 Exercises. Spatial Poisson processes

1. In a certain town at time tf = 0 there are no bears. Brown bears and grizzly bears arrive as
independent Poisson processes B and G with respective intensities B and y.
(a) Show that the first bear is brown with probability B/(6 + y).

(b) Find the probability that between two consecutive brown bears, there arrive exactly r grizzly
bears.

(c) Given that B(1) = 1, find the expected value of the time at which the first bear arrived.

2. Campbell—Hardy theorem. Let IT be the points of a non-homogeneous Poisson process on Rd
with intensity function 4. Let S = S°,e7 g(x) where g is a smooth function which we assume for

74
Markov chain Monte Carlo Exercises [6.13.3]-[6.14.4]

convenience to be non-negative. Show that E(S) = fea g(u)A(u) duand var(S) = fod g(u)2A(u) du,
provided these integrals converge.

3. Let I bea Poisson process with constant intensity 4 on the surface of the sphere of R?3 with radius
1. Let P be the process given by the (X, Y) coordinates of the points projected on a plane passing
through the centre of the sphere. Show that P is a Poisson process, and find its intensity function.

4. Repeat Exercise (3), when TF is a homogeneous Poisson process on the ball {(x1, x2, x3) : xe +
xe + x3 < 1}.
5. You stick pins in a Mercator projection of the Earth in the manner of a Poisson process with
constant intensity 4. What is the intensity function of the corresponding process on the globe? What
would be the intensity function on the map if you formed a Poisson process of constant intensity 4 of
meteorite strikes on the surface of the Earth?
6. Shocks. The rth point 7, of a Poisson process N of constant intensity 1 on R+ gives rise to an
effect X,e-“¢—-T) at time t > Ty, where the X; are independent and identically distributed with
finite variance. Find the mean and variance of the total effect S(t) = yo X-e %¢—T) in terms of
the first two moments of the X;, and calculate cov(S(s), S(t)).

What is the behaviour of the correlation o(S(s), S(t)) as s > co witht — s fixed?

7. Let N be a non-homogeneous Poisson process on R+ with intensity function A. Find the joint
density of the first two inter-event times, and deduce that they are not in general independent.

8. Competitionlemma. Let {N;(t) : r > 1}beacollection of independent Poisson processes on R+
with respective constant intensities {Ay : r > 1}, such that >A, = A < oo. Set N(t) = >, Nr(t),
and let J denote the index of the process supplying the first point in NV, occurring at time 7. Show that

A;
PU =i, T>H)=PU =)PT >H)N= se, i>.

6.14 Exercises. Markov chain Monte Carlo

1. Let P be a stochastic matrix on the finite set © with stationary distribution 7. Define the inner
product (x, y) = S“pe@ xk Yk, and let Par) = {xe R® : (x, x) < oo}. Show, in the obvious
notation, that P is reversible with respect to 7 if and only if (x, Py) = (Px, y) for all x, y € Por).

2. Barker’s algorithm. Show that a possible choice for the acceptance probabilities in Hastings’s
general algorithm is
Tj 8 iji
bij => —_—, — .
Hj 8ij + Hj 8ji
where G = (g;;) is the proposal matrix.

3. Let S be a countable set. For each j € S, the sets Ajx, k € S, form a partition of the interval
[0, 1]. Let g : S x [0,1] > S be given by g(j,u) = k ifu € Ajz. The sequence {X, : » > O} of
random variables is generated recursively by Xy41 = g(Xn,Un4 1), n => 0, where {Uy : n > 1} are
independent random variables with the uniform distribution on [0, 1]. Show that X is a Markov chain,
and find its transition matrix.

4. Dobrushin’s bound. Let U = (us;) be a finite |S| x |T| stochastic matrix. Dobrushin’s ergodic
coefficient is defined to be

1
dU) = 3 sup Y> |uie — ujel-
LIES pep

(a) Show that, if V is a finite |7| x |U| stochastic matrix, then d(UV) < d(U)d(V).

75

[6.15.1]-[6.15.7] Exercises Markov chains
(b) Let X and Y be discrete-time Markov chains with the same transition matrix P, and show that

So |P(Xn =k) -P(¥, =1| < dP)” So|P(Xo = & — P(X =).
k k

6.15 Problems

1. Classify the states of the discrete-time Markov chains with state space S = {1, 2,3, 4} and
transition matrices

(a) (b)

Oflenl—uje
O° Ne uN
of- OO
SNi= Oo O
Co rw ©
oo one
RP OONe
Oo Own ©

In case (a), calculate /4(n), and deduce that the probability of ultimate absorption in state 4, starting
from 3, equals 5. Find the mean recurrence times of the states in case (b).

2. A transition matrix is called doubly stochastic if all its column sums equal 1, thatis, if }7; pij = 1
for all j € S.

(a) Show that if a finite chain has a doubly stochastic transition matrix, then all its states are non-null
persistent, and that if it is, in addition, irreducible and aperiodic then p; j(n) > N —lasn — ov,
where N is the number of states.

(b) Show that, if an infinite irreducible chain has a doubly stochastic transition matrix, then its states
are either all null persistent or all transient.

3. Prove that intercommunicating states of a Markov chain have the same period.
4. (a) Show that for each pairi, 7 of states of an irreducible aperiodic chain, there exists N = N(i, j)
such that pj;j (1) > O for alln > N.

(b) Let X and Y be independent irreducible aperiodic chains with the same state space S and transition
matrix P. Show that the bivariate chain Z, = (Xn, Yn), n > 0, is irreducible and aperiodic.

(c) Show that the bivariate chain Z may be reducible if X and Y are periodic.

5. Suppose {Xy, : n > 0} is a discrete-time Markov chain with Xg = i. Let N be the total number
of visits made subsequently by the chain to the state 7. Show that

1— fi ifn =0,
P(N =”) = |
fig fp” -A—-fij) ifn>1,

and deduce that P(N = oo) = 1 if and only if fjj = fjj = 1.

6. Leti and j be two states of a discrete-time Markov chain. Show that if i communicates with /,
then there is positive probability of reaching j from i without revisiting i in the meantime. Deduce
that, if the chain is irreducible and persistent, then the probability f,; of ever reaching j from i equals
1 for alli and j.

7. Let {Xp : n > 0} bea persistent irreducible discrete-time Markov chain on the state space S with
transition matrix P, and let x be a positive solution of the equation x = xP.

(a) Show that
xj . .
qij (2) = ~ Pji(), i,jeS,n>1,
t

76
Problems Exercises [6.15.8]~[6.15.13]

defines the n-step transition probabilities of a persistent irreducible Markov chain on S whose
first-passage probabilities are given by

xj ae
E

where Jj;(1) = P(Xn =i, T >| Xo = j) and T = min{m > 0: Xm = j}.

(b) Show that x is unique up to a multiplicative constant.

(c) Let Tj = min{n > 1: Xn = j} and define hj; = P(Z; < T; | Xo =i). Show that x;hjj = xjhjj
foralli,j €S.

8. Renewal sequences. The sequence u = {uy : n > 0} is called a ‘renewal sequence’ if
n
ug = 1, un = >> fini forn > 1,

for some collection f = {f, : n > 1} of non-negative numbers summing to 1.

(a) Show that u is a renewal sequence if and only if there exists a Markov chain X on a countable
state space S such that u, = P(X, =s | Xp =), for some persistent s € S and all n > 1.

(b) Show that if u and v are renewal sequences then so is {uyvpy, in > O}.

9. Consider the symmetric random walk in three dimensions on the set of points {(x, y, z) : x, y,Z
0, +1, £2, ...}; this process is a sequence {X,, : 2 > 0} of points such that P(X,,1 = X, +€) =
for € = (+1, 0, 0), (0, +1, 0), (0, 0, +1). Suppose that Xp = (0, 0, 0). Show that

1\2" (Qn)! 1\2" /2n ni \?
P(Xan = (0,0.0)) = (2) »> Gt jtkD2 = (5) (") a Ca 7A)

i+j+k=n i+j+k=n

1
6

and deduce by Stirling’s formula that the origin is a transient state.

10. Consider the three-dimensional version of the cancer model (6.12.16). If « = 1, are the empires
of Theorem (6.12.18) inevitable in this case?

11. Let X bea discrete-time Markov chain with state space S = {1, 2}, and transition matrix

l-a a
P= .
( B ip)
Classify the states of the chain. Suppose that a6 > 0 and af # 1. Find the n-step transition

probabilities and show directly that they converge to the unique stationary distribution as n — oo.
For what values of a and 6 is the chain reversible in equilibrium?

12. Another diffusion model. N black balls and N white balls are placed in two urns so that each
contains N balls. After each unit of time one ball is selected at random from each urn, and the two
balls thus selected are interchanged. Let the number of black balls in the first urn denote the state
of the system. Write down the transition matrix of this Markov chain and find the unique stationary
distribution. Is the chain reversible in equilibrium?

13. Consider a Markov chain on the set S = {0, 1,2, ...} with transition probabilities p;;.1 = a;,
Pi,g = 1 —a;, i > O, where (a; : i = 0) is a sequence of constants which satisfy 0 < a; < 1 for alli.
Let bg = 1, bj = aga, ---a;_1 fori > 1. Show that the chain is

(a) persistent if and only if bj > 0 asi — oo,

(b) non-null persistent if and only if 7; bj < 00,

and write down the stationary distribution if the latter condition holds.

77

[6.15.14]-[6.15.19] Exercises Markov chains

Let A and 6 be positive constants and suppose that aj = 1 — Ai —B for all large i. Show that the
chain is
(c) transient if 6 > 1,
(d) non-null persistent if 6 < 1.
Finally, if 6 = 1 show that the chain is
(e) non-null persistent if A > 1,
(f) null persistent if A < 1.
14, Let X be a continuous-time Markov chain with countable state space S and standard semigroup
{P;}. Show that p;;(t) is a continuous function of t. Let g(t) = —log pj; (t); show that g is a
continuous function, g(0) = 0, and g(s +t) < g(s)+ g(t). We say that g is ‘subadditive’, and a well
known theorem gives the result that

t t
lim se) =A existsand A= sup st) < oo.

110 ¢ t>0

Deduce that gj; = lim;)9 tr! {pi (t) — 1} exists, but may be —oo.

15, Let X be acontinuous-time Markov chain with generatorG = (g;;) and suppose that the transition

semigroup P; satisfies P; = exp(tG). Show that X is irreducible if and only if for any pair i, j of

States there exists a sequence k;, ky,..., kn of states such that gj £, 8k, ,ky °° * Sknj 7% 9-

16. (a) Let X = {X(t) : —co < t < co} be a Markov chain with stationary distribution 7, and
suppose that X (0) has distribution w. We call X reversible if X and Y have the same joint
distributions, where Y(t) = X(-1).

G) If X(t) has distribution w for all t, show that Y is a Markov chain with transition probabilities
P; j (t) = (1; /m;) pji(t), where the p;;(t) are the transition probabilities of the chain X.
(ii) If the transition semigroup {P;} of X is standard with generator G, show that 7; 93; = 10j 8ji
(for all i and j) is a necessary condition for X to be reversible.
(iii) If P; = exp(tG), show that X(z) has distribution w for all t and that the condition in (ii) is
sufficient for the chain to be reversible.
(b) Show that every irreducible chain X with exactly two states is reversible in equilibrium.
(c) Show that every birth—-death process X having a stationary distribution is reversible in equilibrium.

17. Show that not every discrete-time Markov chain can be imbedded in a continuous-time chain.
More precisely, let

P= 4", i) for some 0 <a <1

be a transition matrix. Show that there exists a uniform semigroup {P;} of transition probabilities in
continuous time such that P; = P, if and only if 4 <a < 1. Inthis case show that {P;} is unique and
calculate it in terms of a.

18. Consider an immigration—death process X(t), being a birth-death process with rates A, = A,
[in = nw. Show that its generating function G(s, t) = E(s* ®) is given by

G(s,t) = 1+ @ — De} exp{p(s — DU — e #4}

where p = A/j and X(0) = I. Deduce the limiting distribution of X(t) as t — oo.

19, Let N be a non-homogeneous Poisson process on R_ = [0, oo) with intensity function A. Write
down the forward and backward equations for NV, and solve them.

Let N(0) = 0, and find the density function of the time T until the first arrival in the process. If
A(t) = c/(1 +1), show that E(T) < 00 if and only ifc > 1.

78

Problems Exercises [6.15.20]-[6.15.27]

20. Successive offers for my house are independent identically distributed random variables X1,
X7,..., having density function f and distribution function F. Let Yj; = X1, let Y be the first offer
exceeding Y,, and generally let Y,, 41 be the first offer exceeding Y, . Show that Y,, Y2,... are the times
of arrivals in a non-homogeneous Poisson process with intensity function A(t) = f(t)/C — F(a).
The Y; are called ‘record values’.

Now let Z, be the first offer received which is the second largest to date, and let Zz be the second
such offer, and so on. Show that the Z; are the arrival times of a non-homogeneous Poisson process
with intensity function A.

21. Let N be a Poisson process with constant intensity A, and let Y;, Y2,... be independent random
variables with common characteristic function ¢ and density function f. The process N*(t) =
Y, + Y2 +--++ Yq) is called a compound Poisson process. Yp, is the change in the value of N* at
the nth arrival of the Poisson process N. Think of it like this. A ‘random alarm clock’ rings at the
arrival times of a Poisson process. At the nth ring the process N* accumulates an extra quantity Y).
Write down a forward equation for N* and hence find the characteristic function of N*(t). Can you
see directly why it has the form which you have found?

22. If the intensity function 2 of a non-homogeneous Poisson process N is itself a random process,
then N is called a doubly stochastic Poisson process (or Cox process). Consider the case when
A(t) = A for all t, and A is arandom variable taking either of two values A, or Az, each being picked

with equal probability 5. Find the probability generating function of N(t), and deduce its mean and
variance.

23. Show that a simple birth process X with parameter 4 is a doubly stochastic Poisson process with
intensity function A(t) = AX(#).

24. The Markov chain X = {X(t) : t > 0} is a birth process whose intensities A; (t) depend also on
the time t and are given by

1+ uk

1+ pt

P(X@+h)=k+1] XQ =k) = h + o(h)

as h | 0. Show that the probability generating function G(s, t) = E(s* ) satisfies

aG —1 aG
alae Cruse), O<s <1.
at 1+ pt as

Hence find the mean and variance of X (+) when X (0) = I.

25. (a) Let X be a birth—-death process with strictly positive birth rates 49, A ,,... and death rates
41, #2,-.--. Let n; be the probability that X(t) ever takes the value 0 starting from X (0) = i. Show
that
Ajnjai — Aj + j)nj + wynj—-1 = 9, J=1,
and deduce that 7; = 1 for all i so long as ~~ ej = 00 where ej = yg + + hj /(A1AQ-+ + Aj).
(b) For the discrete-time chain on the non-negative integers with
P+G+y 1 PEG +N?
find the probability that the chain ever visits 0, starting from 1.

Pj,jt41=

26. Find a good necessary condition and a good sufficient condition for the birth-death process X of
Problem (6.15.25a) to be honest.

27, Let X be a simple symmetric birth-death process with Ay = zn = nd, and let T be the time until
extinction. Show that

Ax I
Pr <1 X= = (7) ,

79

[6.15.28]-[6.15.33] Exercises Markov chains

and deduce that extinction is certain if P(X (0) < oo) = 1.

Show that PAT/I <x | X(0) =1) > e71/* as I > oo.
28. Immigration—death with disasters. Let X be an immigration—death—disaster process, that is, a
birth-death process with parameters A; = A, 4; = ijs, and with the additional possibility of ‘disasters’

which reduce the population to 0. Disasters occur at the times of a Poisson process with intensity 6,
independently of all previous births and deaths.

(a) Show that X has a stationary distribution, and find an expression for the generating function of
this distribution.

(b) Show that, in equilibrium, the mean of X(t) is A/(6 + 42).
29. With any sufficiently nice (Lebesgue measurable, say) subset B of the real line R is associated a
random variable X (B) such that
(i) X(B) takes values in {0, 1, 2,...},
(ii) if By, Bo,..., By are disjoint then X(B,), X(Bo), ..., X (Bn) are independent, and furthermore
X(By U By) = X(By) + X (Bo),
(iii) the distribution of X(B) depends only on B through its Lebesgue measure (‘length’) | B|, and
P(X(B) > 1)

————— > 1 as|B|->0.
P(X(B) = 1)

Show that X is a Poisson process.

30. Poisson forest. Let N be a Poisson process in R? with constant intensity A, and let Ray < Ray <
++ be the ordered distances from the origin of the points of the process.

(a) Show that Ri): Roy; ... are the points of a Poisson process on Ry = [0, 00) with intensity A.
(b) Show that Rig has density function

Doar (amr2)k-le—Anr?
(k — 1)! ,

r>0.

fir) =

31. Let X be a n-dimensional Poisson process with constant intensity 1. Show that the volume of
the largest (n-dimensional) sphere centred at the origin which contains no point of X is exponentially
distributed. Deduce the density function of the distance R from the origin to the nearest point of
X. Show that E(R) = T(1/n)/{n(ac)!/"} where c is the volume of the unit ball of R” and I is the
gamma function.

32. A village of N + 1 people suffers an epidemic. Let X(t) be the number of ill people at time r,
and suppose that X(0) = 1 and X is a birth process with rates A; = Ai(N + 1 —i). Let T be the
length of time required until every member of the population has succumbed to the illness. Show that

N

1 1
ET) = - ——_—___—_.
() 5 E41
and deduce that o(log N J
ogN +y —2
E(T) = ————— + O(N
(T) XN FD) +O(N™)

where y is Euler’s constant. It is striking that E(T) decreases with N, for large N.

33. A particle has velocity V(t) at time t, where V(t) is assumed to take values in {n + 4 in > O}.
Transitions during (t,t + A) are possible as follows:

(vt 4)h+o(h) ifw=v4+1,
P(V(t+h) =w|V(@t)=v) =¢ 1-2vh+0(h)  ifw=0,
(v—})ht+o(h) ifw=v-1.

80

Problems Exercises [6.15.34]-[6.15.39]

Initially V(0) = 4. Let

CO
G(s,t) =) s"P(V@) =n t+ 4).
n=0

(a) Show that
aG

aG
— =(1—sy— -(1—s)G
ot ( s) os ( s)
and deduce that G(s,t) ={1+(1—s)t}7!.
(b) Show that the expected length m,(T) of time for which V = n+ 5 during the time interval [0, T]
is given by

T
my (T) = i P(V(t)=n+4)dt

and that, for fixed k, mg (T) — log T > — a i-1asT > oo.

(c) What is the expected velocity of the particle at time t?
34, A random sequence of non-negative integers {X, : n > 0} begins Xo = 0, X, = 1, and is
produced by

x Xn+Xpn—1 with probability 4,

+1= . as
. [Xp — Xy,-1| with probability I

Show that ¥, = (X,—1, Xn) is a transient Markov chain, and find the probability of ever reaching
(1, 1) from (1, 2).
35. Take a regular hexagon and join opposite corners by straight lines meeting at the point C. A
particle performs a symmetric random walk on these 7 vertices, starting at A. Find:
(a) the probability of return to A without hitting C,
(b) the expected time to return to A,
(c) the expected nmber of visits to C before returning to A,
(d) the expected time to return to A, given that there is no prior visit to C.

36. Diffusion, osmosis. Markov chains are defined by the following procedures at any time n:

(a) Bernoulli model. Two adjacent containers A and B each contain m particles; m are of type I and
m are of type II. A particle is selected at random in each container. If they are of opposite types
they are exchanged with probability a if the type I is in A, or with probability 6 if the type I is in
B. Let Xy be the number of type I particles in A at time n.

(b) Ehrenfest dog-flea model. Two adjacent containers contain m particles in all. A particle is
selected at random. If it is in A it is moved to B with probability a, if it is in B it is moved to A
with probability 8. Let Y, be the number of particles in A at time n.

In each case find the transition matrix and stationary distribution of the chain.

37. Let X be an irreducible continuous-time Markov chain on the state space S with transition prob-
abilities p;,(¢) and unique stationary distribution 7, and write P(X(t) = j) = aj(¢). If c(x) isa
concave function, show that d(t) = Dies 1;C(a;(t)/m;) increases to c(1) as t > oo.

38. With the notation of the preceding problem, let u,(t) = P(X(t) = k | X(O) = 0), and suppose
the chain is reversible in equilibrium (see Problem (6.15.16)). Show that ug(2t) = > j (mo/1j)uj (0),
and deduce that ug(t) decreases to 19 as t > 00.

39, Perturbing a Poisson process. Let ITI be the set of points in a Poisson process on R? with
constant intensity A. Each point is displaced, where the displacements are independent and identically
distributed. Show that the resulting point process is a Poisson process with intensity A.

81

[6.15.40]-[6.15.46] Exercises Markov chains

40. Perturbations continued. Suppose for convenience in Problem (6.15.39) that the displacements
have a continuous distribution function and finite mean, and that d = 1. Suppose also that you are
at the origin originally, and you move to a in the perturbed process. Let Lp be the number of points
formerly on your left that are now on your right, and Ry, the number of points formerly on your right
that are now on your left. Show that E(Lp) = E(R,) if and only if a = jz where yz is the mean
displacement of a particle.

Deduce that if cars enter the start of a long road at the instants of a Poisson process, having
independent identically distributed velocities, then, if you travel at the average speed, in the long run
the rate at which you are overtaken by other cars equals the rate at which you overtake other cars.

41, Ants enter a kitchen at the instants of a Poisson process N of rate A; they each visit the pantry
and then the sink, and leave. The rth ant spends time X; in the pantry and Y, in the sink (and X; + Y,
in the kitchen altogether), where the vectors V, = (X,, Y,) and Vs are independent for r ~ s. At
time t = 0 the kitchen is free of ants. Find the joint distribution of the numbers A(t) of ants in the
pantry and B(t) of ants in the sink at time t. Now suppose the ants arrive in pairs at the times of the
Poisson process, but then separate to behave independently as above. Find the joint distribution of the
numbers of ants in the two locations.

42. Let {X, : r > 1} be independent exponential random variables with parameter 1, and set S, =

yor, Xr. Show that:

(a) Ye = Sx/Sn, 1 < k <n —1, have the same distribution as the order statistics of independent
variables {U; : 1 < k <n — 1} which are uniformly distributed on (0, 1),

(b) Z, = Xx/Sn, 1 < k < n, have the same joint distribution as the coordinates of a point
(U,,..., Un) chosen uniformly at random on the simplex an ur = 1, uy > O for all r.

43. Let X be a discrete-time Markov chain with a finite number of states and transition matrix
P = (pij) where p;; > 0 for alli, 7. Show that there exists 1 € (0, 1) such that | pj; (2) — m;| < A”,
where w is the stationary distribution.

44. Under the conditions of Problem (6.15.43), let V;(n) = a, T;x,=i} be the number of visits of
the chain to i before time n. Show that

1 2
&(|Eim—x >0 asnow.

Show further that, if f is any bounded function on the state space, then

n-1 2
(|? L1H) - Yi fom| } +0

ieS

45. Conditional entropy. Let A and B = (Bo, By,..., Bn) be a discrete random variable and
vector, respectively. The conditional entropy of A with respect to B is defined as H(A | B) =
E(E{—log f(A | B)|B}) where f(a | b) = P(A =a | B=b). Let X be an aperiodic Markov chain
on a finite state space. Show that

H(Xn41 | Xo, X1..- Xn) = H(Xn41 | Xn),

and that
A(Xns1 | Xn) > —Somi SS) py log pij asn > 00,
i j
if X is aperiodic with a unique stationary distribution w.

46. Coupling. Let X and Y be independent persistent birth-death processes with the same parameters
(and no explosions). It is not assumed that Xg = Yo. Show that:

82

Problems Exercises [6.15.47]-[6.15.49]

(a) for any A C R, |P(X; € A) — P(¥; € A)| > Oast > ~,

(b) if P(X < Yo) = 1, then Ef[g(X:)] < Elg(¥;)] for any increasing function g.

47, Resources. The number of birds in a wood at time ¢ is a continuous-time Markov process X. Food
resources impose the constraint 0 < X(t) < n. Competition entails that the transition probabilities

obey
Pke+i(h) = An — kh + off), Pkk—-1(h) = wkh + o(h).

Find E(s*), together with the mean and variance of X(t), when X(0) = r. What happens as
t—> 00?

48. Parrando’s paradox. A counter performs an irreducible random walk on the vertices 0, 1, 2 of
the triangle in the figure beneath, with transition matrix

0 po qo
P=|q O pi
p2 492 OO

where p; + qj = 1 for all i. Show that the stationary distribution w has

_ 1—qopi
3 — 41 P0 — 92P1 — GoP2

ro

with corresponding formulae for 7, 72.

0 2

Suppose that you gain one peseta for each clockwise step of the walk, and you lose one peseta
for each anticlockwise step. Show that, in equilibrium, the mean yield per step is

_ _ 3@poP1P2 — PoP1 — P1P2— P2Po + Po + Pi + p2— 1)
y= 2p; - Dm = 3
; — 41 P0 — 92P1 — GoP2

Consider now three cases of this process:
A. Wehave p; = 5 —a for eachi, where a > 0. Show that the mean yield per step satisfies ya < 0.

B. We have that pp = tb —a,pj=pr.= ; — a, where a > 0. Show that yg < 0 for sufficiently
small a.

C. At each step the counter is equally likely to move according to the transition probabilities of
case A or case B, the choice being made independently at every step. Show that, in this case,
Po= a —a, pi, = pr2= 3 — a. Show that yc > 0 for sufficiently small a.

The fact that two systematically unfavourable games may be combined to make a favourable game is
called Parrando’s paradox. Such bets are not available in casinos.

49. Cars arrive at the beginning of a long road in a Poisson stream of rate 4 from time ¢ = 0 onwards.
A car has a fixed velocity V > 0 which is a random variable. The velocities of cars are independent
and identically distributed, and independent of the arrival process. Cars can overtake each other freely.
Show that the number of cars on the first x miles of the road at time ¢ has the Poisson distribution with
parameter AE[V~! min{x, Vf}].

83

[6.15.50]-[6.15.51] Exercises Markov chains

50. Events occur at the times of a Poisson process with intensity 4, and you are offered a bet based
on the process. Let t > 0. You are required to say the word ‘now’ immediately after the event which
you think will be the last to occur prior to time t. You win if you succeed, otherwise you lose. If no
events occur before t you lose. If you have not selected an event before time ¢ you lose.

Consider the strategy in which you choose the first event to occur after a specified time s, where
O<s <t.
(a) Calculate an expression for the probability that you win using this strategy.
(b) Which value of s maximizes this probability?
(c) If At > 1, show that the probability that you win using this value of s is el,

51. Anew Oxbridge professor wishes to buy a house, and can afford to spend up to one million pounds.
Declining the services of conventional estate agents, she consults her favourite internet property page
on which houses are announced at the times of a Poisson process with intensity 2 per day. House
prices may be assumed to be independent random variables which are uniformly distributed over the
interval (800,000, 2,000,000). She decides to view every affordable property announced during the
next 30 days. The time spent viewing any given property is uniformly distributed over the range (1, 2)
hours. What is the moment generating function of the total time spent viewing houses?

84

7

Convergence of random variables

7.1 Exercises. Introduction

1. Letr > 1, and define ||X||- = (E|X"|}!/". Show that:

(a) IlcX |r = le] - ||X|lr fore ER,

(b) [|X +¥ llr < [Xr + IV Ir,

(c) ||X||- = 0if and only if P(X = 0) = 1.

This amounts to saying that || - ||, is a norm on the set of equivalence classes of random variables on a

given probability space with finite rth moment, the equivalence relation being given by X ~ Y if and
only if P(X = Y) = 1.

2. Define (X, Y) = E(XY) for random variables X and Y having finite variance, and define || X || =
W(X, X). Show that:

(a) (aX + bY, Z) =a(X, Z) + OLY, Z),

(b) |X + YI? +X — YI? = 2X? + ||¥ ||), the parallelogram property,

(c) if (X;, Xj) =O for alli A 7 then

n
> Xi
i=l

2 n
2
=o 1x.
i=l

3. Lete > 0. Let g,h : [0,1] — R, and define de(g,h) = ter dx where E = {u € [0,1]:
|g(u) — h(u)| > €}. Show that d, does not satisfy the triangle inequality.

4. Lévy metric. For two distribution functions F and G, let
d(F, G) = inf {é > 0: F(x — 8) —8 < G(x) < F(x +6) +8 forall x € R}.
Show that d is a metric on the space of distribution functions.

5. Find random variables X, X;, X2,... such that E(|X, — X|°) — Oasn — ov, but E|X,| = 00
for all n.

7.2 Exercises. Modes of convergence

1. (a) Suppose X, —> X where r > 1. Show that E|X”| > E|X’|.
(b) Suppose Xp 4 X. Show that E(X;,) — E(X). Is the converse true?
(c) Suppose Xp 4 X. Show that var(X,) — var(X).

85

[7.2.2]-{7.3.3] Exercises Convergence of random variables

2. Dominated convergence. Suppose |X,| < Z for alln, where E(Z) < oo. Prove that if Xp, & X
then Xp, 4 X.

3. Give a rigorous proof that E(XY) = E(X)E(Y) for any pair X, Y of independent non-negative
random variables on (Q, ¥,P) with finite means. [Hint: For k > 0, n > 1, define X, = k/n if

k/n < X < (K+ 1)/n, and similarly for Y,. Show that X, and Y, are independent, and X, < X,
and Y, < Y. Deduce that EX, — EX and EY, — EY, and also E(X,Y,) > E(XY).]

4. Show that convergence in distribution is equivalent to convergence with respect to the Lévy metric

of Exercise (7.1.4).

5. (a) Suppose that Xp, 5 X and Y, a c, where c is aconstant. Show that X,Yp » cX, and that
Xn/Yn > X/cifc £0.

(b) Suppose that X, » 0 and Y, # Y, and let ¢: IR2 —> R be such that g(x, y) is a continuous

function of y for all x, and g(x, y) is continuous at x = 0 for all y. Show that g(Xn, Yn) a
g(0, Y).
[These results are sometimes referred to as ‘Slutsky’s theorem(s)’.]
6. Let X,, X2,... be random variables on the probability space (Q, ¥, P). Show that the set
A = {wm €  : the sequence X,,(w) converges} is an event (that is, lies in #), and that there exists a

random variable X (that is, an #-measurable function X : Q — R) such that X,(w) > X(@) for
MEA.

7. Let {X,} be a sequence of random variables, and let {cy} be a sequence of reals converging to the
limit c. For convergence almost surely, in rth mean, in probability, and in distribution, show that the
convergence of X, to X entails the convergence of cy Xp tocX.

8. Let {X,} be a sequence of independent random variables which converges in probability to the
limit X. Show that X is almost surely constant.

9. Convergence in total variation. The sequence of discrete random variables X,, with mass
functions fy, is said to converge in total variation to X with mass function f if

Ss" in(z) — f)| +0 as n> oo.

Suppose X, — X in total variation, and u : R > R is bounded. Show that E(u(Xn)) ~ E(u(X)).

10. Let {X; : r => 1} be independent Poisson variables with respective parameters {A; :r > 1}. Show
that 57°, X; converges or diverges almost surely according as }-?°, Ay converges or diverges.

7.3 Exercises. Some ancillary results

1. (a) Suppose that Xy, a X. Show that {X,} is Cauchy convergent in probability in that, for all
€ > 0, P(\Xn — Xm| > €) ~ 048 n,m — ov. In what sense is the converse true?

(b) Let {Xp} and {Y,} be sequences of random variables such that the pairs (X;, X;) and (Y;, Yj)

have the same distributions for all i, 7. If Xp a X, show that Y, converges in probability to
some limit Y having the same distribution as X.

2. Show that the probability that infinitely many of the events {A, : n > 1} occur satisfies
P(A, 1.0.) > lim supy_,o9 P(An).

3. Let {S, : 2 > 0} be a simple random walk which moves to the right with probability p at each
step, and suppose that Sg = 0. Write Xp = Sy — Sy-1.

86

Some ancillary results Exercises  [7.3.4]-[7.3.10]

(a) Show that {S, = 0 1.0.} is not a tail event of the sequence {Xp}.
(b) Show that P(S, = Oi.o.) = Oif p # 4.
(c) Let T, = S;,/./n, and show that

{lim inf Tr < =x} M ¢ lim sup Th > x}

nh WOO

is a tail event of the sequence {Xp}, for all x > 0, and deduce directly that P(S, = Oi.o.) = Lif

p= 3.
4. Hewitt-Savage zero—one law. Let X1, X2,... be independent identically distributed random
variables. The event A, defined in terms of the X», is called exchangeable if A is invariant un-
der finite permutations of the coordinates, which is to say that its indicator function I, satisfies
TA(X1, X2,---,Xn.---) = [A(Kiy, Xin ++, Xin» Xn41,---) for all n > 1 and all permutations
(ij, i2,-.-, in) of (1, 2,..., 2). Show that all exchangeable events A are such that either P(A) = 0
or P(A) = 1.

5. Returning to the simple random walk S of Exercise (3), show that {S, = 0 i.o.} is an exchangeable
event with respect to the steps of the walk, and deduce from the Hewitt-Savage zero—one law that it
has probability either 0 or 1.

6. Weierstrass’s approximation theorem. Let f : [0, 1] — R be a continuous function, and let
Sn be a random variable having the binomial distribution with parameters n and x. Using the formula
E(Z) = E(ZI4) + E(ZI 4c) with Z = f(x) — f(n!S,) and A = {|n—!S, — x| > 5}, show that

F(a) — Of K/n) (") xk xy"*! <0.
k=0

lim sup
NO Ney <]

You have proved Weierstrass’s approximation theorem, which states that every continuous function
on [0, 1] may be approximated by a polynomial uniformly over the interval.

7, Complete convergence. A sequence X;, X2,... of random variables is said to be completely
convergent to X if
SSPUXn -X|>€)<00  foralle > 0.
n

Show that, for sequences of independent variables, complete convergence is equivalent to a.s. conver-
gence. Find a sequence of (dependent) random variables which converges a.s. but not completely.

8. Let X,, X2,... be independent identically distributed random variables with common mean yz
and finite variance. Show that

-1
(;) > XX; > p2 asn—> oo.

l<i<j<n

9. Let {X, : n > 1} be independent and exponentially distributed with parameter 1. Show that

Xx
P (1imsup 2 = 1) =1.

noo log n

10. Let {X, :n > 1} be independent N(0, 1) random variables. Show that:

. IXnl )
P { lims =J/2\= 1,
@) (iim sup Jlogn

87

[7.3.11]-[7.5.1] Exercises Convergence of random variables

0 if}), P(X1 > an) < ~,
1 if 0, P(X, > an) = ow.
11. Construct an example to show that the convergence in distribution of X, to X does not imply the
convergence of the unique medians of the sequence Xp.

(b) P(X > an io.) = {

12. (i) Let {X; : r > 1} be independent, non-negative and identically distributed with infinite mean.
Show that lim sup,_,,) X,/r = oo almost surely.
(ii) Let {X;} be a stationary Markov chain on the positive integers with transition probabilities
—— ifk=j+1,
j+2 J
— ifk=1.
j+2

(a) Find the stationary distribution of the chain, and show that it has infinite mean.
(b) Show that lim sup,_, 4 X,/r < 1 almost surely.

13. Let {X; : 1 <7 <n} be independent and identically distributed with mean yz and finite variance
o?. Let X =n—! S>"_, X;. Show that

do — w) /
r=1

converges in distribution to the N(0, 1) distribution as n — oo.

7.4 Exercise. Laws of large numbers

1. Let X2, X3,... be independent random variables such that

1

P(X, =n) = P(X, = —n) = wloen’

» PX, =0=1-

2n logn

Show that this sequence obeys the weak law but not the strong law, in the sense that n! VX:
converges to 0 in probability but not almost surely.

2. Construct a sequence {X; : r > 1} of independent random variables with zero mean such that
n—!yo"_| X, > —oo almost surely, as n > 00.

3. Let N be a spatial Poisson process with constant intensity 2 in R@, where d > 2. Let S be the
ball of radius r centred at zero. Show that N(S)/|S| — A almost surely as r > oo, where | S| is the
volume of the ball.

7.5 Exercises. The strong law

1. Entropy. The interval [0, 1] is partitioned into n disjoint sub-intervals with lengths p), po, ...,
Pn, and the entropy of this partition is defined to be

i
h=—)° pjlog pj.
i=1

88

Martingales Exercises [7.5.2]-[7.7.4]

Let X,, X2,... be independent random variables having the uniform distribution on [0, 1], and let
Zm (i) be the number of the X;, X2,..., Xm which lie in the ith interval of the partition above. Show
that

n
i=1

satisfies m—! log Rm — —h almost surely as m — ov.

2. Recurrent events. Catastrophes occur at the times 7; , T,... where T) = X;+X2+---+X; and
the X; are independent identically distributed positive random variables. Let N(t) = max{n : Ty < t}
be the number of catastrophes which have occurred by time t. Prove that if E(X,) < oo then
N(t) > co and N(t)/t > 1/E(X1) as t > oo, almost surely.

3. Random walk. Let X,, X2,... be independent identically distributed random variables taking
values in the integers Z and having a finite mean. Show that the Markov chain S = {S,} given by
Sn = 21 X; is transient if E(X1) 4 0.

7.6 Exercise. Law of the iterated logarithm

1. A function ¢(x) is said to belong to the ‘upper class’ if, in the notation of this section, P(S, >
o(n)./n io.) = 0. A consequence of the law of the iterated logarithm is that ./a log log x is in the
upper class for all a > 2. Use the first Borel—Cantelli lemma to prove the much weaker fact that
o(x) = fo log x is in the upper class for all a > 2, in the special case when the X; are independent
N(O, 1) variables.

7.7 Exercises. Martingales
1. Let X1, X2,... be random variables such that the partial sums Sy = X,; + X2+---+ Xn
determine a martingale. Show that E(X;X;) = 0 if i ¥ j.

2. Let Z, be the size of the nth generation of a branching process with immigration, in which the
family sizes have mean jt (# 1) and the mean number of immigrants in each generation is m. Suppose
that E(Zg) < o, and show that

_ n

is a martingale with respect to a suitable sequence of random variables.

3. Let Xo, X1, Xz, ... bea sequence of random variables with finite means and satisfying E(X,,41 |
Xo, X1,---,Xn) = aXpn + bX,_; forn > 1, whereO < a,b < 1landa+b=1. Finda value of a
for which S, = aX», + Xy,_-1,n = 1, defines a martingale with respect to the sequence X.

4. Let X, be the net profit to the gambler of betting a unit stake on the nth play in a casino; the
Xn may be dependent, but the game is fair in the sense that E(X,41 | X1, X2,-.., Xn) = 0 for all
n. The gambler stakes Y on the first play, and thereafter stakes f,(X 1, X2,..., Xn) on the (n + 1)th
play, where f,, f2,... are given functions. Show that her profit after n plays is

n
Sn = >) Xi fi-1(X1, X2,---, Xi-1),

i=1

where fo = Y. Show further that the sequence S = {S,} satisfies the martingale condition E(S,+ |
X 1, X2,...,Xn) = Sn, n = 1, if Y is assumed to be known throughout.

89
[7.8.1]-[7.9.5] Exercises Convergence of random variables

7.8 Exercises. Martingale convergence theorem

1. Kolmogorov’s inequality. Let X;, X2,... be independent random variables with zero means
and finite variances, and let S, = X, + X2 +---+ Xy. Use the Doob—Kolmogorov inequality to
show that

l<jxn

1 n
P ( max Isj1>) Sy vax) fore > 0.
j=l

2. Let X,,X2,... be independent random variables such that 5, n2 var(Xn) < co. Use Kol-
mogorov’s inequality to prove that

3 X; — E(Xi) as,
—————_ —> Y asn > oOo,

; i
i=l

for some finite random variable Y, and deduce that
1< as.
-5 (Xj — EX;) —>0 asn —> oo,
n
i=]

(You may find Kronecker’s lemma to be useful: if (ay) and (by) are real sequences with by, + oo and
3; 4;/bj < 00, then bz! ye a > Oasn > 00.)

3. Let S be a martingale with respect to X, such that E(S2) < K < oo for some K € R. Suppose
that var(S,) —> 0 as n — ov, and prove that S = limy-so9 Sp exists and is constant almost surely.

7.9 Exercises. Prediction and conditional expectation

1. Let Y be uniformly distributed on [—1, 1] and let X¥ = Y?.
(a) Find the best predictor of X given Y, and of Y given X.
(b) Find the best linear predictor of X given Y, and of Y given X.

2. Let the pair (X, Y) have a general bivariate normal distribution. Find E(Y | X).

3. Let X1, X2,.--, Xn be random variables with zero means and covariance matrix V = (v;;), and
let Y have finite second moment. Find the linear function h of the X; which minimizes the mean
squared error E{(Y — h(Xj,..., Xn))7}.
4. Verify the following properties of conditional expectation. You may assume that the relevant
expectations exist.
@) E{E(Y | $)} = EY).
(ii) E(@aY + BZ | 3) = aE(Y | $) + BE(Z | 9) fora, BER.
(iii) EY | 9) = Of Y > 0.
(iv) E(Y | 9) = E{E(Y | 4) | g}if 9c #.
(v) EY | $) = E(Y) if Y is independent of Ig for every G € §.
(vi) Jensen’s inequality. g{E(Y | %)} < E{g(Y) | $} for all convex functions g.
(vii) If Y, “SS Y and |Y;| < Z a.s. where E(Z) < oo, then E(¥n | 9) “SS E(Y | 9).
(Statements (ii)-(vi) are of course to be interpreted ‘almost surely’.)

5. Let X and Y have joint mass function f(x, y) = {x@ + yy} for x = y = 1,2,.... Show that
E(Y | X) < « while E(Y) = oo.

90

Problems Exercises [7.9.6]-[7.11.5]

6. Let (2, F, P) be a probability space and let % be a sub-o-field of F. Let H be the space of
g-measurable random variables with finite second moment.

(a) Show that H is closed with respect to the norm || - |j2.

(b) Let Y be a random variable satisfying E(Y 2) < co, and show the equivalence of the following
two statements for any M € H:
G) E{(Y — M)Z} = 0 forall Z € H,
(ii) E{(Y — M)Ig} = 0 forall G € G.

7.10 Exercises. Uniform integrability

1. Show that the sum {X, + Y,} of two uniformly integrable sequences {X,} and {Y,} gives a
uniformly integrable sequence.

2. (a) Suppose that X;, > X where r > 1. Show that {|X,|" : > 1} is uniformly integrable, and
deduce that E(X7,) — E(X") if r is an integer.

(b) Conversely, suppose that {|X,|" : » = 1} is uniformly integrable where r > 1, and show that
Xn > XifXp > X.

3. Let g : [0, 00) — [0, co) be an increasing function satisfying g(x)/x — oo as x — oo. Show

that the sequence {X, : n > 1} 1s uniformly integrable if sup, E{g(|Xn|)} < oo.

4. Let {Z, : n > O} be the generation sizes of a branching process with Zo = 1, E(Z,) = 1,

var(Z1) #4 0. Show that {Zy, : n > 0} is not uniformly integrable.

5. Pratt’s lemma. Suppose that Xn < Yn < Zn where X, > X,Y, —> Y,and Z, + Z. If

E(Xn) > E(X) and E(Z,) > E(Z), show that E(Y,) > E(Y).

6. Let {X, :n > 1} be a sequence of variables satisfying E(sup, |Xn|) < oo. Show that {Xp} is
uniformly integrable.

7.11 Problems

1. Let X, have density function

n

fn) = al +n2x2)’ n>.

With respect to which modes of convergence does X», converge as n — 00?

2. (i) Suppose that X, 7s X and Y, °s Y, and show that X, + Y, “+ X + Y. Show that the
corresponding result holds for convergence in rth mean and in probability, but not in distribution.

(ii) Show that if X, 2S: xX and Yn 2S: ¥ then X nin +; XY. Does the corresponding result hold
for the other modes of convergence?

3. Let g :R — R be continuous. Show that g(X») an g(X) if Xn as Xx.

4. Let Yj, Y2,... be independent identically distributed variables, each of which can take any value
in {0, 1,..., 9} with equal probability b- Let Xn = Ty Y; 10~#. Show by the use of characteristic

functions that X, converges in distribution to the uniform distribution on [0, 1]. Deduce that X,, 2s y
for some Y which is uniformly distributed on [0, 1].

5. Let N(t) be a Poisson process with constant intensity on R.

91

[7.11.6]-[7.11.13] Exercises Convergence of random variables

(a) Find the covariance of N(s) and N(t).

(b) Show that N is continuous in mean square, which is to say that E({N (t+h)-—N (t)}?) — Oas
h->0.

(c) Prove that N is continuous in probability, which is to say that P(|N (t+h)— N(t)| > €) — Oas
h — 0, for all ¢ > 0.

(d) Show that N is differentiable in probability but not in mean square.

6. Prove thatn—! 1 Xj +5; 0 whenever the X j are independent identically distributed variables
with zero means and such that E(X4) <M.

7. Show that Xy, #5; X whenever Yn EUXn — XI") < 00 for some r > 0.

8. Show that if X, 2 X thenaX, +b 2, aX + b for any real a and b.
9, If X has zero mean and variance oa’, show that
2

o
+ 1?

fort > 0.
o2

PIX >t) <

10. Show that X,, +> 0 if and only if

e() —-0 asnow.
1+ |Xal

11. The sequence {X,} is said to be mean-square Cauchy convergent if E{(Xy — Xm)"} —> Oas
m,n — ov. Show that {X,} converges in mean square to some limit X if and only if it is mean-square
Cauchy convergent. Does the corresponding result hold for the other modes of convergence?

12. Suppose that {X,,} is a sequence of uncorrelated variables with zero means and uniformly bounded
variances. Show that n~! S07_, X; aS. 0.

13. Let X1, X2,... be independent identically distributed random variables with the common dis-
tribution function F, and suppose that F(x) < 1 for all x. Let My, = max{X 1, X2,..., Xn} and
suppose that there exists a strictly increasing unbounded positive sequence a,,a2,... such that
P(My/an <x) — H(x) for some distribution function H. Let us assume that H is continuous
with 0 < H(1) < 1; substantially weaker conditions suffice but introduce extra difficulties.

(a) Show that n[1 — F(ay,x)] ~ — log H() as n — o and deduce that
1— F(ay,x) . log H(x)
1— F(an) log H(i)

ifx > 0.

(b) Deduce that if x > 0
1— F(tx) log H(x)

> ast —> ©.
1—-F(@) log H(1)

(c) Set x = x,x2 and make the substitution

() = log H(e*)
8) = Tos HA)
to find that g(x + y) = g(x)g(y), and deduce that

—ax 7B) i
Hos) = { mr ax?) ifx>0,

ifx <0,

92

Problems Exercises [7.11.14]~[7.11.19]

for some non-negative constants a and p.
You have shown that H is the distribution function of Y—!, where Y has a Weibull distribution.
14. Let X1, X2,..., Xp be independent and identically distributed random variables with the Cauchy

distribution. Show that M, = max{X,, X2,..., Xn} is such that 7 M,/n converges in distribution,
the limiting distribution function being given by H(x) = e7!/* if x > 0.
15. Let X,, Xo,... be independent and identically distributed random variables whose common
characteristic function ¢ satisfies @’(0) = ij. Show that n7! dj=1 Xj 4 [L.
16. Total variation distance. The total variation distance dpy(X, Y) between two random variables
X and Y is defined by

apy(X,Y)= — sup EGC) — E(u(Y))|

u:|Ulloo=
where the supremum is over all (measurable) functions u : R — R such that |ju|loo = sup, |u(x)|
satisfies |[u|loo = 1.
(a) If X and ¥ are discrete with respective masses f, and gy at the points x, show that

dpy(X,Y) = 3° | fn — gn| = 2 sup |P(X € A) — PY € A)j.
n ACR

(b) If X and Y are continuous with respective density functions f and g, show that

OO

dpy(X,Y) = / Lf (x) — g(x)|dx = 2 sup [P(X € A) -— PY € A).

(c) Show that dpy(Xn, X) — 0 implies that X, — X in distribution, but that the converse is false.

(d) Maximal coupling. Show that P(X 4 Y) > 5dty(X , Y), and that there exists a pair X’, Y’
having the same marginals for which equality holds.
(e) If X;, Y; are independent random variables, show that

n n n
dry (> Xi) v) < So arv(%, ¥)).
i=l i=l

i=1

17, Let g : R — R be bounded and continuous. Show that

a) k
2 #ik/m) “ e ™ _, (4) asn— oo.

18. Let X;, and Yj, be independent random variables having the Poisson distribution with parameters
n and m, respectively. Show that
(Xn —1)—- Yn —m) D
—> N(O,1) asm,n— oo.
VXn + Yn

19. (a) Suppose that X;, X2, ... isa sequence of random variables, each having a normal distribution,

and such that X, ®. x. Show that X has a normal distribution, possibly degenerate.
(b) For each n > 1, let (Xn, Yn) be a pair of random variables having a bivariate normal distribution.

Suppose that X, x X and Y, EA Y, and show that the pair (X, Y) has a bivariate normal
distribution.

93

[7.11.20]-[7.11.26] Exercises Convergence of random variables

20. Let X1, X2,... be random variables satisfying var(X;) < c for all n and some constant c. Show
that the sequence obeys the weak law, in the sense that n=! 21 (X; — EX;) converges in probability
to 0, if the correlation coefficients satisfy either of the following:

(i) p(X;, Xj) < 0 for alli F j,
(ii) o(X;, Xj) > Oas fi — j| > o.

21. Let X;, X2,... be independent random variables with common density function

0 if |x| < 2,
FI=) f  tiei > 2,
x” log |x|

: P
where c is a constant. Show that the X; have no mean, but n} an X; — Oasn — oo. Show that
convergence does not take place almost surely.

22. Let X, be the Euclidean distance between two points chosen independently and uniformly from
the n-dimensional unit cube. Show that E(X;,)/./n > 1 [V6 asn — oo.

23. Let X 1, X2,... be independent random variables having the uniform distribution on [—1, 1].
Show that
7 (

n
x"
i=]

1 1
> Jn) J asn > ©.

24. Let X,, X2,... be independent random variables, each X; having mass function given by
1
(Xp = k) = P(X ) 52
1 1 .
P(X, =) = POX = -D = 5 l-a ifk > 1.

Show that U, = >7] X; satisfies U,/./n » N(O, 1) but var(U,/./n) > 2 asn > 0.
25. Let X1, X2,... be random variables, and let Ny, Nz,... be random variables taking values in
the positive integers such that Nz an oo as k — oo. Show that:
(i) if Xp, 2 X and the X, are independent of the Nz, then XN, ms Xakoow,
(ii) if Xn 2 X then Xy, > X ask > ov.
26. Stirling’s formula.

(a) Leta(k,n) = nk /(k — 1)! for 1 <k <n-+1. Use the fact that 1—x < e~* ifx > 0 to show that

ah) <P) tk > 0.
a(n+1,n) — ~

(b) Let X,, X2,... be independent Poisson variables with parameter 1, and let S, = X; +---+ Xp.
Define the function g : R > R by

(x) {o. ifO0>x>-M,
x)=
8 0 otherwise,

where M is large and positive. Show that, for large n,

Sn — -n
B(«{ ah) = Se facn + 1m) — a(n ~ km}
94

Problems Exercises [7.11.27]-[7.11.33]

where k = [Mn!/ 21. Now use the central limit theorem and (a) above, to deduce Stirling’s

formula:
nie”

i
n"+?./n

27. A bag contains red and green balls. A ball is drawn from the bag, its colour noted, and then it
is returned to the bag together with a new ball of the same colour. Initially the bag contained one
ball of each colour. If R, denotes the number of red balls in the bag after n additions, show that
Sn = Rp/(n 4+ 2) is a martingale. Deduce that the ratio of red to green balls converges almost surely
to some limit as n + ov.

—>1 asn — OOo.

28. Anscombe’s theorem. Let {X; : i > 1} be independent identically distributed random variables
with zero mean and finite positive variance o”, and let Sy = 7] X;. Suppose that the integer-valued

: _ P : eye
random process M (ft) satisfies ¢ lu (t) — 6 ast — oo, where @ is a positive constant. Show that

Ss AY
2M) Bovo,1) and —“@®_ 2 wo,1) ast > 0.
oVét o/M(t)

You should not assume that the process M is independent of the X;.

29. Kolmogorov’s inequality. Let X,, X2,... be independent random variables with zero means,
and Sy = Xj +Xo+-:-+Xn. Let My, = max) <p<y |5,%j and show that E(S2TA,) > c2P(Ag) where
Ay = {Mg_1 <c < My} andc > 0. Deduce Kolmogorov’s inequality:

E 2
P ( max ISkl >) < (Sn) c> 0.

l<k<n c2

30. Let X 1, X2,... be independent random variables with zero means, and let S, = X; + X2+
--- + X,. Using Kolmogorov’s inequality or the martingale convergence theorem, show that:

(i) S772) Xi converges almost surely if S772, E(X?) < 0,
(ii) if there exists an increasing real sequence (b,) such that by, — ov, and satisfying the inequality
Sree E(XP)/bz < 00, then by! S700, X, *S Oasn > ov.

31. Estimating the transition matrix. The Markov chain Xo, X1,... , Xn has initial distribution
Ff, = P(Xo = i) and transition matrix P. The log-likelihood function A(P) is defined as A(P) =
log( Xo PX9,X1 PX1,X_ °°" PX,-1,Xn)- Show that:

(a) ACP) = log fxg + yi, j Ni; log pij where N;; is the number of transitions from i to j,

(b) viewed as a function of the p;;, A(P) is maximal when p;; = pjj where Dj; = Nij/3°, Nik.
(c) if X is irreducible and ergodic then p; ij aS; Pij ASN > OO.

32. Ergodic theorem in discrete time. Let X be an irreducible discrete-time Markov chain, and let
j4; be the mean recurrence time of state i. Let V;(n) = yr I,x,=i} be the number of visits to i up
ton — 1, and let f be any bounded function on S. Show that:

(a) n—!V;(n) aS uj! asn > o,
(b) if 4; < 00 for all i, then

1%
hn So f(r) => Sof @O/m: asn —> oo.
r=0

ieS

33. Ergodic theorem in continuous time. Let X be an irreducible persistent continuous-time Markov
chain with generator G and finite mean recurrence times j;.

95

[7.11.34]-[7.11.37] Exercises Convergence of random variables

1 t
(a) Show that “ff Tix(s)=j} as ass ast > &;
0

Hj 8j
(b) deduce that the stationary distribution w satisfies 7; = 1/(u; gj);
(c) show that, if f is a bounded function on S,

-[ f(X(s)) ds =4s Ease as t > 00.

34. Tail equivalence. Suppose that the sequences {X, :n > 1} and {Y, : n > 1} are tail equivalent,
which is to say that )>7°., P(Xn # Yn) < 00. Show that:

(a) S372, Xn and S>7°., Yn converge or diverge together,
(b) any (Xn — Yn) converges almost surely,

(c) if there exist a random variable X and a sequence a, such that a, t oo and a,, 1 ae Xy 3S. Xx ;
then
Ty as.
—SoY, = Xx.
an
r=1

35. Three series theorem. Let {X, : 2 > 1} be independent random variables. Show that eat Xn
converges a.s. if, for some a > 0, the following three series all converge:

(a) So, PUXal > @),

(b) Sy var(Xnlx_|<a})

(C) oy, E(Xnl x, |<a})-

[The converse holds also, but is harder to prove.]

36. Let {Xp :n > 1} be independent random variables with continuous common distribution function

F. We call X, a record value for the sequence if X, > X; for 1 <r < k, and we write J; for the
indicator function of the event that X; is a record value.

(a) Show that the random variables J; are independent.
(b) Show that Rm = > 71 Ir satisfies Rm /logm “5; Lasm > 00.

37. Random harmonic series. Let {X;, : 2 > 1} be a sequence of independent random variables
with P(X, = 1) = P(X, = -1) = 4 Does the series }°”_, X;/r converge a.s. asm > 00?

96

8

Random processes

8.2 Exercises. Stationary processes

1. Flip-flop. Let {X,} be a Markov chain on the state space S = {0, 1} with transition matrix

l-a a
P= ;
( B 9)
where a + B > 0. Find:

(a) the correlation p(Xm, Xm-+n), and its limit as m — oo with n remaining fixed,
(b) littn+oon! 1 P(X = 1).
Under what condition is the process strongly stationary?

2. Random telegraph. Let {N(t) : t > 0} be a Poisson process of intensity 4, and let 7p be
an independent random variable such that P(Ty = +1) = 4. Define T(t) = To(-1)N®. Show
that {7 (¢) : t > O} is stationary and find: (a) e(T(s), T(s + ¢)), (b) the mean and variance of
X(t) = fp T(s)ds.

3. Korolyuk—Khinchin theorem. An integer-valued counting process {N(t) : t > 0} with N(0) =
0 is called crudely stationary if px(s,t) = P(N(s +t) — N(s) = k) depends only on the length ¢ and
not on the location s. It is called simple if, almost surely, it has jump discontinuities of size 1 only.
Show that, for a simple crudely stationary process N, lim, jo t PIN (t) > 0) = E(N(1)).

8.3 Exercises. Renewal processes

1. Let (f; : n = 1) bea probability distribution on the positive integers, and define a sequence
(un :n > 0) by ug = 1 and uy = D0? _, f-un—r, n = 1. Explain why such a sequence is called a
renewal sequence, and show that u is a renewal sequence if and only if there exists a Markov chain U
and a state s such that u, = P(U, = s | Up = 5).

2. Let {X; : i > 1} be the inter-event times of a discrete renewal process on the integers. Show
that the excess lifetime B, constitutes a Markov chain. Write down the transition probabilities of the
sequence {By} when reversed in equilibrium. Compare these with the transition probabilities of the
chain U of your solution to Exercise (1).

3. Let (u, :n > 1) satisfy ug = 1 and up = 77_, frun—r forn > 1, where (f; :r > lisa
non-negative sequence. Show that:

(a) Uv, = pup is a renewal sequence if p > Oand 0°.) op" fn = 1,

(b) asn — ©0, p”™uy, converges to some constant c.

97

[8.3.4]~[8.4.5] Exercises Random processes

4. Events occur at the times of a discrete-time renewal process N (see Example (5.2.15)). Let uy be
the probability of an event at time n, with generating function U(s), and let F(s) be the probability
generating function of a typical inter-event time. Show that, if |sj < 1:

oo oo k
y E(N(r))s" = a and > E (rn) six ae fork > 0.
—s5 _
=0

r=0

5. Prove Theorem (8.3.5): Poisson processes are the only renewal processes that are Markov chains.

8.4 Exercises. Queues

1. The two tellers in a bank each take an exponentially distributed time to deal with any customer;
their parameters are A and jz respectively. You arrive to find exactly two customers present, each
occupying a teller.

(a) You take a fancy to arandomly chosen teller, and queue for that teller to be free; no later switching
is permitted. Assuming any necessary independence, what is the probability p that you are the
last of the three customers to leave the bank?

(b) If you choose to be served by the quicker teller, find p.

(c) Suppose you go to the teller who becomes free first. Find p.

2. Customers arrive at a desk according to a Poisson process of intensity 1. There is one clerk, and
the service times are independent and exponentially distributed with parameter . At time 0 there is
exactly one customer, currently in service. Show that the probability that the next customer arrives
before time ¢ and finds the clerk busy is

Xr

d—- e Ot Ey
A+ Mb

3. Vehicles pass a crossing at the instants of a Poisson process of intensity A; you need a gap of
length at least a in order to cross. Let T be the first time at which you could succeed in crossing to
the other side. Show that E(T) = (e@* — 1)/A, and find Eel),

Suppose there are two lanes to cross, carrying independent Poissonian traffic with respective rates
i and jt. Find the expected time to cross in the two cases when: (a) there is an island or refuge between
the two lanes, (b) you must cross both in one go. Which is the greater?

4. Customers arrive at the instants of a Poisson process of intensity 4, and the single server has
exponential service times with parameter y. An arriving customer who sees n customers present
(including anyone in service) will join the queue with probability (7 + 1)/(m + 2), otherwise leaving
for ever. Under what condition is there a stationary distribution? Find the mean of the time spent in
the queue (not including service time) by a customer who joins it when the queue is in equilibrium.
What is the probability that an arrival joins the queue when in equilibrium?

5. Customers enter a shop at the instants of a Poisson process of rate 2. At the door, two represen-
tatives separately demonstrate a new corkscrew. This typically occupies the time of a customer and
the representative for a period which is exponentially distributed with parameter 1, independently of
arrivals and other demonstrators. If both representatives are busy, customers pass directly into the
shop. No customer passes a free representative without being stopped, and all customers leave by
another door. If both representatives are free at time 0, show the probability that both are busy at time

‘2 2,-2t 4 ,—-St
tis g — ze + 75e .

98
Problems Exercises [8.5.1]-[8.7.4]

8.5 Exercises. The Wiener process

1. Fora Wiener process W with W(0) = 0, show that

1 1
P(W(s) >0, Wi) > 0) =] + on sin~! ‘A fors <t.

Calculate P(W(s) > 0, W(t) > 0, Wi) > 0) whens <t <u.
2. Let W be a Wiener process. Show that, for s < t < u, the conditional distribution of W(t) given
W(s) and W(w) is normal

N (S —-)W()+t-s)Wiu) u—-net— >)

u—s “u—S

Deduce that the conditional correlation between W(t) and W(u), given W(s) and W(v), where s <

t<u<v,is
(v —u)(t —s)
(v-t)u—s)
3. For what values of a and b is aW, + bW2 a standard Wiener process, where W, and W) are

independent standard Wiener processes?

4. Show that a Wiener process W with variance parameter o? has finite quadratic variation, which
is to say that

n—1
Vw G + Wt/n) — W(jt/n)}* eS 07t asn > o.
j=0

5. Let W be a Wiener process. Which of the following define Wiener processes?
(a) —W(t), (b) VW), (© W(2t) — We).

8.7 Problems

1. Let {Z,} be a sequence of uncorrelated real-valued variables with zero means and unit variances,
and define the ‘moving average’

r
Yn = So oi Zn-is
=0

for constants a9, a1, ..., @,-. Show that Y is stationary and find its autocovariance function.

2. Let {Z,} be a sequence of uncorrelated real-valued variables with zero means and unit variances.
Suppose that {Y,,} is an ‘autoregressive’ stationary sequence in that it satisfies Y, = a@Y¥,_1 + Zn,
—oo <n < on, for some real a satisfying |a| < 1. Show that Y has autocovariance function
c(m) = al! /(1 — a).

3. Let {X,} be a sequence of independent identically distributed Bernoulli variables, each taking
values 0 and 1 with probabilities 1 — p and p respectively. Find the mass function of the renewal
process N(r) with interarrival times {Xp}.

4. Customers arrive in a shop in the manner of a Poisson process with parameter 1. There are
infinitely many servers, and each service time is exponentially distributed with parameter 4. Show
that the number Q(t) of waiting customers at time ¢ constitutes a birth-death process. Find its
stationary distribution.

99

[8.7.5]-[8.7.7] Exercises Random processes

5. Let X(t) = Y cos(6t) + Z sin(@t) where Y and Z are independent N(O, 1) random variables, and
let X(t) = Rcos(Ot + W) where R and W are independent. Find distributions for R and W such that
the processes X and X have the same fdds.

6. Bartlett’s theorem. Customers arrive at the entrance to a queueing system at the instants of
an inhomogeneous Poisson process with rate function A(t). Their subsequent service histories are
independent of each other, and a customer arriving at time s is in state A at time s + ¢ with prob-
ability p(s, t). Show that the number of customers in state A at time ¢ is Poisson with parameter

Jogo Mw) p(u, t — u) du.

7. Ina Prague teashop (U MySaka), long since bankrupt, customers queue at the entrance for a
blank bill. In the shop there are separate counters for coffee, sweetcakes, pretzels, milk, drinks, and
ice cream, and queues form at each of these. At each service point the customers’ bills are marked
appropriately. There is a restricted number WN of seats, and departing customers have to queue in order
to pay their bills. If interarrival times and service times are exponentially distributed and the process
is in equilibrium, find how much longer a greedy customer must wait if he insists on sitting down.
Answers on a postcard to the authors, please.

100

9

Stationary processes

9.1 Exercises. Introduction

1. Let...,Z_1, Zo, Z1, Zo, ... be independent real random variables with means 0 and variances
1, and let a, 8 € R. Show that there exists a (weakly) stationary sequence {W,} satisfying W, =
aW,—-1 + BWn-2 + Zn, n = ...,—1,0,1,..., if the (possibly complex) zeros of the quadratic
equation z2 — wz — 6 = O are smaller than 1 in absolute value.

2. Let U be uniformly distributed on [0, 1] with binary expansion U = aa Xx 7277. Show that
the sequence

CO
Vn = >_Xign2!, n=O,

i=l
is strongly stationary, and calculate its autocovariance function.

3. Let {X,:n=...,—1,0,1,...} bea stationary real sequence with mean 0 and autocovariance
function c(m).
(i) Show that the infinite series 0 anX pn converges almost surely, and in mean square, whenever
co lanl < 09.
Gi) Let
le ¢)
Yn = So aK Xn n=...,—1,0,1,...
k=0

where “729 |ax| < oo. Find an expression for the autocovariance function cy of Y, and show
that

oO

d= lev()| < 00.

m=—-CO

4. LetX = {X, : n > 0}bea discrete-time Markov chain with countable state space S and stationary
distribution 7, and suppose that Xo has distribution 2. Show that the sequence { f (Xn): n > O} is
strongly stationary for any function f : S > R.

9.2 Exercises. Linear prediction

1. Let X be a (weakly) stationary sequence with zero mean and autocovariance function c(m).
(i) Find the best linear predictor X ntl Of Xn41 given Xn.
(ii) Find the best linear predictor xX n+1 Of Xn41 given Xp and Xp,_}1.

101

[9.2.2]-[9.4.2] Exercises Stationary processes

(ii) Find an expression for D = E{(Xn41 — Xn41)2} — E{(Xn41 — Xn41)7}, and evaluate this
expression when:
(a) X, = cos(nU) where U is uniform on [—2, x],
(b) X is an autoregressive scheme with c(k) = o:!*l where ja] < 1.

2. Suppose |a| < 1. Does there exist a (weakly) stationary sequence {X, : —oo <n < oo} with
zero means and autocovariance function

1 ifk =0,

(k) = Cif [kl =1
Cc = L+a2 =t,
0 if |k| > 1.

Assuming that such a sequence exists, find the best linear predictor Xn of X», given X,-1,
Xy—-2,---, and show that the mean squared error of prediction is (1 + a*)—!_ Verify that {Xp} is
(weakly) stationary.

9.3 Exercises. Autocovariances and spectra

1. Let X, = Acos(nd) + Bsin(nd) where A and B are uncorrelated random variables with zero
means and unit variances. Show that X is stationary with a spectrum containing exactly one point.

2. Let U be uniformly distributed on (—7, 2), and let V be independent of U with distribution func-
tion F. Show that X, = e!(U—Y") defines a stationary (complex) sequence with spectral distribution
function F’.

3. Find the autocorrelation function of the stationary process {X(t) : —oo <t < oo} whose spectral
density function is:

@ N,V, i) f(x) = Fe7 FI, -00 < x < 00.

4. Let X,, X2,... beareal-valued stationary sequence with zero means and autocovariance function

c(m). Show that
Len _ sin(nd/2) \*
(5%) = c(0) _ (Tinos) oe

where F is the spectral distribution function. Deduce that n-! jal Xj ™S, 0 if and only if
F(0) — F(O—) = 0, and show that

n—-1
1
c(O){F (0) — FO-)} = lim, — Y/c(7).
j=0

9.4 Exercises. Stochastic integration and the spectral representation
1. Let S be the spectral process of a stationary process X with zero mean and unit variance. Show
that the increments of S have zero means.

2. Moving average representation. Let X be a discrete-time stationary process having zero means,
continuous strictly positive spectral density function f, and with spectral process S. Let

y, asa.
“=f Jan f () ).

102

Gaussian processes Exercises [9.4.3]-[9.6.2]

Show that ..., ¥_1, Yo, Y1,... is a sequence of uncorrelated random variables with zero means and
unit variances.

Show that X, may be represented as a moving average Xp = je—o0 aj;Y,—; where the a; are
constants satisfying

J2nf a) = = aj we th for A € (—1, 7].

3. Gaussian process. Let X be a discrete-time stationary sequence with zero mean and unit vari-
ance, and whose fdds are of the multivariate-normal type. Show that the spectral process of X has
independent increments having normal distributions.

9.5 Exercises. The ergodic theorem

1. Let T = {1,2,...} and let £ be the set of invariant events of (RP , Br), Show that £ is a o-field.

2. Assume that X,, X,... is a stationary sequence with autocovariance function c(m). Show that

(5%) = 2S oq -O.

j=li=0
Assuming that 7 -1 J 5 c(i) — o” as j > oo, show that
ic 2
ar ere > Oo asn — Oo.
i=1
3. Let X1, X2,... be independent identically distributed random variables with zero mean and unit

variance. Let
io.¢]
Yn= So ajXn4i forn>1
i=0
where the a; are constants satisfying 5"; a? < oo. Use the martingale convergence theorem to show

that the above summation converges almost surely and in mean square. Prove that n—! 1 Y); 2 0
a.s. and in mean, as n —> oo.

9.6 Exercises. Gaussian processes

1. Show that the function c(s, t) = min{s, t} is positive definite. That is, show that

n

S C(t, tj ZZ > 0

jk=l
for allO < t) < t2 <--++ < ft, and all complex numbers 21, 2Z2,..., Z, at least one of which is
non-zero.
2. Let X,, X,... be a stationary Gaussian sequence with zero means and unit variances which

satisfies the Markov property. Find the spectral density function of the sequence in terms of the
constant p = cov(X, X2).

103

[9.6.3]-[9.7.7] Exercises Stationary processes

3. Show that a Gaussian process is strongly stationary if and only if it is weakly stationary.

4. Let X bea stationary Gaussian process with zero mean, unit variance, and autocovariance function
c(t). Find the autocovariance functions of the processes X2 = {X (t)? : -co <t < oo} and
B= (x(t)? 1-00 <t < coh.

9.7 Problems

1, Let...,X_1, X9, X1,... be uncorrelated random variables with zero means and unit variances,
and define

io.¢]
Yn =Xn tad. Bi! Xy i for —o<n<o,
i=1
where a and f are constants satisfying |B] < 1, |B —a@| < 1. Find the best linear predictor of Y¥,41
given the entire past Y,, Y,-1,....

2. Let {¥, : -oo < k < oo} bea stationary sequence with variance oF, and let

r
Xn = So ak Yn—k: —oO <n<O,
k=0
where ao, a, ..., 4, are constants. Show that X has spectral density function

oF idy)2
fx) = — fr lGale”)|
oX

where fy is the spectral density function of Y, oY = var(X;), and Ga(z) = >“; agzk.

Calculate this spectral density explicitly in the case of ‘exponential smoothing’, when r = oo,
ay = we —p),and0O<yp <1.

3. Suppose that Yast = aY, + BY,_1 is the best linear predictor of Y,4 1 given the entire past
Yn, Yn—1,--- Of the stationary sequence {Y; : —oo < k < oo}. Find the spectral density function of
the sequence.

4. Recurrent events (5.2.15). Meteorites fall from the sky at integer times 7), T>,... where T, =
X,+Xo+---+Xpy. We assume that the X; are independent, X2, X3,... are identically distributed,
and the distribution of X 1 is such that the probability that a meteorite falls at time n is constant for
all n. Let Y, be the indicator function of the event that a meteorite falls at time n. Show that {Y,} is
stationary and find its spectral density function in terms of the characteristic function of X>.

5. Let X = {X, :n > 1} be given by X, = cos(nU) where U is uniformly distributed on [—z, zr].
Show that X is stationary but not strongly stationary. Find the autocorrelation function of X and its
spectral density function.

6. (a) Let N be a Poisson process with intensity 4, and leta > 0. Define X(t) = N(t +a) — N(t)
for t > 0. Show that X is strongly stationary, and find its spectral density function.

(b) Let W be a Wiener process and define X = {X(t) : t > 1} by X(@1) = W(t) — We — 1).
Show that X is strongly stationary and find its autocovariance function. Find the spectral density
function of X.

7. Let Z1, Z2,... be uncorrelated variables, each with zero mean and unit variance.

(a) Define the moving average process X by X, = Zp +a@Z,_1 where @ is a constant. Find the
spectral density function of X.

104

Problems Exercises [9.7.8]-[9.7.16]

(b) More generally, let Yn = )~}-9 @jZn—i, Where ag = 1 and aj,..., a, are constants. Find the
spectral density function of Y.

8. Show that the complex-valued stationary process X = {X(t) : —co < t < oo} has a spectral
density function which is bounded and uniformly continuous whenever its autocorrelation function o
is continuous and satisfies oe je(t)| dt < co.

9, Let X = {X, : n > 1} be stationary with constant mean w = E(X,) for all n, and such that
cov(Xo, Xn) > Oasn —> oo. Show that n—! Yj=1 Xj mS, [L.

10. Deduce the strong law of large numbers from an appropriate ergodic theorem.

11, Let Q be a stationary measure on (R’ , BT) where T = {1, 2,...}. Show that Q is ergodic if and
only if

1 n
- > Y; > E(Y) a.s. and in mean
n

i=1

for all Y : R’? — R for which E(Y) exists, where Y; : R? = Ris given by Y;(x) = Y(ri-! (x)). As
usual, t is the natural shift operator on R?.

12. The stationary measure Q on (R? , BT) is called strongly mixing if Q(AN t~" B) > Q(A)Q(B)
asin — oo, forall A, B € B7; as usual, T = {1,2,...} and t is the shift operator on R?. Show that
every strongly mixing measure is ergodic.

13. Ergodic theorem. Let (Q, ¥, P) be a probability space, and let T : 82 — Q be measurable and
measure preserving (i.e., P(T~!A) = P(A) for all A € F). Let X : Q — R be a random variable,
and let X; be given by X;(@) = x(ri-} (w)). Show that

1 n
- >> Xi > EX | £) a.s. and in mean
n

i=]

where £ is the o-field of invariant events of T.
If T is ergodic (in that P(A) equals 0 or 1 whenever A is invariant), prove that E(X | £) = E(X)
almost surely.

14. Consider the probability space (Q, ¥, P) where Q = [0, 1), Fis the set of Borel subsets, and P
is Lebesgue measure. Show that the shift T : Q — Q defined by T(x) = 2x (mod 1) is measurable,
measure preserving, and ergodic (in that P(A) equals 0 or 1 if A = T —TA),

Let X : 2 — R be the random variable given by the identity mapping X(w) = w. Show that the

proportion of 1’s, in the expansion of X to base 2, equals 5 almost surely. This is sometimes called

‘Borel’s normal number theorem’.

15. Let g : R — R be periodic with period 1, and uniformly continuous and integrable over [0, 1].
Define Z, = g(X +(n- Da), n > 1, where X is uniform on [0, 1] and a is irrational. Show that,

asn — oo,
i 1
af g(u)du a.s.
no 0

16. Let X = {X(t) : t > 0} be a non-decreasing random process such that:

(a) X(0) = 0, X takes values in the non-negative integers,

(b) X has stationary independent increments,

(c) the sample paths {X(t, w) : t > 0} have only jump discontinuities of unit magnitude.
Show that X is a Poisson process.

105

[9.7.17]-[9.7.22] Exercises Stationary processes

17. Let X be a continuous-time process. Show that:

(a) if X has stationary increments and m(t) = E(X(t)) is a continuous function of t, then there exist
a and f such that m(t) =a + ft,

(b) if X has stationary independent increments and v(t) = var(X (t) — X (0)) is a continuous function
of t then there exists 0” such that var(X(s +1) — X(s)) = 07t for alls.

18. A Wiener process W is called standard if W(0) = 0 and W(1) has unit variance. Let W be a

standard Wiener process, and let w be a positive constant. Show that:

(a) aW(t/ o:”) is a standard Wiener process,

(b) W(t +a) — W(q) is a standard Wiener process,

(c) the process V, given by V(t) = tW(1/t) fort > 0, V(O) = 0, is a standard Wiener process,

(d) the process W(1) — W(1 — 1) is a standard Wiener process on [0, 1].

19, Let W be a standard Wiener process. Show that the stochastic integrals

f t
x= [ dW(u), r= f e "aw,  t>0,
0 0

are well defined, and prove that X(t) = W(t), andthat Y has autocovariance functioncov(Y (s), Y(¢t)) =
3 (e784 —e 5) s <t.
20. Let W be a standard Wiener process. Find the means of the following processes, and the autoco-
variance functions in cases (b) and (c):
(a) XO =|WE)I,
b) Yt) = e¥,
(c) Z(t) = fg Ww) du.
Which of these are Gaussian processes? Which of these are Markov processes?
21. Let W be a standard Wiener process. Find the conditional joint density function of W(t) and
W (t3) given that W(t;) = W(t4) = 0, where t) < fo < 13 < 14.
Show that the conditional correlation of W (tz) and W (t3) is

p= (t4 — t3)(t2 — th)
(tq — t2)(t3 — 1)
22. Empirical distribution function. Let U,, U2,... be independent random variables with the
uniform distribution on [0, 1]. Let 7; («) be the indicator function of the event {U; < x}, and define

1 n
Fa(x) = ~ Ue), O<x<1.
mH

The function Fy, is called the ‘empirical distribution function’ of the U;.

(a) Find the mean and variance of F,, (x), and prove that ./n(F, (x) — x) 2, Y(x) asn — oo, where
Y (x) is normally distributed.

(b) What is the (multivariate) limit distribution of a collection of random variables of the form
{./n(Fr(x;) — xj): 1 <i < kj}, where O < xy < x9 < +--+ < xy <1?

(c) Show that the autocovariance function of the asymptotic finite-dimensional distributions of
/n(Fy(x) — x), in the limit as n > oo, is the same as that of the process Z(t) = W(t) -¢W(1),
0 <+ < 1, where W is a standard Wiener process. The process Z is called a ‘Brownian bridge’
or ‘tied-down Brownian motion’.

106

10

Renewals

In the absence of indications to the contrary, {X, :n > 1} denotes the sequence of interarrival times
of either a renewal process N or a delayed renewal process N’ qd” In either case, F4 and F are the
distribution functions of X; and X> respectively, though F4 # F only if the renewal process is
delayed. We write 44 = E(X2), and shall usually assume that 0 < yz < oo. The functions m and m
denote the renewal functions of N and N4. We write T, = S<#_, X;, the time of the nth arrival.

10.1 Exercises. The renewal equation

1. Prove that E(e2%©) < 00 for some strictly positive 9 whenever E(X,) > 0. [Hint: Consider
the renewal process with interarrival times X k = €1{x,>e} for some suitable €.]

2. Let N be a renewal process and let W be the waiting time until the length of some interarrival
time has exceeded s. That is, W = inf{t : C(t) > s}, where C(t) is the time which has elapsed (at
time 1) since the last arrival. Show that

F w={t ifx <s,
WO 1 = Fs) + [8 Fw —u)dFu) ifx>s,

where F is the distribution function of an interarrival time. If N is a Poisson process with intensity 4,

show that
A-86

ow, _
ee) = Fa Ge0Ws

ford <A,
and E(W) = (e*5 — 1)/A. You may find it useful to rewrite the above integral equation in the form of
a renewal-type equation.

3. Find an expression for the mass function of N(f) in a renewal process whose interarrival times
are: (a) Poisson distributed with parameter 1, (b) gamma distributed, ['(A, b).

4. Let the times between the events of a renewal process N be uniformly distributed on (0, 1). Find
the mean and variance of N(t) forO <1 < 1.

10.2 Exercises. Limit theorems

1. Planes land at Heathrow airport at the times of a renewal process with interarrival time distribution
function F. Each plane contains a random number of people with a given common distribution and
finite mean. Assuming as much independence as usual, find an expression for the rate of arrival of
passengers over a long time period.

2. Let Z1, Z2,... be independent identically distributed random variables with mean 0 and finite
variance o?, and let Ty = "_, Z;. Let M be a finite stopping time with respect to the Z; such that
E(M) < oo. Show that var(Ty) = E(M)o?.

107

(10.2.3]-[10.4.1] Exercises Renewals

3. Show that E(Ty(1)44) = w(m(t)+4) for all k > 1, but that it is not generally true that E(Ty(r)) =
pm(t).

4, Show that, using the usual notation, the family {N(t)/t : 0 < t < oo} is uniformly integrable.
How might one make use of this observation?

5. Consider a renewal process N having interarrival times with moment generating function M, and
let T be a positive random variable which is independent of N. Find E(s%@)) when:

(a) T is exponentially distributed with parameter v,

(b) N is a Poisson process with intensity 4, in terms of the moment generating function of T. What
is the distribution of N(T) in this case, if T has the gamma distribution T'(v, b)?

10.3 Exercises. Excess life

1. Suppose that the distribution of the excess lifetime E(t) does not depend on t. Show that the
renewal process is a Poisson process.

2. Show that the current and excess lifetime processes, C(t) and E(t), are Markov processes.

3. Suppose that X; is non-arithmetic with finite mean pj.
(a) Show that E(t) converges in distribution as t > oo, the limit distribution function being

* 1
Ha) = [ “0 Fopiay.
0
(b) Show that the rth moment of this limit distribution is given by
x’ dH(x) = ———~,,
ih wrth)
assuming that this is finite.
(c) Show that
t
E(E(t)") = E({(X1 1) Ty) + | h(t —x)dm(x)
0
for some suitable function h to be found, and deduce by the key renewal theorem that E(E(1)") >
EXT) / (ur + 1)} as t > oo, assuming this limit is finite.

4. Find an expression for the mean value of the excess lifetime E(t) conditional on the event that
the current lifetime C(t) equals x.

5. Let M(t) = N(t) +1, and suppose that Xj has finite non-zero variance o?.
(a) Show that var(Tya) — uM(t)) = o7(m(@) + DD.

(b) In the non-arithmetic case, show that var(M (t))/t > a2 / we ast — 00.

10.4 Exercise. Applications

1. Find the distribution of the excess lifetime for a renewal process each of whose interarrival times is
the sum of two independent exponentially distributed random variables having respective parameters
A and yz. Show that the excess lifetime has mean

1 Ae TOtHMEL

— + ey

ib AA +B)

108

Problems Exercises [10.5.1]-[10.5.6]

10.5 Exercises. Renewal-reward processes

1. If X(@) is an irreducible persistent non-null Markov chain, and u(-) is a bounded function on the
integers, show that

1 t
-f u(X(s)) ds = S~ nul),
F JO ieS

where z is the stationary distribution of X(t).

2. Let M(t) be an alternating renewal process, with interarrival pairs {X,, Y, : r > 1}. Show that

1 EX,

t
“| TM (s) is even} 28 me ——————-_ aS t > OO.
t JO EX, + EY,

3. Let C(s) be the current lifetime (or age) of a renewal process N(t) with a typical interarrival time
X. Show that

1 ft E(x?
‘ff C(s) ds 2s ma as t > 00.

Find the corresponding limit for the excess lifetime.

4. Let j and k be distinct states of an irreducible discrete-time Markov chain X with stationary
distribution 2. Show that

1/7
Ej | Xo =k) + EC | Xo = J)

PT) < Te | Xo =) =

where 7; = min{n > 1: X, = i} is the first passage time to the state i. [Hint: Consider the times of
return to j having made an intermediate visit to k.]

10.5 Problems

1. (a) Show that P(N(t) > coast + oo) = 1.
(b) Show that m(t) < oo if uw £0.
(c) More generally show that, for all k > 0, E(N (ty )< ooifu £0.

2. Let v(t) = E(N(t)?). Show that
t
v(t) = mt) + 2 | m(t — s)dm(s).
0
Find v(t) when N is a Poisson process.
3. Suppose that o? = var(X 1) > 0. Show that the renewal process N satisfies

N(t)— (t/u) D
Vore Xe 1), ast > oo.

4. Find the asymptotic distribution of the current life C(t) of N ast + oo when X is not arithmetic.

5. Let N bea Poisson process with intensity A. Show that the total life D(¢) at time t has distribution
function P(D(t) < x) = 1—(1+Amin{t, x})e~** for x > 0. Deduce that E(D(t)) = (2—e7*4)//a.

6. A Type 1 counter records the arrivals of radioactive particles. Suppose that the arrival process
is Poisson with intensity 4, and that the counter is locked for a dead period of fixed length T after

109

[10.5.7]-[10.5.12] Exercises Renewals

each detected arrival. Show that the detection process N is a renewal process with interarrival time
distribution F(x) = 1 — e~*@—) if x > T. Find an expression for P(N (t) > k).

7. Particles arrive at a Type 1 counter in the manner of a renewal process N; each detected arrival
locks the counter for a dead period of random positive length. Show that

P(X, <x) = [ [1 — F(x — y) FL) dm(y)

where F7, is the distribution function of a typical dead period.

8. (a) Show that m(t) = Sat - ae — eat ) if the interarrival times have the gamma distribution
Pa, 2).

(b) Radioactive particles arrive like a Poisson process, intensity A, at a counter. The counter fails to
register the nth arrival whenever n is odd but suffers no dead periods. Find the renewal function

m of the detection process N.

9. Show that Poisson processes are the only renewal processes with non-arithmetic interarrival times
having the property that the excess lifetime E(t) and the current lifetime C(t) are independent for
each choice of t.

10. Let N; be a Poisson process, and let N2 be a renewal process which is independent of Nj with
non-arithmetic interarrival times having finite mean. Show that N(t) = Nj (t) + No(t) is a renewal
process if and only if Nz is a Poisson process.

11. Let N be a renewal process, and suppose that F is non-arithmetic and that 02 = var(X 1) < oo.
Use the properties of the moment generating function F*(—@) of X1 to deduce the formal expansion

1 2_ ,,2
m0) = 5 +a FO as 0 —> 0.

Invert this Laplace—Stieltjes transform formally to obtain

2 2

t —
m(t) =~ +7 —* + 0(1) as t > 00.
i 2b

Prove this rigorously by showing that
t t
m() =< — Fe) + [ UL Fete — same)

where Fg is the asymptotic distribution function of the excess lifetime (see Exercise (10.3.3)), and
applying the key renewal theorem. Compare the result with the renewal theorems.

12. Show that the renewal function m4 of a delayed renewal process satisfies
t
m4(t) = F4(t) + | m4(t — x) d F(x).
0
Show that v4(r) = E(N4(t)*) satisfies
t
vit) = m4(t) + 2 | m(t — x) dm(x)
0

where m is the renewal function of the renewal process with interarrival times X2, X3,....

110

Problems Exercises [10.5.13]-[10.5.19]

13. Let m(t) be the mean number of living individuals at time t in an age-dependent branching process
with exponential lifetimes, parameter A, and mean family size v (> 1). Prove that m(t) = I e—Dat
where J is the number of initial members.

14. Alternating renewal process. The interarrival times of this process are Zg, Yj, Z1, Yo,...,
where the ¥; and Z; are independent with respective common moment generating functions My and
Mz. Let p(t) be the probability that the epoch ¢ of time lies in an interval of type Z. Show that the
Laplace-Stieltjes transform p* of p satisfies

1— Mz(-6)
— My(—0)Mz(—8)

PO=5

15. Type 2 counters. Particles are detected by a Type 2 counter of the following sort. The incoming
particles constitute a Poisson process with intensity A. The jth particle locks the counter for a length
Y; of time, and annuls any after-effect of its predecessors. Suppose that Y;, Y2, ... are independent of
each other and of the Poisson process, each having distribution function G. The counter is unlocked
at time 0.

Let L be the (maximal) length of the first interval of time during which the counter is locked.
Show that H(t) = P(L > 1) satisfies

t
HQ) =e“[1- GQ] +f Hi —x){l— G(x)Jae7* dx.

Solve for H in terms of G, and evaluate the ensuing expression in the case G(x) = 1 — e~¥* where
p> 0.

16. Thinning. Consider a renewal process N, and suppose that each arrival is ‘overlooked’ with
probability g, independently of all other arrivals. Let M(t) be the number of arrivals which are
detected up to time t/p where p = 1 — q.

(a) Show that M is a renewal process whose interarrival time distribution function Fp is given by
Fy(x) = ean pq —IF. (x/p), where F,, is the distribution function of the time of the nth
arrival in the original process N.

(b) Find the characteristic function of F, in terms of that of F, and use the continuity theorem to
show that, as p | 0, Fp(s) > 1- e~5/¥ for s > 0,80 long as the interarrival times in the original
process have finite mean jz. Interpret!

(c) Suppose that p < 1, and M and N are processes with the same fdds. Show that N is a Poisson
process.

17. (a) A PC keyboard has 100 different keys and a monkey is tapping them (uniformly) at random.
Assuming no power failure, use the elementary renewal theorem to find the expected number of
keys tapped until the first appearance of the sequence of fourteen characters ‘W. Shakespeare’.
Answer the same question for the sequence ‘omo’.

(b) Acoin comes up heads with probability p on each toss. Find the mean number of tosses until the
first appearances of the sequences (i) HHH, and (ii) HTH.

18. Let N be a stationary renewal process. Let s be a fixed positive real number, and define X(t) =
N(s +t) — N(t) for t = 0. Show that X is a strongly stationary process.

19. Bears arrive in a village at the instants of a renewal process; they are captured and confined at a
cost of $c per unit time per bear. When a given number B bears have been captured, an expedition
(costing $d) is organized to remove and release them a long way away. What is the long-run average
cost of this policy?

111

11

Queues

11.2 Exercises. M/M/1

1, Consider a random walk on the non-negative integers with a reflecting barrier at 0, and which
moves rightwards or leftwards with respective probabilities o/(1 + 9) and 1/(1 + e); when at 0, the
particle moves to 1 at the next step. Show that the walk has a stationary distribution if and only if
p < 1, and in this case the unique such distribution z is given by 7p = 4 (1— ?),%, = 4 (1—p?)p" 7!
forn > 1.

2. Suppose now that the random walker of Exercise (1) delays its steps in the following way. When
at the point n, it waits a random length of time having the exponential distribution with parameter
6, before moving to its next position; different ‘holding times’ are independent of each other and of
further information concerning the steps of the walk. Show that, subject to reasonable assumptions
on the @,, the ensuing continuous-time process settles into an equilibrium distribution v given by
Vn = Crp /Oy, for some appropriate constant C.

By applying this result to the case when 69 = A, 6, = A+ forn > 1, deduce that the equilibrium
distribution of the M(A)/M(z)/1 queue is vz = (1 — e)p”, n > 0, where p = A/p < 1.

3. Waiting time. Consider a M(A)/M(jz)/1 queue with o = 1/, satisfying p < 1, and suppose that
the number Q(0) of people in the queue at time 0 has the stationary distribution x, = (1 — p)p”,
n > 0. Let W be the time spent by a typical new arrival before he begins his service. Show that the
distribution of W is given by P(W < x) = 1— pe *#-A) for x > 0, and note that P(W = 0) = 1—p.

4. A box contains i red balls and j lemon balls, and they are drawn at random without replacement.
Each time a red (respectively lemon) ball is drawn, a particle doing a walk on {0, 1,2, ...} moves
one step to the right (respectively left); the origin is a retaining barrier, so that leftwards steps from
the origin are suppressed. Let (n; i, 7) be the probability that the particle ends at position n, having
started at the origin. Write down a set of difference equations for the (n; i, j), and deduce that

u(n;i, jf) =AM;i,j)-Amt+lij) fori<jtna

where A(n;i, j) = ()/ CP").

n

5. Let Q be a M(A)/M(2)/1 queue with Q(0) = 0. Show that p,(t) = P(Q(t) = n) satisfies

x iat Jott
pat) = So mnzi,f) (‘ a (‘“

1,j20

where the m(n; i, j) are given in the previous exercise.

6. Let Q(t) be the length of an M(A)/M(yz)/1 queue at time t, and let Z = {Z,} be the jump chain
of Q. Explain how the stationary distribution of Q@ may be derived from that of Z, and vice versa.

112

G/G/1 Exercises [11.2.7]-[11.5.2]

7. Tandem queues. Two queues have one server each, and all service times are independent and

exponentially distributed, with parameter 4; for queue i. Customers arrive at the first queue at the

instants of a Poisson process of rate A (< min{jz1, 22}), and on completing service immediately enter

the second queue. The queues are in equilibrium. Show that:

(a) the output of the first queue is a Poisson process with intensity 4, and that the departures before
time f are independent of the length of the queue at time f,

(b) the waiting times of a given customer in the two queues are not independent.

11.3 Exercises. M/G/1

1. Consider M(A)/D(d)/1 where o = Ad < 1. Show that the mean queue length at moments of
departure in equilibrium is 5p(2 — p)/A—-p).

2. Consider M()/M(j2)/1, and show that the moment generating function of a typical busy period
is given by

_ Atn-s)-VA+p—5% —4dy
2r

for all sufficiently small but positive values of s.

3. Show that, for a M/G/1 queue, the sequence of times at which the server passes from being busy
to being free constitutes a renewal process.

11.4 Exercises. G/M/1

1, Consider G/M(j)/1, and let aj = E((ux)4 e BX /j'!) where X is a typical interarrival time.
Suppose the traffic intensity p is less than 1. Show that the equilibrium distribution w of the imbedded
chain at moments of arrivals satisfies

foe)
In = So ain si-1 forn > 1.
=

Look for a solution of the form 2, = 6” for some @, and deduce that the unique stationary distribution
is given by 2; = (1 — 7)n/ for j > 0, where 7 is the smallest positive root of the equation s =
Mx (us — 1)).

2. Consider a G/M(;z)/1 queue in equilibrium. Let 7 be the smallest positive root of the equation
x = Mx(u(x — 1)) where My is the moment generating function of an interarrival time. Show that
the mean number of customers ahead of a new arrival is n(1 — n)7?, and the mean waiting time is
mud — nyt.

3. Consider D(1)/M(j)/1 where jz > 1. Show that the continuous-time queue length Q(r) does not
converge in distribution as t — oo, even though the imbedded chain at the times of arrivals is ergodic.

11.5 Exercises. G/G/1

1. Show that, for a G/G/1 queue, the starting times of the busy periods of the server constitute a
renewal process.

2. Consider a G/M(j)/1 queue in equilibrium, together with the dual (unstable) M(jz)/G/1 queue.
Show that the idle periods of the latter queue are exponentially distributed. Use the theory of duality

113

[11.5.3]-[11.7.5] Exercises Queues

of queues to deduce for the former queue that: (a) the waiting-time distribution is a mixture of an
exponential distribution and an atom at zero, and (b) the equilibrium queue length is geometric.

3. Consider G/M(w)/1, and let G be the distribution function of S — X where S and X are typical
(independent) service and interarrival times. Show that the Wiener—-Hopf equation

F(x) = [ F(x—y)dG(y), x >0,

—w

for the limiting waiting-time distribution F is satisfied by F(x) = 1 — ne~#('-™)*, x > 0. Here, 7
is the smallest positive root of the equation x = My(y(x — 1)), where My is the moment generating
function of X.

11.6 Exercise. Heavy traffic

1. Consider the M(A)/M(j2)/1 queue with p = A/u < 1. Let Qp be a random variable with the
equilibrium queue distribution, and show that (1 — ¢)Q» converges in distribution as p ¢ 1, the limit
distribution being exponential with parameter 1.

11.7 Exercises. Networks of queues

1. Consider an open migration process with c stations, in which individuals arrive at station j at
rate v;, individuals move from i to j at rate 1;;;(n;), and individuals depart from i at rate 42;;(7;),
where n; denotes the number of individuals currently at station i. Show when ¢;(n;) = n; for all i
that the system behaves as though the customers move independently through the network. Identify
the explicit form of the stationary distribution, subject to an assumption of irreducibility, and explain
a connection with the Bartlett theorem of Problem (8.7.6).

2. Let Q be an M(A)/M(j2)/s queue where A < sj, and assume Q is in equilibrium. Show that
the process of departures is a Poisson process with intensity 4, and that departures up to time f are
independent of the value of Q(t).

3. Customers arrive in the manner of a Poisson process with intensity A in a shop having two servers.
The service times of these servers are independent and exponentially distributed with respective
parameters jt; and 42. Arriving customers form a single queue, and the person at the head of the
queue moves to the first free server. When both servers are free, the next arrival is allocated a server
chosen according to one of the following rules:

(a) each server is equally likely to be chosen,

(b) the server who has been free longer is chosen.

Assume that A < zy + j42, and the process is in equilibrium. Show in each case that the process of
departures from the shop is a Poisson process, and that departures prior to time t are independent of
the number of people in the shop at time ft.

4. Difficult customers. Consider an M(A)/M(jz)/1 queue modified so that on completion of service
the customer leaves with probability 5, or rejoins the queue with probability 1-3. Find the distribution
of the total time a customer spends being served. Hence show that equilibrium is possible if 4 < éu,
and find the stationary distribution. Show that, in equilibrium, the departure process is Poisson, but if
the rejoining customer goes to the end of the queue, the composite arrival process is not Poisson.

5. Consider an open migration process in equilibrium. If there is no path by which an individual
at station k can reach station j, show that the stream of individuals moving directly from station j to
station k forms a Poisson process.

114
Problems Exercises [11.8.1]-[11.8.7]

11.8 Problems

1. Finite waiting room. Consider M(A)/M(j1)/k with the constraint that arriving customers who
see N customers in the line ahead of them leave and never return. Find the stationary distribution of
queue length for the cases k = 1 and k = 2.

2. Baulking. Consider M(A)/M(j2)/1 with the constraint that if an arriving customer sees n customers

in the line ahead of him, he joins the queue with probability p(n) and otherwise leaves in disgust.

(a) Find the stationary distribution of queue length if p(m) = (vn + 17),

(b) Find the stationary distribution 2 of queue length if p(n) = 2~”, and show that the probability
that an arriving customer joins the queue (in equilibrium) is (1 — 7)/A.

3. Series. Ina Moscow supermarket customers queue at the cash desk to pay for the goods they want;
then they proceed to a second line where they wait for the goods in question. If customers arrive in the
shop like a Poisson process with parameter A and all service times are independent and exponentially
distributed, parameter jz; at the first desk and j2 at the second, find the stationary distributions of
queue lengths, when they exist, and show that, at any given time, the two queue lengths are independent
in equilibrium.

4. Batch (or bulk) service. Consider M/G/1, with the modification that the server may serve up to m
customers simultaneously. If the queue length is less than m at the beginning of a service period then she
serves everybody waiting at that time. Find a formula which is satisfied by the probability generating
function of the stationary distribution of queue length at the times of departures, and evaluate this
generating function explicitly in the case when m = 2 and service times are exponentially distributed.

5. Consider M(A)/M(j1)/1 where 4 < ys. Find the moment generating function of the length B of a

typical busy period, and show that E(B) = (yu — aw} and var(B) = (A+ p)/(u —- )3. Show that
the density function of B is

af [A
fp@) = EIR Ow (2x/Au) forx > 0

where J is a modified Bessel function.

6. Consider M(A)/G/1 in equilibrium. Obtain an expression for the mean queue length at departure
times. Show that the mean waiting time in equilibrium of an arriving customer is } AE(S?) /U—-p)
where S is a typical service time and p = AE(S).

Amongst all possible service-time distributions with given mean, find the one for which the mean
waiting time is a minimum.
7. Let W; be the time which a customer would have to wait ina M(A)/G/1 queue if he were to arrive
at time t. Show that the distribution function F(x; t) = P(W; < x) satisfies

OF OF
at ax
where S is a typical service time, independent of W;.

Suppose that F(x, t) > H(x) for all x as t > ov, where Z is a distribution function satisfying
0=h-\AH+AP(U +S <x) for x > 0, where U is independent of S with distribution function H,
and h is the density function of H on (0, 00). Show that the moment generating function My of U
satisfies

(1 — p)é

Mu (0) = 63M)

where p is the traffic intensity. You may assume that P(S = 0) = 0.

115

[11.8.8]-[11.8.14] Exercises Queues

8. Consider a G/G/1 queue in which the service times are constantly equal to 2, whilst the interarrival
times take either of the values 1 and 4 with equal probability 5 Find the limiting waiting time
distribution.

9. Consider an extremely idealized model of a telephone exchange having infinitely many channels
available. Calls arrive in the manner of a Poisson process with intensity 4, and each requires one
channel for a length of time having the exponential distribution with parameter jz, independently of
the arrival process and of the duration of other calls. Let Q(7) be the number of calls being handled
at time f, and suppose that Q(0) = [.

Determine the probability generating function of Q(t), and deduce E(Q(t)), P(Q(t) = 0), and
the limiting distribution of Q(t) as t > oc.

Assuming the queue is in equilibrium, find the proportion of time that no channels are occupied,
and the mean length of an idle period. Deduce that the mean length of a busy period is (e4/# — 1) /d.

10. Customers arrive in a shop in the manner of a Poisson process with intensity A, where 0 < A < 1.
They are served one by one in the order of their arrival, and each requires a service time of unit length.
Let Q(t) be the number in the queue at time t. By comparing Q(t) with Q(t + 1), determine the
limiting distribution of Q(t) as t — oo (you may assume that the quantities in question converge).
Hence show that the mean queue length in equilibrium is A(1 — 2) /(A—-A).

Let W be the waiting time of a newly arrived customer when the queue is in equilibrium. Deduce
from the results above that E(W) = 40/0 — ).

11. Consider M(A)/D(1)/1, and suppose that the queue is empty at time 0. Let T be the earliest time

at which a customer departs leaving the queue empty. Show that the moment generating function Mr
of T satisfies

s
log (1—~) + log Mr(s) = (s -4)(1- Mr(s)),
and deduce the mean value of 7, distinguishing between the cases A < 1 andA > 1.

12. Suppose A < ys, and consider a M(A)/M(jz)/1 queue Q in equilibrium.

(a) Show that Q is a reversible Markov chain.

(b) Deduce the equilibrium distributions of queue length and waiting time.

(c) Show that the times of departures of customers form a Poisson process, and that Q(f) is indepen-
dent of the times of departures prior to f.

(d) Consider a sequence of K single-server queues such that customers arrive at the first in the manner
of a Poisson process, and (for each j) on completing service in the jth queue each customer moves
to the (j + 1th. Service times in the jth queue are exponentially distributed with parameter j2;,
with as much independence as usual. Determine the (joint) equilibrium distribution of the queue
lengths, when A < j4; for all j.

13. Consider the queue M(A)/M(j)/k, where k > 1. Show that a stationary distribution 7 exists if
and only if A < ky, and calculate it in this case.
Suppose that the cost of operating this system in equilibrium is

CO
Ak +BY \(n—k+ Um,

n=k

the positive constants A and B representing respectively the costs of employing a server and of the
dissatisfaction of delayed customers.

Show that, for fixed jz, there is a unique value A* in the interval (0, jz) such that it is cheaper to
have k = 1 than k = 2 if and only if A < A*.

14. Customers arrive in a shop in the manner of a Poisson process with intensity 4. They form a
single queue. There are two servers, labelled 1 and 2, server i requiring an exponentially distributed

116
Problems Exercises [11.8.15]-[11.8.19]

time with parameter j1; to serve any given customer. The customer at the head of the queue is served
by the first idle server; when both are idle, an arriving customer is equally likely to choose either.

(a) Show that the queue length settles into equilibrium if and only if 4 < 1 + p42.

(b) Show that, when in equilibrium, the queue length is a time-reversible Markov chain.

(c) Deduce the equilibrium distribution of queue length.

(d) Generalize your conclusions to queues with many servers.

15. Consider the D(1)/M(jz)/1 queue where 4 > 1, and let Q» be the number of people in the
queue just before the nth arrival. Let Q,, be a random variable having as distribution the stationary
distribution of the Markov chain {Q,}. Show that (1 — po!) QO, converges in distribution as yz | 1,
the limit distribution being exponential with parameter 2.

16. Taxis arrive at a stand in the manner of a Poisson process with intensity t, and passengers
arrive in the manner of an (independent) Poisson process with intensity 7. If there are no waiting
passengers, the taxis wait until passengers arrive, and then move off with the passengers, one to each
taxi. If there is no taxi, passengers wait until they arrive. Suppose that initially there are neither
taxis nor passengers at the stand. Show that the probability that n passengers are waiting at time ¢ is

1
(x /t)2"e— + 7, (24/1), where In (x) is the modified Bessel function, i.e., the coefficient of z”
in the power series expansion of exp(4x(z +27).

17. Machines arrive for repair as a Poisson process with intensity 4. Each repair involves two stages,
the ith machine to arrive being under repair for a time X; + Y;, where the pairs (X;, ¥;),i = 1,2,...,
are independent with a common joint distribution. Let U(t) and V(t) be the numbers of machines in
the X-stage and Y-stage of repair at time t. Show that U(t) and V(t) are independent Poisson random
variables.

18. Ruin. An insurance company pays independent and identically distributed claims {Ky :n > 1}
at the instants of a Poisson process with intensity 4, where AE(K,) < 1. Premiums are received at
constant rate 1. Show that the maximum deficit M the company will ever accumulate has moment
generating function
1—- p)0
E(e®™) = _ =p)
A+6—AE(ePX)

19. (a) Erlang’s loss formula. Consider M(1.)/M(j)/s with baulking, in which a customer departs
immediately if, on arrival, he sees all the servers occupied ahead of him. Show that, in equilibrium,
the probability that all servers are occupied is

Sig!
Ns = els where p = A/wU.
j=—0 ps /j!
(b) Consider an M(A)/M(2)/co queue with channels (servers) numbered 1,2,.... On arrival, a

customer will choose the lowest numbered channel that is free, and be served by that channel. Show
in the notation of part (a) that the fraction p, of time that channel c is busy is pe = p(%e_1 — Hc) for
c > 2, and py = 7}.

117
12
Martingales

12.1 Exercises. Introduction

1. G)If (Y, F) is a martingale, show that E(Y,) = E(Yp) for all n.

Gi) If (Y, ¥) is a submartingale (respectively supermartingale) with finite means, show that E(Y,,) >
E(Yo) (respectively E(Y,) < E(Y)).

2. Let (Y, F) be a martingale, and show that E(Y;+m | Fn) = Yn for alln,m > 0.

3. Let Zy, be the size of the nth generation of a branching process with Zp = 1, having mean family
size yz and extinction probability 7. Show that Z,4—” and n2" define martingales.

4. Let {S, : n > 0} be a simple symmetric random walk on the integers with Sg = k. Show that Sy
and Ss? —nare martingales. Making assumptions similar to those of de Moivre (see Example (12.1.4)),
find the probability of ruin and the expected duration of the game for the gambler’s ruin problem.

5. Let (Y, ¥) be a martingale with the property that E(¥2) < 00 for all n. Show that, fori < j <k,
E{(Ye — ¥;)¥;} = 0, and E{(¥%, — ¥j)? | Fi} = EYP | F) - E(Y? | #;). Suppose there exists K
such that E(Y2) < K for all n. Show that the sequence {Y,,} converges in mean square as n — co.

6. Let Y be a martingale and let u be a convex function mapping R to R. Show that {u(Y,) : n > 0}
is a submartingale provided that E(u(Y,)*) < oo for all n.

Show that |Y,|, Y?, and Y,* constitute submartingales whenever the appropriate moment condi-
tions are satisfied.

7. Let Y be a submartingale and let u be a convex non-decreasing function mapping R to R. Show
that {u(Y,) : n > O} is a submartingale provided that E(u(Y,)+) < oo for all n.

Show that (subject to a moment condition) Y,t constitutes a submartingale, but that |Y,| and y2
need not constitute submartingales.

8. Let X be a discrete-time Markov chain with countable state space S and transition matrix P.
Suppose that y : S > R is bounded and satisfies Dijes Pij>W(A) < AY@ for some A > 0 and all

i € S. Show that A7~"y(X,,) constitutes a supermartingale.

9, Let G(s) be the probability generating function of the size Z, of the nth generation of a branching
process, where Zo = 1 and var(Z,) > 0. Let H, be the inverse function of the function Gy, viewed
as a function on the interval [0, 1], and show that M, = {Hy (s)}2" defines a martingale with respect
to the sequence Z.

118

Crossings and convergence Exercises [12.2.1]-[12.3.4]

12.2 Exercises. Martingale differences and Hoeffding’s inequality

1. Knapsack problem. It is required to pack a knapsack to maximum benefit. Suppose you have n
objects, the ith object having volume V; and worth W;, where V;, V2,..., Vn, Wy, Wo. ..., Wn are
independent non-negative random variables with finite means, and W; < M for all i and some fixed
M. Your knapsack has volume c, and you wish to maximize the total worth of the objects packed in
it. That is, you wish to find the vector 21, z2,..., Zn of 0’s and 1’s such that }77 2;V; <c and which
maximizes S~Y 2; W,;. Let Z be the maximal possible worth of the knapsack’s contents, and show that

P(|Z —EZ| > x) < 2exp{—x?/(2nM2)} for x > 0.

2. Graph colouring. Given n vertices v1, v2,..., Un, for each 1 < i < j <n we place an edge
between v; and v; with probability p; different pairs are joined independently of each other. We call
v; and v; neighbours if they are joined by an edge. The chromatic number x of the ensuing graph is
the minimal number of pencils of different colours which are required in order that each vertex may
be coloured differently from each of its neighbours. Show that P(jx — Ex| > x) < 2exp{- 5x? jn}
for x > 0.

12.3 Exercises. Crossings and convergence

1, Give a reasonable definition of a downcrossing of the interval [a, b] by the random sequence

Yo, Y1,..--

(a) Show that the number of downcrossings differs from the number of upcrossings by at most 1.

(b) If (Y, F) is a submartingale, show that the number D,,(a, b; Y) of downcrossings of [a, b] by Y
up to time n satisfies

E{(¥n — b)*}

—a

EDy(a, b; Y) <

2. Let (Y, &) be a supermartingale with finite means, and let U,(a, b; Y) be the number of upcross-
ings of the interval [a, b] up to time n. Show that

E{Y¥n —a)7}

EU, (a, b; Y) <
b-a

Deduce that EU, (a, b; Y) < a/(b — a) if Y is non-negative and a > 0.

3. Let X be a Markov chain with countable state space S and transition matrix P. Suppose that X is
irreducible and persistent, and that yy : S > Sis abounded function satisfying }~ jeS Pij ¥G)<¥@
for i € S. Show that w is a constant function.

4. Let Z,, Zo, ... be independent random variables such that:

an with probability }n~?,

Zn=4 0 with probability 1 — n—?,
—ay with probability 5n~?,

where aj = 2 and ay, = 4 a;. Show that Y, = j=l Z; defines a martingale. Show that
Y = lim Y, exists almost surely, but that there exists no M such that E|Y,| < M for all xn.

119

[12.4.1]-[12.5.5] Exercises Martingales

12.4 Exercises. Stopping times

1. If 7, and 7> are stopping times with respect to a filtration ¥, show that T, + T), max{T,, 7},
and min{7,, 72} are stopping times also.

2. Let X 1, X2,... be a sequence of non-negative independent random variables and let N(t) =
max{n : X; + X2+---+ Xy < t}. Show that M(t) + 1 is a stopping time with respect to a suitable
filtration to be specified.

3. Let (Y, ¥) be a submartingale and x > 0. Show that

1
P( max Ym > x) < -E(Y,S).
x

O0<m<n

4. Let (Y, F) be a non-negative supermartingale and x > 0. Show that

1
P( max Yj, > x) < 5 Bo).

O<m<n

5. Let (Y, &) be a submartingale and let S and T be stopping times satisfying 0 < S < T < N for
some deterministic N. Show that EYp < EY; < EY; < EYy.

6. Let {S,} be a simple random walk with Sg = 0 such thatO < p = P(Sy = 1) < h. Use de
Moivre’s martingale to show that E(sup,, Sm) < p/(1 — 2p). Show further that this inequality may
be replaced by an equality.

7. Let Fbe a filtration. For any stopping time T with respect to F, denote by ¥7 the collection of
all events A such that, for alln, AN {T <n} € F,. Let S and T be stopping times.

(a) Show that #7 is a o-field, and that T is measurable with respect to this o-field.

(b) If A € Fs, show that AN {S < T} € Fr.

(c) Let S and T satisfy S$ < T. Show that 5 C Fr.

12.5 Exercises. Optional stopping

1. Let (Y, ¥) be a martingale and T a stopping time such that P(T < 00) = 1. Show that E(Yy) =
E(Yo) if either of the following holds:

(a) E(sup, |¥ranl) < 00,  (b) E(¥ran|'*8) < c for some c, 6 > 0 and all n.

2. Let (Y, F) be a martingale. Show that (Yr,,, Fy) is a uniformly integrable martingale for any
finite stopping time T such that either:

(a) Ej¥7| < co and E(¥ni7r>n}) > 0 asin > 0, or

(b) {Yn} is uniformly integrable.

3. Let (Y, ¥) be auniformly integrable martingale, and let S and T be finite stopping times satisfying

S < T. Prove that Yr = E(Yoo | Fr) and that Ys = E(Yr7 | Fs), where Yoo is the almost sure limit
asn — ooof ¥Yy.

4. Let {S, : n > 0} be a simple symmetric random walk with 0 < Sg < N and with absorbing
barriers at 0 and N. Use the optional stopping theorem to show that the mean time until absorption is
E{So(N — So)}.

5. Let {S, : 2 => 0} be a simple symmetric random walk with Sg = 0. Show that

_ cosfALSn — 36 ~ @)]}
n (cos A)”

120

Problems Exercises [12.5.6]-[12.9.2]

constitutes a martingale if cosA 4 0.

Let a and b be positive integers. Show that the time T until absorption at one of two absorbing
barriers at —a and b satisfies

_py _ cos{5a(b — a)}
B({eosa™") = cos{a(b +a)}' < Sora

6. Let {S, :n > 0} bea simple symmetric random walk on the positive and negative integers, with
So = 0. For each of the three following random variables, determine whether or not it is a stopping
time and find its mean:

U =min{n >5:S_,=Sp-5+5}, V=U-5, W=min{n: S, = 1).

7. LetS, =a+>°?_, Xr beasimple symmetric random walk. The walk stops at the earliest time T
when it reaches either of the two positions 0 or K whereO < a < K. Show that My, = 0 Sr—- 43
is a martingale and deduce that E(S~/_9 Sy) = 4(K? — a?)a +a.

8. Gambler’s ruin. Let X; be independent random variables each equally likely to take the values

+1, and let T = min{n : S, € {—a,b}}. Verify the conditions of the optional stopping theorem
(12.5.1) for the martingale Ss? — nand the stopping time T.

12.7 Exercises. Backward martingales and continuous-time martingales

1. Let X be a continuous-time Markov chain with finite state space S and generator G. Let 7 =
{n(@i) : i € S} be a root of the equation Gy’ = 0. Show that 4(X(t)) constitutes a martingale with
respect to F; = o ({X(u) : u < t}).

2. Let N be a Poisson process with intensity A and N(O) = 0, and let Tz = min{r : N(t) = a},
where a is a positive integer. Assuming that E{exp(yTz)} < oo for sufficiently small positive w, use
the optional stopping theorem to show that var(Tz) = an,

3. Let S, = ee Xy, m <n, where the X; are independent and identically distributed with finite
mean. Denote by U;, U2, ... , Un the order statistics of n independent variables which are uniformly
distributed on (0, t), and set U,,1 = t. Show that Rm = Sm/Um+i, 0 < m <n, is a backward
martingale with respect to a suitable sequence of o-fields, and deduce that

P(Rm = 1 for some m <n| Sp = y) < min{y/t, 1}.

12.9 Problems

1. Let Z, be the size of the nth generation of a branching process with immigration in which the
mean family size is jz (4 1) and the mean number of immigrants per generation is m. Show that

_ 1— pe”
Yn =U {zn mi}

defines a martingale.

2. In an age-dependent branching process, each individual gives birth to a random number of off-
spring atrandom times. At time 0, there exists a single progenitor who has N children at the subsequent

121
[12.9.3]-[12.9.8] Exercises Martingales

times B; < By <--- < By; his family may be described by the vector (N, By, Bo,..., By). Each
subsequent member x of the population has a family described similarly by a vector (N(x), By(x),...,
By x) @)) having the same distribution as (N, B,,..., By) and independent of all other individuals’
families. The number N(x) is the number of his offspring, and B;(x) is the time between the births
of the parent and the ith offspring. Let {B,,, : r > 1} be the times of births of individuals in the nth
generation. Let M, (0) = >>, e~98x,r, and show that Y, = My (0)/E(M,(@))" defines a martingale
with respect to Fy, = o ({Bm,r :m <n, r >= 1)), for any value of @ such that EM (0) < o.

3. Let (Y, F) be a martingale with EY, = 0 and E(Y2) < o¢ for all n. Show that

x > 0.

2
) - BaD

P({ max ¥>x —sa
( b>) = £2) +2

1<k<n

4. Let (Y, £) beanon-negative submartingale with Yp = 0, and let {c,} be a non-increasing sequence
of positive numbers. Show that

1 n
P( max cyY¥, > x) <- So EN — Yy-1), x>0.
l<k<n x lel

Such an inequality is sometimes named after subsets of Hajek, Rényi, and Chow. Deduce Kol-
mogorov’s inequality for the sum of independent random variables. [Hint: Work with the martingale
Zn = Cnn — Dipat CKE(X« | Fe—1) + has (Ce-1 — ce) Ye—1 where Xy = Ye — Ye-1.]

5. Suppose that the sequence {X,, : n > 1} of random variables satisfies E(Xy | X1, X2,..., Xn—-1)
= 0 for all n, and also S772, E(Xxl")/k" < 00 for some r € [1,2]. Let Sy = 577.) Z; where
Z; = X;/i, and show that

l<k<

1
P( max |Sn+k — Sml >) < —E(|Sm+n — Sml”), x > 0.
n x

Deduce that S, converges a.s. as n — oo, and hence that nol yi Xe 25 0. [Hint: In the case
1 <r < 2, prove and use the fact that h(u) = |u|" satisfies h(v)—h(u) < (v—u)h’(u)+2h((v—u)/2).
Kronecker’s lemma is useful for the last part.]

6. Let X1, X2,... be independent random variables with

1 with probability (2n)—},
Xn = 0 with probability 1—n7!,
—1 with probability (2n)~!.

Let Y; = X, and forn > 2

{ Xn if Y,-; = 9,

a= :

nY,—1|Xn| if Yn_-1 #0.

Show that Y;, is a martingale with respect to F, = 0(¥1, Yo,..., Yn). Show that Y, does not converge

almost surely. Does Y, converge in any way? Why does the martingale convergence theorem not
apply?

7. Let X1, X2,... be independent identically distributed random variables and suppose that M(t) =
E(e’*1) satisfies M(t) = 1 for some t > 0. Show that P(S; > x for some k) < e~* for x > 0 and
such a value of t, where Sy = Xj + Xo+---+ Xx.

8. Let Z, be the size of the nth generation of a branching process with family-size probability
generating function G(s), and assume Zo = 1. Let & be the smallest positive root of G(s) = s.

122

Problems Exercises [12.9.9]-[12.9.14]

Use the martingale convergence theorem to show that, if 0 < € < 1, then P(Z, — 0) = & and
P(Z, > wo) = 1-6.

9. Let (Y, F) be a non-negative martingale, and let ¥Y* = max{Y, : 0 < k <n}. Show that
e€
E(Y*) < 1 + E(¥p (log ¥n)*)}.

[Hint: alogt b < alogt a+ b/e if a, b > 0, where log* x = max{0, log x}.]

10. Let X = {X(t) : t > 0} be a birth-death process with parameters 4;, 4;, where 4; = 0 if and
only if i = 0. Define h(0) = 0, (1) = 1, and

Lye
wey = i 5 Malas j>2.

Show that 4(X(t)) constitutes a martingale with respect to the filtration F; = o({X(u) :0 <u < t}),
whenever Eh(X(t)) < 00 for all t. (You may assume that the forward equations are satisfied.)

Fix n, and let m <n; let x(m) be the probability that the process is absorbed at 0 before it reaches
size n, having started at size m. Show that r(m) = 1 — {h(m)/h(n)}.

11. Let (Y, ¥) be a submartingale such that E(Y, +) < M for some M and all n.

(a) Show that M, = limm—oo EY? nim | Fn) exists (almost surely) and defines a martingale with
respect to F-

(b) Show that Y, may be expressed in the form Y, = Xy,— Zy» where (X, F) is a non-negative
martingale, and (Z, F) is a non-negative supermartingale. This representation of Y is sometimes
termed the ‘Krickeberg decomposition’.

(c) Let (Y, ¥) be a martingale such that E/Y,| < M for some M and all n. Show that Y may be
expressed as the difference of two non-negative martingales.

12. Let £Y, be the assets of an insurance company after n years of trading. During each year it
receives a total (fixed) income of £P in premiums. During the nth year it pays out a total of £Cy in
claims. Thus ¥,41 = Yn + P —C,41. Suppose that Cy, C2,... are independent N (uw, o”) variables
and show that the probability of ultimate bankruptcy satisfies
2(P — wy
P(Y, <0 for some n) < exp poor} .

o2

13. Polya’s urn. A bag contains red and blue balls, with initially r red and b blue where rb > 0. A
ball is drawn from the bag, its colour noted, and then it is returned to the bag together with a new ball
of the same colour. Let Ry, be the number of red balls after n such operations.

(a) Show that Y, = Ry/(n +r +b) is a martingale which converges almost surely and in mean.
(b) Let T be the number of balls drawn until the first blue ball appears, and suppose that r = b = 1.
Show that E{(T + 2)~!} =

(c) Suppose r = b = 1, and show that P(Y, > ; for some n) < q

14. Here is a modification of the last problem. Let {Ay : n => 1} be a sequence of random variables,
each being a non-negative integer. We are provided with the bag of Problem (12.9.13), and we add
balls according to the following rules. At each stage a ball is drawn from the bag, and its colour noted;
we assume that the distribution of this colour depends only on the current contents of the bag and not
on any further information concerning the A,. We return this ball together with A, new balls of the
same colour. Write Ry, and B, for the numbers of red and blue balls in the urn after n operations, and

123
[12.9.15]-[12.9.19] Exercises Martingales

let Fp = o({ Rg, By 2 0 < k < n}). Show that Y, = R,/(Rn + By) defines a martingale. Suppose
Ro = Bo = 1, let T be the number of balls drawn until the first blue ball appears, and show that

1+Ar 1
E a ST, = 73>
2+ Vjnt Ai 2
-l
so long as >, (2+ Ly Ai) = was.

15. Labouchere system. Here is a gambling system for playing a fair game. Choose a sequence
X1,%2,-..,X» Of positive numbers.

Wager the sum of the first and last numbers on an evens bet. If you win, delete those two numbers;
if you lose, append their sum as an extra term x,41 (= x1 + x») at the right-hand end of the sequence.

You play iteratively according to the above rule. If the sequence ever contains one term only, you
wager that amount on an evens bet. If you win, you delete the term, and if you lose you append it to
the sequence to obtain two terms.

Show that, with probability 1, the game terminates with a profit of }77 x;, and that the time until
termination has finite mean.

This looks like another clever strategy. Show that the mean size of your largest stake before
winning is infinite. (When Henry Labouchere was sent down from Trinity College, Cambridge, in
1852, his gambling debts exceeded £6000.)

16. Here is a martingale approach to the question of determining the mean number of tosses of a coin
before the first appearance of the sequence HHH. A large casino contains infinitely many gamblers
G , Go,..., each with an initial fortune of $1. A croupier tosses a coin repeatedly. For each n,
gambler G,, bets as follows. Just before the mth toss he stakes his $1 on the event that the nth toss
shows heads. The game is assumed fair, so that he receives a total of $ po if he wins, where p is the
probability of heads. If he wins this gamble, then he repeatedly stakes his entire current fortune on
heads, at the same odds as his first gamble. At the first subsequent tail he loses his fortune and leaves
the casino, penniless. Let S, be the casino’s profit (losses count negative) after the nth toss. Show
that S,, is a martingale. Let N be the number of tosses before the first appearance of HHH; show that
N is a stopping time and hence find E(N).

Now adapt this scheme to calculate the mean time to the first appearance of the sequence HTH.

17. Let {(Xx, Yx) : k = 1} be a sequence of independent identically distributed random vectors such
that each X, and Y; takes values in the set {—1,0, 1,2,...}. Suppose that E(X,) = E(¥j) = 0
and E(X,¥,) = c, and furthermore X, and Y, have finite non-zero variances. Let Ug and Vo
be positive integers, and define (Un41, Va41) = (Un. Vn) + (Xn41, Yn41) for each > 0. Let
T = min{n : UnVp_ = 0} be the first hitting time by the random walk (Un, Vn) of the axes of R?.
Show that E(T) < oo if and only if c < 0, and that E(T) = —E(UgVo)/c in this case. [Hint: You
might show that U,V, — cn is a martingale.]

18. The game ‘Red Now’ may be played by a single player with a well shuffled conventional pack of
52 playing cards. At times m = 1,2, ...,52 the player turns over a new card and observes its colour.
Just once in the game he must say, just before exposing a card, “Red Now”. He wins the game if the
next exposed card is red. Let R, be the number of red cards remaining face down after the nth card
has been turned over. Show that X, = R,/(52 —n),0 <n < 52, defines a martingale. Show that
there is no strategy for the player which results in a probability of winning different from 7.

19. A businessman has a redundant piece of equipment which he advertises for sale, inviting “offers
over £1000”. He anticipates that, each week for the foreseeable future, he will be approached by
one prospective purchaser, the offers made in week 0,1,... being £1000Xo, £1000X1,..., where
Xo, X1,... are independent random variables with a common density function f and finite mean.
Storage of the equipment costs £1000c per week and the prevailing rate of interest is w (> 0) per

124

Problems Exercises [12.9.20]-[12.9.24]

week. Explain why a sensible strategy for the businessman is to sell in the week T, where T is a
stopping time chosen so as to maximize

Tr
wT) = e{( +a)? Xr-S + are

n=1

Show that this problem is equivalent to maximizing E{(1 + a)~7 Zr} where Zy = Xy +c/a.
Show that there exists a unique positive real number y with the property that

CO
ay -| P(Zyn > y) dy,
Y

and that, for this value of y, the sequence V, = (1+a@)~” max{Z,, y} constitutes a supermartingale.
Deduce that the optimal strategy for the businessman is to set a target price t (which you should
specify in terms of y) and sell the first time he is offered at least this price.

In the case when f (x) = 2x73 forx > 1,andc =a = ay find his target price and the expected
number of weeks he will have to wait before selling.
20. Let Z be a branching process satisfying Zp = 1, E(Z,) < 1, and P(Z, > 2) > 0. Show that
E(sup,, Zn) < n/(n— 1), where n is the largest root of the equation x = G(x) and G is the probability
generating function of Z.
21. Matching. In a cloakroom there are K coats belonging to K people who make an attempt to
leave by picking a coat at random. Those who pick their own coat leave, the rest return the coats and

try again at random. Let N be the number of rounds of attempts until everyone has left. Show that
EN = K and var(N) < K.

22. Let W be a standard Wiener process, and define
t
M(t) = [ W(u) du — 4 wa)?
0

Show that M(t) is a martingale, and deduce that the expected area under the path of W until it first
reaches one of the levels a (> 0) or b (< 0) is —zab(a +b).

23. Let W = (W), Wo,..., Wa) be a d-dimensional Wiener process, the W; being independent
one-dimensional Wiener processes with W;(0) = 0 and variance parameter o2 =d7!. Let Rity* =
Wi (t)? + W(t)? So Wa(t)?, and show that R(t)? — tis a martingale. Deduce that the mean
time to hit the sphere of R@ with radius a is a”.

24. Let W be a standard one-dimensional Wiener process, and let a,b > 0. Let T be the earliest

time at which W visits either of the two points —a, b. Show that P(W(T) = b) = a/(a + b) and
E(T) = ab. In the case a = b, find E(e~*T) for s > 0.

125

13

Diffusion processes

13.3 Exercises. Diffusion processes

1. Let X = {X(t) : t = 0} be a simple birth-death process with parameters 1, = nA and fy, = np.
Suggest a diffusion approximation to X.

2. Bartlett’s equation. Let D be a diffusion with instantaneous mean and variance a(t, x) and
b(t, x), and let M(t, 0) = E(e?P©), the moment generating function of D(t). Use the forward
diffusion equation to derive Bartlett’s equation:

1
OM _ a(t, *.\m+—0% (1,2)
at a6 2 36

where we interpret

fc) a" M
t,—|}M= t
«( a) Dont aan
if g(t, x) = P29 ya(t)x".

3. Write down Bartlett’s equation in the case of the Wiener process D having drift m and instanta-
neous variance 1, and solve it subject to the boundary condition D(O) = 0.

4. Write down Bartlett’s equation in the case of an Ornstein-Uhlenbeck process D having instan-
taneous mean a(t, x) = —x and variance b(t, x) = 1, and solve it subject to the boundary condition
DO) = 0.

5. Bessel process. If W,(t), W2(t), W3(t) are independent Wiener processes, then R(t) defined as
R? = we + ws + we is the three-dimensional Bessel process. Show that R is a Markov process. Is
this result true in a general number n of dimensions?

6. Show that the transition density for the Bessel process defined in Exercise (5) is
a
f@y|s,x)= gy PRO <y | R(s) = x)

_ vie Jef o-)_. f_ ot
~ Vingeaos) )P\ 2G—s) Pl a¢-H ] f-

7. If W is a Wiener process and the function g : R > R is continuous and strictly monotone, show
that g(W) is a continuous Markov process.

8. Let W bea Wiener process. Which of the following define martingales?
(a) WO, )cWt/c?), (©) tWW) — fy WOs)ds.

126

Stochastic calculus Exercises [13.3.9]-[13.7.1]

9. Exponential martingale, geometric Brownian motion. Let W be a standard Wiener process

and define S(t) = e#+8W). Show that:

(a) Sis a Markov process,

(b) S is a martingale (with respect to the filtration generated by W) if and only if a + 4p = 0, and
in this case E(S(t)) = 1.

10. Find the transition density for the Markov process of Exercise (9a).

13.4 Exercises. First passage times

1. Let W bea standard Wiener process and let X(t) = exp{i0 W(t) + 4672} where i = /—1. Show
that X is a martingale with respect to the filtration given by F; = o ({W(u) : u < t}).

2. Let T be the (random) time at which a standard Wiener process W hits the ‘barrier’ in space-time
given by y = at+bwherea <0, b > 0; thatis, T = inf{t : W(t) = at+b}. Use theresult of Exercise
(1) to show that the moment generating function of T is given by E(e¥? ) = exp{—b(./a? — 2y+a)}
for y < 5a’. You may assume that the conditions of the optional stopping theorem are satisfied.

3. Let W be a standard Wiener process, and let T be the time of the last zero of W prior to time t.
Show that P(T <u) = (2/m)sin7! /u/t, O<u <t.

13.5 Exercise. Barriers

1. Let D be a standard Wiener process with drift m starting from D(0) = d > 0, and suppose that
there is a reflecting barrier at the origin. Show that the density function f(t, y) of D(t) satisfies
f't, y) > 0 ast > co ifm > 0, whereas f(t, y) > 2|\mle~2!"I9 for y > 0, ast > ooifm <0.

13.6 Exercises. Excursions and the Brownian bridge

1. Let W be a standard Wiener process. Show that the conditional density function of W(t), given
that W(u) > Ofor 0 <u <t, is g(x) = (x/t)e7?/20), x>0.

2. Show that the autocovariance function of the Brownian bridge is c(s,t) = min{s, 7} — st,0 <
syt<1.

3. Let W bea standard Wiener process, and let W(t) = W(t)—1W (1). Show that (W(t) :0 <1 < 1}
is a Brownian bridge.

4. If W is a Wiener process with W (0) = 0, show that Wit) =(1-—-fWct/d —1t)) for0 <t <1,
W(1) = 0, defines a Brownian bridge.

5. LetO < s < +? < 1. Show that the probability that the Brownian bridge has no zeros in the
interval (s, t) is (2/2) cos! /@ —s)/[F —5)].

13.7 Exercises. Stochastic calculus

1. Doob’s Lz inequality. Let W be a standard Wiener process, and show that

B( max |Ws!?) < 4E(W2).
t

O<s<

127

[13.7.2]-[13.8.6] Exercises Diffusion processes

2. Let W be a standard Wiener process. Fix t > 0, > 1, and let 6 = t/n. Show that Z, =
725 (Wyss — Wis)? satisfies Z, — t in mean square as n > oo.

3. Let W be a standard Wiener process. Fix t > 0, > 1, and let = t/n. Let Vj = Wjs and
A; = Vj;11 — V;. Evaluate the limits of the following as n > oo:
J i+ J
(a) hn) =D VjAy,
(b) Inn) = 00; Visi Aj,
© Ba) = dj 4Vjait Vay.
(d) I4(n) = vi Wo sdysAi-

4. Let W be a standard Wiener process. Show that U(t) = et W (e2Bt ) defines a stationary
Ornstein—Uhlenbeck process.

5. Let W be a standard Wiener process. Show that U; = W; — B to e 8E-5) w, ds defines an
Ormstein—Uhlenbeck process.

13.8 Exercises. The It6 integral

In the absence of any contrary indication, W denotes a standard Wiener process, and ¥; is the smallest
o-field containing all null events with respect to which every member of {W, : 0 < u < t} is
measurable.

t t
1. (a) Verify directly that | sdWs =tW; — | W; ds.
0 0

t t
(b) Verify directly that [ W, dW; = 3W? - [ W, ds.

2
(c) Show inate (| [ We dW,| ) = [e0n2vas,
0

2. Let X;= fo W, ds. Show that X is a Gaussian process, and find its autocovariance and autocor-
relation function.

3. Let (&, F, P) be a probability space, and suppose that Xy, ™S xX asn — oo. If 9 < F, show
that E(Xn | ) > E(X | 9).

4. Let yy and wW be predictable step functions, and show that

fo. @]
ELI (WI (¥2)} = e( [ OYA) ar),
whenever both sides exist.

5. Assuming that Gaussian white noise G; = dW;/dt exists in sufficiently many senses to appear
as an integrand, show by integrating the stochastic differential equation dX; = —BX; dt + dW; that

t
X,=W;- pf e PC-S) W, ds,
0

if Xp = 0.
6. Let y% be an adapted process with || || < oo. Show that ||7(v) ilo = iv].

128

Option pricing Exercises [13.9.1]-[13.10.1]

13.9 Exercises. It6’s formula

In the absence of any contrary indication, W denotes a standard Wiener process, and F; is the smallest
o-field containing all null events with respect to which every member of (Wy, : 0 < u < t} is
measurable.

1, Let X and Y be independent standard Wiener processes. Show that, with R? = X? + Y?,

a4 ty,
Z= | dx [ee
t [ Rs s+ 0 Rs Ss

is a Wiener process. [Hint: Use Theorem (13.8.13).] Hence show that R? satisfies
t
R? =2/ Rs dWs + 2t.
0

Generalize this conclusion to n dimensions.

2. Write down the SDE obtained via It6’s formula for the process ¥; = w; , and deduce that
EW) = 317.

3. Show that Y; = 1W,; is an It6 process, and write down the corresponding SDE.

4. Wiener process on a circle. Let Y; = e!%_ Show that Y = X 1 +iX2 is a process on the unit
circle satisfying

dX, = —4Xx, dt—XzdW, dX2z= —4Xdt +X ,dw.

5. Find the SDEs satisfied by the processes:

(a) Xr = Wi/U +1),

(b) X; = sin W,,

(c) [Wiener process on an ellipse] X; = acos W;, Y; = b sin W;, where ab #4 0.

13.10 Exercises. Option pricing

In the absence of any contrary indication, W denotes a standard Wiener process, and F; is the smallest
o-field containing all null events with respect to which every member of {W, : 0 < u < ft} is
measurable. The process S; = exp((u — 507) +oW,) is a geometric Brownian motion, and r > 0
is the interest rate.

1. (a) Let Z have the N(y, 12) distribution. Show that

E((ae~ —K)*) = acVt30 @ (eecry + r) —Ko (Ree +r)

where ® is the N(0, 1) distribution function.
(b) Let Q be a probability measure under which oW is a Wiener process with drift r — uw and
instantaneous variance a2. Show for 0 <t<T that

Eg((Sr — K)+ | F) = Sre"F—9 @ (dy, S:)) — K (dot, S:))
where

log(x/K) + (r + 402)(T —1)
oV/T —-t ,

129

di(t,x) = dy(t, x) = dy (t,x) —oVT —t.

(13.10.2]-[13.12.2] Exercises Diffusion processes

2. Consider a portfolio which, at time rt, holds &(t, S) units of stock and w(t, S) units of bond, and
assume these quantities depend only on the values of S,, for 0 < u < t. Find the function y such that
the portfolio is self-financing in the three cases:

(a) &(¢, S) = 1 for allt, S,

(b) &@, S) = Sz,
) &tt y= fs dv
(c = R py dv.

3. Suppose the stock price S; is itself a Wiener process and the interest rate r equals 0, so that a
unit of bond has unit value for all time. In the notation of Exercise (2), which of the following define
self-financing portfolios?

(a) €(¢t, S) = w@, S) = 1 for all ¢, S,

(b) (1, 8) = 2S;, W(t, S) = -S? -1,

(c) &(t, S) = —-t, Wit, S) = fg Ss ds,

) &(t,5) = fo Ssds, W(t, S) = — fg S? ds.

4. An ‘American call option’ differs from a European call option in that it may be exercised by the
buyer at any time up to the expiry date. Show that the value of the American call option is the same
as that of the corresponding European call option, and that there is no advantage to the holder of such
an option to exercise it strictly before its expiry date.

5. Show that the Black-Scholes value at time 0 of the European call option is an increasing function
of the initial stock price, the exercise date, the interest rate, and the volatility, and is a decreasing
function of the strike price.

13.11 Exercises. Passage probabilities and potentials

1. Let G be the closed sphere with radius € and centre at the origin of R@ where d > 3. Let W bea
d-dimensional Wiener process starting from W(0) = w ¢ G. Show that the probability that W visits
Gis (e/r)4-2, where r = |w|.

2. Let G be an infinite connected graph with finite vertex degrees. Let A, be the set of vertices x
which are distance n from 0 (that is, the shortest path from x to 0 contains n edges), and let Ny be
the total number of edges joining pairs x, y of vertices with x € An, y € Ayn+1. Show that a random
walk on G is persistent if }7; i = 0.

3. Let G be aconnected graph with finite vertex degrees, and let H be a connected subgraph of G.
Show that a random walk on 7 is persistent if a random walk on G is persistent, but that the converse

is not generally true.

13.12 Problems

1. Let W beastandard Wiener process, that is, a process with independent increments and continuous
sample paths such that W(s +1) — W(s) is N(O, t) fort > 0. Let a be a positive constant. Show that:

(a) aW(t /o) is a standard Wiener process,
(b) W(t +a) — W(q) is a standard Wiener process,
(c) the process V, given by V(t) = tW(1/t) for t > 0, V(O) = 0, is a standard Wiener process.

2. Let X = {X(t) : t > 0} be a Gaussian process with continuous sample paths, zero means, and
autocovariance function e(s, t) = u(s)v(t) for s < ¢ where u and v are continuous functions. Suppose

130

Problems Exercises [13.12.3]-[13.12.9]

that the ratio r(¢) = u(t)/v(t) is continuous and strictly increasing with inverse function r—!, Show
that W(t) = X(r7!(a)) / v(r—1(t)) is a standard Wiener process on a suitable interval of time.
If c(s,t) =s(. —t) fors <+t < 1, express X in terms of W.

3. Let B > 0, and show that U(t) = e—F! w(e2t — 1) is an Ornstein—-Uhlenbeck process if W is a
standard Wiener process.

4. Let V = {V(t) : t > 0} be an Ornstein—Uhlenbeck process with instantaneous mean a(t, x) =
— Bx where B > 0, with instantaneous variance b(t, x) = o”, and with U (0) = u. Show that V(£) is
N(ue~P#, o2(1 — e~ Ft) /(2B)). Deduce that V(t) is asymptotically N(0, 507/B) as t —> oo, and
show that V is strongly stationary if V (0) is N(O, 5a? /B).

Show that such a process is the only stationary Gaussian Markov process with continuous auto-
covariance function, and find its spectral density function.

5. Let D = {D(t) : t > 0} be a diffusion process with instantaneous mean a(t, x) = ax and
instantaneous variance b(f, x) = Bx where a and 6 are positive constants. Let D(O) = d. Show that
the moment generating function of D(t) is

2adbe™ \

MEO = exp (yea ey Fa

Find the mean and variance of D(t), and show that P(D(t) = 0) > e 240/B ast —> oo.

6. Let D be an Ornstein—Uhlenbeck process with D(0) = 0, and place reflecting barriers at —c and
d where c,d > 0. Find the limiting distribution of D as t > oo.

7. Let Xo, X1,... be independent N(0, 1) variables, and show that

_ it. 2 S sin(kt)
Wo exo ES rae

defines a standard Wiener process on [0, zr].

8. Let W bea standard Wiener process with W (0) = 0. Place absorbing barriers at —b and b, where
b > 0, and let W2 be W absorbed at these barriers. Show that W(t) has density function

1 & (y — 2kb)”
“Vy, 1) = = —1) exp ¢ -~——_ 3, -b<y<b,
ro J 20t eS P 2t y
which may also be expressed as
CO
b
F*(y,1) = So ane" sin (=o) ,  -b<y<b,

n=1
where an = b—! sin(fnm) and Ay = n?x?/(8b?).
Hence calculate P(supp<; <, |W(s)| > 5) for the unrestricted process W.

9. Let D bea Wiener process with drift m, and suppose that D(0) = 0. Place absorbing barriers at
the points x = —a and x = b where a and b are positive real numbers. Show that the probability pg
that the process is absorbed at —a is given by

_ e2mb |
Pa =~ om(at+b) — 1°

131
[13.12.10]-13.12.18] Exercises Diffusion processes

10. Let W be a standard Wiener process and let F (u, v) be the event that W has no zero in the interval
(u, v).

(a) If ab > 0, show that P(F (0, t) | W(0) = a, W(t) = b) = 1- e774 /t,

(b) If W(0) = 0 and 0 < tg < ty < ty, show that

-—1
sin" /t9/t2
P(F (to, f2) | F(to, t1)) = sin” Jig iy”
o/t1

(c) Deduce that, if W(0) = 0 and 0 < ty < ty, then P(F (0, to) | F(O, t))) = /t1/ho.
11. Let W be a standard Wiener process. Show that

2t
P ( sup |W(s)| => ») < 2P(\W()| = w) < > for w > 0.
w

O<s<t

Set t = 2” and w = 22"/3 and use the Borel—Cantelli lemma to show that twa) — Oas. as
to oo.

12. Let W be a two-dimensional Wiener process with W(0) = w, and let F be the unit circle. What
is the probability that W visits the upper semicircle G of F before it visits the lower semicircle H?

13. Let W, and W2 be independent standard Wiener processes; the pair W(t) = (W(t), W2(t))
represents the position of a particle which is experiencing Brownian motion in the plane. Let / be
some straight line in R?, and let P be the point on / which is closest to the origin O. Draw a diagram.
Show that

(a) the particle visits /, with probability one,

(b) if the particle hits / for the first time at the point R, then the distance PR (measured as positive or
negative as appropriate) has the Cauchy density function f(x) = d/{x (d?+x2)}, —-0O <x <M,
where d is the distance OP,

(c) the angle POR is uniformly distributed on [—47, 47].

14. Let 6% +iy) = u(x, y) +iv(, y) be an analytic function on the complex plane with real part
u(x, y) and imaginary part v(x, y), and assume that

au \? du \2

—) +{(—) =1.

ox ay
Let (W,, W2) be the planar Wiener process of Problem (13) above. Show that the pair u(W 1, W2),
v(W,, W2) is also a planar Wiener process.

15. Let M(t) = maxo<s<; W(s), where W is a standard Wiener process. Show that M(t) — W(t)
has the same distribution as M(t).

16. Let W be a standard Wiener process, u € R, and let Z = {t : W(t) = u}. Show that Z is a null
set (i.e., has Lebesgue measure zero) with probability one.

17, Let M(t) = maxo<s<; W(s), where W is a standard Wiener process. Show that M(t) is attained
at exactly one point in [0, r], with probability one.

18. Sparre Andersen theorem. Let sg = 0 and sj, = Yjel xj, where (xj : 1 < j <n) isa given
sequence of real numbers. Of the n! permutations of (x; : 1 < j < a), let Ay be the number of
permutations in which exactly r values of (sm : 0 < m < n) are strictly positive, and let B, be the
number of permutations in which the maximum of (sy, : 0 < m < n) first occurs at the rth place.
Show that Ay = B; forO <r <n. [Hint: Use induction on n.]

132
Problems Exercises [13.12.19]-[13.12.24]

19. Arc sine laws. For the standard Wiener process W, let A be the amount of time u during the
time interval [0, t] for which W(u) > 0; let L be the time of the last visit to the origin before ¢;
and let R be the time when W attains its maximum in [0, t]. Show that A, L, and R have the same
distribution function F(x) = (2/7) sin—! /x/f for 0 < x < +t. [Hint: Use the results of Problems
(13.12.15)-(13.12.18).]

20. Let W be a standard Wiener process, and let Ux be the amount of time spent below the level x
(= 0) during the time interval (0, 1), that is, U, = fo Tew a)<xj dt. Show that Ux has density function

fu, ! ex x O<u<1
4“) = —_—_= ~—-—], .
Ux a/fu(1 — un) P\ on

Show also that . .
V i sup{t <1: W; =x} if this set is non-empty,
x =

1 otherwise,
has the same distribution as U;.

21. Let sign(x) = 1 if x > Oand sign(x) = —1 otherwise. Show that V; = fo sign(Ws) dWs defines
a standard Wiener process if W is itself such a process.

22. After the level of an industrial process has been set at its desired value, it wanders in a random
fashion. To counteract this the process is periodically reset to this desired value, at times 0, T, 2T,....
If W, is the deviation from the desired level, ¢ units of time after a reset, then {W; :O0 <t < T} can
be modelled by a standard Wiener process. The behaviour of the process after a reset is independent
of its behaviour before the reset. While W; is outside the range (—a, a) the output from the process
is unsatisfactory and a cost is incurred at rate C per unit time. The cost of each reset is R. Show that
the period T which minimises the long-run average cost per unit time is T*, where

T* a a2
R=C —— | dt.
[ Vian P \

23. An economy is governed by the Black-Scholes model in which the stock price behaves as a
geometric Brownian motion with volatility o, and there is a constant interest rate r. An investor likes
to have a constant proportion y (€ (0, 1)) of the current value of her self-financing portfolio in stock
and the remainder in the bond. Show that the value function of her portfolio has the form V; = f(t)S, y

where f(t) = cexp{(1 — yy(gvo? + r)t} for some constant c depending on her initial wealth.

24. Let u(t, x) be twice continuously differentiable in x and once int, for x € R and t € [0, T]. Let
W be the standard Wiener process. Show that u is a solution of the heat equation

du -1:0u
at

if and only if the process U; = u(T —t, Wr), 0 <1 < T, has zero drift.

133
1

Events and their probabilities

1.2 Solutions. Events as sets

1. (a) Leta € (UA;)°. Thena ¢ UAj, so that a € Af for all i. Hence (UJ A;)° € () Af.
Conversely, if a € (| Aj, then a ¢ A; for every i. Hence a ¢ \) A;, and so (| Af © (U A;)°. The
first De Morgan law follows.

(b) Applying part (a) to the family {Af : i € I}, we obtain that (Uj; Ag)° = 11;(A9)S = 1); Ai-
Taking the complement of each side yields the second law.

2. Clearly

(i) AN B= (ASU BY),

(ii) A\ B= AN BS = (AS UB),
(iii) AA B= (A\ B)U(B\ A) = (ASU BYU (AU BY.

Now F is closed under the operations of countable unions and complements, and therefore each of
these sets lies in F.

3. Letusnumberthe players 1, 2, ...,2” in the order in which they appear in the initial table of draws.
The set of victors in the first round is a point in the space V, = {1,2} x {3,4} x --. x {2" —1,2”}.
Renumbering these victors in the same way as done for the initial draw, the set of second-round
victors can be thought of as a point in the space V,,_1, and so on. The sample space of all possible
outcomes of the tournament may therefore be taken to be V, x V,_1 x +--+ x Vj, a Set containing
g2rtg2"? |. ol — 92"—1 points.

Should we be interested in the ultimate winner only, we may take as sample space the set

{1,2,...,2"} of all possible winners.

4, We must check that % satisfies the definition of a o-field:

(a) @ € F, and therefore @ = ON B € G,

(b) if Ay, Ag, ... € F, then Uj, (A; 9 B) = (U; Ai) NB € G,

(c) if A € F, then A° € ¥ so that B\ (AN B) = ASN Be §.

Note that %, is a o-field of subsets of B but not a o-field of subsets of Q, since C € % does not imply
that C° = Q\ Ce §.

5. (a), (b), and (d) are identically true; (c) is true if and only if A CC.

1.3 Solutions. Probability

1. (i) We have (using the fact that P is a non-decreasing set function) that

P(AM B) = P(A) + P(B) — P(AU B) > P(A) + P(B) — 1 = %.

135

[1.3.2]-[1.3.4] Solutions Events and their probabilities

Also, since AN BC Aand AM BC B, P(AN B) < min{P(A), P(B)} = 5.

These bounds are attained in the following example. Pick a number atrandom from {1, 2, ..., 12}.
Taking A = {1,2,...,9} and B = {9, 10, 11, 12}, we find that AN B = {9}, and so P(A) = 3,
P(B) = 4, (AN B) = 75. To attain the upper bound for P(A M B), take A = {1,2,...,9} and
B = {1, 2, 3, 4}.

(ii) Likewise we have in this case P(A U B) < min{P(A) + P(B),1} = 1, and P(A U B) =
max{P(A), P(B)} = 3. These bounds are attained in the examples above.

2. (i) We have (using the continuity property of P) that
P(no head ever) = lim P(no head in first n tosses) = lim 27" =0,
n->OO now

so that P(some head turns up) = 1 — P(no head ever) = 1.

(ii) Given a fixed sequence s of heads and tails of length k, we consider the sequence of tosses arranged
in disjoint groups of consecutive outcomes, each group being of length k. There is probability 2—*
that any given one of these is s, independently of the others. The event {one of the first n such groups
is s} is a subset of the event {s occurs in the first nk tosses}. Hence (using the general properties of
probability measures) we have that

P(s turns up eventually) = im, P(s occurs in the first nk tosses)
> lim P(s occurs as one of the first n groups)
n> CO

=1- jim, P(none of the first n groups is s)

=1- lim (1—27*)” =1.
n> CO

3. Lay out the saucers in order, say as RRWWSS. The cups may be arranged in 6! ways, but since
each pair of a given colour may be switched without changing the appearance, there are 6!— (2!1)3 =90
distinct arrangements. By assumption these are equally likely. In how many such arrangements is
no cup on a saucer of the same colour? The only acceptable arrangements in which cups of the
same colour are paired off are WWSSRR and SSRRWW; by inspection, there are a further eight
arrangements in which the first pair of cups is either SW or WS, the second pair is either RS or SR,

and the third either RW or WR. Hence the required probability is 10/90 = 5:

4. We prove this by induction on n, considering first the casen = 2. Certainly B = (AN B)U(B\ A)
is a union of disjoint sets, so that P(B) = P(AN B) + P(B \ A). Similarly AU B = AU (B\ A), and
sO

P(AU B) = P(A) + P(B \ A) = P(A) + {P(B) — P(AN B)}.

Hence the result is true for n = 2. Let m > 2 and suppose that the result is true for n < m. Then it is
true for pairs of events, so that

m+1 m m
P( U Ai) = P(U Ai) + P(Am+1) - Pf (Ua:) n Amat}
1 1 1
= P(U Ai) +P(Am41) -»{ Ui 0 Ame)
1

1

Using the induction hypothesis, we may expand the two relevant terms on the right-hand side to obtain
the result.

136

Conditional probability Solutions [1.3.5]-[1.4.1]

Let A,, Az, and A3 be the respective events that you fail to obtain the ultimate, penultimate, and
ante-penultimate Vice-Chancellors. Then the required probability is, by symmetry,

3
1- P(U Ai) = 1 — 3P(Aj) + 3P(Ay M Ag) — P(Ay 9 Az N A3)
1
= 1-3(9)° +3(8)° — (3)°.

5. By the continuity of P, Exercise (1.2.1), and Problem (1.8.11),

r=]
n

n
= i c i cy _
=1 - Jime(U A) >1- lim > Par) =1.
r=

r=1

r<§

n
6. We have that 1 = P(U Ar) = >> P(4,) — )>P(Ar 1 As) = np — 4n(n — 1)q. Hence
1 r

p2=n7!, and 5n(n— 1)qg=np-1<n-1.

7. Since at least one of the A; occurs,

1=P(U4r) = > P(Ar) — S>P(AY As) +S) P(Ar OAs Ar)
1 r

r<s r<s<t

Since at least two of the events occur with probability 7

}=P(Ua-n as) =S°P(ArNAs)— 5° SS P(A ASN APN Au) +o
r<s res r<s

t<u
(r,s) A(t.)

By acareful consideration of the first three terms in the latter series, we find that

= GG) 6)

Hence 3 = np — (3)x, so that p > 3/(2n). Also, (5)q = 2np — 3, whence g < 4/n.

1.4 Solutions. Conditional probability

1. By the definition of conditional probability,

PLAN B)  P(BO A) P(A) = PCB | ay)

P(A | B) = > =
(B) P(A) P(B) P(B)

137

[1.4.2]-[1.4.5] Solutions Events and their probabilities

if P(A)P(B) # 0. Hence
P(AIB) PBA)

P(A) ~—P(B)

>

whence the last part is immediate.

2. Set Ag = Q for notational convenience. Expand each term on the right-hand side to obtain

The( A«)=II P(N Ab) =P (ai).

r=l PN Ax) 1

3. Let M be the event that the first coin is double-headed, R the event that it is double-tailed, and
N the event that it is normal. Let Hj be the event that the lower face is a head on the ith toss, Ti the
event that the upper face is a tail on the ith toss, and so on. Then, using conditional probability ad
nauseam, we find:

0) P(A!) = PCH! | M) + deca! | R) + 2P(H) | NW) = 3 4042-4 = 3.
« PALO) PM) 953 2
(ii) P(A | Hi) = = =2/322.
| u P(A) P(A) 5/5 = 3
(iii) P(A? | Hd) =1-P(M | Hd) + 4PCV | ))
= PH! | Hy +41 — Pua | ab) = 344-4 = 3.
(iv)
2 1 2 2
p(H2 | Hn #2) = PU Hy 0 Hy) _ PM) ~_3__4
1 u u 1 5} i 5) i 5"
P(Ay 9 Ay) 1-P(M)+4-PIr) 244

(v) From (iv), the probability that he discards a double-headed coin is 4, the probability that he

discards a normal coin is 7 (There is of course no chance of it being double-tailed.) Hence, by
conditioning on the discard,

PCH) = SPCH} | M) + $PUHe |) = $(4+3-°2) +503 43° a) =

Nie

(

ae

4. The final calculation of 5 refers not to a single draw of one ball from an urn containing three, but
rather to a composite experiment comprising more than one stage (in this case, two stages). While it
is true that {two black, one white} is the only fixed collection of balls for which a random choice is
black with probability 3, the composition of the urn is not determined prior to the final draw.

After all, if Carroll’s argument were correct then it would apply also in the situation when the urn
originally contains just one ball, either black or white. The final probability is now 3, implying that
the original ball was one half black and one half white! Carroll was himself aware of the fallacy in
this argument.

5. (a) One cannot compute probabilities without knowing the rules governing the conditional prob-
abilities. If the first door chosen conceals a goat, then the presenter has no choice in the door to be
opened, since exactly one of the remaining doors conceals a goat. If the first door conceals the car, then
a choice is necessary, and this is governed by the protocol of the presenter. Consider two ‘extremal’
protocols for this latter situation.

G) The presenter opens a door chosen at random from the two available.

(ii) There is some ordering of the doors (left to right, perhaps) and the presenter opens the earlier

door in this ordering which conceals a goat.

Analysis of the two situations yields p = 5 under (i), and p = 4 under (ii).

138
Independence Solutions [1.4.6]-[1.5.3]

Leta e€ 5. 41, and suppose the presenter possesses a coin which falls with heads upwards with
probability 8 = 6a — 3. He flips the coin before the show, and adopts strategy (i) if and only if the

coin shows heads. The probability in question is now 5B + 51 — Bp) =a.
You never lose by swapping, but whether you gain depends on the presenter’s protocol.
(b) Let D denote the first door chosen, and consider the following protocols:
Gii) If D conceals a goat, open it. Otherwise open one of the other two doors at random. In this
case p = 0.

(iv) If D conceals the car, open it. Otherwise open the unique remaining door which conceals a
goat. In this case p = 1.

As in part (a), a randomized algorithm provides the protocol necessary for the last part.
6. This is immediate by the definition of conditional probability.

7. Let C; be the colour of the ith ball picked, and use the obvious notation.
(a) Since each urn contains the same number n — 1 of balls, the second ball picked is equally likely to
be any of the n(n — 1) available. One half of these balls are magenta, whence P(C2 = M) = 4

(b) By conditioning on the choice of urn,

P(C), C2 = M) = Sr @anmar-y 12

P(C; =M) 3

P(C, =M|C =M) = nn —1in—2) / 2 > 3

1.5 Solutions. Independence
1. Clearly

P(AS NB) = P(B \ {AN B}) = P(B) — P(AN B)
= P(B) — P(A)P(B) = P(A°)P(B).

For the final part, apply the first part to the pair B, A°.

2. Suppose i < j andm <n. If j < m, then Aj; and Aj» are determined by distinct independent
rolls, and are therefore independent. For the case j = m we have that

P(Ajj O Ajn) = PGth, jth, and nth rolls show same number)
6
= )> ¢P(jth and nth rolls both show r | ith shows r) = 4g = P(Ajj)P(Ajn),

r=]

as required. However, ifi # j #k,
P(A A Aje O Aik) = ae # ag = P(A )P(AjR)PCAix)-

3. That (a) implies (b) is trivial. Suppose then that (b) holds. Consider the outcomes numbered
i, i2,...,im, and let uj € {H,T} for 1 < j < m. Let S; be the set of all sequences of length M =
max{ij : 1 < j < m} showing uj in the i;th position. Clearly |Sj| = 2%~! and |, 5;| = 24-™.

Therefore,
M-1 1 9M —m 1

2
PS) = SM = 9° »(NS) = 2M ~ 3m?
J

139

[1.5.4]-[1.7.2] Solutions Events and their probabilities

so that P((; $;) = [Ij P(S)).

4. Suppose |A| = a, |B| = b, |AM B| = c, and A and B are independent. Then P(AN B) =
P(A)P(B), which is to say that c/p = (a/p)- (b/p), and hence ab = pc. If ab # O then p | ab
(i.e., p divides ab). However, p is prime, and hence either p | a or p | b. Therefore, either A = Q or
B = Q (or both).

5. (a) Flip two coins; let A be the event that the first shows H, let B be the event that the second
shows H, and let C be the event that they show the same. Then A and B are independent, but not
conditionally independent given C.

(b) Roll two dice; let A be the event that the smaller is 3, let B be the event that the larger is 6, and let
C be the event that the smaller score is no more than 3, and the larger is 4 or more. Then A and B are
conditionally independent given C, but not independent.

(c) The definitions are equivalent if P(C) = 1.

6 (a) <4

7. (a)P(AN B)= 3 = =}. 1 = P(A)P(B), and (BNC) = =}-} = P(B)P(C).

(b) PAN C) =0 4 P(ADP(C).

(c) Only in the trivial cases when children are either almost surely boys or almost surely girls.
(d) No.

8 No. P(all alike) = 4.

9. P(ist shows r and sum is 7) = % = é . é = P(1st shows r)P(sum is 7).

1.7 Solutions. Worked examples

1. Write EF for the event that there is an open road from E to F, and EF® for the complement of
this event; write E < F if there is an open route from E to F, and E # F if there is none. Now
{A < C} = ABM BC, so that

P(AB,A GC) P(AB,B¢C)__(1— p?)p?
PAC)  1-PASC) 1-(1— p2)?’

P(AB| A #C) =

By a similar calculation (or otherwise) in the second case, one obtains the same answer:

(1 — p?)p? _ (= p*)p*

PAB | A ©) = 7G na, ad -p) 17d py

2. Let A be the event of exactly one ace, and KK be the event of exactly two kings. Then P(A |
KK) = P(A 1 KK)/P(KK). Now, by counting acceptable combinations,

moan ((QCI/ C3} me (GEG)

so the required probability is

(616/000):

140
Problems Solutions [1.7.3]-[1.8.2]

3. First method: Suppose that the coin is being tossed by a special machine which is not switched
off when the walker is absorbed. If the machine ever produces N heads in succession, then either the
game finishes at this point or it is already over. From Exercise (1.3.2), such a sequence of N heads
must (with probability one) occur sooner or later.

Alternative method: Write down the difference equations for p;, the probability the game finishes at
O having started at k, and for px, the corresponding probability that the game finishes at N; actually
these two difference equations are the same, but the respective boundary conditions are different.
Solve these equations and add their solutions to obtain the total 1.

4. Itisa tricky question. One of the present authors is in agreement, since if P(A | C) > P(B | C)
and P(A | C°) > P(B {| C°) then
P(A) = P(A | C)P(C) + P(A | COP(CS)
> P(B | CYP(C) + P(B | CYP(CS) = PCB).
The other author is more suspicious of the question, and points out that there is a difficulty arising

from the use of the word ‘you’. In Example (1.7.10), Simpson’s paradox, whilst drug I is preferable
to drug II for both males and females, it is drug I that wins overall.

5. Let Lx be the label of the kth card. Then, using symmetry,

P(Lg =m) 1/1
P(L = m| Le > Ly for <r <k) = 5 a == / gp =k/m.

1.8 Solutions to problems

1. (a) Method I: There are 36 equally likely outcomes, and just 10 of these contain exactly one six.
The answer is therefore 19 = ze

Method IT: Since the throws have independent outcomes,

P(first is 6, second is not 6) = P(first is 6)P(second is not 6) = é : 3 = >.
There is an equal probability of the event {first is not 6, second is 6}.
(b) A die shows an odd number with probability i. by independence, P(both odd) = I . i = i
(c) Write S for the sum, and {i, 7} for the event that the first is i and the second 7. Then P(S = 4) =
P(1, 3) + P(2, 2) + PG, 1) = #.
(d) Similarly

P(S divisible by 3) = P(S = 3) + P(S = 6) + P(S = 9) + P(S = 12)
= {PC, 2) + P(2, 1)}
+ {PC, 5) + P(2, 4) + PG, 3) + PG, 2) + PG, 1}
+ {PG, 6) + P(4, 5) + PCS, 4) + P(6, 3)} + PC, 6)

-~L_1!1
~ 36 ~ 3°

2. (a) By independence, P(n — 1 tails, followed by a head) = 2~”.

(b) If n is odd, P(# heads = # tails) = 0; #A denotes the cardinality of the set A. If n is even, there
are (n2) sequences of outcomes with hn heads and 5n tails. Any given sequence of heads and tails

has probability 2~”; therefore P(# heads = # tails) = 27” (nj2)-

141

[1.8.3]-[1.8.9] Solutions Events and their probabilities

(c) There are (5) sequences containing 2 heads and n — 2 tails. Each sequence has probability 2~,
and therefore P(exactly two heads) = (5)2~".
(d) Clearly

P(at least 2 heads) = 1 — P(no heads) — P(exactly one head) = 1 — 27” — (") 2".

3. (a) Recall De Morgan’s Law (Exercise (1.2.1)): ; Ai = (U; Af)°. which lies in F since it is
the complement of a countable union of complements of sets in F.
(b) # is a o-field because:

(i) @ € Fand @ € §; therefore @ € #.

(ii) If Ay, Ag, ... is a sequence of sets belonging to both ¥ and G, then their union lies in both Fand
G,, which is to say that Jf is closed under the operation of taking countable unions.

(iii) Likewise AS is in # if A is in both F and §.
(c) We display an example. Let

Q={a,b,ch, F= {{a}, {b,c}, 2, Q}, $= {{a, d}, {c}, B, Qh.

Then H = FU GF is given by H = {{a}, {c}, {a, b}, {b, c}, @, 2}. Note that {a} € H and {c} € H,
but the union {a, c} is not in #€, which is therefore not a o-field.

4. Ineach case # may be taken to be the set of all subsets of 82, and the probability of any member
of ¥ is the sum of the probabilities of the elements therein.

(a) Q = {H, TP, the set of all triples of heads (H) and tails (T). With the usual assumption of
independence, the probability of any given triple containing / heads and t = 3 — h tails is p'(—p),
where p is the probability of heads on each throw.

(b) In the obvious notation, @ = {U, V}* = {UU, VV, UV, VU}. Also P(UU) = P(VV) = 2 . 4 and
P(UV) = P(VU) = §- 3.

(c) Q is the set of finite sequences of tails followed by a head, {T”H : n > 0}, together with the infinite
sequence T®™ of tails. Now, P(T’H) = (1 — p)" p, and P(T®) = limy—+oo(1 — p)” = 0if p £0.
5. Asusual, P(A A B) = P((AU B)\ P(AN B)) = P(AU B) — (ANB).

6. Clearly, by Exercise (1.4.2),

P(AU BUC) = P((ASN BE NC) = 1 — P(ASN BENC®)
=1—P(AS | BSN C)P(BS | CYP(CS).

7. (a) If A is independent of itself, then P(A) = P(AN A) = P(A)2, so that P(A) = Oor 1.

(b) If P(A) = 0 then 0 = P(AN B) = P(A)P(B) for all B. If P(A) = 1 then P(AN B) = P(B), so
that P(A N B) = P(A)P(B).

8 QU = Qand QNS = @, and therefore 1 = P(QUS) = P(Q) + P(S) = 14 P(S), implying
that P(@) = 0.

9 @ QS) = PS | B) = 0. Also Q(Q) = P(Q | B) = P(B)/P(B) = 1.

(ii) Let Ay, Az,... be disjoint members of F. Then {A; M B : i > 1} are disjoint members of F,
_ implying that

a(Ua.) = P(Ua

_ PUPINB) APANB A
B) a = = Lean,

142

Problems Solutions [1.8.10]-[1.8.13]

Finally, since Q is a probability measure,

QANC)  P(ANC|B) _ P(ANBNC)
Qc) ~— PC|B) ~~ P(BNC)

QA | C)= =P(A| BNC).

The order of the conditioning (C before B, or vice versa) is thus irrelevant.
10. As usual,

io, @) io.) io.)
P(A) = P(Uia n B))) = S>P(AN Bj) = >> P(A | By)P(B)).
1 1 1

11. The first inequality is trivially true ifn = 1. Let m > 1 and assume that the inequality holds for
n<m. Then

m+1 m m

P(U 41) =P(Uas) + Pnsv - P(Ut4i 9 Ans)
1 1 1
m+l

< p(Uas) +P(Am+1) < 5 P(Ai),
1 1

by the hypothesis. The result follows by induction. Secondly, by the first part,
n n c n n
P(M4i) = P((Ua%) ) =1- p(Ua?) >1-—5> PAS).
1 1 1 1

12. We have that

=1— 5° P(Af) + S$ P(E AS) = + ayre() 43) by Exercise (1.3.4)

i<j 1
=1—n+5~P(Aj) + (;) —S>P(Aj U Aj) - (3) foe

i<j

n
+(-1)" (") _ c'e(U Ai) using De Morgan’s laws again
n
1
n
=(1-1)" + S> P(A) see y'e(U Ai) by the binomial theorem.
i 1

13. Clearly,

P(M)= >> P(M 4143):
SCU 20m) ieS j¢S

For any such given S, we write Ag = (\;-5 Aj. Then

(1) Ai () 45) = P(As) — S> P(Asuyy) + > PlAsugj) —

ieS fj ¢S js i<k
JkES

143

[1.8.14]-[1.8.16] Solutions Events and their probabilities

by Exercise (1.3.4). Hence

k+1 _
P(N) = S> P(As)- SO ( k Jes) (oF «({ pear os Ap
|S|=k |S|=k+1

where a typical summation is over all subsets S of {1, 2,..., 2} having the required cardinality.
Let A; be the event that a copy of the ith bust is obtained. Then, by symmetry,

w-o-()

where a; is the probability that the j most recent Vice-Chancellors are obtained. Now a3 is given in
Exercise (1.3.4), and a4 and as may be calculated similarly.

14, Assuming the conditional probabilities are defined,
P(AJNB) PCB A;)P(AjZ) _— «PCB | Aj) PAs)
P(B) P(BO (UT A4i)) = 1 PCB | A)PCAi)

P(A; | B) =

15. (a) We have that

PIN =2|/S—4) = PUN EAS = 4p P(S =4| N =2)P(N =2)

P(S = 4) yo; PS =4| N =i)P(N =i)
iil
_ 124
1 1 1 3 1 1 1°
6° 2+ 12°47 716° 8+ M6

(b) Secondly,

P(S=4|N=214+P(S =4|N=44
P(S =4/j N even) = ( | 4 ( | 16

P(N even)
Jiil,i ii 233
_ 2 4t ete 43° +1
~“T7 1 ~ 4433
atagte: 443
(c) Writing D for the number shown by the first die,
PIN =2,S=4,D=1) é-e-4
PIN =2|S=4,D=1)= P(S =4,D=1 —T.1.1,1.2.1,1.1°
== 6°6°446°36°8 + GF 16

(d) Writing M for the maximum number shown, if 1 <r < 6,

ad oo .
j r\J 1 r r\-l r
s = < =jf2J= > = = -— = .
P(M <r) DPM SIN j)2 XG) ws =p(-5) =p

16. (a) € B if and only if, for all n, w € UP, A;, which is to say that w belongs to infinitely many
of the Ap.

(b) w € C if and only if, for some n, w € Wen Aj, which is to say that w belongs to all but a finite
number of the Ay.

144

Problems Solutions [1.8.17]-[1.8.20]

(c) It suffices to note that B is a countable intersection of countable unions of events, and is therefore
an event.
(d) We have that

CO oO

Ch = () 4i S An S LU 4: = Bn,

tn i=n
and therefore P(Cy,) < P(An) < P(B,). By the continuity of probability measures (1.3.5), if Ch > C
then P(C,) > P(C), and if B, — B then P(B,) > P(B). If B = C = A then

P(A) = P(C) < lim P(An) < P(C) = P(A).

17. If By, and Cy are independent for all n then, using the fact that Cy, C By,
P(Bp)P(Cn) = P(Ban ON Cn) = P(Ch) > PCC) asn—> oo,

and also P(B,)P(Cn) — P(B)P(C) as n > o0, so that P(C) = P(B)P(C), whence either P(C) = 0
or P(B) = 1 or both. In any case P(B NC) = P(B)P(C).
If An > Athen A = B =C so that P(A) equals 0 or 1.

18. Itis standard (Lemma (1.3.5)) that P is continuous if it is countably additive. Suppose then that P is
finitely additive and continuous. Let Ay, Az, ... be disjoint events. Then UP? Aj = limn—+oo Uf Ai,
so that, by continuity and finite-additivity,

oe) nt A CO
P(Ua) = jin.P(Ua) ~ zg, 2 PCAs) = 2 PAD)

19. The network of friendship is best represented as a square with diagonals, with the corners labelled
A, B, C, and D. Draw a diagram. Each link of the network is absent with probability p. We write EF
for the event that a typical link EF is present, and EF® for its complement. We write A < D for the
event that A is connected to D by present links.
(d) P(A D| AD‘) = P(A & D| ADS NBC)p + P(A & D | AD® NBC)(1 — p)

= {1-(1— (=p)? }p + (1— py — p).
(c) P(A + D| BC) =P(A & Dj] ADS NBC)p + P(A @ D| BC’ NAD)(1 — p)

= {1-(—(—py)?}p+(1- p).
(b) P(A <= D] ABS) = P(A @ D] ABS NAD*)p + P(A & Dj ABS NAD) — p)

=(1—- p){1- pd-(—p))}p + — p).
(a) P(A oD) =P(A@D| AD )p+P(A <D]| AD)(1 — p)

= {1-1-1 ~ p)*)"}p? + - pp — p) + 0 - p).

20. We condition on the result of the first toss. If this is a head, then we require an odd number of
heads in the next n — 1 tosses. Similarly, if the first toss is a tail, we require an even number of heads
in the next n — 1 tosses. Hence

Pa = Pl — pa-1) + - p)pa-1 = (1 — 2p) pn_1 t+ Pp

with po = 1. As an alternative to induction, we may seek a solution of the form py = A+ BA”.
Substitute this into the above equation to obtain

A+ BA" =(1—2p)A+ (1 —2p) Ba"! 4 p

145

[1.8.21]-[1.8.23] Solutions Events and their probabilities

and A+B =1. Hence A=4,B=4,4=1-2p.

21. Let A = {run of r heads precedes run of s tails}, B = {first toss is a head}, and C = {first s tosses
are tails}. Then

P(A | BS) = P(A | BS NC)P(C | BS) + P(A | BSN C*)P(CS | B°) =0+ P(A] B)A 9°),

where p = 1 — q is the probability of heads on any single toss. Similarly P(A | B) = p” “14 P(A |
B°)(1— p’—!). We solve for P(A | B) and P(A | B®), and use the fact that P(A) = P(A | B)p+P(A |
B°)gq, to obtain

pid —4q)

P(A) = .
pro! +5! _ pr—qs-]

22. (a) Since every cherry has the same chance to be this cherry, notwithstanding the fact that five
are now in the pig, the probability that the cherry in question contains a stone is a = i.

(b) Think about it the other way round. First a random stone is removed, and then the pig chooses
his fruit. This does not change the relevant probabilities. Let C be the event that the removed cherry
contains a stone, and let P be the event that the pig gets at least one stone. Then P(P | C) is the
probability that out of 19 cherries, 15 of which are stoned, the pig gets a stone. Therefore

P(P | C) = 1 — P(pig chooses only stoned cherries |C) = 1 — 3 . ig . 8: iz: i5-

23. Label the seats 1, 2,...,2n clockwise. For the sake of definiteness, we dictate that seat 1 be
occupied by a woman; this determines the sex of the occupant of every other seat. For 1 < k < 2n,
let Ag be the event that seats A, k + 1 are occupied by one of the couples (we identify seat 2n + 1 with
seat 1). The required probability is

2n 2n
p(Ma") =1-P(Uai) = t= DUP y+ SOP(A;N AJ) — +
1 1

i<j

Now, P(A;) = n(n — 1)!?/n!, since there are n couples who may occupy seats i andi + 1, (n — 1)!
ways of distributing the remaining n — 1 women, and (n — 1)! ways of distributing the remaining n — 1
men. Similarly, if 1 <i < j <2n, then

@-2P 0,
P(A) Aj) = nn) if |i —-j| #1

0 if|i- jl=1,
subject to P(A, M Az,) = 0. In general,

nt (n-k)P?  m-b!
(M—ky! nl? a!

P(A; NA N--- NAR) =

if iy < ig <--> < iy andij4; —ij > 2for1 < j < k, and 2n +i; — ix = 2; otherwise this

probability is 0. Hence
2n w !
P(A") = ren Gh's,,
1 k=0

where Sj,» is the number of ways of choosing k non-overlapping pairs of adjacent seats.

Finally, we calculate Sx; ,. Consider first the number Nx, of ways of picking k non-overlapping
pairs of adjacent seats from a line (rather than a circle) of m seats labelled 1, 2, ..., m. There is a one—
one correspondence between the set of such arrangements and the set of (m — k)—vectors containing

146

Problems Solutions [1.8.24]-[1.8.27]

k 1’s and (m — 2k) 0’s. To see this, take such an arrangement of seats, and count 0 for an unchosen
seat and 1 for a chosen pair of seats; the result is such a vector. Conversely take such a vector, read its
elements in order, and construct the arrangement of seats in which each 0 corresponds to an unchosen

seat and each 1 corresponds to a chosen pair. It follows that Nem = (™*).

Turning to Sx,, either the pair 2n, 1 is chosen or it is not. If it is chosen, we require another
k —1 pairs out of a line of 2 — 2 seats. If it is not chosen, we require k pairs out of a line of 2n seats.
Therefore

2n—k—1 2n—k 2n—k\ 2n
Skin = Ne-1,2n-2 + Nerm=4 ,_, tl , J =, Japce

24, Think about the experiment as laying down the b + r balls from left to right in a random order.
The number of possible orderings equals the number of ways of placing the blue balls, namely ou ).
The number of ways of placing the balls so that the first k are blue, and the next red, is the number of
ways of placing the red balls so that the first is in position A + 1 and the remainder are amongst the

r+b—k-—1 places to the right, namely (” teh), The required result follows.

The probability that the last ball is red is r/(r + b), the same as the chance of being red for the
ball in any other given position in the ordering.

25. We argue by induction on the total number of balls in the urn. Let pg- be the probability that the
last ball is azure, and suppose that pac = 4 whenever a,c > 1,a+c<k. Leta ando be such that
a,o0 >1,a+o0=k-+1. Let A; be the event that i azure balls are drawn before the first carmine
ball, and let C; be the event that j carmine balls are drawn before the first azure ball. We have, by
taking conditional probabilities and using the induction hypothesis, that

a G
Pao = So Pa-ise P(Ai) + > Pa,a—jP(Cj)

i=l j=l
a-l o-l
= po,oP(Aa) + Pa,oP(Co) + 5 >, P(Ai) + 3 >, PCC).
i=l j=l

Now po,o = 0 and py.o = 1. Also, by an easy calculation,

a a-—1l 1 ato!
ato ato-1 o+1  (a+oa)!

P(Aq) = = P(Co).

It follows from the above two equations that

Pao = 1(SoPaD + YPC) + 4(P(Co) — P(Aw)) = 4.

i=] j=l

26. (a) If she says the ace of hearts is present, then this imparts no information about the other card,
which is equally likely to be any of the three other possibilities.

(b) In the given protocol, interchange hearts and diamonds.

27, Writing A if A tells the truth, and A° otherwise, etc., the only outcomes consistent with D telling
the truth are ABCD, AB°C°D, A°BC°D, and A°B°CD, with a total probability of at. Likewise, the
only outcomes consistent with D lying are A°B°C°DS, ACBCDS, AB°CD*, and ABC°D*S, with a total
probability of 3. Writing S for the given statement, we have that

PO |S) = P(DNS) _ 13
~ PIDNS)+P(DENS) By 2s ~ Al’

147

[1.8.28]-[1.8.33] Solutions Events and their probabilities

Eddington himself thought the answer to be rae hence the ‘controversy’. He argued that a truthful
denial leaves things unresolved, so that if, for example, B truthfully denies that C contradicts D, then
we cannot deduce that C supports D. He deduced that the only sequences which are inconsistent with
the given statement are AB°CD and AB°C°D*, and therefore

25 25

8r
PDS) = a5" ae = 77 -
ate 7

Which side are you on?

28. Let B, be the event that the rth vertex of arandomly selected cube is blue, and note that P(B,) =
tb: By Boole’s inequality,

8 8

P(U B,) < UPB) = i <1,
r=] r=1

so at least 20 per cent of such cubes have only red vertices.

29. (a) P(B | A) = P(AN B)/P(A) = P(A | B)P(B)/P(A) > P(B).

(b) P(A | BS) = P(AN B°)/P(B°) = {P(A) — P(AN B)}/P(BS) < P(A).

(c) No. Consider the case ANC = @.

30. The number of possible combinations of birthdays of m people is 365”; the number of combina-
tions of different birthdays is 365!/(365 — m)!. Use your calculator for the final part.

1
(c) 7
r!

«/()
HI /0)

32. In the obvious notation, P(wS, xH, yD, zC) = (3) (3) (") (7) / (73) . Now use your calcula-
tor. Turning to the ‘shape vector’ (w, x, y,z) withhw >x > y>z,

4P(wS,xH, yD,zC) ifwAx=y=z,

12P(wS,xH, yD,zC) ifw=x¢y#z,

on counting the disjoint ways of obtaining the shapes in question.

P(w,x,y,z) = {

ef) 0) OE) MOOG:
CVO: °C)

148

Problems Solutions [1.8.34]-[1.8.36]

34. Divide each of the following by 6°.

615! 615! 615!
31227 38(2H3" aE GBN2’
61 615! . 615!

41312! | (42’
615!
(51)?

35. Let S, denote the event that you receive r similar answers, and T the event that they are correct.
Denote the event that your interlocutor is a tourist by V. Then TM V° = @, and

PITAVNS,) _ PIT OS; | V)P(V)
P(S;) ~ P(Sr)

PT | Sr) =

Hence:
(a) P(T | S}) =} x 3/1 = 35.
(b) PCT | So) =)" - §/[{G@ + @}4 + 3] = 2
(©) PCT | $3) = GY - 3/[{(4)? + @?}3 + 3] = wy.
@ PCF | Sa) = ()*- 3/[{G)* + G)"}3 + 3] = 96-
(e) If the last answer differs, then the speaker is surely a tourist, so the required probability is

(q+ 4
(D3 x 44 G33

ale

36. Let E (respectively W) denote the event that the answer East (respectively West) is given.
(a) Using conditional probability,

23
PCE | East correct €°§.
P (Fast correct | E) = <4 | East correct) _ 34
+3

PE) te4 Gd
Gay)
eb+h+z3q-9 |
67 3 3°74

P(East correct | W) =

(b) Likewise, one obtains for the answer EE,

€- 2(3)2
3G _
«-33)24+ 0-09 (30)? +4)

and for the answer WW,
2/1,\2 1
(3g +3)
e-2+(1-632
(c) Similarly for EEE,

9
DG {eG + 1 - 933+ $)} = Goa

149
[1.8.37]-[1.8.39] Solutions Events and their probabilities

and for WWW,
e{(3)q)> +4} _ lle
[G+ 5)+0-83G)3 9426"

Then for € = a the first is SL. the second is 7 as you would expect if you look at Problem (1.8.35).
37. Use induction. The inductive step employs Boole’s inequality and the fact that

n+l n n
P(U Ar) = P(An41) +P(U Ar) -P(Uu, Ane):
r=1

r=1 r=1

38. We propose to prove by induction that

P(U Ar) < STP CA) — $2 P(A, 9 Aj).
r=1 r=]

2<r<n

There is nothing special about the choice of A; in this inequality, which will therefore hold with any
suffix k playing the role of the suffix 1. Kounias’s inequality is then implied.

The above inequality holds trivially when n = 1. Assume that it holds for some value of n (> 1).
We have that

n+l n n
°(U Ar) =( Ar) + P(An+1) =P(Anai n U*)

r=]

<SoP(Ar) — SD P(r Ay) + P(Ang1) — P(Ant nU Ar)
r=]

2<r<n r=]
n+1
<SUP(Ar)— D0 P(Ar NAY)
r=1 2<r<ntl

since P(An41 9 At) < P(An41 MUSE Ar):

39, We take n > 2. We may assume without loss of generality that the seats are labelled 1,2,...,n,
and that the passengers are labelled by their seat assignments. Write F for the event that the last
passenger finds his assigned seat to be free. Let K (> 2) be the seat taken by passenger 1, so that
P(F) = (n— pT! he a, wherea, = P(F | K =k). Note thata, = 0. Passengers 2,3,..., K—1
occupy their correct seats. Passenger K either occupies seat 1, in which case all subsequent passengers
take their correct seats, or he occupies some seat L satisfying L > K. In the latter case, passengers
K+1,K+2,...,L—1 are correctly seated. We obtain thus that

1
Oe = oe tht Mkt + 42 Fo + Mn), 2<k <n.

Therefore a, = 5 for 2 <k <n, by induction, and so P(F) = 4(n — 2)/(n - 1).

150

2

Random variables and their distributions

2.1 Solutions. Random variables

1. @Ifa > 0,x € R, then {w : aX@) < x} = [wm : X(w) < x/a} € F since X is a random
variable. If a < 0,

(w: aX(w) <4} = (0: X@) > x/a)={U {o:x(o) <*— ohh
a

n>l1 n
which lies in ¥ since it is the complement of a countable union of members of F. If a = 0,

@ ifx <0,

fo: ax(w) sx) ={ Q ifx>0:

in either case, the event lies in F.
(ii) For w € Q, X(w) — X(@) = 0, so that X — X is the zero random variable (that this is a random
variable follows from part (i) with a = 0). Similarly X(w) + X(w) = 2X (@).

2. Set ¥Y =aX +b. We have that

P(X <(y —b)/a) = F((y —b)/a) ifa > 0,

PY <y)= { P(X > (y —b)/a) =1—-limypyy—sy/a F(x) ifa <0.

Finally, if a = 0, then Y = b, so that P(Y < y) equals Oifb > yand1lifb<y.
3. Assume that any specified sequence of heads and tails with length n has probability 2~”. There
are exactly (7) such sequences with k heads.

If heads occurs with probability p then, assuming the independence of outcomes, the probability of
any given sequence ofk heads andn—k tails is p*(1—p)"—*. The answer is therefore (7) p*(1—p)"—*.

4. Write H =AF+(1—A)G. Then limy_,—.9 H(x) = 0, limy_5o9 H(x) = 1, and clearly H is
non-decreasing and right-continuous. Therefore H is a distribution function.

5. The function g(F (x)) is a distribution function whenever g is continuous and non-decreasing on
(0, 1], with g(0) = 0, g(1) = 1. This is easy to check in each special case.

151

[2.2.1]-[2.4.1] Solutions Random variables and their distributions

2.2 Solutions. The law of averages

1. Let p be the potentially embarrassed fraction of the population, and suppose that each sampled
individual would truthfully answer “yes” with probability p independently of all other individuals.
In the modified procedure, the chance that someone says yes is p + (1 —-p= 5 + p). If the
proportion of yes’s is now ¢, then 2¢ — 1 is a decent estimate of p.

The advantage of the given procedure is that it allows individuals to answer “yes” without their
being identified with certainty as having the embarrassing property.

2. Clearly H, + T, =n, so that (Hy, — Ty)/n = (2H, /n) — 1. Therefore
1 1
P(2p-1-e< + (Fy = Ts) $ 2p ~1 +6) =P ([o tp <5) >i

as n — oo, by the law of large numbers (2.2.1).

3. Let J, (x) be the indicator function of the event {X, < x}. By the law of averages, no} =| I)
converges in the sense of (2.2.1) and (2.2.6) to P(X, <x) = F(x).

2.3 Solutions. Discrete and continuous variables

1. With 6 = sup,, |am — @m—1|, we have that
|F(x) — G(x)| < |F(@m) — F(@m—1)| < |F@ + 8) — Fe — 8)}
for x € [am_—1, 4m). Hence G(x) approaches F (x) for any x at which F is continuous.

2. For y lying in the range of g, {Y < y}={X < eg lM} eF

3. Certainly Y is arandom variable, using the result of the previous exercise (2). Also
P(Y < y)=P(F7'(X) < y) = P(X < FQ) = FO)

as required. If F is discontinuous then F —l(x) is not defined for all x, so that Y is not well defined.
If F is non-decreasing and continuous, but not strictly increasing, then F—! (x) is not always defined
uniquely. Such difficulties may be circumvented by defining F —l¢x) = inffy : F(y) = x}.

4. The function Af+(1—A)g is non-negative and integrable over R to 1. Finally, fg is not necessarily
a density, though it may be: e.g.,if f= g=1,0<x <1then f@&)g(@x) =1,0<x <1.

5. (a)Ifd > 1, then fro cx 4 dx = c/(d —1). Therefore f is a density function if c = d — 1, and
F(x) = 1—x—@—) when this holds. If d < 1, then J has infinite integral and cannot therefore be a
density function. .

(b) By differentiating F(x) = e*/(1 + e*), we see that F is the distribution function, and c = 1.

2.4 Solutions. Worked examples
1 @ilfy>0,
P(X? < y) =P(X < Vy) — P(X < -J/¥) = F(/9) — F(-V9).
(b) We must assume that X > 0. If y > 0,

P(VX < y)=PO<X <y’) = F(y”).

152

Random vectors Solutions [2.4.2]-[2.5.6]

()If-l<y<1l,

~

P(sinX <y)= S> P((Qn+1)x -sin” y < X < (Qn+2)n +sin“! y)
n=—0o
OO
= > {F(@n+2)x + sin“! y) — F(Qn + 1m - sin! y)}.
n=—-0o0

(d) P(G71(X) < y) = P(X < G(y)) = F(G0)).

(e) If0 < y < 1, then P(F(X) < y) = P(X < F71(y)) = F(F71(y)) = y. There is a small
difficulty if F is not strictly increasing, but this is overcome by defining F~!(y) = sup{x : F(x) = y}.
() PG" (F(X)) < y) = P(F(X) < G(y)) = GQ).

2. It is the case that, for x € R, Fy (x) and Fz(x) approach F(x) as a > —00, b > oo.

2.5 Solutions. Random vectors

1. Write few = P(X =x, W = w). Then foo = for = 4, fio = 4, and frw = 0 for other pairs
Xx, W.

2. (a) We have that

p if (x, y) = (1, 9),
fxy@.y) = | 1-p if@,y)=(@,1)),
0 otherwise.

(b) Secondly,
1-—p if @,z)= (0,0),

fx,z(%.z) = P if @, z) = (1, 0),

0 otherwise.
3. Differentiating gives fy y(x, y) = e-*/{w(1 + y2)}, x>0,yeER.
4. LetA={X <b,c<Y<d},B={a<X <b, Y <d}. Clearly
P(A) = F(6,d)— F(b,c), P(B) = F,d)— F(a,d), P(AU B)= F(, d) — F(a, c);

now P(A NM B) = P(A) + P(B) — P(A U B), which gives the answer. Draw a map of R? and plot the
regions of values of (X, Y) involved.

5. The given expression equals
PX =x, Y¥<y)-PX =x, Ys y-1=P(X =x, Y=y).

Secondly, for 1 <x < y < 6,
_ 1] r _ r _ -1 r
1\"
(z) ifx=y.

6. No, because F is twice differentiable with 02 F /axdy <0.

f@yY=

153

[2.7.1]-[2.7.6] Solutions Random variables and their distributions
2.7 Solutions to problems
1. By the independence of the tosses,
P(X > m) = P(first m tosses are tails) = (1 — p)”.

Hence
px sx)={ 1-(—pyi fx 20,
ifx <0.
Remember that |x | denotes the integer part of x.
2. (a) If X takes values {x; : i > 1} then X = 7%, xj14, where Aj = (X = xj}.

(b) Partition the real line into intervals of the form [k2~™”, (k + 1)2-"), -—0oo < k < o, and
define Xm = po _ 4g kK27 Ik,m where Ix m is the indicator function of the event {k2-" < X <
(k + 1)27™}. Clearly Xj is a random variable, and X(w) ¢ X(w) as m — oo for all w.

(c) Suppose {Xm} is a sequence of random variables such that Xm(@) t X(@) for all w. Then
{X <x} =]\.{Xm < x}, which is a countable intersection of events and therefore lies in F.

3. (a) We have that

(K+¥<x}=f] U (Xsrn¥sx-rtn7})
n=l reqt

where Q* is the set of positive rationals.

In the second case, if XY is a positive function, then X Y = exp{log X + log Y}; now use Exercise
(2.3.2) and the above. For the general case, note first that |Z| is a random variable whenever Z is
a random variable, since {|Z| < a} = {Z < a}\ {Z < —a} fora > 0. Now, if a > 0, then
{X¥ <a} = {XY <0} U{|XY] < a} and

{XY <0} = ({X <O}N{¥ > O}) U ({X > OFN{Y <0}).

Similar relations are valid if a < 0.
Finally {min{X, Y} > x} = {X > x}M{Y > y}, the intersection of events.

(b) It is enough to check that aX + BY is arandom variable whenever a, B € R and X, Y are random
variables. This follows from the argument above.

If Q is finite, we may take as basis the set {74 : A € ¥} of all indicator functions of events.
4, (a) F(3) — F(4) = 5.
(b) F(2) — F(1) = 5.
(c) P(X? < X) = P(X <1) =}.
(d) P(X < 2X*) = P(X > 5) = 3.
(e) P(X +X? < 7) = P(X < 4) =f.
() (VX <2) = P(X <2”) = 42” if0 <2 < V2.
5. P(X =-1)=1-p, P(X =0)=0, P(X > 1)= 4p.
6. There are 6 intervals of 5 minutes, preceding the arrival times of buses. Each such interval has

probability ra = ab. so the answer is 6- ib = ;-

154

Problems Solutions [2.7.7]-[2.7.12]

7. Let T and B be the numbers of people on given typical flights of TWA and BA. From Exercise
(2.1.3),

10\ (9\* 7 1\'0* 20\ (9\" (1 \70-*
r=n=(') (3) (10) ro=n=(?) (5) (i)

Now
P(TWA overbooked) = P(T = 10) = (2,)!°,
P(BA overbooked) = P(B > 19) = 20()!9() + 3)”,
of which the latter is the larger.

8. Assuming the coins are fair, the chance of getting at least five heads is (5)° + 6(4)° = &.
9. (a) We have that

raxt<xy={% ifx <0,
Fa) ifx>0.
(b) Secondly,
ifx <0,

P(X” <x) { 0
x)=
~ 1—limys-, Fy) ifx>0.

(c) P(\X| <x) = P(—x < X <x) ifx > 0. Therefore

0 ifx <0,
P(X| <x) = . .
F(x) —limyt-x F(Qy) fx = 0.

(d) P(~X <x) =1-limy4_,y Fy).
10. By the continuity of probability measures (1.3.5),

P(X = xo) = lim Py < X < x9) = F(xo) — lim F(y) = F(xo) — F@o-).
ytxo ytxo

using general properties of F. The result follows.

11. Define m = sup{x : F(x) < 4}. Then F(y) < 5 for y < m, and F(m) > 5 (if F(m) < 4 then
F(n') < 4 for some m’ > m, by the right-continuity of F, a contradiction). Hence m is a median,
and is smallest with this property.

A similar argument may be used to show that M = sup{x : F(x) < 5} is a median, and is largest
with this property. The set of medians is then the closed interval [m, M].

12. Let the dice show X and Y. Write S = X + Y and f; = P(X =i), g; = P(Y =i). Assume that
P(S = 2) = P(S = 7) = P(S = 12) = 7}. Now

P(S = 2) = P(X = )P(Y = 1) = figs.
P(S = 12) = P(X = 6)P(Y = 6) = fogs,
P(S = 7) > P(X = DP(Y = 6) + P(X = O)P(Y = 1) = Sige + fo81-

It follows that f,g1 = /gg6, and also

1 & , Joy _ 4b :
77 7 PS = D2 fiat (%+#)=-5 («+=)

155

[2.7.13]-[2.7.14] Solutions Random variables and their distributions

where x = g6/g,. However x + x7! > 1 for all x > 0, a contradiction.
13. (a) Clearly dy, satisfies (i). As for (ii), suppose that d.(F, G) = 0. Then

F(x) <lim{G(x +6) +€} = Ga)
€10

and
F(y) = lim{G(y — «) —e} = GQ).
€10

Now G(y—) > G(x) if y > x; taking the limit as y | x we obtain

F(x) = lim G(y—) = G(x),
yx

implying that F(x) = G(x) for all x.

Finally, if F(x) < G@w +e) +e and G(x) < A(x + 6) +4 6 for all x and some ¢, 5 > 0, then
F(x) < H(x+6+6€)+6€46 for all x. A similar lower bound for F(x) is valid, implying that
dL (F, H) < dL (F, G)+4(G, H).

(b) Clearly dy satisfies (i), and dpy(X, Y) = O if and only if P(X = Y) = 1. By the usual triangle
inequality,

[P(X =k) -P(Z =k)| < |P(X =) -PY =’ | +|PY =H -P(Z=4)I,

and (iii) follows by summing over k.
We have that

2|P(X € A) — P(Y € A)| = |(P(X € A) — PY € A)) — (P(X € AS) - PLY € AY))|

= Dex =k) -P(Y =k) Ja’)
k

where J4(k) equals 1 if k € A and equals —1 if k € A°. Therefore,

2|P(X € A) — P(Y € A)| < )o|P(X = — PY =4)| - Jal < arv(X, ¥).
k
Equality holds if A = {k : PX =k) > P(Y = k)}.
14, (a) Note that
OF
axdy
so that F is not a joint distribution function.
(b) In this case

-e* <0, x,y >0,

a°F =f ifO0<x<y,
axdy lO if0<y <x,

I i dx dy =1.
0 Jo axdy

Hence F is a joint distribution function, and easy substitutions reveal the marginals:

and in addition

— ij —1_p7*
FxQx) = lim FG, y) = 1 e*, x>Q0,

Fy(y)= lim Fx, y)=1-e%—ye%, y20.

156

Problems Solutions [2.7.15]-[2.7.20]

15. Suppose that, for some i # j, we have p; < p; and B; is to the left of B;. Write m for the
position of B; and r for the position of B;, and consider the effect of interchanging B; and B;. For
k<mandk > r,P(T > k) is unchanged by the move. Form < k <r, P(T > k) is decreased by an
amount p; — p;, since this is the increased probability that the search is successful at the mth position.
Therefore the interchange of B; and B; is desirable.

It follows that the only ordering in which P(T = k) can be reduced for no k is that ordering in
which the books appear in decreasing order of probability. In the event of ties, it is of no importance
how the tied books are placed.

16. Intuitively, it may seem better to go first since the first person has greater choice. This conclusion
is in fact false. Denote the coins by C, C2, C3 in order, and suppose you go second. If your opponent

chooses C; then you choose C3, because P(C3 beats Cy) = 2 + z . 3 = 38 > 5}. Likewise

P(C, beats Cz) = P(C> beats C3) = 3 > 5. Whichever coin your opponent picks, you can arrange
to have a better than evens chance of winning.

17. Various difficulties arise in sequential decision theory, even in simple problems such as this one.
The following simple argument yields the optimal policy. Suppose that you have made a unsuccessful
searches “ahead” and b unsuccessful searches “behind” (if any of these searches were successful, then
there is no further problem). Let A be the event that the correct direction is ahead. Then

P(current knowledge | A)P(A)
P(current knowledge)
_ (1 — p)*a
~ (l= p)ta+ (1 = phd - a)’

P(A [ current knowledge) =

which exceeds 5 if and only if (1 — p)*a > (1 - py? (i — a). The optimal policy is to compare
(1 — p)*a with (1 — py? (1 — a). You search ahead if the former is larger and behind otherwise; in
the event of a tie, do either.

18. (a) There are (S$) possible layouts, of which 8+8+2 are linear. The answer is 18/ - ).

(b) Each row and column must contain exactly one pawn. There are 8 possible positions in the first
row. Having chosen which of these is occupied, there are 7 positions in the second row which are

admissible, 6 in the third, and so one. The answer is 8!/ (S').

19, (a) The density function is f(x) = F’(x) = 2x e7*”, x>0.
(b) The density function is f(x) = F’(x) = x2e7!/*, x > 0.
(c) The density function is f(x) = F’(x) = 2(e* + e7*)72, xeéER.
(d) This is not a distribution function because F’(1) < 0.
20. We have that
PU = y= f fu,v(u, v)dudv =0.
{(u,v):u=v}

The random variables X, Y are continuous but not jointly continuous: there exists no integrable
function f : [0, 17 — R such that

x Py
rx sx. ¥sy)= [ / ft,v)dudv, O<x,y<1.
u=0 /v=0

157
3

Discrete random variables

3.1 Solutions. Probability mass functions

1 (a Cmh = eh 2-* = 1.

(b) C7! = 09° 2-*/k = log 2.

(c) C71 = Ok? = 1/6.

(d) C7! = of? 2k kt = e? - 1.

2 (i) $31 — (log 2)7!; 1 — 62 ~*; (e? — 3) /(e? — 1).

(ii) 1; 1; 1; 1 and 2.

(iii) It is the case that P(X even) = S772, P(X = 2k), and the answers are therefore

(a) 5, (b) 1 — Cog 3)/(log 4), (c) f. (d) We have that
2k 2 + (-2)!

— 1= ler + e727) -1
: 2 ?
i (2k)! Far 2!)
so the answer is tq - e~2)

5 .

3. The number X of heads on the second round is the same as if we toss all the coins twice and count
the number which show heads on both occasions. Each coin shows heads twice with probability p’,
so P(X =k) = (7) p* (1 — p?y"-*,

4. Let Dx be the number of digits (to base 10) in the integer k. Then

P(N =k) = P(N =k|T = Dy) P(T = Dy) = 2~Pk,

ISD,

5. (a) The assertion follows for the binomial distribution because k(n — k) < (n —k +1)(K +1).
The Poisson case is trivial.

(b) This follows from the fact that k® > (k2 — 1)*.

(c) The geometric mass function f(k) = gp, k>0.

3.2 Solutions. Independence
1. We have that
PX =1,Z=)=PX=1,Y=)=}=P(X =1)P(Z = 1).

158

Independence Solutions [3.2.2]-[3.2.3]

This, together with three similar equations, shows that X and Z are independent. Likewise, Y and Z
are independent. However

PX =1LY=1,Z2=-1)=04 4 =P(X = )PY = )P(Z =-1),
so that X, Y, and Z are not independent.
2. (a)Ifx > 1,
P(min{X, Y} <x) =1—-P(X >x,¥ >x)=1-—P(X > x)P(Y > x)
=1-27*.2%*=1-4%.
(b) P(Y > X) = P(Y < X) by symmetry. Also
PY > X)+P(Y < X)4+P(Y=X)=1.

Since

PY =X)=>) PY =X =x)=) 2-27 = 3,

we have that P(Y > X) = }-
(c) 5 by part (b).

CO

(d) P(X >kY) = S$ P(X =kY,Y =y)
y=l1
le ¢)

P(X >ky, Y=y)=

Me

P(X > ky)P(Y = y)

Il
_

y

Co w 2
gk+1 4
242?
le ¢)
(e) P(X divides ¥) = )° P(Y = kX) = 3 3 P(Y = kx, X =x)
k=1 k=1x=1
— kx gx _
=e = eta"
k=1 x=1 k=1
(f) Let r = m/n where m and n are coprime. Then
fo) lo. @) 1
_ _ _ _ _ —kma—kn
POX =r) = DPX = km, Y= kn) = D2 2" =
k=] =]

3. (a) We have that

P(X, < Xz < X3)= S> (1— py) — po) — ps) pit ph pk

i<j<k
“eg
= S01 pi) = po)p | Ps PS
i<j
-> (1 — pi) — pa) pi! (paps)! pa

; 1— prp3
_ (= pi) = pa) p2P3
(1 — p2p3)(1 — pi p2P3)

159

[3.2.4]-[3.2.5] Solutions Discrete random variables

(b) P(X, <X_<X3)= So (1 py — po) — ps) pit ps" pk!
ixjsk
_ (= pi) = pa)
(1 — pop3) — pi p2p3)

4. (a) Either substitute py = pz = p3 = 2 in the result of Exercise (3b), or argue as follows, with
the obvious notation. The event {A < B < C} occurs only if one of the following occurs on the first
round:

(i) A and B both rolled 6,
(ii) A rolled 6, B and C did not,
Gii) none rolled 6.
Hence, using conditional probabilities,

P(A < B<C)= (2)? +4(3)*P(B < C) + (3)°P(A < B< ©),
In calculating P(B < C) we may ignore A’s rolls, and an argument similar to the above tells us that

P(B < C) = (2)°P(B<C) +}.
Hence P(B < C) = t yielding P(A < B<C)= fuer:
(b) One may argue as above. Alternatively, let N be the total number of rolls before the first 6 appears.
The probability that A rolls the first 6 is

P(Ne{1L,4,7,...})= >> (3) = %.
k=1,4,7,...

Once A has thrown the first 6, the game restarts with the players rolling in order BCABCA. ... Hence
the probability that B rolls the next 6 is ag also, and similarly for the probability that C throws the

third 6. The answer is therefore (38)°.

5. The vector (—X; : 1 <r <n) has the same joint distribution as (X; : 1 <r <n), and the claim
follows.
Let X +2 and Y + 2 have joint mass function f, where fj; is the (@, 7)th entry in the matrix

1<i,j <3.

Ae One
i ertcalleete Lent
Nears

Then

PX =-D=P(X=)=WY=-)=PY=)=4, P(X =0)=P(Y =0) = 4,
P(X +¥=-2)=%4 fp =P(X+Y =2).

160

Expectation Solutions [3.3.1]-[3.3.3]

3.3 Solutions. Expectation

1. (a) No!
(b) Let X have mass function: f(—1) = 5: f()= $, f¢ fQ)= 5. Then
4+2=
9

E(X)=-g+$4+$8=1=-$4+84+3 =E(1/%).
2. (a) If you have already j distinct types of object, the probability that the next packet contains
a different type is (c — j)/c, and the probability that it does not is j/c. Hence the number of days
required has the geometric distribution with parameter (c — j)/c; this distribution has mean c/(c — j).
(b) The time required to collect all the types is the sum of the successive times to collect each new

type. The mean is therefore
c-1

j=0

3. (a) Let Jj; be the indicator function of the event that players i and 7 throw the same number. Then
6 2
E(Vij)=PUj =YD=>0(b)=§ FFA.

The total score of the group is S = }7;<; 1ij, 80

B(S) = ) Ej) = :(; )

i<j

We claim that the family {J;; : i < j} is pairwise independent. The crucial calculation for this is
as follows: ifi < j < k then

6
E(1j1jk) = PG, j, and k throw same number) = > (1)? = 4 = E(hyj E(x).
r=]
Hence
var(S) = var (x Ij) = S> var(1j;) = (; var(112)
i<j i<j

by symmetry. But var(/}2) = z(1 - 2).
(b) Let X;; be the common score of players i and j, so that X;; = 0 if their scores are different. This
time the total score is S = }7;.; Xij, and

1 7 7
B(S) = (3) = (5) 627 12 ()

The Xj; are not pairwise independent, and you have to slog it out thus:

2
var(S) = E | (Xxs) — E(S)*

i<j

= (;) eat + (3) cnax + (3) _ (;) _ (:) Et 7 ) (:)

161

[3.3.4]-[3.4.1] Solutions Discrete random variables

4. The expected reward is 77°, 2-*.2k = oo. If your utility function is w, then your ‘fair’ entrance
fee is we 2-ky (2k). For example, if u(k) = c(1 — k~®) for k => 1, where c, a > O, then the fair

fee is
k k 1
_ ok, _ _
oD? d-2 )=e(1 az):

This fee is certainly not ‘fair’ for the person offering the wager, unless possibly he is a noted philan-
thropist.

5. We have that E(X%) = S792, x®/{x(x + 1)}, which is finite if and only if a < 1.
6. Clearly )

var(a + X) = E({(a +X) —E(a + X)}”) = E({X — E(X)}*) = var(X).

7. For each r, bet {1 + a(r)}—! on horse r. If the rth horse wins, your payoff is {z(r) + 1}{1+
x(r)}~! = 1, which is in excess of your total stake 37, {1 (k) + 1}7!.

8. We may assume that: (a) after any given roll of the die, your decision whether or not to stop
depends only on the value V of the current roll; (b) if it is optimal to stop for V = r, then it is also
optimal to stop when V > r.

Consider the strategy: stop the first time that the die shows r or greater. Let S(r) be the expected
score achieved by following this stategy. By elementary calculations,

S(6) = 6- P(6 appears before 1) + 1 - P(1 appears before 6) = 5,

and similarly $(5) = 4, S(4) = 4, 53) = 2, S(2) = 4. The optimal strategy is therefore to stop at
the first throw showing 4, 5, or 6. Similar arguments may be used to show that ‘stop at 5 or 6’ is the
rule to maximize the expected squared score.

9. Proceeding as in Exercise (8), we find the expected returns for the same strategies to be:
S@)=4-3c, S(6)=4-2c, S(4)=4-3e, SBV=P-$c, S2=F-c.

Ifc= 7 it is best to stop when the score is at least 4; if c = 1, you should stop when the score is at
least 3. The respective expected scores are ui and 13.

3.4 Solutions. Indicators and matching

1. Let J; be the indicator function of the event that the outcome of the (j + 1)th toss is different from
the outcome of the jth toss. The number R of distinct runs is given by R = 1+ a I;, Hence

E(R) =14+@—DE()) =14+ @—-—1)2pq,
where g = 1 — p. Now remark that J; and J;, are independent if |j — &| > 1, so that
n—-l

>

j=l

2
E{((R — 17} = e{ ( ij) \ = (n— DE(h) +2 — DE)
+ {(n— 1)? — @—1) -2(n — 2) }E()?.

162
Indicators and matching Solutions [3.4.2]-[3.4.5]
Now E(I,) = 2pq and E(, Jy) = p*q + pq” = pq, and therefore

var(R) = var(R — 1) = (n ~ 1)E(1)) + 2 — 2)E( a) — { — 1) + 2(n — 2) JE)?
= 2pq(2n — 3 — 2pq(3n — 5)).

2. The required total is T = yy X;, where X; is the number shown on the ith ball. Hence
E(T) = kE(X)) = Sk(n + 1). Now calculate, boringly,

2
e (> xi) \ = KE(X?) + kk — 1)E(X1 X2)

2 kk-)D,
=2b + m1) 1) 2d

i>j

Ky _1
~ {4n@+ In +2)— jan +0}

kKkK-DG, a
rors py IGN)

= hk(n + 1)Qn +1) + pyk(k — Gn + 2(n + 1.
Hence

var(T) = k(n + 1){ $Qn +1) + yk — Gn +2) — Fkn t+ D} = Feet Din - 8.

3. Each couple survives with probability
2n—2 2n m m
( m \/() =(1-3) (1-54).
m m
1-—){1- .
” ( 7) ( 2n — :)

4. Any given red ball is in urn R after stage k if and only if it has been selected an even number of
times. The probability of this is

and the mean number of such red balls is n times this probability.

so the required mean is

5. Label the edges and vertices as in Figure 3.1. The structure function is
(x) =X5+1- xs {a — X1)X4[X3 + (1 — X3)X2X6]
+X1 [Xp + (1 = Xp)(X3(Xo + X4(1 — X6)))| \

163

[3.4.6]-[3.4.9] Solutions Discrete random variables

s 4

Figure 3.1. The network with source s and sink t.

For the reliability, see Problem (1.8.19a).
6. The structure function is I;s>j;;, the indicator function of {S > k} where S = "=| Xc. The
reliability is therefore )7?_, (7) p'(1 — p)??.

7. Independently colour each vertex livid or bronze with probability 5 each, and let L be the random

set of livid vertices. Then EN; = 51E |. There must exist one or more possible values of Nz, which
are at least as large as its mean.

8. Let J, be the indicator function that the rth pair have opposite polarity, so that X = 1+ yr} I.
We have that P(J; = 1) = 5 and PUI, = I,41 = 1) = 4, whence EX = $(n + 1) and var X =
4 — 1).

9. (a) Let A; be the event that the integer i remains in the ith position. Then

P(U ar) = Pan) = PrN As) ++ DP) Ar)
r=] r r

r<s

ee 2 ee eT
ma (C\aont OD

Therefore the number M of matches satisfies

1 1 nl
P(M =0)= tI.

Now

P(M=r)= (") P(r given numbers match, and the remaining n — r are deranged)
r

_ n! (n—r)! (G-gte bene + ).

“rin—-r)! al

(b)
n+l

dng = S #{derangements with 1 in the rth place}
r=2
= n{#{derangements which swap 1 with 2}
+ #{derangements in which 1 is in the 2nd place and 2 is not in the 1st place}}
= ndy—-1 + ndn >

164

Dependence Solutions [3.5.1]-[3.6.2]

where #A denotes the cardinality of the set A. By rearrangement, d,4 1 —(n+1)dn = —(dn—ndy_}).
Set uy = dy — nd,_1 and note that uz = 1, to obtain u, = (—1)", n > 2, and hence

ni ont

dn = 3 — 3

i}
fet (I,
n!

Now divide by n! to obtain the results above.

3.5 Solutions. Examples of discrete variables

1. There are n!/(n,!n!---n;!) sequences of outcomes in which the ith possible outcome occurs
n; times for each i. The probability of any such sequence is p;! p,? --- p;*, and the result follows.

2. The total number H of heads satisfies

[o.¢) 0° n Aner
P(A =x)= >> PH =x|N=n)P(N=n)= > ("ora = py *—

n=x n=xX
_ pyre? S (al — pyr %e UP

x! oat (n—x)!

The last summation equals 1, since it is the sum of the values of the Poisson mass function with
parameter A(1 — p).

3. dpn/dd = Pp—1 — Pn Where p_, = 0. Hence (d/dd)P(X <n) = pp(A).

4. The probability of a marked animal in the nth place is a/b. Conditional on this event, the chance
of n — 1 preceding places containing m — 1 marked and n — m unmarked animals is

(521) (028) /Cc))

as required. Now let X; be the number of unmarked animals between the j — 1th and jth marked
animals, if all were caught. By symmetry, EX; = (6 — a)/(a + 1), whence EX = m(EX; +1) =
m(b + 1)/(a4+ 1).

3.6 Solutions. Dependence

1. Remembering Problem (2.7.3b), it suffices to show that var(aX + bY) < o if a,b € R and
var(X), var(Y) < oo. Now,

var(aX + bY) = E({aX + bY - B(aX +bY)}’)
= a” var(X) + 2ab cov(X, Y) + b* var(Y)
< a’ var(X) + 2ab/var(X) var(Y) + b” var(Y)
2
= (ay/var(X) + by/var(¥))

where we have used the Cauchy—Schwarz inequality (3.6.9) applied to X — E(X), Y — E(Y).

2. Let N; be the number of times the ith outcome occurs. Then Nj; has the binomial distribution
with parameters n and p;.

165
[3.6.3]-[3.6.8] Solutions Discrete random variables

3. Forx =1,2,...,

-¥${ —
~ 2\l@+y-Dat+y) @&t+ty@tyFt)
C

_ _Ccfl i)
~Ox(xt1) 2\x xti1/)’

and hence C' = 2. Clearly Y has the same mass function. Finally E(X) = S792) @ + 1)~! = 0, so
the covariance does not exist. ‘

4, Max{u, v} = su tov)+ 5 lu — v|, and therefore
E(max{X, ¥7}) = 4E(X? + ¥?) + ZE|(X —Y)(X+Y)|
<1+ 4 /E((x — Y)?)E((X + ¥)2)
=144/2—29)2 4 2p) =14+/1-p?,

where we have used the Cauchy—Schwarz inequality.

5. (a) logy < y — 1 with equality if and only if y = 1. Therefore,

E (108 we) E [ae _ | -0,
fx(X) fx(X)

with equality if and only if fy = fy.
(b) This holds likewise, with equality if and only if f(x, y) = fx (x) fy (y) for all x, y, which is to
say that X and Y are independent.

6 (alatb+ec = E{htysy} + Kysz) + Yz>x}} = 2, whence min{a, b,c} < 4. Equality is
attained, for example, if the vector (X, Y, Z) takes only three values with probabilities f(2, 1,3) =
£,2,1) = f(, 3,2) = 4.

(b) P(X < Y) = P(Y < X), etc.

(c) We have thatc = a = pandb=1— p?. Also sup min{p, (1 — p*)} = 4/5 — 1).

7. Wehave for 1 <x <9 that

1
fx(x) = do (14 5 —~)= eT (14 ios 5) =e (145).

=0

By calculation, EX ~ 3.44.

k i ak aes j
a kala e?(j ta)a/
8. 0 = ed ie G+ ie} ae —

Gi) 1 = SO fxeG) = 2ace**, whence c = e~74/(2a).
i

. cra’ era™2?
© fe = 2 GH mre

IV

166

Conditional distributions and conditional expectation Solutions [3.7.1]-[3.7.4]

Gi) BK +¥ — 1) = TE DOAN" _

2a. Now E(X) = E(Y), and therefore E(X) = a + 4.

r=1

3.7 Solutions. Conditional distributions and conditional expectation
1. (a) We have that

EQ@Y +bZ|X=x)=) Gy +bz)P(Y =y,Z=z| X =x)

YZ
=a) yPY =y,Z=2|X=x) +b) 2PY =y,Z =2| X =x)
Bare y,Z
=a) yPY =y|X=x)+b) > 2P(Z=z| X =x).
y Zz

Parts (b)—(e) are verified by similar trivial calculations. Turning to (f),

He |X.2 |x =x) = Ef Doe ay Xan 2 aae=n2=21¥=9}

y
PY =y,X =x,Z=7) P(X =x,Z=2)
~ LL? P(X=x,Z=z2) £P(X=x)

_ Soy ay x= E01 x=0
y

=E{EY x) |x =x,Z=z}, by part (e).

2. If ¢ and w are two such functions then E((@(X) — #(X))g(X)) = 0 for any suitable g. Setting
g(X) = Ix=x; for any x € R such that P(X = x) > 0, we obtain ¢(@~) = w(x). Therefore
P@(X) = W(X) = 1.

3. We do not seriously expect you to want to do this one. However, if you insist, the method is to
check in each case that both sides satisfy the appropriate definition, and then to appeal to uniqueness,
deducing that the sides are almost surely equal (see Williams 1991, p. 88).

4. The natural definition is given by
var(Y | X =x) =E({¥ -E(¥ |X =x)}?|X =2).
Now,

var(Y) = E({Y —EY}’) =E [E({Y ~E(Y |X) +E(¥ | X)—EY}*| x)|
= E(var(Y | X)) + var (E(Y | X))

since the mean of E(Y | X) is EY, and the cross product is, by Exercise (1e),

2e|B({Y —E(Y | X)}{E( | X)- EY} | x)]

~= 2 | {EY | X) -EY}E{Y —-E(Y | X)| x} =

167

[3.7.5]-[3.7.10] Solutions Discrete random variables

since E{Y — E(Y | X)| X} =E(Y | X) -E(Y | X) =0.
5. We have that

lo. @)

oo
PT >t+r)
ET -t|T>)=) PT>ttr|T>H)=>> Hsp
r=0 r=0
N-t
(a) EIT —1|T>1) = )) —— =3N-1 +I).
r=0
OO 4-(t4tr)
(b) ET -t|T>)=)) xa =
r=0

6. Clearly
n
E(S | N =n) = e(S>x:) = pn,
i=1

and hence E(S | N) = uN. It follows that E(S) = E{E(S | N)} = E(uN).
7. A robot passed is in fact faulty with probability 7 = {@(1 — 8)}/(1 — $8). Thus the number of
faulty passed robots, given Y, is bin(n — Y, 2), with mean (n — Y){@(1 — 5)}/(1 — $5). Hence

(n— Y)o0 — 8)

E(X|Y)=Y¥+ 38

8. (a) Let m be the family size, 4, the indicator that the rth child is female, and jz; the indicator of
a male. The numbers G, B of girls and boys satisfy

G= Soe, B= our, EG) = m= E(B).

r=1 r=l

(It will be shown later that the result remains true for random m under reasonable conditions.) We
have not used the property of independence.

(b) With M the event that the selected child is male,
m1
E(G | M) = e(> 6r) = 4(m—-1) = E(B).
r=1

The independence is necessary for this argument.

9. Conditional expectation is defined in terms of the conditional distribution, so the first step is not
justified. Even if this step were accepted, the second equality is generally false.

10. By conditioning on X;,_1,
EXn = E[E(Xn | Xn—1)] = E[p(Xn-1 +1) +d — p)(Xn-1 +14 Xn)]

where X, has the same distribution as X,. Hence EX, = (+ EX,_1)/p. Solve this subject to
EX, = pa.

168

Sums of random variables Solutions [3.8.1]-[3.8.5]

3.8 Solutions. Sums of random variables

1. By the convolution theorem,

P(X+¥ =z) =) P(X =HPY =z—h)
k

k+1
srt fO<k<maAn,
(m+1)a@+1)
(mAn)+1 .
= ¢ ——— — ifman<k<mvn,
(m+ 1)(n +1)
m+n+1—k

——_——— _ ifmvn<k<m+n,
(m+1)@+1)

where m An = min{m, n} and m V n = max{m, n}.
2. Ifz>2,

Cc
z(z+1)'

[oe]
P(X+Y¥ =z) => P(X =k, Y=r-k)=
k=]
Also, if z > 0,

P(X -Y =z) =) P(X =k+z,Y =h)

k=]

cy 1

6 Ok +2- Qk +2)Qk +241)

Ine 1 1

=3¢)){ aepecparsy ” CeFDORFzFD}
08 1

-lcye

20) Gye det D

1
By symmetry, if z < 0, P(X — Y =z) = P(X — Y = —z) = P(X — Y = [2)).

zl -1 -1
_ rl _— gye-r-l _ oP {(1 — By - dd - a)’ }
3. > a(l—a)’ “pd — B) = ans .

r=1
4. Repeatedly flip a coin that shows heads with probability p. Let X; be the number of flips after
the r — 1th head up to, and including, the rth. Then X; is geometric with parameter p. The number
of flips Z to obtain n heads is negative binomial, and Z = )~”_, X; by construction.

r

5. Sam. Let X, be the number of sixes shown by 6n dice, so that X,41 = Xn +Y where Y has the
same distribution as X; and is independent of X,. Then,

6
P(Xng1 >a 4+1) = $0 P(Xn = nt+1—-r)PY =r)
r=0
6
=P(Xn =n) + >) [P(Xn =n +1 —r)—P(Xn > n)|PY =r).
r=0

We set g(k) = P(X, = &) and use the fact, easily proved, that g(n) > g(n—1)>--- > g(n—5) to
find that the last sum is no bigger than

6
g(n) )r — LP(Y =r) = g(n)(E(Y) - 1).
r=0

169
[3.8.6]-[3.9.2] Solutions Discrete random variables

The claim follows since E(Y) = 1.

—Ayn g(nje~* yn] —
6. (i) LHS = Srsine nN nt=r¥ oane ike = RHS.
Gi) Conditioning on N and Xy,

LHS = E(E(Sg(S) | N)) = E{NE(Xya(S)|N)}
en Ayn nal

=) fat +x)) dF(x) = RHS.

3.9 Solutions. Simple random walk

1. (a) Consider an infinite sequence of tosses of a coin, any one of which turns up heads with
probability p. With probability one there will appear a run of N heads sooner or later. If the coin
tosses are ‘driving’ the random walk, then absorption occurs no later than this run, so that ultimate
absorption is (almost surely) certain. Let S be the number of tosses before the first run of N heads.
Certainly P(S > Nr) < (1 - pn )’, since Nr tosses may be divided into r blocks of N tosses, each
of which is such a run with probability p’. Hence P(S = s) < (1 — p%)l/J, and in particular
E(S*) < oo for all k > 1. By the above argument, E(T*) < oo also.

2. If So =k then the first step X 1 satisfies

PX = DPW | X1=1) _ prey

POL = TI W) = P(W) _—"

Let T be the duration of the walk. Then

Jy = ECT | So =k, W)
=E(T | So=k, W, X, = 1)P(X, = 1| Sg =k, W)
+E(T | So =k, W, Xy = —DP(X, = —-1 | So=k, W)
= (1+ Jeg1) A + (1+ a) (1 - Pati? )
Pk Pk
PPk+1 Seth (Pk — PPk-+1) Fea
Pk Pk

as required.
Certainly Jo = 0. If p = 5 then py = 1 — (k/N), so the difference equation becomes

(N —k -VDJeg1 —2(N —BD Jp + (N —k + De = 2k — N)
fori <k < N —1. Setting up = (N — k) Jy, we obtain
Up] — 2ue + up = 2(k — N),

with general solution up = A+ Bk — S(N — k) for constants A and B. Now ug = uy = 0, and
therefore A = 4N?, B = —4N?, implying that J, = 4{N? —(N —k)*},0<k <N.

170

Random walk: counting sample paths Solutions [3.9.3]-[3.10.3]

3. The recurrence relation may be established as in Exercise (2). Set uy = (p* _ p’ )J;, and use
the fact that pz = (p* - eXy/ — ep) where p = q/p, to obtain

pugs — (1 —r)ug + quy_1 = e% — p*.

The solution is
k(ok + p)
P-q -
for constants A and B. The boundary conditions, uo = uy = 0, yield the answer.

ug = A+ Bok +

4. Conditioning in the obvious way on the result of the first toss, we obtain

Pmn = PPm—1,n + (1 — P)Pmn-1; ifm,n > 1.
The boundary conditions are pmo = 0, pon = 1. if m,n > 1.
5. Let Y be the number of negative steps of the walk up to absorption. Then E(X + Y) = D, and
y-Y= { N—k_ if the walk is absorbed at N,
—k if the walk is absorbed at 0.
Hence E(X — Y) = (N — k)(1 — pg) — kpx, and solving for EX gives the result.

3.10 Solutions. Random walk: counting sample paths

1. Conditioning on the first step X1,
P(T =2n) = 4P(T = 2n| X= 1) + 5P(T = 2n| X; =—-1)
= 4 f-12n—-1) +3 f1(2n -1)

where f,(m) is the probability that the first passage to b of a symmetric walk, starting from 0, takes
place at time m. From the hitting time theorem (3.10.14),

1 1 2n—1
fiQn —1) = f.yQn—-1) = P(Sn—1 = 1) = NTE \a-@a-D),
2n—1 2n—1 n

which therefore is the value of P(T = 2n).
For the last part, note first that “TP P(T = 2n) = 1, which is to say that P(T < oo) = 1; either
appeal to your favourite method in order to see this, or observe that P(T = 2n) is the coefficient of

s?” in the expansion of F(s) = 1— V1 —s2. The required result is easily obtained by expanding the
binomial coefficient using Stirling’s formula.
2. By equation (3.10.13) of PRP, for r > 0,
P(Mn =r) = P(My =r) —P(Mn > +)
= 2P(Sn =>r+1+P(Sp =r) — 2P(Sp, =r +2) —P(S, =r +1)
= P(Sy, =r) +P(Sp =r+ 1)
= max{P(Sp =r), P(Sn =r + 1}
since only one of these two terms is non-zero.

3. By considering the random walk reversed, we see that the probability of a first visit to Sz, at time
2k is the same as the probability of a last visit to Sp at time 2n — 2k. The result is then immediate
from the arc sine law (3.10.19) for the last visit to the origin.

171

[3.11.1]-[3.11.4] Solutions Discrete random variables

3.11 Solutions to problems

1. (a) Clearly, for alla, b € R,

P(g(X) =a, h(Y) =b) = So P(X =x, Y=y)
x,y:
g(x)=a,h(y)=b

= P(X = x)P(Y = y)

x,y!
&(x)=a,h(y)=b

= YO WxX=x) SO PWY=y)

x:g(x)=a y:h(y)=b
= P(g(X) = a)PC(Y) = 5).

(b) See the definition (3.2.1) of independence.

(c) The only remaining part which requires proof is that X and Y are independent if fy y(x, y) =
g(x)h(y) for all x, y € R. Suppose then that this holds. Then

fx(x) = S° fxy@y)=8@) S040), fro) => fxy@.y) =O) D> 8).
y y x x

Now

1= D0 fx@) = Do s@) DUO),
x x y

so that
fx) fry) = AG) D> 8) DAC) = g@)hO) = fx, y)-
x y

2. If E(X*) = 0, x°P(X = x) = O then P(X = x) = 0 for x 4 0. Hence P(X = 0) = 1.
Therefore, if var(X) = 0, it follows that PX —-EX = 0) = 1.

3. (a)

E(g(X)) = So yP(pQO=y)= 0 DO yP(X =x) = D0 g@)PX = 2)
y x

Y xe@e=y

as required.

(b) E(g(X)A(Y)) = Do a(x)AQ) fx,y ¥) by Lemma(3.6.6)
x,y

= So aay) fx (2) fy) by independence
x,y

= Yo a(x) fx) 0 AO AYO) = E(e(X)EA(Y)).
x y

4, (a)Clearly fy (i) = fyi) = 4 fori = 1, 2,3.
(b) (X+Y)(w1) = 3,(X+Y)(@2) = 5, (X+Y)(w3) = 4, and therefore fy +y (i) = 4 fori = 3, 4,5.
(c) (XY)(w1) = 2, (XY)(w2) = 6, (XY)(@3) = 3, and therefore fyy (i) = 4 for i = 2, 3, 6.

172

Problems Solutions [3.11.5]-[3.11.8]

(d) Similarly fx/y(@) = 4 fori = 5, $,3.

PYY=2,Z=2)  — P(w) 1

(e) friz2 12) = Baa = Bio Uw) 2?

and similarly fy)z(3 | 2) = 7 fy\z(1 | 1) = 1, and other values are 0.
(f) Likewise fzjy (2 | 2) = fzjy(213) = fzjyd |) =1.

5 wy ‘ =k +__t \ ak, and therefore k= 1
. Svna+) 4 \|n ntif” =1.

(b) $772.) kn™ = kt(—o) where ¢ is the Riemann zeta function, and we require a < —1 for the sum
to converge. In this case k = t(-—a)7}.

6. (a) We have that

n Ne pAgn-k k

e*x eeu
PX +Y =n) =D IMX =n- WP =) = DS

k=0 k=0
n! pars k n!
P(X =k,X+¥ =n)
®) OS RIXAV == Ox EY =a)

_ P(X =HPY =n—-k) _ (n\ aku

~  PX+¥=n) \kJA+w™

Hence the conditional distribution is bin(n, A/(A + 44).
7, (i) We have that

P(X =n+k,X >n)
P(X =n+k|X>ny= PX > n)

_ pa- pyitk-1

(ii) Many random variables of interest are ‘waiting times’, i.e., the time one must wait before the
occurrence of some event A of interest. If such a time is geometric, the lack-of-memory property
states that, given that A has not occurred by time 7, the time to wait for A starting from has the same
distribution as it did to start with. With sufficient imagination this can be interpreted as a failure of
memory by the process giving rise to A.

(iii) No. This is because, by the above, any such process satisfies G(k + n) = G(k)G(n) where
G(n) = P(X > n). Hence G(k + 1) = G(1)**! and X is geometric.

8. Clearly,

= pl — p)*! = P(X =b).

k
P(X+¥ =k) =) P(X =k-j,¥ = jf)
j=

k
ML _? _ . n . _}
= (," et ig™ (") ighd
\krd j

k
= pk pmtn-k m n = yk ymta-k morn

[3.11.9]-[3.11.13] Solutions Discrete random variables

which is bin(m +n, p).

9. Turning immediately to the second request, by the binomial theorem,
n\ ,_ n _
(x ty)" + Fy — x)" = 5 (<)> Hak 4 (ah }= ({) 40 k
k k even
as required. Now,

P(N even) = )~ (7) pea — pyr

k even
= }3{(p+1—p)"+(—p-— p)"} = 514+ —2p)"}
in agreement with Problem (1.8.20).

10. There are (°) ways of choosing k blue balls, and (7 fa) ways of choosing n — k red balls. The
total number of ways of choosing n balls is (¥ ), and the claim follows. Finally,

rane (hee bt ON 8)! N=)!

(b-k! (N-b—-n+k! WM!

N-b N—b—n+k+1)(N N-—n+4+1)7!
xX eee a ee
N N

> (°) pe — py*-* as N > oo.

11. Using the result of Problem (3.11.8),
P(X =k)P(Y = N —k)

P(X =k|X+Y=N)=

P(X +Y¥=N)
_ Grea Ey PN Ea NE _ WO (ve)
(Wy) PN g2n-N (w)

12. (a) E(X) =c+d,E(Y) = b+4+d, and E(XY) =d, so cov(X, Y) = d — (c+ d)(b +d), and X
and Y are uncorrelated if and only if this equals 0.
(b) For independence, we require f(i, 7) = P(X =i)P(Y = J) for alli, 7, which is to say that

a=(a+bja+c), b=(a+b\(b+d), c=(c+d)(at+c), d=(b+d)(c4+d).

Now a +b+c+d = 1, and with a little work one sees that any one of these relations implies the
other three. Therefore X and Y are independent if and only if d = (b+ d)(c +d), the same condition
as for uncorrelatedness.

13. (a) We have that

co m-—l fore) fore)
E(X) = 5 mP(X = m) = X= => 3 P(X =m) = > P(X > n).
m=0 m=0 n=0 n=

Om=n+1 n=0
Problems Solutions [3.11.14]-[3.11.14]

(b) First method. Let N be the number of balls drawn. Then, by (a),

r r
E(N) = > P(N >n)= > P(first n balls are red)
=0 n==0
r

-y r r-1or-ntl 3 r! (b+r-—n)!

sp otrbtr—i b+r—-n+l1 4 +)! (r—n)!
_ rib! 3 n+b\_ b+r+l
~ (b+) b J” b4+1?

where we have used the combinatorial identity $77 9 ("5") = (et). To see this, either use

the simple identity (,*,) + (7) = (* ry repeatedly, or argue as follows. Changing the order of
summation, we find that

© r n+b ] oo n+b
yd (P sede")
r=0 n=0 n=0

fore)
b+r+l1
1- “OH = S r
( *) * b+1

r=0

by the (negative) binomial theorem. Equating coefficients of x”, we obtain the required identity.

Second method. Writing m(b, r) for the mean in question, and conditioning on the colour of the first

ball, we find that
r

bt+r-

With appropriate boundary conditions and a little effort, one may obtain the result.

Third method. Withdraw all the balls, and let N; be the number of red balls drawn between the ith and
(i + 1)th blue ball (No = N, and Nz is defined analogously). Think of a possible ‘colour sequence’
as comprising r reds, split by b blues into b + 1 red sequences. There is a one-one correspondence
between the set of such sequences with No = i, Nm = j (for given i, j,m) and the set of such
sequences with No = j, Nm = i; just interchange the ‘Oth’ red run with the mth red run. In particular
E(No) = E(Nm) for all m. Now No + Nj +---+ Np =r, 80 that E(Nm) = r/(b + 1), whence the
claim is immediate.

(c) We use the notation just introduced. In addition, let B, be the number of blue balls remaining after
the removal of the last red ball. The length of the last ‘colour run’ is Np + B;, only one of which is
non-zero. The answer is therefore r/(b + 1) + b/(r + 1), by the argument of the third solution to part
(b).

14. (a) We have that E(X;,) = pz and var(X;,) = px(1 — px), and the claims follow in the usual way,
the first by the linearity of E and the second by the independence of the X;; see Theorems (3.3.8) and
(3.3.11).

(b) Let s = 5°, pg, and let Z be a random variable taking each of the values p;, p2,..., Pn with
equal probability n~!. Now E(Z”) — E(Z)* = var(Z) > 0, so that

1 5 1 \?_ s?
r= (Lm) = 72

k

m(b,r) = at {1 +m(b,r — 1}

with equality if and only if Z is (almost surely) constant, which is to say that py = p2 =--: = pn.
Hence 5
s
var(Y) = > pe - d- vy <s-—
k k

175

[3.11.15]-[3.11.18] Solutions Discrete random variables

with equality if and only if py = pz =--- = pn.
Essentially the same route may be followed using a Lagrange multiplier.

(c) This conclusion is not contrary to informed intuition, but experience shows it to be contrary to
much uninformed intuition.

15. A matrix V has zero determinant if and only if it is singular, that is to say if and only if there is a
non-zero vector x such that xVx’ = 0. However,

2
xV(X)x’ =E (= xe(Xk - BX:))
k

Hence, by the result of Problem (3.11.2), 5°, 4% (X% — EX,) is constant with probability one, and the
result follows.

16. The random variables X + Y and |X — Y| are uncorrelated since
cov(X + Y,|X —Y|) =E{(X + Y)|X — Y|} —E(X + Y)E(X — Y])
=}+44-1.4=0.
However,
Lo pxX+¥=0,|X -Y/=0) #P(X+¥ =OP(X-Yi/=0)=}-4= 4,

so that X + Y and |X — Y| are dependent.

17. Let I, be the indicator function of the event that there is a match in the kth place. Then PU =
1) =n7!, and for k ¥ j,

1

PUe = 1 dj = = PU = 1 = DPC = 1D) = n(n — 1)"

Now X = S-7_, Ig, so that E(X) = S7f#_) n7! = Land

n 2
var(X) = E(X*) — (EX)* = e(> I) -1
1

" 1
= CE)? + EG Kh) —1 = 420) ao -1=1.
; For J 2}n(n—1)

We have by the usual (mis)matching argument of Example (3.4.3) that

11)
Px=nN= >>) jo (OS rsn—2,
po il

which tends to e l/r! as n —> oo.

18. (a)Let Y,, Yo,..., Y, be Bernoulli with parameter p2, and Z), Z2,..., Zn Bernoulli with param-
eter p;/p2, and suppose the usual independence. Define A; = Y;Z;, a Bernoulli random variable that
has parameter P(Aj = 1) = PCY; = 1)P(Z; = 1) = py. Now (Aq, Ao,..-, An) < (M1, %,---, Yn)
so that f(A) < f(¥). Hence e(p1) = E(f(A)) < E(f(Y)) = e(p2).
(b) Suppose first that n = 1, and let X and X’ be independent Bernoulli variables with parameter p.
We claim that

{f£(X) — F(X) }{aQX) — 9(X)} = 0;

176

Problems Solutions [3.11.19]-{3.11.19]

to see this consider the three cases X = X’, X < X’, X > X’ separately, using the fact that f and g
are increasing. Taking expectations, we obtain

E({f(X) — F(X )Ha(X) — 9(X’)}) 2 0,
which may be expanded to find that

0 < E(f(X)g(X)) — E(F(X) gO) — E(F XO) g(X)) +E(F(X)g(X))
= 2{E(f(X)g(X)) — ECf(X)E(@(X)) }

by the properties of X and X’.
Suppose that the result is valid for all n satisfying n < k where k > 2. Now

(*) E(f(X)g(X)) = E {E(f(X)g(X) | X1, X2...., Xx-1)};
here, the conditional expectation given X;, X2,..., X%—1 is defined in very much the same way as in
Definition (3.7.3), with broadly similar properties, in particular Theorem (3.7.4); see also Exercises

(3.7.1, 3). If X1, Xo,..., X,~_1 are given, then f(X) and g(X) may be thought of as increasing
functions of the single remaining variable X;, and therefore

E(f(X)g(X) | X1, Xo,..., Xx-1) = E(F (CX) | X1, X2,.--, Xe-1)E(g(X) | X1, X2,---, Xx-1)
by the induction hypothesis. Furthermore
F(X) =E(f(K) | X1, X2,-.-,Xe-1),  9/(K) = E(g(K)| X1, X2,..-, Xe-1),

are increasing functions of the k—1 variables X; , X2,..., Xz—1, implying by the induction hypothesis
that E( f'(X)g’(X)) = E(f’(X))E(2’(X)). We substitute this into («) to obtain

E(f(X)g(X)) = E(f’(X))E(g'(X)) = E(f CO))E(g(X))

by the definition of f’ and 2’.

19. Certainly R(p) = E([4) = 324 14(@)P(@) and Pw) = pN@g”™-N@) where p+q = 1.
Differentiating, we obtain

R'(p) = S$ La(o) ph gn-NO ([“ _m— we)

1
== S~ 1a@)P(@)(N(@) — mp)
1 t
= —E(I4(N — mp)) = —{E(I4N) — E(I4)E(N)} = —cov(I4, N).
ba Pq Pq

Applying the Cauchy—Schwarz inequality (3.6.9) to the latter covariance, we find that R’(p) <
(pq)7! »/ var(14) var(V). However I, is Bernoulli with parameter R(p), so that var([4) = R(p)U—
R(p)), and finally N is bin(m, p) so that var(N) = mp(1 — p), whence the upper bound for R’(p)
follows.

As for the lower bound, use the general fact that cov(X +Y, Z) = cov(X, Z)+cov(Y, Z) to deduce
that cov([4, N) = cov(I4, [4) + cov(4, N — 14). Now I, and N — I, are increasing functions of
a, in the sense of Problem (3.11.18); you should check this. Hence cov(I4, N) > var(I4) + 0 by the
result of that problem. The lower bound for R’(p) follows.

177

[(3.11.20]-[3.11.21] Solutions Discrete random variables

20. (a) Let each edge be blue with probability p; and yellow with probability po; assume these two
events are independent of each other and of the colourings of all other edges. Call an edge green if it
is both blue and yellow, so that each edge is green with probability p, p2. If there is a working green
connection from source to sink, then there is also a blue connection and a yellow connection. Thus

P(green connection) < P(blue connection, and yellow connection)
= P(blue connection)P(yellow connection)

so that R(p1 p2) < R(p1)R(p2).

(b) This is somewhat harder, and may be proved by induction on the number n of edges of G. Ifn = 1
then a consideration of the two possible cases yields that either R(p) = 1 for all p, or R(p) = p for
all p. In either case the required inequality holds.

Suppose then that the inequality is valid whenever n < k where k > 2, and consider the case
when G has k edges. Let e be an edge of G and write w(e) for the state of e; w(e) = 1 if e is working,
and w(e) = 0 otherwise. Writing A for the event that there is a working connection from source to
sink, we have that

R(p’) = Ppy (A | o(e) = 1) p” + Ppr (A | w(e) = 0)(1 — p”)
< Pp(A | o(e) = 1)" p” + Pp(A | w(e) = 0)" (1 — p”)
where Py is the appropriate probability measure when each edge is working with probability a. The

inequality here is valid since, if w(e) is given, then the network G is effectively reduced in size by one
edge; the induction hypothesis is then utilized for the case n = k — 1. Jt is a minor chore to check that

x¥ p¥ +y%(1—p)’ <{xp+yQ—p)}” — ifx > y = 0;

to see this, check that equality holds when x = y > 0 and that the derivative of the left-hand side with
respect to x is at most the corresponding derivative of the right-hand side when x, y > 0. Apply the
latter inequality with x = P,(A | w(e) = 1) and y = Pp(A | w(e) = 0) to obtain

R(p”) < {Pp(A | of) = Ip + Pp(A | o(e) = O(1 — p)}” = R(p)”.

21. (a) The number X of such extraordinary individuals has the bin(107, 1077) distribution. Hence
EX = 1 and

P(X>1)_ 1-P(X =0)- P(X =1)

P(X >0) 1—P(X =0)

_ 1=(.= 1077)!" — 107. 10-74 — 10-7) 10"!
1 — (1 — 10-7107

P(X>1)/X>H)=

(Shades of (3.5.4) here: X is approximately Poisson distributed with parameter 1.)
(b) Likewise
1—2e7} — se!

Todenl ~ 0.3.

P(X >2|X>2)~

(c) Provided m < N = 10’,

N! 1\™ 1\"-™ el
PO =) = api (Wr) (5) Th

178

Problems Solutions [3.11.22]-[3.11.23]

the Poisson distribution. Assume that “reasonably confident that n is all" means that P(X > n |X >
n) < r for some suitable small number r. Assuming the Poisson approximation, P(X > n) < rP(X =
n) if and only if

ie St
ee > i <r e7! ke
k=n+1 — kan
For any given r, the smallest acceptable value of n may be determined numerically. If r is small, then
very roughly n x 1/r will do (e.g., ifr = 0.05 then n ~ 20).
(d) No level p of improbability is sufficiently small for one to be sure that the person is specified
uniquely. If p = 10~’q, then X is bin(10’, 10~7a), which is approximately Poisson with parameter
a, Therefore, in this case,
—a bot 64
PX >1/X2 Ie ee a Lg gay,
1-—e
An acceptable value of p for a very petty offence might be p ~ 0.05, in which case a ~ 0.1 and so
p = 10-8 might be an acceptable level of improbability. For a capital offence, one would normally
require a much smaller value of p. We note that the rules of evidence do not allow an overt discussion
along these lines in a court of law in the United Kingdom.

22. The number G of girls has the binomial distribution bin(Qn, p). Hence
2n on
P(G > 2n-G)=P(G2>n)= k ,2n—k
(G = 2n-G)=P(G Zn) (7) a

k=n
an\ = _k 2n q
< Ye ekg? * = | | p"q" ——.,

where we have used the fact that Cr) < C7") for all k.
With p = 0.485 and n = 10*, we have using Stirling’s formula (Exercise (3.10.1)) that

n\ nn @ 1 n 0.515
——< 1 — 0.03)(1 + 0.03) }” ——
(0 4 q-p” Tan ya + d 0.03
0.515 9 \ 108 5
= -—— -— 2 a

tS ¢ at) < 1.23 x 10

It follows that the probability that boys outnumber girls for 82 successive years is at least (1 — 1.23 x
107>)®2 > 0.99899.

23. Let M be the number of such visits. If k 4 0, then M > 1 if and only if the particle hits 0 before
it hits N, an event with probability 1 — kKN—! by equation (1.7.7). Having hit 0, the chance of another
visit to 0 before hitting N is 1 — N—!, since the particle at 0 moves immediately to 1 whence there is
probability 1 — N —1 of another visit to 0 before visiting N. Hence

k 1 r-l
PM >| $=" = (1-5) (1-5) ; r>1,

so that

P(M = j | So=k) = P(M > j | So =k) —P(M > j +1] So = 0)

k 1\f/-! 1
={1-—)({1-— —, fel.
N N N

179

[3.11.24]-[3.11.27] Solutions Discrete random variables

24. Either read the solution to Exercise (3.9.4), or the following two related solutions neither of which
uses difference equations.
First method. Let Ty be the event that A wins and exactly & tails appear. Then k < n so that
P(A wins) = ar, P(7T,). However P(7;) is the probability that m + k tosses yield m heads, k tails,
and the last toss is heads. Hence
P(Ti) = (" the ' pak,
m—-1

whence the result follows.

Second method. Suppose the coin is tossed m +n — 1 times. If the number of heads is m or more,
then A must have won; conversely if the number of heads is m — 1 or less, then the number of tails is
n or more, so that B has won. The number of heads is bin(m +n — 1, p) so that

m+n—1
. m+n-—-1 je
P(A wins) = y ( k ) an 1-k

=m
25. The chance of winning, having started from k, is
k le Le
1—- @/p) 1—(@/p)?* 1+ @/p)?” |
_ N 1 lar?
1 @/p) 1—(q/p)2™ 14 (q/p)?%

see Example (3.9.6). If k and N are even, doubling the stake is equivalent to playing the original game
with initial fortune $k and the price of the Jaguar set at 5N . The probability of winning is now

which may be written as

1
1— @/p)?
7
1— (q/p)2™
which is larger than before, since the final term in the above display is greater than 1 (when p < 5).

If p= 5. doubling the stake makes no difference to the chance of winning. If p > 7 it is better
to decrease the stake.

26. This is equivalent to taking the limit as N — oo in the previous Problem (3.11.25). In the limit
when p # 7 the probability of ultimate bankruptcy is

in ———————_——_. =

(q/py* —(q/py’ _ { (@/p)* if p> 5,
N>co 1-—(q/p)%

1 ifp< 7
where p+ g=1.Ifp= 4, the corresponding limit is limy—oo(1 —k/N) = 1.
27, Using the technique of reversal, we have that
P(Rn = Ry_-1 +1) = P(Syp_-1 ¥ Sn, Sp—2 F Sn, ---, So F Sn)
= P(Xn £0, Xn_-1 + Xn FO,..., Xp +--- + Xn #0)
= P(X; £0, X2+ X1 #0,...,X_pt+--- +X, #0)
= P(S| £0, Sp £0,..., Sp 0) = P(S1S2--- Sy AO).

It follows that E(Ry) = E(R,~-1) + P(S1S2---S, 4 0) for n > 1, whence

1 1 n
-E(R,) = — fis S> P(S1S2-++ Sn 20)! —> P(S,% £0 for all k > 1)
n n m=l

180

Problems Solutions [3.11.28]-[3.11.29]

since P(S1S2---Sm #9) | P(S, #0 for allk > 1) asm > ow.

There are various ways of showing that the last probability equals |p — q|, and here is one.
Suppose p > q. If X, = 1, the probability of never subsequently hitting the origin equals 1 — (¢/p),
by the calculation in the solution to Problem (3.11.26) above. If X, = —1, the probability of staying
away from the origin subsequently is 0. Hence the answer is p(1 — (¢/p)) +¢-0= p—gq.

If g > p, the same argument yields g — p, andif p = q = 5 the answer is 0.

28. Consider first the event that M>, is first attained at time 2k. This event occurs if and only if: (i)
the walk makes a first passage to Sz, (> 0) at time 2k, and (ii) the walk thereafter does not exceed
S2,. These two events are independent. The chance of (i) is, by reversal and symmetry,

P(S2x-1 < Sox, Sox—2 < Sop, +--+, So < Sox)
= P(X2x > 0, Xox—1 + Xan > 0,...,X1 +++ + XK > 0)
= P(X, > 0,X,+ Xo >0,...,X, +--- +X, > 0)
= P(S; > O for 1 <i < 2k) = 4P(S; A0 for 1 <i < 2k)
= P(Sx,=0) by equation (3.10.23).

As for the second event, we may translate 5S, to the origin to obtain the probability of (ii):
P(Sap41 < Sap, +--+ San < Soe) = PMoy—2~ = 0) = PSan—-24 = 9),

where we have used the result of Exercise (3.10.2). The answer is therefore as given.

The probabilities of (1) and (ii) are unchanged in the case i = 2k + 1; the basic reason for this is
that So, is even, and S2,41 odd, for all r.

29, Let up = P(S, = 0), fe = P(S, = 0, S; #0 for 1 <i < k), and use conditional probability (or
recall from equation (3.10.25)) to obtain

n
(*) won = > Won—2k fok-
k=1

Now N = 2, and therefore it suffices to prove that E(N,) = E(N,—-1) forn > 2. Let Ni_y be
the number of points visited by the walk 51, 52,..., Sy, exactly once (we have removed So). Then

Ni_, +1. if S 4 Soforl<k<n,
Nn = N)_1—1. if Sy = So for exactly one k in {1,2,...,n},
Ni_y otherwise.
Hence, writing a, = P(S,; 4 Oforl1 <k <n),
E(Nn) = E(N,_1) + an — P(Sg = So exactly once)
= E(Nn-1) + on — { fotin—2 + fadn—4 +--+ + fojn2y}

where [x] is the integer part of x. Now a2 = G24] = “2m by equation (3.10.23). If n = 2k is
even, then

E(N9x) — E(N2¢—1) = 4x — (folae—2 +--+ fox} =O by ().
If n = 2k + 1 is odd, then
E(N2¢41) — E(Nox) = box — { foure—-2 +--+ + fox} =O by (%).

181

[3.11.30]-[3.11.34] Solutions Discrete random variables

In either case the claim is proved.

30. (a) Not much.

(b) The rhyme may be interpreted in any of several ways. Interpreting it as meaning that families stop
at their first son, we may represent the sample space of a typical family as {B, GB, G2B, ...}, with
P(G"B) = 2—-+D_ The mean number of girls is 7°, nP(G"B) = S702, n2-“+) = 1; there is
exactly one boy.

The empirical sex ratio for large populations will be near to 1:1, by the law of large numbers.
However the variance of the number of girls in a typical family is var(#girls) = 2, whilst var(#boys) =
0; #A denotes the cardinality of A. Considerable variation from 1:1 is therefore possible in smaller
populations, but in either direction. In a large number of small populations, the number of large
predominantly female families would be balanced by a large number of male singletons.

31. Any positive integer m has a unique factorization in the form m = []; pro for non-negative
integers m(1), m(2),.... Hence,

1 1 _mi)\2 = C
Pa =m =[] 70 =m) =I (1-25) res = (IL °) =3
i Pi) Pj j

i i i i

where C = [];(1 — p)’). Now >>, P(M = m) = 1, so thatC-! = >, mF.

32. Number the plates 0, 1,2, ..., N where 0 is the starting plate, fix k satisfying 0 < k < N, and
let Ax be the event that plate number k is the last to be visited. In order to calculate P(A), we cut
the table open at k, and bend its outside edge into a line segment, along which the plate numbers read
k,k+1,...,N,0,1,...,k in order. It is convenient to relabel the plates as —(N + 1 — k), -(N —
k),...,—-1,0,1,...,k. Now Ag occurs if and only if a symmetric random walk, starting from 0,
visits both —(N — k) and k — 1 before it visits either —(N + 1 — k) ork. Suppose it visits —(N — k)
before it visits k — 1. The (conditional) probability that it subsequently visits k — 1 before visiting
—(N + 1 -— k) is the same as the probability that a symmetric random walk, starting from 1, hits N
before it hits 0, a probability of N~! by (1.7.7). The same argument applies if the cake visits k — 1
before it visits —(N — k). Therefore P(A,) = N “1,

33. With j denoting the jth best vertex, the walk has transition probabilities pj, = (j — 1)~! for
1<k < j. By conditional expectation,

j-1

1
alts te r, = 0.

Induction now supplies the result. Since r; ~ log j for large j, the worst-case expectation is about
log (7).
34. Let py denote the required probability. If (n;,m,4+,) is first pair to make a dimer, then m
is ultimately uncombined with probability p,_,. By conditioning on the first pair, we find that
Pn = (Pi + p2 +-+> + Pn—2)/(n — 1), giving 2(Pnt1 — Pn) = —(Pn — Pn—1). Therefore,
(Pati — Pn) = (-1)"7!(p2 — pt) = (—1)", and the claim follows by summing.

Finally,

n
EUn = >_ P(m, is uncombined) = pn + py Pn—1 + +++ + Pn—1P1 + Pn,
r=]

since the rth molecule may be thought of as an end molecule of two sequences of lengthr andn—r-+1.
Now pn > e~/asn —> oo, and itis an easy exercise of analysis to obtain that n'EU, > e,

182

Problems Solutions [3.11.35]-[3.11.36]

35. First,
k
= (S) = SEP Pry ss Pre = KY DY Pry Pry s+ Pres
i TY 61Q,- Nk {rp o- PR}
where the last summation is over all subsets {r,,... , rg} of k distinct elements of {1,2,..., 7}.
Secondly,
k
{rp s-sreh i PY eves TK—2
k k-1
TL y- Vk J i
Hence
+ e
{ry ,-Fk}

By Taylor’s theorem applied to the function log (1—x), there exist 0, satisfying 0<6 < {2(1- —c)*)}7}
such that

(#) [a - pr) = J] expt, — 6 pr} = exp{—a — 20(max p;) }.
r=1 r

Finally,

Pr, +++ Pr
po = = ([Ja- 2) » = pr) Pa)

{rpo-sFK}
The claim follows from (*) and (**).

36. It is elementary that
nna! Nv 1X an
(Y) = — SUE) = oar
r=] r=1
We write Y — E(Y) as the mixture of indicator variables thus:

r=1
It follows from the fact
non-l
EG;;) = — i i,
Gi=— yoy fA

(3.11.37]-{3.11.38] Solutions Discrete random variables

HAnNnc Oe?
YP
C Hnc eee ee ene
Y —
ANC
HOC
|; ha) Ge (-y)a
l-y

Cc

ANC

Figure 3.2. The tree of possibility and probability in Problem (3.11.37). The presence of the
disease is denoted by C, and hospitalization by H; their negations are denoted by C and H.

that

il
Mz
Spas

=

var(Y)

=~
i
[=

ee”

eae (4-H) (y- HS

ifj
xjxj Jnn—-1 n2
n2 )NN-1 N2

Daya
~ i“] ON — DN2
we n(N —1)N

N N
2
wat - “D ry ye Ee

E{(H-H)

ll
Wo
|

ry “NS
tM oY

I
eae
2|s
—

|
2/5
—

Some

ll
M=
*
“i
>| =
wo} |
=

l
fal

Nii
N N
N-n 1
_ _ nel i _x)2
- syn {2 r Nx mN= DN De x)".

37. The tree in Figure 3.2 illustrates the possibilities and probabilities. If G contains n individuals,
X is bin(n, yp + (1 — y)a) and Y is bin(n, yp). It is not difficult to see that cov(X, Y) = nyp(1—v)
where v = yp+(1—y)a. Also, var(Y) = nyp(1 — yp) and var(X) = nk(1—v). The result follows
from the definition of correlation.

38. (a) This is an extension of Exercise (3.5.2). With P, denoting the probability measure conditional
on N =n, we have that

Pa(Xi = 7 for <i <k) = ff (2)? --- fA! -— FY,

rytrol---rgts!

184
Problems Solutions [3.11.39]-{3.11.39]

where s =n — we r;. Therefore,

CO
P(X; =7j for 1 <i <k) = > Pa(Xj = 7 for 1 <i < P(N =n)
n=0

“II ct vri fone \y 3 v5 (1 — F(k))§ e7¥(I-F®),
s=0

s!

The final sum is a Poisson sum, and equals 1.

(b) We use an argument relevant to Wald’s equation. The event {7 < n—1} depends only on the random
variables X,, X2,..., X,_1, and these are independent of X,. It follows that X, is independent of
the event {T > n}={T <n-—1}°. Hence,

E(S) = DO E(Xifroay) = SO E(KDEC rai) = DL E(K)PM = 1)
i=] i=] i=]

ow ow t
DVO MPT =H =v PT =O LV FO

i=] t=1 t=] i=1

o.4)
v SORT =1t)F(t) = E(F(T)).

t=1

39. (a) Place an absorbing barrier at a + 1, and let pg be the probability that the particle is absorbed
at 0. By conditioning on the first step, we obtain that

1
Pu= a (Pot Pit pats + Prt), 1snsa.

The boundary conditions are po = 1, pg+1 = 0. It follows that py+) — pn = (n+ D(Pn — Pn-1)
for 2 <n <a. We have also that pp — py = p, — 1, and

Patt — Pn = 4(n + 1)! (p2 — pt) = 44+ 1)! (v1 — Po).

Setting n = a we obtain that —p,g = AG + 1)! (py — 1). By summing over 2 <n <a,
a
Pa — P\ = (P1 ~ Po) + 3(P1 ~ Po) Dj!

and we eliminate p; to conclude that

(a+ 1)!
443!+4!+---4+@41!

Pa =

It is now easy to see that, for givenr, py = py(a) > 1 asa — on, so that ultimate absorption
at 0 is (almost) certain, irrespective of the starting point.

(b) Let 4, be the probability that the last step is from 1 to 0, having started at r. Then

(*) Ay = 3(1+A1 +49),
(&*) (r+ 2)Ap = Ay +AQH+-+-+A,44, r>

v
N

185

[3.11.40]-[3.11.40] Solutions Discrete random variables

It follows that

1
Ap —Ap-1 = rp Art — Ar), r > 3,

whence
1

3M = 45. +)

(r+ — Ar), r>3.

Letting r — oo, we deduce that A3 = A2 so that A, = A2 forr > 2. From (**) withr = 2,A2 = SAL
and from («) A, = Z.

(c) Let yz; be the mean duration of the walk starting from r. As above, jzg = 0, and

1
Hr = V+ (er + a ts + Mr $1), r21,

whence pp44 — Mr = (7 + Dur — My_1) — 1 forr > 2. Therefore, vp4.. = (up41 — er)/(r+D!
satisfies v,41 — ve = —1/(r +1)! forr > 2, and some further algebra yields the value of jz .

40. We label the vertices 1, 2,...,, and we let 2 be a random permutation of this set. Let K be the
set of vertices v with the property that 7(w) > 2(v) for all neighbours w of v. It is not difficult to
see that K is an independent set, whence a(G) > |K|. Therefore, a(G) > E|K| = 5°, Pv € K).
For any vertex v, a random permutation is equally likely to assign any given ordering to the set
comprising v and its neighbours. Also, v € K if and only if v is the earliest element in this ordering,
whence P(v € K) = 1/(dy + 1). The result follows.

186

4

Continuous random variables

4.1 Solutions. Probability density functions

1
1. (a) {x(1 — x)} 2 is the derivative of sin~! (2x — 1), and therefore C = 27!.
(b) C = 1, since

—x —x
—x— = ji _ K _ .
/ exp(—x —e “)dx im [exp( e | k=l

fond ©, 0)

(c) Substitute v = (1 + x2j)71 to obtain

OO d. 1 3 1
—_O _ = |v 21) 2 dv = BA, m—})
co 1 +x2" Jo

where B(., -) is a beta function; see paragraph (4.4.8) and Exercise (4.4.2). Hence, if m > i,

Co! = BGy,m-5)= POrap D)
2. (i) The distribution function Fy of Y is

FyQ) =PY < y)=P@X < y)= P(X < y/a) = Fx (y/a).
So, differentiating, fy (y) = a7! fx (y/a).

(ii) Certainly
Fy) =PCX <x) =P(X = -x) = 1—-—P(X < -x)

since P(X = —x) = 0. Hence f_y(x) = fx(—x). If X and —X have the same distribution function
then f_x(x) = fx(x), whence the claim follows. Conversely, if fy(—x) = fx (x) for all x, then,
by substituting u = —x,

foe) y y
P(-X < y) =P(X > -y)= / far(x)dx = / fax(-u) du = / farlu) du = P(X <y),
-y 00 00

whence X and —X have the same distribution function.

3. Sincea > 0, f > 0, and g > 0, it follows thataf + (1 —a@)g > 0. Also
[tars a-me}dx =a f far+a—a) f gdx=a41-a=1.
R R R

187

[4.1.4]-[4.2.4] Solutions Continuous random variables

If X is arandom variable with density f, and Y arandom variable with density g, thena f+(1—a)g
is the density of a random variable Z which takes the value X with probability a and Y otherwise.

Some minor technicalities are necessary in order to find an appropriate probability space for such
a Z. If X and Y are defined on the probability space (Q, ¥, P), it is necessary to define the product
space (Q, F, P) x (2, §, Q) where © = {0, 1}, % is the set of all subsets of &, and Q(0) = a,
Q(l) = 1-—a. Forw x 0 € Q x &, we define

X() ifo=0,

Zo xo)= { Y) ifo=1.

1F@+hA)-F@) _ f()

4. (a) By definition, r(x) = lim

hloh 1—F(x) ~ T= F(x)
(b) We have that
H d (1 f* 1 f* 1 f*
@_¢ {-f ronay} = OS Prova = 3 [re -ronay,
x dx (x Jo x x* Jo x Jo

which is non-negative if r is non-increasing.
(c) H(x)/x is non-decreasing if and only if, for 0 <a < 1,

1 1
—H(ax) < —H(x) for all x > 0,
ax x

which is to say that —a7! log[1 — F(ax)] < —log[1 — F(x)]. We exponentiate to obtain the claim.
(d) Likewise, if H(x)/x is non-decreasing then H(at) < wH(t) forO0 <a@ < 1 andt > 0, whence
HA(at) + Att — at) < H(t) as required.

4.2 Solutions. Independence

1. Let N be the required number. Then P(N = n) = F(Ky*—"[1 — F(K)] for n > 1, the geometric
distribution with mean {1 — F(K)]~!.

2. (i) Max{X, Y} < v if and only if X < v and Y < v. Hence, by independence,
P(max{X, Y} < v) = P(X <0, ¥ <v) =P(X <v)P(Y <v) = Fv)’.
Differentiate to obtain the density function of V = max{X, Y}.
(ii) Similarly min{X, Y} > wu if and only if X > u and Y > u. Hence
PU <u)=1—-P(U > u) =1—-P(X > w)PYY > vw) =1-[1- Fw),

giving fy(u) = 2f (u)[1 — F(u)).

3. The 24 permutations of the order statistics are equally likely by symmetry, and thus have equal
probability. Hence P(X) < X2 < X3 < X4) = 4, and P(X; > Xz < X3 < X4) = 3, by
enumerating the possibilities.

4. PYYO) > H= F(yy* fork > 1. Hence EY(y) = F(y)/[1 — F(Q))] > coas y > ow.
Therefore,

P(Y(y) > EY(y)) = {1 [1 — Foy POE Foo

F
~ exp {1-t1- FoI | ()

lo Fa) |} se! as y > ©.

188

Expectation Solutions [4.3.1]-[4.3.5]

4.3 Solutions. Expectation

1. (a) E(X*%) = f° x%e~* dx < 00 if and only if a > —-1.

(b) In this case
CO Clx|*
E(\x|* -| ——,— d.
(|X|") “oo AED x < 00

if and only if —1 <a@ < 2m —1.
2. We have that

An xX; n
1=E (He ‘) = > E(X;/Sn).
i=]
By symmetry, E(X;/Sn) = E(X1/S») for all i, and hence 1 = nE(X,/S,). Therefore

E(Sm/Sn) =) | E(Xi/Sn) = mE(X1/Sn) = m/n.

i=l
3. Either integrate by parts or use Fubini’s theorem:

OO OO le ¢)
rf x’ P(X > x) dx =r | xt! { foyay} dx
0 0 y

=x

= [rot fe #} dy = [ y" f(y) dy.

An alternative proof is as follows. Let I, be the indicator of the event that X > x, so that
for I, dx = X. Taking expectations, and taking a minor liberty with the integral which may be made
rigorous, we obtain EX = Io° EC) dx. A similar argument may be used for the more general case.

4. We may suppose without loss of generality that 4 = 0 ando = 1. Assume further that m > 1.
In this case, at least half the probability mass lies to the right of 1, whence E(XJ;y>m}) = 3 Now
0 = E(X) = E{X[I(x>mj + [tx <m)]}, implying that EX Ix <m)) < —}. Likewise,
E(X7Ix>m}) = 4, E(X?Ix<m}) < 4.
By the definition of the median, and the fact that X is continuous,
E(X |X <m)<-—l, E(X2|X <m) <1.

It follows that var(X | X < m) <0, which implies in turn that, conditional on {X < m}, X is almost
surely concentrated at a single value. This contradicts the continuity of X, and we deduce that m < 1.
The possibility m < —1 may be ruled out similarly, or by considering the random variable — X.

5. Itis a standard to write X = X* — X~ where X* = max{X, 0} and X~ = — min{X, 0}. Now
Xt and X~ are non-negative, and so, by Lemma (4.3.4),

p= EX) = BX+)— BA) = [Pex > dx [Px < —x)dx
0 0

lee) ioe) 0
= [urea [ F(-x)dx = | = Faide- [ F(x)dx.
0 0 0

—0O
It is a triviality that

i jb
w= Fajds + [ [1 — F(x) dx
0 0

and the equation follows with a = y. It is easy to see that it cannot hold with any other value of a,
since both sides are monotonic functions of a.

189

[4.4.1]-[4.4.6] Solutions Continuous random variables
4.4 Solutions. Examples of continuous variables
1. (i) Integrating by parts,
OO OO
T(t) -| xP Te dx = t- »f xt e-* dx = (t—- 1) (t—- VD.
0 0

If n is an integer, then it follows that "(1) = (#—1)P (2-1) =--- = (#—1)!TC) where P(1) = 1.
(ii) We have, using the substitution u2 = x, that

2 2
roy = { [ote ax} = f° de" au

fore) 2 foe) 2 co opm /2 2
=4/ e # auf e” dv=4 | [ e rdrdd=x
0 0 r=0 J@=0

as required. For integral n,

_ (Qn)!
~ Ahn}

Pa+5)=@-5)Pn-f) =.= (nH) — 3)--- 4G) Jr.

2. By the definition of the gamma function,
io.¢] OO oO OO
P(a)P(b) -| xa-le-* ax [ yee) dy = [ [ e Ft) ~4-1y bl gy dy,
0 0 0 0
Now setu =x+y,u=x/(@ + y), obtaining

co fl 1,a-1 1
| / e #ytte-1 yal — yy?! du du
u=0 /v=0

lore) 1
= [ yttb—le-u au [ v2! — vy?! dv = Pa + b)BAa, b).
0 0

3. If g is strictly decreasing then P(g(X) < y) = P(X = ge!) =1- gy) so long as
0 < g—!(y) < 1. Therefore P(g(X) < y) = 1—e7, y > 0, if and only if g—!(y) = e~, which is
to say that g(x) = —logx for0 <x <1.

4. We have that

x 1 1 1. 14
P(X <x)= > du = = + — tan x.
~oo W(1+u*) 2 =

g(x) = [> ee
= fod 4ay

Also,

is finite if and only if |a| < 1.

5. Writing ® for the N(0, 1) distribution function, P(Y < y) = P(X < log y) = ®(log y). Hence

1 1 1! 2
Py) = 5 Fx (logy) = Tene 3 (logy) 0<y<oo.

6. Integrating by parts,

OO 1 —
LHS = [ 2(x) {o -W=6 (¢ = HN dx
_— co oo _—
=- letnoe (—)| + [soo (=*) dx = RHS.
oo de

190

Dependence Solutions [4.4.7]-[4.5.3]

7. (ara@)= aBx8-1,

(b)r@) =A.

hae + (1 — ae #*
ae** + (1 —a)eT#*

8. Clearly ¢’ = —x@. Using this identity and integrating by parts repeatedly,

co co al oo at
1-0) = [ ow du=— [ OO ty = 2 4 ¢'@) 4,

x x w
_ ¢@) $x) -[* 30") 4, 9) _ 9) , 36) -[* 156 (u)

x x3 uo

(c)r(@®) =

, which approaches min{A, 44} as x > oo.

du.

uw x x3 x

4.5 Solutions. Dependence

1. (As the product of non-negative continuous functions, f is non- negative and continuous. Also
_ oo 1 _1,2,2 _
g(x) = der Fl / ae” dy = Se
-—0o V20x—

if x + 0, since the integrand is the N(0, x~*) density function. It is easily seen that g(0) = 0, so that

g is discontinuous, while
OO. 00
/ g(x)dx = / ze Fl dx =1.
—0CO —-w

(ii) Clearly fg = 0 and

co foo 0° hn
[. [fom naray= 3 (3) -1L=1.

n=1

Also fg is the uniform limit of continuous functions on any subset of R? of the form [—M, M] x R;
hence fg is continuous. Hence fg is a continuous density function. On the other hand

ioe) 0° n
[. fo, y)dy=>> (3) g(x — Gn),

n=1

where g is discontinuous at 0.
(iti) Take Q to be the set of the rationals, in some order.

2. We may assume that the centre of the rod is uniformly positioned in a square of size a x b, while
the acute angle between the rod and a line of the first grid is uniform on (0, hn). If the latter angle is
6 then, with the aid of a diagram, one finds that there is no intersection if and only if the centre of the
rod lies within a certain inner rectangle of size (a — r cos@) x (b —r sin@). Hence the probability of
an intersection is

2 ”e 2r 1
<a | {ab — (a—rcos8)(b — rsind)} dd = —— (a + b— 5r).

3. (i) Let J be the indicator of the event that the first needle intersects a line, and let J be the indicator
that the second needle intersects a line. By the result of Exercise (4.5.2), EU) = E(J) = 2/z; hence
Z=1+ J satisfies E(4Z) = 2/7.

191

[4.5.4]-[4.5.6] Solutions Continuous random variables

(ii) We have that

var(}Z) = 4 {B(7) + EJ?) + 2EWJ)} — EZ)?

=! -4i_4,}
= g{E@) + EV) + 2E(J)} - <= = — = + SEU).

In the notation of (4.5.8), if0 <6 < 51, then two intersections occur if z < 5 min{sin 0, cos 6}
orl-—z< 5 min{sin 8, cos@}. With a similar component when 50 <6 < 127, we find that

4
E(/ J) = P(two intersections) = — | (2,6): dzd6é
u
<z<4 min{sin @,cos 6}
0<d<40
4 pr 4 7/4 4 1
--/ J min(sin@, cos0}do = = f sino do = * (1-2).
x Jo x Jo 1 J/2
and hence a
1 4 1 3-72 4
var(3Z) = —- —+—Q-Vv2)= - =.
cans iu iu iu

(iii) For Buffon’s needle, the variance of the number of intersections is (2/7) — (2/ mn) which exceeds
var(3 Z). You should therefore use Buffon’s cross.

4. (i) Fy(u) = 1-—(1—4) (1-4) if0 < u < 1,andsoE(U) = fo 2u(1—u) du = 3 (Alternatively,
place three points independently at random on the circumference of a circle of circumference 1.
Measure the distances X and Y from the first point to the other two, along the circumference clockwise.
Clearly X and Y are independent and uniform on [0,1]. Hence by circular symmetry, EU) =

E(V -U) =E(1—V) = 4.)
(ii) Clearly UV = XY, so that E(UV) = E(X)E(Y) = 4. Hence

cov(U, V) = EUV) -EU)E(V) = 4-40 -) =,

since E(V) = 1 — E(UV) by ‘symmetry’.
5. (i) If X and Y are independent then, by (4.5.6) and independence,

E(e(X)h(¥)) = | / gix)h(y) fry (es y) dx dy
= / glx) far(x) dx / h(y) fy (9) dy = E(e(X))E(R(Y)).

(ii) By independence

1 1 oo 4 2
E(e2*+¥)) — Be2*)? = {f ee ax =4,
0

6. If O is the centre of the circle, take the radius OA as origin of coordinates. That is, A = (1, 0),
B = (1,0), C = (1, ®), in polar coordinates, where we choose the labels in such a way that
0 < @ < ®. The pair ©, & has joint density function f (0, ) = (217)~! for0 <0 < ¢ <2n.
The three angles of ABC are 4, 3( — @), m — 5®. You should plot in the /¢ plane the set
of pairs (0, @) such that 0 < 6 < ¢@ < 27 and such that at least one of the three angles exceeds xz.

192

Conditional distributions and conditional expectation Solutions [4.5.7]-[4.6.3]

Then integrate f over this region to obtain the result. The shape of the region depends on whether or
not x < 5. The density function g of the largest angle is given by differentiation:

The expectation is found to be ig?

7. We have that E(X) = y, and therefore E(X, — X) = 0. Furthermore,

E{X(X, — X)}

1 2 1 — _
w= (= XrXs) —E(X’) = =o? + ny") — (var(X) + E(X)*)

1 2
={o? + ny} - (= + ?) = 0.
n n

8. The condition is that E(Y) var(X) + E(X) var(Y) = 0.

9. If X and Y are positive, then S positive entails T positive, which displays the dependence. Finally,
S?=XandT? =.

4.6 Solutions. Conditional distributions and conditional expectation

1. The point is picked according to the uniform distribution on the surface of the unit sphere, which
is to say that, for any suitable subset C of the surface, the probability the point lies in C is the surface
integral [. (47)—! dS. Changing to polar coordinates, x = cos@ cos¢, y = sin@ cos¢, z = sing,
subject to xe 4 y? +z* = 1, this surface integral becomes (4)7! Jc | cos 6| dO dg, whence the joint
density function of © and ® is

f0,6)= Llcosgl, #1 S $x, 050 < 2x.
TT

The marginals are then f@(0) = (Qx)7}, fo(¢) = I cos ¢|, and the conditional density functions
are

1
foo 16)=5-, — foo 16) = gl cos,

for appropriate 6 and @. Thus © and ® are independent. The fact that the conditional density functions
are different from each other is sometimes referred to as ‘Borel’s paradox’.

2. We have that © fy y( )
X,YW,y

= sous gd

¥@) [2 fia

and therefore

CO CO
E(W(X)e(X)) = [ 7 [ LED glx) fils) dx dy

CO OO
= / [ ly eh fxr (x, y)dx dy = E(¥e(X)).

3. Take Y to be arandom variable with mean oo, say fy (y) = y forl1 < y <o,andlet ¥ = Y.
Then E(Y | X) = X which is (almost surely) finite.

193

[4.6.4]-[4.6.6] Solutions Continuous random variables
4, (a) We have that

OO
fx@)= / ne dy =)e*, 0<x<0,
x

so that fy|x(y | x) = re*@-Y), for0 <x <y < 00.
(b) Similarly,

oe)
fun) = | xe OTD dy = e7*, 0O<x <M,
0

so that fy|x(y | x) = xe-*Y, for 0 < y < oo.
5. We have that

_ 1 _ 1 n k next (1 — xP}
py =) = | PY =k |X =s)fxdx = [ (7); di —-x) Bah

_ {n Biat+k,n—-—k+b)

“Ak B(a, b) ‘

In the special case a = b = 1, this yields

n\TR+)ra—-k+1) 1
PY =kKh= = », O<k<n,
( ) (7) T(n +2) n+1 sear
whence Y is uniformly distributed.
We have in general that
1 na
BY) = f BY |X =x)fxte)dx =
0 a+b

and, by a similar computation of E(Y 2).

nab(a+b-+n)
(a+b(at+b+1)

var(Y) =

6. By conditioning on X1,
x Xx
Gr(x) = P(N > n) = i G,-1@ —u)du= [ Gy_1(v) dv.
0 0

Now Go(v) = | for all v € (0, 1), and the result follows by induction. Now,

oe)
EN = 5 P(N >n) =e".

n=0
More generally,
= n = n xn x” SX
Gnis)= Ds"PW =n) = Dos oD! at = (s — 1)e** +1,
A= a=

whence var(N) = G4, (1) + Gy (1) — Gy (1)? = 2xe* + e* — €?*,

194

Functions of random variables Solutions [4.6.7]-[4.7.1]

7. We may assume without loss of generality that EX = EY = 0. By the Cauchy—Schwarz
inequality,
E(XY)? = E(XE(Y | X))? < E(X?)E(E(Y | X)?).

Hence,

E(XyYy2

~— (1 — 72
E(xdy = (1 #7 vart).

E(var(Y | X)) = E(Y?) — E(E(Y | X)*) < EY? -

8. One way is to evaluate

OO OO OO
[ / / Apve*-HY-Y2 dx dy dz.
0 x y

Another way is to observe that min{Y, Z} is exponentially distributed with parameter yz + v, whence
P(X < min{Y, Z}) =A/A+ uv). Similarly, POY < Z) = u/(u + v), and the product of these
two terms is the required answer.

9. By integration, for x, y > 0,

y foe)
fr) = [ fee. yde= foe, fee) = [fee yay = ore,

whence c = 1. It is simple to check the values of fxjy(x | y) = f(x, y)/fy(y) and fy\x(y | x),
and then deduce by integration that E(X | Y = y)= Y and E(Y | X =x) =x+4+2.

10. We have that N > nif and only if XQ is largest of {Xp, X1,..., Xn}, an event having probability
1/(7 +1). Therefore, P(N = n) = 1/{n(v + 1)} forn > 1. Next, on the event {N = n}, Xp is the
largest, whence

ee 3 Fa@yrth | 3 F(x}

P(Xy < = o_o = F ;
Ov 3) = Tae n n+11*
n=1 n=1 n=1
as required. Finally,
P(M =m) = P(Xo = X1 = > Xm_-1) — P(Xo0 = X1 = > Xm) =— y
=mn)= oz AlLz 2 Am-1 02412 = Am) = 7 (m+)!

4.7 Solutions. Functions of random variables
1. We observe that, if0 <u <1,
P(XY <u) =PUXY <u, Y <u) +PXY <u,¥ >u) =P(Y <u) +P(X <u/Y, Y>u)
nut [i dy =u loge,
By the independence of XY and Z,
P(XY <u, Z* <v) = P(XY <uw)P(Z < Vv) =uJv(1—logu), O<u,v<1.

195
[4.7.2]-[4.7.5] Solutions Continuous random variables

Differentiate to obtain the joint density function

gu, v) = —. O<u,v<l.

Hence
P(XY < Z?) = Il 7 oe du dv = §.
dcwevel >

Arguing more directly,

P(XY < Z’)= /I/ dx dy dz = 3.

O<x,y,z<1
xyse*

2. The transformation x = uv, y = u — uv has Jacobian

_|ov u
r=|,2, —u

Hence |J| = |u|, and therefore fyy,y (u, v) = ue, forO <u <00,0<v <1. Hence U and V are
independent, and fy (v) = 1 on [0, 1] as required.

3. Arguing directly,
. _ 2 1
P(sin X < y) = P(X < sin y= > sin y, O<y<l,

so that fy (y) = 2/(x+/1 — y2), for 0 < y < 1. Alternatively, make a one-dimensional change of
variables.

4. (a) P(sin—! X < y) = P(X < siny) = siny, forO < y < 50. Hence fy(y) = cosy, for
O<y< 50.

(b) Similarly, Poin! X¥ < y) = (1 + sin y), for 30 <y< 5m, so that fy(y) = 5 60s y, for

1 1
xT SYS AM.

5. Consider the mapping w = x, z = (y—px)/V/1 — p2 with inverse x = w, y = pw+zv/1 — p2
and Jacobian
1 0

p vV1—
The mapping is one-one, and therefore W (= X) and Z satisfy
V1 —p2 { 1
exp 4 —
Py 2a

In V/1 — p2 — p*)

2

J= = 4/1—°.

Lo _ dy? 4,2
fw,z(w, 2) = (1 — p*)(w* +2)} = xe +2°)

implying that W and Z are independent N(0, 1) variables. Now

(X= 0,7 s0= {Ww >0Z>—We /Vi-p}.

and therefore, moving to polar coordinates,

1,2 57 1
P(X > 0, reo- [ E —_ ~2 rdrdo= | — dé
=o 21 a 20

196

Functions of random variables Solutions [4.7.6]-[4.7.9]

where a = —tan7! (o/V1 _ p?) = —sin7! p-

6. We confine ourselves to the more interesting case when p # 1. Writing X = U, Y = pU +
V1 —*V, we have that U and V are independent N(0, 1) variables. It is easy to check that Y > X
if and only if (1 — p)U < ./1 — o2V. Turning to polar coordinates,

1.2
00 pe 3h woe ¥
E X,Yp= 6+ry/1—p? sind > do
(max{X, Y}) i on [/ {pr eos +r p* sin \ +f

C2 —n

rcosé ao] dr

where tan yy = ./(1 — e)/(1 + p). Some algebra yields the result. For the second part,
E(max{X, Y}?) = E(X7I{x>¥)) + EW? Iy>xy) = E(X* Ix cy}) + EP My <xy),

by the symmetry of the marginals of X and Y. Adding, we obtain 2E(max{X, Y}?) = E(x?) +
E(Y?) =2.
7. We have that

rs
P(X <¥, Z> 2 =P@<X <¥) =e OH = PX < Y)PCZ > 2).
a

Xr
a) P(X = Z) = PX < Y) = ——.
(a) P(X = Z) =P <¥) =
(b) By conditioning on Y,

thw for w > 0.

P(X -¥)+ =0) =P SY) =<, P((X ~Y)t >w)= sy

+

By conditioning on X,

P(V > v) = P(X -Y|> n= [ew > vtxdfxcnde+ [~ Po <x—v)fx(x)dx
v

_ pe” 4 pe HY

; v>0.
Ath

(c) By conditioning on X, the required probability is found to be

wax [* a a
| heg «f pe dydx = {e7M — eT HY.
0 t—x h-A

8. Either make a change of variables and find the Jacobian, or argue directly. With the convention
that /r2 — u2 = 0 when r2 — u2 < 0, we have that

x
F(x) =P(R <r, X <x) = - | Jaw du,
-r

a2 F or

f(x)= bran > pV a

9. As in the previous exercise,

|jxf<r <1.

z
P(R <1, Z<20= z/ nr? — w) dw.

—r

197
[4.7.10]-[4.7.14] Solutions Continuous random variables

Hence f(r, z) = 3r for |z| < r < 1. This question may be solved in spherical polars also.

10. The transformation s = x + y,r = x/(x + y), has inverse x = rs, y = (1 —1r)s and Jacobian
J =s. Therefore,

CO o.4)
far) = | frstr.syds = [ fxy(rs, (1 —r)s)s ds

0 Xr
= | ne ATS pe HU-M)S 5 ds _ ——_E O<r<l.
0 {ar + wl —1)}

11. We have that

pO <= (x2 > 2-1) = 2P(x <- <-1),
Y y

whence

1
fr) = 2fx(-V@ly) — 1) = Ta O<y<a.

12. Using the result of Example (4.6.7), and integrating by parts, we obtain

ramaran= [oe {i-0(Fa85)h a
a —p

=(1-e@t-e@l+ [1- ea ba px Pdx.
a V1—p2 1— 2

Since [1 — ®(x)]/@() is decreasing, the last term on the right is no greater than

—70 b— px p d
6@) Ja eco = 1p

which yields the upper bound after an integration.

13. The random variable Y is symmetric and, for a > 0,

a7! d a 2g
PY > a)=PO<X <a!) = [ aos aed
0 ml+u?) Joo r(1 +07?)
by the transformation v = 1/u. For another example, consider the density function
5x? ifx > 1,
faye i

14, The transformation w = x + y,z = x/(x + y) has inverse x = wz, y = (1 — z)w, and Jacobian
J = w, whence

AQwz)%leAwz (A — z)w)yB-leAC 2)

Fw, 2) = ws Tey TB)
_ AAw)*tB-1 Aw zl _ ze!
= r@ +A) : Bia. B) ; w>0,0<z<1.

Hence W and Z are independent, and Z is beta distributed with parameters a and f.

198

Sums of random variables Solutions [4.8.1]-[4.8.4]

4.8 Solutions. Sums of random variables

1. By the convolution formula (4.8.2), Z = X + Y has density function
% x
fz@ = | Ape Me“ HE) dy = SE (e* - e Ht) , 220,
) w-A
if A ~ yw. What happens if A = 4? (Z has a gamma distribution in this case.)
2. Using the convolution formula (4.8.2), W = aX + BY has density function

lore) 1 1
fu w) = [. wa(i + Ja) | mB(1+ ((w—x/BP)

which equals the limit of a complex integral:

im | %._1 | d
1 —_ .
R>oJp m2 z2+a2 (z—w)? + p2

where D is the semicircle in the upper complex plane with diameter [—R, R] on the real axis. Evalu-
ating the residues at z = ia and z = w + iB yields

) _ eB azi fe 1 1 1 \
fw) = Vie” (ia — w)? + p2 + 3ip (w+ iB? 402
1 1

~ r@+p) 1+(w/@tAP

after some manipulation. Hence W has a Cauchy distribution also.

3. Using the convolution formula (4.8.2),
ey 1,2
fz(@) = [ nee “dy=nze%, 7220.

4. Let f, be the density function of S,. By convolution,

x A A
—Ayx 2 —Agx 1 —Arx s
=A 14%. x 2 = x r
F2(%) 1€ Moa + Mt —do > ré | | Ae -dy
r=] s=1
sr
This leads to the guess that
n n X
= —Arx s
(*) fn) = Do ape Hse n>2,
r=1 s=l
sr

which may be proved by induction as follows. Assume that (+) holds for n < N. Then

x N N 4

r=l sai 2s ~4r
sr
N N+1 X
= S ape r* T] —2— 4 Ae Nt,
» r II hs — Ap
r=1 s=l
sor

199

[4.8.5]-[4.8.8] Solutions Continuous random variables

for some constant A. We integrate over x to find that

Nyl A
a> I mre + Nal ,

-r

s=
RY
and (*) follows with n = N + 1 on solving for A.

5. The density function of X + Y is, by convolution,

fax) {3 ifO<x <1,
x)=
2 2—-x ifl<x <2.

Therefore, for 1 < x < 2,

1 1 x—1
Aw) = [ pee-»ay= [ @-yay+ [2-4 y= 9-6-2,

Likewise,

fax) 5x? if0<x <1,
3K) =
4G—x) if2<x <3.

A simple induction yields the last part.
6. The covariance satisfies cov(U, V) = E(X 2_ Y2) = 0, as required. If X and Y are symmetric
random variables taking values +1, then

PU =2,V=2)=0 but PU =2)P(V =2)>0.
17,242
If X and Y are independent N(0, 1) variables, fy,y(u, v) = (42) 1e7 4% + ) which factorizes as
a function of u multiplied by a function of v.

7, From the representation KX = opU + o0./1— p2V, Y = tU, where U and V are independent

N(O, 1), we learn that

B(X | ¥ = y) =E@pU |U = y/t) = 2.

Similarly,
oC 2
E(X? | ¥ = y) = E((opU)? +021 — p2)V2 |U = y/t) = (=) +02(1 — p’)

whence var(X | Y) = o7(1 _ p”). For parts (c) and (d), simply calculate that cov(X, X + Y) =
o*+ pot, var(X + Y) = oF + Qport +t, and
t7(1 — p*)

1— p(x, X Y)? = —— —*.
pt +¥) o24+2p0t + 12

8. First recall that P(|X| < y) = 2®(y) — 1. We shall use the fact that U = (X + Y)/V¥2,
= (X — Y)/./2 are independent and N(0, 1) distributed. Let A be the triangle of R* with vertices
(0, 0), (, Z), (Z, 0). Then

P(Z <z|X>0, Y > 0) =4P((X,Y)€ A) =P(|U|< z/V2,|V| < z/V2) by symmetry
= 2{2(z/V2) — 1},

200

Multivariate normal distribution Solutions [4.9.1]-[4.9.5]

whence the conditional density function is
f@) = 22{20(z/V2) — 1}6(z/v2).
Finally,

E(Z | X > 0, Y > 0) = 2E(X | X > 0, Y > 0)
Co x 1,2

= 2E(X | X > 0) = 4E(XIjx30)) = af
0

4.9 Solutions. Multivariate normal distribution

1. Since V is symmetric, there exists a non-singular matrix M such that M’ = M7~! and V =
MAM7!, where A is the diagonal matrix with diagonal entries the eigenvalues Aj, A2,..., An of V.

1 1
Let A2 be the diagonal matrix with diagonal entries ./A1, /A2,...,-/An; A2 is well defined since
1
V is non-negative definite. Writing W = MA2M’, we have that W = W’ and also

Ww = (MA2M~!)(MA2M~!) =MAM-!=V

1
as required. Clearly W is non-singular if and only if A2 is non-singular. This happens if and only if
A; > 0 for all i, which is to say that V is positive definite.

2. By Theorem (4.9.6), ¥ has the multivariate normal distribution with mean vector 0 and covariance
matrix
w-'vw-! = wo! ow4yw-l =

3. Clearly Y = (K — pa’ + pra’ where a = (a1, @2,..., an). Using Theorem (4.9.6) as in the
previous solution, (X — yz)a’ is univariate normal with mean 0 and variance aVa’. Hence Y is normal
with mean ja’ and variance aVa’.

4. Make the transformation u = x + y, v = x — y, with inverse x = su +v) y= 5(u — v), SO
that |J| = 5. The exponent of the bivariate normal density function is

ny ay Eg ay 2
apy * 2oxy + y*) 4nd (L—p)+v°(1+ p)},

and therefore U = X + Y, V = X — Y have joint density

flu, v) 1 ex uw v?
u,v) = - - ,
4nJi-p2?| 40 +0) 40—p)

whence U and V are independent with respective distributions N(0, 2(1 + e)) and N(O, 2(1 — ¢)).

5. That Y is N(O, 1) follows by showing that P(Y < y) = P(X < y) for each of the cases y < ~a,
ly| <a, y>a.
Secondly,

p(a) = E(XY) = a x°o(x) dx — [. x°o(x) dx — [ Pomax =1-— af x(x) dx.

a —ooO a a

201

[4.9.6]-[4.10.1] Solutions Continuous random variables

The answer to the final part is no; X and Y are N(0, 1) variables, but the pair (X, Y) is not bivariate
normal. One way of seeing this is as follows. There exists a root a of the equation p(a) = 0. With
this value of a, if the pair X, Y is bivariate normal, then X and Y are independent. This conclusion is
manifestly false: in particular, we have that P(X > a, Y > a) # P(X > a)P(Y > a).

6. Recall from Exercise (4.8.7) that for any pair of centred normal random variables
cov(X, Y)

E(X | ¥) = SY, var(X | ¥) = {1 ~ o(X, Y)?} var X.

The first claim follows immediately. Likewise,

Lor CjrCk
var(X; | Xx) = {1 — p(X, Xx)"} var Xj = { - do,

2 2
Jr Cir Lr cky JF

7. As in the above exercise, we calculate a = E(X, | 0] X-) and b = var(X | >>] X,) using the
facts that var Xj = v4, var(S7 X;) = ij vjj, and cov(X1, i X,) =), Up.

8 Letp =P(X >0, Y>0, Z>0) = P(X <0, Y <0, Z <0). Then

1— p=P({X > O}U{Y > 0} U{Z > 0})
= P(X > 0)+P(Y > 0)+ P(Z > 0)4+p
—P(X > 0, Y > 0) -—P(Y > 0, Z > 0) —- P(X > 0, Z > 0)
3

3 1 — —_
=F +p [F+ sbi 1 oy + sin ! oy + sin 1 03} .

9. Let U, V, W be independent N(0, 1) variables, and represent X,Y, Zas X =U, Y = pyU +

- 1 =p? —p2—p2+2
Z=pu+@ ey 4 P{ — Pz — PZ PLP2P3 yy

J1-e (1 — pf)

We have that U = X, V = (Y — 9, X)/,/1- pe and E(Z | X, Y) follows immediately, as does the
conditional variance.

4.10 Solutions. Distributions arising from the normal distribution

1. First method. We have from (4.4.6) that the x2(m) density function is

1 1
fn (x) = g—m/2.9M-1e-3% ye >,

Tm /2)

The density function of Z = X, + X9 is, by the convolution formula,
zZ1 1 1 1
g(z) = cf x21 e- 3% (z — x) 2716-2) dy
0
1 1, fii 1
= cgimtm-tende f u2™—ly - u)2"—! du
0

202
Distributions arising from the normal distribution Solutions [4.10.2}-[4.10.6]

by the substitution u = x/z, where c is a constant. Hence g(z) = clzzmtn)-1,— 32 for z > 0, for
an appropriate constant c’, as required.

Second method. If m and n are integral, the following argument is neat. Let Z1, Zo,..., Zm4n be
independent N(0, 1) variables. Then X1 has the same distribution as Z} + Z ++--+ Z2,, and X2
the same distribution as Z?,, | + Z2,.,5 +--+ +Z24, (see Problem (4.14.12)). Hence X1 + Xz has
the same distribution as Z{ -+---+ Z2,,,, Le., the x2(m +2) distribution.

2. (i) The ¢(r) distribution is symmetric with finite mean, and hence this mean is 0.
(ii) Here is one way. Let U and V be independent x2(r) and x28) variables (respectively). Then

U/r _ s -1

by independence. Now U is rd, 5r) and V is rd, $s), so that E(U) =r and

7s a Vv e@ v= T i Vv e 2"dv =
vrs) ar(ds) Jo Ps—) s—2

09 1 9-5/2 LoL 00 -$(s-2)
E(V~!) -| 12 As-1 -3¥g _ Pgs —D 2 8-2 1 1
0

if s > 2, since the integrand is a density function. Hence

E Ulr -— if s > 2.
V/s s—2

(ii) If s < 2 then E(V—!) = 00.
3. Substitute r = 1 into the ¢(r) density function.

4, First method. Find the density function of X/Y, using a change of variables. The answer is
F(2, 2).

Second method. X and Y are independent x2(2) variables (just check the density functions), and
hence X/Y is F(2, 2).

5. The vector (X, X; — X, Xz — X,..., Xn — X) has, by Theorem (4.9.6), a multivariate normal
distribution. We have as in Exercise (4.5.7) that cov(X, X, — ¥) = 0 for all r, which implies that X¥
is independent of each X;. Using the form of the multivariate normal density function, it follows that
X is independent of the family {X, — X : 1 < r <n}, and hence of any function of these variables.
Now S? = (n — 1)—! 0. (X; — X)? is such a function.

6. The choice of fixed vector is immaterial, since the joint distribution of the X; is spherically
symmetric, and we therefore take this vector to be (0, 0,... ,0, 1). We make the change of variables
U? = Q? + X?2, tanW = Q/Xn, where Q? = S-"~} xX? and Q > 0. Since Q has the x?(n — 1)
distribution, and is independent of Xy, the pair Q, X», has joint density function

1,2 1 m@_3) -1
Jn Tr(4(n— 1)

xeéER, g>0.

The theory is now slightly easier than the practice. We solve for U, VY, find the Jacobian, and deduce
the joint density function fy w(u, ¥) of U, YW. We now integrate over u, and choose the constant so
that the total integral is 1.

203

{4.11.1]-[4.11.7] Solutions Continuous random variables

4.11 Solutions. Sampling from a distribution

1. Uniform on the set {1, 2,... , n}.
2. The result holds trivially when n = 2, and more generally by induction on n.

3. We may assume without loss of generality that A = 1 (since Z/A is (A, t) if Zis TC, t)). Let U,
V be independent random variables which are uniformly distributed on [0, 1]. We set X = —tlog V
and note that X has the exponential distribution with parameter 1/t. It is easy to check that

1
ro

xf 1p < cfx(x) for x > 0,

where c = t'e—'t!/ T(z). Also, conditional on the event A that

xt-le-t Xx
U < ——— te *!,
= "Fo
X has the required gamma distribution. This observation may be used as a basis for sampling using
the rejection method. We note that A = {log U < (n — 1) (log(X/n) — (X/n) + 1)}. We have that
P(A) = 1/c, and therefore there is a mean number c of attempts before a sample of size 1 is obtained.

4. Use your answer to Exercise (4.11.3) to sample X from (1, w) and Y from T'(1, B). By Exercise
(4.7.14), Z = X/(X + Y) has the required distribution.

5. (a) This is the beta distribution with parameters 2, 2. Use the result of Exercise (4).

(b) The required (1, 2) variables may be more easily obtained and used by forming X = — log(U, U2)
and Y — log(U3U4) where {U; : 1 <i < 4} are independent and uniform on [0, 1].

(c) Let U1, Uz, U3 be as in (b) above, and let Z be the second order statistic U2). That is, Z is the
middle of the three values taken by the U;; see Problem (4.14.21). The random variable Z has the
required distribution.

(d) As a slight variant, take Z = max{U,, U2} conditional on the event {Z < U3}.

(e) Finally, let X = /U/(./U) + /U2), ¥Y = /U, + /U3. The distribution of X, conditional on
the event {Y < 1}, is as required.

6. Weuse induction. The result is obvious when n = 2. Letn > 3 and let p = (pj, p2,-..., Pn) be
a probability vector. Since p sums to 1, its minimum entry pcj) and maximum entry pi») must satisfy

1 1 1— pay _ 1+(@—2)pqay . 1

<e- —— > .
PY Se? Pay + Pin) = Pay + n—-1 n-1 n-1

We relabel the entries of the vector p such that py = pj) and py = pin), and set vy = ((a- 1)py,1-
(n — 1)p1,0,...,0). Then

n-1l

—2 1
Pn-1 Where py] = 0, py + pz - ———
n—-1 n—-2 n—1

1 n
P= —TVW+
n—-1

Lasso +P)

is a probability vector with at most n — 1 non-zero entries. The induction step is complete.

It is a consequence that sampling from a discrete distribution may be achieved by sampling from
a collection of Bernoulli random variables.
7. Jtisanelementary exercise to show that P(R2 <)= im, and that, conditional on this event, the

vector (J,, T+) is uniformly distributed on the unit disk. Assume henceforth that R2 < 1, and write
(R, ©) for the point (7, T>) expressed in polar coordinates. We have that R and © are independent
with joint density function fr.@(7, 0) = r/x,0<r<1,0< 6 < 2. Let (Q, W) be the polar

204

Coupling and Poisson approximation Solutions [4.11.8]-[4.12.1]

, 192
coordinates of (X, Y), and note that ¥ = © and e~ 22° = R?. The random variables Q and W are

independent, and, by a change of variables, Q has density function fg(q) = ge? , q > 0. We
recognize the distribution of (Q, VY) as that of the polar coordinates of (X, Y) where X and Y are
independent N(0, 1) variables. [Alternatively, the last step may be achieved by a two-dimensional
change of variables.]

8. We have that
log U
log gq

px =H =P(| |=«-1) =P(qgk <U <q ')=qgF1—q), k>1.

9. The polar coordinates (R, ©) of (X, Y) have joint density function

H<O< 50.

Ne
Ne

2.
froin O=—, O<r<i1-
Tw

Make a change of variables to find that Y/ X = tan © has the Cauchy distribution.
10. By the definition of Z,

m-1
P(Z =m) =h(m) [J (1-20)
r=0
= P(X > O)P(X > 1|X>0)---P(X=m|X>m—-) =P(X =m).

11. Suppose g is increasing, so that h(x) = —g(1 — x) is increasing also. By the FKG inequality of
Problem (3.11.18b), « = cov(g(U), —g(i — U)) = 0, yielding the result.

Estimating J by the average (2n)7! ria g(U,) of 2n random vectors U; requires a sample
of size 2n and yields an estimate having some variance 2nc?. If we estimate J by the average
(2n)7! {7.1 8(Ur) + gl — U,)}, we require a sample of size only n, and we obtain an estimate

with the smaller variance 2n (a2 —K).

12. (a) By the law of the unconscious statistician,

E feo gy) fx)

fr (¥) / froy 122%

(b) This is immediate from the fact that the variance of a sum of independent variables is the sum of
their variances; see Theorem (3.3.11b).

(c) This is an application of the strong law of large numbers, Theorem (7.5.1).

13. (a) If U is uniform on [0, 1], then X = sin(37 U) has the required distribution. This is an example
of the inverse transform method.

(b) If U is uniform on [0, 1], then 1 — U? has density function g(x) = {2/T—x}"', O0< x <1.
Now g(x) = (1/4) f(x), which fact may be used as a basis for the rejection method.

4.12 Solutions. Coupling and Poisson approximation
1. Suppose that E(u(X)) > E(u (Y)) for all increasing functions u. Let c € R and set u = I, where

1 ifx>ec,
0 ifx<c,

Ie(x) = {

205

[4.12.2]-[4.13.1] Solutions Continuous random variables

to find that P(X > c) = E(ie(X)) = EUc(Y)) = P(Y > c).

Conversely, suppose that X >st Y. We may assume by Theorem (4.12.3) that X and Y are
defined on the same sample space, and that P(X > Y) = 1. Let u be an increasing function. Then
P(u(X) > u(Y)) > P(X > Y) = 1, whence E(u(X) — u(Y)) => 0 whenever this expectation exists.

2. Leta = w/A, and let {J, : r => 1} be independent Bernoulli random variables with parameter a.
Then Z = an T, has the Poisson distribution with parameter Aa = ys, and Z < X.

3. Use the argument in the solution to Problem (2.7.13).

4. Forany ACR,

P(X #£Y) > P(X¥ € A, Y € AS) = P(X € A)—-P(X EA, YEA)
> P(X € A) —P(Y € A),

and similarly with X and Y interchanged. Hence,

P(X # Y) > sup |P(X € A) — P(Y € A)| = 4drv(X, Y).
ACR

5. For any positive x and y, we have that (y — x)+ +x Ay = y, where x A y = min{x, y}. It
follows that

Stfx@ — frit = fr ® — fe@yt =1- 30 fea fr®,
k k k

and by the definition of dty(X, Y) that the common value in this display equals sary (X,Y) = 6.
Let U be a Bernoulli variable with parameter 1 — 5, and let V, W, Z be independent integer-valued
variables with

P(V =k) = {fx — fr®}*/8,
P(W =k) = {fytk) — fx(k)}* /8,
P(Z =k) = fx(k) A fr (K)/(L — 8).

Then X‘ = UZ+ (1 —U)V and Y’ = UZ + (1 — U)W have the required marginals, and P(X’ =
Y’) = PU = 1) = 1 —8. See also Problem (7.11.16d).

6. Evidently dpy(X, Y) = |p — q|, and we may assume without loss of generality that p > g. We
have from Exercise (4.12.4) that P(X = Y) < 1 — (p—q). Let U and Z be independent Bernoulli
variables with respective parameters 1— p+g andq/(1—p+gq). Thepair X’ = U(Z—1)4+1,Y’ = UZ
has the same marginal distributions as the pair X, Y, and P(X’ = Y’) = P(U = 1) = 1— p+qas
required.

To achieve the minimum, we set X” = 1 — X’ and Y” = Y’, so that P(X” = ¥”) = 1-P(X’ =
Y’)=p-q.

4.13 Solutions. Geometrical probability

1. The angular coordinates Y and & of A and B have joint density f(y, o) = (27)~?. We make
the change of variables from (p, 9)  (,o) by p = cos{5(o —-wW}a= 30 +o+ 7), with
inverse

w=o- 3x — cos! p, o=6- 3m + cos! p,

and Jacobian |J| = 2/,/1 — p?.

206
Geometrical probability Solutions [4.13.2]-[4.13.6]

2. Let A be the left shaded region and B the right shaded region in the figure. Writing 4 for the
random line, by Example (4.13.2),

P(A meets both S$; and $7) = P(A meets both A and B)
= P(A meets A) + P(A meets B) — P(A meets either A or B)
« b(A) + b(B) — b(A) = b(X) — b(A),

whence P(A meets S32 | A meets $1) = [b(X) — b(H)]/b(S}).
The case when Sz C Sj is treated in Example (4.13.2). When S, M Sp # @ and S; A Sp # @,
the argument above shows the answer to be [b(S,) + b(S2) — b(H)]/B(S1).

3. With |/| the length of the intercept J of A, with S2, we have that P(A meets 1) = 2|7|/b(S}),
by the Buffon needle calculation (4.13.2). The required probability is

Lee 21] dpd@ _ ™ |S! 16 = EIS
2Jo Joo b(S1) B(S1) Jo. (81)? BES)?”

4, Ifthe two points are denoted P = (X), Y;), and Q = (Xo, Yo), then

E(Z7) = E(|PQ|*) = 2E((X1 — X2)”) = 4var(X)) = a5 5 x? Va? ~x2 dx =a".

We use Crofton’s method in order to calculate E(Z). Consider a disc D of radius x surrounded by an
annulus A of width h. We set A(x) = E(Z | P, Q € D), and find that

A(x +h) = A(x) ¢ - < =) + 2E(Z | Pe D, Qe A) (+ + 0h).

Now

1

2 3m 2x cos @ 32.

E(Z|PeD, Qe A)=—5 [ 2 dr dO +0(1) = —,
mx* Jo 0 On

whence
dx _ 4. 128

dx x On’
which is easily integrated subject to 4(0) = 0 to give the result.

5. (i) We may assume without loss of generality that the sphere has radius 1. The length X = |AO|}
has density function f(x) = 3x? for 0 <x < 1. The triangle includes an obtuse angle if B lies either
in the hemisphere opposite to A, or in the sphere with centre 5X and radius 5X , or in the segment
cut off by the plane through A perpendicular to AO. Hence,

1
P(obtuse) = 5 + E((4.X)%) + (2) 1E (/ m(1 — y’) in)
Xx
_!1 1 4)—-1y/2 ly3, _ 5
=atyet(s) EG -X+3X)= 3.
(ii) In the case of the circle, X has density function 2x for 0 < x < 1, and similar calculations yield

1 1 1
P(obtuse) = 5 + 3 + —E (cos X-XV1-X?)= >

6. Choose the x-axis along AB. With P = (X, Y) and G = (7, 2),
E|ABP| = 5|AB] E(Y) = IABly2 = |ABG}.

207
[4.13.7]-[4.13.11] Solutions Continuous random variables
7. We use Exercise (4.13.6). First fix P, and then Q, to find that
E/|APQ| = E[E(|APQ| | P)] = E|APG2| = |AG;Gp|.

With b = |AB| and A the height of the triangle ABC on the base AB, we have that |G,G2| = 5b and
the height of the triangle AG; Go is fh. Hence,

E|APQ| = 4 - 4b- 3h = SABC].

8. Let the scale factor for the random triangle be X, where X € (0, 1). For a triangle with scale
factor x, any given vertex can lie anywhere in a certain triangle having area (1 — x)*|ABC]. Picking
one at random from all possible such triangles amounts to supposing that X has density function
f@ =3da- x)*, 0 <x <1. Hence the mean area is

1
|ABC| E(X?) = jasc} | 3x7(1 — x)’ dx = 7y|ABC].
0

9. We have by conditioning that, for 0 < z <a,

a 2 2a da
F(z,a+da) = F(z, a) aida + P(X 2 a2) ay + 0a)
2d 2d
= F(z, a) (1 - *<) +2." +o(da),

and the equation follows by taking the limit as da | 0. The boundary condition may be taken to be
F(a, a) = 1, and we deduce that

2. 2
FG, a) = = - (=) ' O0<z<a.
a a
Likewise, by use of conditional expectation,
2d 2d
mr(a +da) = m,(a) (1 - ae) +E((a _ x)’) . << + o(da).

Now, E((a—X)") = a’ /(r +1), yielding the required equation. The boundary condition is m; (0) = 0,
and therefore
2a’
m= CDE ED
10. If n great circles meet each other, not more than two at any given point, then there are 2(5)
intersections. It follows that there are 4(5) segments between vertices, and Euler’s formula gives the
number of regions as n(n — 1) + 2. We may think of the plane as obtained by taking the limit as
R — oo and ‘stretching out’ the sphere. Each segment is a side of two polygons, so the average
number of sides satisfies
4n(n — 1)

—————- > 4 asn— 0O.
2+n(n — 1)

11. By making an affine transformation, we may without loss of generality assume the triangle has
vertices A = (0, 1), B = (0,0), C = (1, 0). With P = (X, Y), we have that

x x Y Y
L (25). Cesabesal N (0. 3)

208

Problems Solutions [4.13.12]-[4.14.1]

Hence,

1 2
xy x a 3
BIBLN|=2 | © ——»——-dxdy= | (-x-——1togx) dx = 2-2,
IBLN| ees vay [(- i) 6 2

and likewise E|CLM] = E|ANM] = 47? — 3. It follows that EILMN| = 3(10 — 2?) = (10 —
m7)|ABCI.
12. Let the points be P, Q, R, S. By Example (4.13.6),

NI

P(one lies inside the triangle formed by the other three) = 4P(S € PQR) =4- i.

13. We use Crofton’s method. Let m(a) be the mean area, and condition on whether points do or do
not fall in the annulus with internal and external radii a, a + h. Then

6
m(a +h) = m(a) () + [* + o( m(a),

where m(a) is the mean area of a triangle having one vertex P on the boundary of the circle. Using
polar coordinates with P as origin,

2acos@ p2acosy
n7a‘m(a) = SEES [ r3r§ dry dra sin|6 — v\ldddw

_ 32a° 65

of [sie 6 sin? w sin |@ — vidody = 2

Letting # | 0 above, we obtain
dm 6m 6 35a”

da ata 36m

whence m(a) = (35a”)/(48z).

14, Let a be the radius of C, and let R be the distance of A from the centre. Conditional on R, the
required probability is (@ — R)?/a?, whence the answer is E((a — R)? /a”) = fo (l—r)*2r dr = é

15. Leta be the radius of C, and let R be the distance of A from the centre. As in Exercise (4.13.14),
the answer is E((a — R)?/a3) = fia —r)3r? dr = a

4.14 Solutions to problems

1. (a) We have that

IL eo 49) dy dy = IL, en rdrdd =n.
R R

Secondly, f > 0, and it is easily seen that iho f()dx = 1 using the substitution y = (@ —
p)/(oV2).

: OO 1 - 12 : : - 1,2 . : :
(b) The mean is f° go *(2%) Ze 2° dx, which equals 0 since xe 2" is an odd integrable function.

. . OO 2 - 1 - 1 x2 : : :
The variance is [°° x“(21)” 2e” 2” dx, easily integrated by parts to obtain 1.

209

{4.14.2]-[4.14.5] Solutions Continuous random variables
(c) Note that

d _1,2 _1,2

— {rte 2” \. -(+y)e7 2,

dy

d 1,2 _1,2
— {or —y ye 2? \ = -(1-3y4)e72,
dy

1,2
and also 1 — 3y~4 < 1 < 1+ y~?. Multiply throughout these inequalities by e 2” /./27, and
integrate over [x, 00), to obtain the required inequalities. More extensive inequalities may be found
in Exercise (4.4.8).

(d) The required probability is a(x) = [1 — ®(@ + a/x)]/[1 — ®@)j]. By (©),
ena @ta/x)?
a(x) = (L +0(1))——{4-—— > e 4 aS xX —> 00.
e2*
2. Clearly f > 0 if and only if0 <a < B <1. Also
- B 2 1,22 2 1723 3
= [=a dx = $6? 0) — $0? - 03),
a

3. The A; partition the sample space, andi — 1 < X(w) <i if € Aj. Taking expectations and
using the fact that E(/;) = P(A;), we find that S < E(X) < 1+ S where

oo i-1
S= Ye pPay =O Pa) = 30 3 FA) = YP ph
i=2j=1 j=li=j+l

4. (a) (i) Let F~!(y) = sup{x : F(x) = y}, so that
P(F(X) <y)=P(X<F'Q))=FF'0)=y, O<y<l

(ii) P(— log F(X) < y) = P(F(X) > e7”) =1—e7 ify >0.
(b) Draw a picture. With D = PR,

a, ——— 1
P(D < d) = P(tan POR < d) = P(POR < tan~! @) = — (= + tan7 0)
bd
Differentiate to obtain the result.
5. Clearly
P(X>s+x) e+) ax
PX >stx[X>sy= PO > 5) = is =e

if x, s > 0, where A is the parameter of the distribution.

Suppose that the non-negative random variable X has the lack-of-memory property. Then G(x) =
P(X > x) is monotone and satisfies G(O) = 1 and G(s + x) = G(s)G(x) for s,x > 0. Hence
G(s) = e7 5 for some A; certainly 1 > 0 since G(s) < 1 for alls.

Let us prove the hint. Suppose that g is monotone with 9(0) = 1 and g(s +1) = g(s)g(t)
for s,t > 0. For an integer m, g(m) = g(l1)g(m — 1) = --- = g(1)”. For rational x = m/n,
g(x)” = g(m) = g(1)” so that g(x) = g(1)*; all such powers are interpreted as exp{x log g(1)}.
Finally, if x is irrational, and g is monotone non-increasing (say), then g(u) < g(x) < g(v) for all

210

Problems Solutions [4.14.6]-[4.14.9]

rationals u and v satisfying v < x < u. Hence g(1)” < g(x) < g(1)”. Take the limits asu | x and
v ¢ x through the rationals to obtain g(x) = e“* where pz = log g(1).

6. If X and Y are independent, we may take g = fy and hk = fy. Suppose conversely that
f(. y) = g@)hQy). Then

CO OO
f(s) = a(x) f h(y) dy, fro) = ho) [ g(x) dx

i=[ fray = f g(x) dx f h(y) dy.

Hence fx (x) fy (vy) = g@)h(y) = FC, y), So that X and Y are independent.

7. They are notindependentsinceP(Y <1, X > 1) = OwhereasP(Y < 1) > OandP(X > 1) > 0.
As for the marginals,

fo) y
pula) = [2% dy = 26, fy = [26 dx = 227 =e),
x 0

for x, y => 0. Finally,

OO oO
E(XY) = | / xy2e*~9 dx dy = 1
x=0 Jy=x

and E(X) = 4, E(Y) = 3, implying that cov(X, Y) = 4.

8. Asin Example (4.13.1), the desired property holds if and only if the length X of the chord satisfies
xX < J. Writing R for the distance from P to the centre O, and © for the acute angle between the chord

and the line OP, we have that X = 2/1 — R2 sin” ©, and therefore P(X < J/3) = P(Rsin® > 3):
The answer is therefore

1
1 2 m 1
P(R>— -=[* P| R>— dé,
2sin® x Jo 2sin@

which equals z — J¥3/ (20) in case (a) and : +71 log tan(r/12) in case (b).
9. Evidently,

1
Bu) =P < a(x) = ff dx dy = [ g(x) dx,
O<x,y<l 0

ysg(x)

1
E(V) = E(g(X)) = [ g(x) dx,

1 1
Bow) = $f {e(x) +e —x)}dx = [ g(x) dx.

Secondly,

1
E(U?)=E(U)=J,  -E(V”) -| g(x)? dx <J sinceg <1,
0
1 1
E(w? ~4{2/ soyrdx +2 [ sna —ar|
0 0
1
= (V4) — iff g(x){g(x) — g(1 —x)} dx
1
= E(V*) — if {g(x) — gd — x)’ dx < E(V?).
0

211

[4.14.10]-[4.14.11] Solutions Continuous random variables

Hence var(W) < var(V) < var(U).

10. Clearly the claim is true for n = 1, since the P(A, 1) distribution is the exponential distribution.
Suppose it is true forn < k where k > 1, and consider the casen = k+ 1. Writing f;, for the density
function of S,, we have by the convolution formula (4.8.2) that

x x yk qktle—Ax px
_ _y)dy= k-1,-Ayy p-MXx-Y) dy = “ae tl kl gy,
Fit) [ Fx fr @ — y) dy [ Te”? * , r® bb”

which is easily seen to be the P(A, k + 1) density function.

11. (a) Let Z1, Z,..., Zmin be independent exponential variables with parameter 1. Then, by
Problem (4.14.10), X’ = Z) +---+Zm is TA,m), ¥’ = Zm41 + +--+ Zmtn is T(A,n), and
X’+ Y’ is [(A,m +n). The pair (X, Y) has the same joint distribution as the pair (X’, Y’), and
therefore X + Y has the same distribution as X’ + Y’,i.e., P(A,m +n).

(b) Using the transformation u = x + y, v = x/(x + y), with inverse x = uv, y = u(1 — v), and

Jacobian
v u

=-4u
l—-v -u ,

=|

we find that U = X + Y, V = X/(X + Y) have joint density function
pinta

fu,v, v) = fx,y (uv, ul — v)) |u| = Tomr@ wr” te —v)}t ley

-{ qmtn umintemt| yt-1q — yt}
Pm +n) Bim,n)

for u > 0,0 <v < 1. Hence U and V are independent, U being (A, m +n), and V having the beta
distribution with parameters m and n.
(c) Integrating by parts,

OO rm
P(X >nh= / "le * dx
t

(m — 1)!
amt m—1 ,-Ax ° oo ml m—2_,—Ax
-|-Soe “|, +f mami
war ty"!

where X’ is '(A, m — 1). Hence, by induction,
P(X a Gk Z
( >= re er = PC <m).
k=0

(d) This may be achieved by the usual change of variables technique. Alternatively, reflect that, using
the notation and result of part (b), the invertible mapping uw = x + y, v = x/(x + y) maps a pair
X,Y of independent ([(A, m) and (A, n)) variables to a pair U, V of independent (T(A, m +7) and
B(m, n)) variables. Now UV = X, so that (figuratively)

“‘TA,m+n) x Bim,n) =T(A,m)”.

Replace n by n — m to obtain the required conclusion.

212

Problems Solutions [4.14.12]-[4.14.15]

R. @Z= x? satisfies
vz 4 12
fz(z) = SPX < a e 2" dus =
z 0

the rG, 5) or x2) density function.
(b) Ifz > 0, Z = X? + X3 satisfies

244,2
P(Z <2) =P(X7 4 X3 <2) = If J oye +9) dx dy
24y2<z Qn

20 1 1
=|" of. —e —3” rdrd@ =1—e72¢,
=0 Jo=o 270

the x22) distribution function.

(c) One way is to work in n-dimensional polar coordinates! Alternatively use induction. It suffices
to show that if X and Y are independent, X being x2(n) and Y being x72Q) where n > 1, then
Z=X+Yis x20 + 2). However, by the convolution formula (4.8.2),

z —n/2
0 n

for some constant c. This is the x2(n + 2) density function as required.

13. Concentrate on where x occurs in fx,y (x | y); any multiplicative constant can be sorted out later:

_ fxy@y) _ 1 x? 2x 2px(y — 2)
fxyy@ |y) = fr) e1(y) v0 - xp (5 a? 310) J}

by Example (4.5.9), where c1 (y) depends on y only. Hence

[x — w1 — poy(y — ea  xeR

Fxiy @ | y) = co(y) exp {- 20 — pa?

for some c2(y). This is the normal density function with mean 41 + po (y — 42)/o2 and variance
oF(1 - p”). See also Exercise (4.8.7).

14. Setu = y/x, v = x, with inverse x = v, y = uv, and |J| = |u|. Hence the pair U = Y/X,
V = X has joint density fy,y(u, v) = fx,y(v, uv)|v| for —oo < u,v < oo. Therefore fy(u) =
[eo fv, uv)|v| dv.

15. By the result of Problem (4.14.14), U = Y/X has density function
OO
fou) =f forfuy)lyldy,
—ooO

and therefore it suffices to show that U has the Cauchy distribution if and only if Z = tan—! U is
uniform on (52, 5m). Clearly

P(Z <6) =PU < tan@), 40 <@< 5m,

213

[4.14.16]-[4.14.17] Solutions Continuous random variables

whence f7(0) = fy (tan@) sec” 6. Therefore fz@= xo! (for || < ea) if and only if

1
mi +u2)’

“MO <uUu< Ww.

fut) =
When f is the N(0, 1) density,
°° © 4, 1.9,,.2
[- feotanitax =2 [Phe PO vax,
—0o 0 2

which is easily integrated directly to obtain the Cauchy density. In the second case, we have the
following integral:

fore) 2 x|
4 aa OX
—oo (1 +x*)(1 + x*y*)
In this case, make the substitution z = x” and expand as partial fractions.
16. The transformation x = rcos@, y =r sin@ has Jacobian J = r, so that
1 _1,2
fr,e(r,0) = ~-re 2 , r>0, 0<6 <2n.

20

Therefore R and © are independent, © being uniform on [0, 277), and R?2 having distribution function

Ja 4 1
P(R? <a) = [ re-2” dr =1—e724;
0

this is the exponential distribution with parameter 4 (otherwise known as ry, 1) or x2(2)). The

1,2
density function of R is fr(r) =re 2" forr > 0.
Now, by symmetry,
x? 1_ { x?+4y¥? 1
Ei —; = -—E | ——,—_ =-,.
R2 2 R2 2

In the first octant, ie., in {(x, y) : 0 < y < x}, we have min{x, y} = y, max{x, y} = x. The
joint density fx y is invariant under rotations, and hence the expectation in question is

8 | = fy, ydxdy=8 | [. an’ re 2” dr dé = — log?.
O<y<x ¥ =0 On” Xu

17. G) Using independence,

PU <u) =1—-P(X >u,¥ > u)=1-(1-FxyW)(1—- Fylu)).
Similarly

PVV <v)=PXX <0,Y <v) = Fy(v)Fy().
(ii) (a) By @), PU <u) = 1 — ee foru > 0.
(b) Also, Z = X + 4Y satisfies
OO v io.¢]
P(Z>v)= [ P(Y > 2(v —x)) fx(@x) dx = [ e 2X) 9% ay +/ e* dx
0 0

v

=e%e" — 1) +e? = 1—-(1-e 2 = PV > 0).

214

Problems Solutions [4.14.18]-[4.14.19]
Therefore E(V) = E(X) + 5E(Y) = 3, and var(V) = var(X) + 1 var(Y) =i by independence.
18, (a) We have that

OO OO XA
P(X <Y) = | P(X < y)we7#Y dy = | (ee dy =
0 0 w+A

(b) Clearly, for w > 0,

PU <u,W>w)=PU <u,W>w,X <Y)+PU <u,W>w,X>/Y).

Now

u

PU <u,W>w,X <Y)=P(X <u,Y > X+w) -| ne AX e“HOTW) ay
0
A o-nw -(+n)
=——e (l-e BM)
A+ Ee

and similarly

PU <u,W>w,X > Y) = ed — ety,
Ath

Hence, for0 <u <u+w <oo,
Xr
PU <u,W > w) =(1—e7 O44) (oe + oe) .
A+ A+ pb

an expression which factorizes into the product of a function of u with a function of w. Hence U and
W are independent.

19. U=X+Y,V =X have joint density function fy(u — v) fy(v), 0 < v <u. Hence

fu,vU, v) ___ fru v) fx(v)
fu) fo fru —y)fxo)dy’

fyju@ | 4) =

(a) We are given that fyjy(v | u) = u—! for0 <v <u; then

1 ue
feu —»)fx(v) =~ | fru —y) f(y) dy
u Jo

is a function of u alone, implying that

fru—v) fx) = fr@)fx() _ by setting v = 0
= fy(O)fx(u) _ by setting vy =u.

In particular fy(u) and fy (u) differ only by a multiplicative constant; they are both density functions,
implying that this constant is 1, and fy = fy. Substituting this throughout the above display, we
find that gx) = fx(x)/fx () satisfies g(0) = 1, g is continuous, and g(u — v)g(v) = g(u) for
0<v <u. From the hint, g(x) = e~** for x > 0 for some A > 0 (remember that g is integrable).

(b) Arguing similarly, we find that
fy — v) fx (0) = apg v Mu — vy [ fru — y) fx) dy

215

[4.14.20]-[4.14.22] Solutions Continuous random variables

for 0 < v < u and some constant c. Setting fy(v) = x(v)v2-1, fr) = n(y)y87!, we obtain
n(u — v)x(v) = h(u) for 0 < v < u, and for some function h. Arguing as before, we find that 7 and
X are proportional to negative exponential functions, so that X and Y have gamma distributions.

20. We are given that U is uniform on [0, 1], so thatO < X,Y < 5 almost surely. For 0 < € < i,

€=P(X+Y <€) <P(X <6, ¥ <6) =PW(X <0)’,
and similarly
e=P(X+Y¥>1-6€)<P(X>4-6¥>4-GH =P(X> 4-06),
implying that P(X < €) > ./e and P(X > 5 -€) > Je. Now
2 =P(5-€ <X4+¥<}+6e)>P(X> 5 —€,¥ <e)+P(X <e,¥>}-€)
= 2P(X > 4 —€)P(X <€) > Je).

Therefore all the above inequalities are in fact equalities, implying that P(X < €) = P(X > 5 -eE)=
Ve if0O<e< . Hence a contradiction:

g=P(X+Y <}) =P(X,Y < })-P(X,Y <3, X4+V¥ 24) < PX <5, ¥ < b= 3.
21. Evidently

P(X) S y1.---,X qm) S yn) = S°P (Xm, Sy,.--)Xaq <n, Xn, < +++ < Xm)
u
where the sum is over all permutations 7 = (71, %2,...,%n) of (1,2,...,n). By symmetry, each
term in the sum is equal, whence the sum equals
nl P(X, < y1,---,Xn S Yn, X1 < XQ < +++ < Xp).

The integral form is then immediate. The joint density function is, by its definition, the integrand.
22. {a) In the notation of Problem (4.14.21), the joint density function of X),..., X(n) is

y2

g2002.---.9n) = [ 80V15-+-+ Yn) dy1

—O

=nLoz,..., nF OIF OWDF03) ++: £On)

where F is the common distribution function of the X;. Similarly X(3), ..., Xm) have joint density

83(V3, -+-5 Yn) = an! L(y3,---5 In) F (3) £093) FOn)s

and by iteration, X (4), ..., Xm) have joint density
n! k-1
8k kr +++) Yn) = Eat POb WF OW) SK) ++ FOn)-
We now integrate over yn, Yn—1,---, Ye+1 in turn, arriving at
n! _ _
IxXa Qn) = &-Dia_b! Fon) ‘1 — F(Qy,)}" * fOK).

216

Problems Solutions [4.14.23]-[4.14.25]

(b) It is neater to argue directly. Fix x, and let J, be the indicator function of the event {X, < x}, and
let S= 1, +: In +---+ Jy. Then S is distributed as bin(n, F (x)), and

l

P(Xq) <x) =PS>KH =>) (7) F(x - F@))".
l=k

Differentiate to obtain, with F = F(x),

fxg @= 0 (7) f(x) {ira ~ FY @-pFU ryt}

=k (7) f@F Ra — Fy

by successive cancellation of the terms in the series.

23. Using the result of Problem (4.14.21), the joint density function is g(y) = n!L(y)T~" for
O<y; <T,1<i <n, where y = (y1, yo,..., Yn).

24, (a) We make use of Problems (4.14.22)-(4.14.23). The density function of Xqj is fx(*«) =
k(q)x*-1 (1 — x)"-* for 0 < x < 1, so that the density function of nX ty is

-—D--Mm— —k
kn 1)---@ EY) et (y 2)" _ 1 kr lex

1
nthe) = Fi nk n k-D!

as n —> og. The limit is the (1, &) density function.
(b) For an increasing sequence x(1), x(2), .--, Xn) in [0, 1], we define the sequence un = —n log x(n),
ug = —k log (xg) /*x@41)) for 1 < k <n. This mapping has inverse

n
xm =e", xa) = xg ete! = exp{ - itu,
i=k

with Jacobian J = (—1)"e~“1~“2—""-“" /n!, Applying the mapping to the sequence X (1), X(2), ---;
Xn) we obtain a family U), U2, ..., Un of random variables with joint density g(u1, u2,...,4¥n) =
e *1—42—""—4n for u; > 0,1 <i <n, yielding that the U; are independent and exponentially
distributed, with parameter 1. Finally log X%) = — Vyx imlU;.

(c) In the notation of part (b), Z, = exp(—U,) for 1 < k <n, a collection of independent variables.
Finally, U; is exponential with parameter 1, and therefore

P(Zy < 2) = P(Ug > —logz) =e8% =z, 90 <z <1.

25. (i) (X1, X2, X3) is uniformly distributed over the unit cube of R°, and the answer is therefore the
volume of that set of points (x1, x2, x3) of the cube which allow a triangle to be formed. A triangle
is impossible if x1 > x2 + x3, or x2 > x1 + x3, Or x3 > x; + 4X2. This defines three regions of
the cube which overlap only at the origin. Each of these regions is a tetrahedron; for example, the
region x3 > x, + x2 is an isosceles tetrahedron with vertices (0, 0, 0), (1, 0, 1), (0, 1, 1), (0, 0, 1),
with volume é Hence the required probability is 1 — 3 - = 5.

(ii) The rods of length x1, x2, ..., Xp fail to form a polygon if either xj > x2 +.x3-+--++2» or any of
the other n — 1 corresponding inequalities hold. We therefore require the volume of the n-dimensional
hypercube with n corners removed. The inequality x; > x2 +x3+---+ x, corresponds to the convex
hull of the points (0, 0,..., 0), (1,0,..., 0), (1, 1,0,...,0), 1,0, 1,0,...,0),...,(1,0,...,0, 1).

217
[4.14.26]-[4.14.27] Solutions Continuous random variables

Mapping x; t> 1 — x1, we see that this has the same volume V,, as has the convex hull of the origin
0 together with the n unit vectors ej, €2,...,€,. Clearly Vo = 5, and we claim that V, = 1/n!.
Suppose this holds for n < k, and consider the case n = k. Then

1
w= dx, Vy_1(O, x1e2,..., x1€)

where V;_1 (0, x1e2, ..., x ex) is the (k — 1)-dimensional volume of the convex hull of 0, xje2,...,
xje,. Now
k-1 xf)
V1 (0, x1e2,...,x1€%) = xp Ve = @—

so that

v= [ xy dx =2
Rh &-DI OO

The required probability is therefore 1 — n/(n!) = 1 — {(n -— 1}.
26. (i) The lengths of the pieces are U = min{X , X2}, V = |X, — X2|,W =1-U — V, and we
require that U < V + W, etc, as in the solution to Problem (4.14.25). In terms of the X; we require

either: =X) <$, |Xy—-X2l<4, 1-X. <4,

or: X2 <5, |X1-Xl< 4, 1-X1 <}f-.

Plot the corresponding region of IR2. One then sees that the area of the region is i, which is therefore
the probability in question.
(ii) The pieces may form a polygon if no piece is as long as the sum of the lengths of the others. Since

the total length is 1, this requires that each piece has length less than 5. Neglecting certain null events,
this fails to occur if and only if the disjoint union of events Ag U A, U---U Ay occurs, where

Ao = {no break in (0, sy}, Ax = {no break in (Xx, X% + 5]} forl<k<n;

remember that there is a permanent break at 1. Now P(Ag) = 3)", and fork > 1,

1
PAD = [ P(Ak | Xe=x)dx = [dy dx = (4)";

Hence P(Ag U Ay U---U An) = (n + 1)2~ whence the required probability is 1 — (+ 1)2™.

27. (a) The function g(t) = (t?/p) + (t~4/q), fort > 0, has a unique minimum at ¢ = 1, and hence
g(t) > g(1) = 1 fort > 0. Substitute ¢ = xl/dy~l/p where

eee A
{E|XP|}1/P’ {E|¥9|}1/4’

(we may as well suppose that P(XY = 0) ¥ 1) to find that

|X|? ly |? . [XY|
pE|XP| — gE|¥9| ~ {E|X?|}!/P(e|¥9|}!/4

Holder’s inequality follows by taking expectations.

218

Problems Solutions [4.14.28]-[4.14.31]
(b) We have, with Z = |X + Y|,

E(Z?) =E(Z- Z?-1) < E(X|Z?-!) + E(\y|Z?~})
< {E|X?|}1/P (e(Z?)}1/4 + (EY? |y/? (E(ZP)} 1/4

by Hélder’s inequality, where p~! + q—! = 1. Divide by {E(Z?)}!/@ to get the result.

28. Apply the Cauchy—Schwarz inequality to |Z [20-@) and |Z| p(b+a) where 0 < a < b, to obtain
{E|Z?|}2 < E|Z°-4| £|Z°+4|. Now take logarithms: 2g(b) < g(b — a) + g(b +a) for0 <a <b.
Also g(p) > g(0) = 1 as p | 0 (by dominated convergence). These two properties of g imply that
g is convex on intervals of the form [0, M) if it is finite on this interval. The reference to dominated
convergence may be avoided by using Holder instead of Cauchy—Schwarz.

By convexity, g(x)/x is non-decreasing in x, and therefore g(r)/r > g(s)/s if0 <s <r.

29. Assume that X, Y, Z are jointly continuously distributed with joint density function f. Then

f(@, y,2) dx.

E(X|Y¥=y,Z=o= dx =
(X|¥=y.Z=2) [sfarzely.2 ee [ae

Hence

E{B(X1Y,Z)|¥=y}= [BX =y,Z= ofall yz

= [[-222 fy,z0.2 dxdz
fy,zQ.z) fr)

f (&, y, z)
= LM dxdt =E(X | ¥ =).
[Pp ogew (f1¥=y)

Alternatively, use the general properties of conditional expectation as laid down in Section 4.6.

30. The first car to arrive in a car park of length x + 1 effectively divides it into two disjoint parks
of length Y and x — Y, where Y is the position of the car’s leftmost point. Now Y is uniform on
[0, x], and the formula follows by conditioning on Y. Laplace transforms are the key to exploring the
asymptotic behaviour of m(x)/x as x > oo.

31. (a) If the needle were of length d, the answer would be 2/7 as before. Think about the new needle
as being obtained from a needle of length d by dividing it into two parts, an ‘active’ part of length L,
and a ‘passive’ part of length d — L, and then counting only intersections involving the active part.
The chance of an ‘active intersection’ is now (2/7)(L/d) = 2L/(ad).

(b) As in part (a), the angle between the line and the needle is independent of the distance between
the line and the needle’s centre, each having the same distribution as before. The answer is therefore
unchanged.

(c) The following argument lacks a little rigour, which may be supplied as a consequence of the
statement that S has finite length. For € > 0, let x1, x2,..., x, be points on S, taken in order along
S, such that xo and x, are the endpoints of S, and |xj41 — x;| < € forO <i <n; |x — y| denotes
the Euclidean distance from x to y. Let J; be the straight line segment joining x; to xj+1, and let J;
be the indicator function of {J; 1A 4 @). If € is sufficiently small, the total number of intersections
between Jp U Jy U-++U J,_1 and S has mean

n-1 2 n-1
a) = > Ixiz1 — xl
=0 i=0

219

[4.14.32]-[4.14.34] Solutions Continuous random variables

by part (b). In the limit as « | 0, we have that 5°; E(/;) approaches the required mean, while

2L(S)
— 25 lxi+1 — x11 > Td

32. (i) Fix Cartesian axes within the gut in question. Taking one end of the needle as the origin,
the other end is uniformly distributed on the unit sphere of R°. With the X-ray plate parallel to the
(x, y)-plane, the projected length V of the needle satisfies V > v if and only if |Z| < /1—- v2, where
Z is the (random) z-coordinate of the ‘loose’ end of the needle. Hence, for 0 < uv < 1,

4/1] — 12
PV >v) =P(-VI1-w <Z< Vi-v) = VE ‘ “= V1 -2?,

since 4x 1/1 — v2 is the surface area of that part of the unit sphere satisfying |z| < \/1— v2 (use
Archimedes’s theorem of the circumscribing cylinder, or calculus). Therefore V has density function
fy(v) =0/V1—-v? forO <u <1.

Gi) Draw a picture, if you can. The lengths of the projections are determined by the angle © between
the plane of the cross and the X-ray plate, together with the angle W of rotation of the cross about an
axis normal to its arms. Assume that © and W are independent and uniform on [0, $m]. If the axis
system has been chosen with enough care, we find that the lengths A, B of the projections of the arms
are given by

A= \cos2 W + sin? & cos2 ©, B= \/ sin? W + cos? W cos ©,

1— A2
@ =cos~! V A2 + B2 -1, Ww =tan—! »/ ——..
cos + an 1-R2

Some slog is now required to calculate the Jacobian J of this mapping, and the answer will be
fa,p(a, b) = 4|J|2—2 for 0 < a,b < 1,a* +b? > 1.

33. The order statistics of the X; have joint density function

n
h(x], x2, .-+)Xn) =A"n! exo(- ox)
i=l

with inverse

on the set J of increasing sequences of positive reals. Define the one-one mapping from J onto
(0, 00)” by
y1 = 7x1, yr =(n+1—r)@, —x--1) forl<r<n,

with inverse xy = )~y.-) yk/(n —k + 1) forr = 1. The Jacobian is (n 1)—1, whence the joint density
function of Y,, ¥2,..., Yn is

1 * ‘
— in! exp (- 2H w) =a" ex(- » vx).
ix =

34. Recall Problem (4.14.4). First, Z; = F(X;), 1 <i <n, is a sequence of independent variables
with the uniform distribution on [0, 1]. Secondly, a variable U has the latter distribution if and only
if — log U has the exponential distribution with parameter 1.

220

Problems Solutions [4.14.35]-[4.14.37]

It follows that L; = — log F(X;), 1 <i <n, isasequence of independent variables with the expo-
nential distribution. The order statistics L(y), ..., L(,) are in order — log F(X(n)), ..., — log F(X),
since the function — log F(-) is non-increasing. Applying the result of Problem (4.14.33), Ey =
—n log F(X my) and

Erp =—-(a+1- r) {log F(X(n41-r) — log F(X(n42-r))}> l<r<n,

are independent with the exponential distribution. Therefore exp(—£,), 1 <r <n, are independent
with the uniform distribution.

35. One may be said to be in state j if the first 7 — 1 prizes have been rejected and the jth prize has
just been viewed. There are two possible decisions at this stage: accept the jth prize if it is the best
so far (there is no point in accepting it if it is not), or reject it and continue. The mean return of the
first decision equals the probability j/n that the jth prize is the best so far, and the mean return of the
second is the maximal probability f(j) that one may obtain the best prize having rejected the first /.
Thus the maximal mean return V(j) in state j satisfies

VQ) = max{j/n, f(/)).

Now j/n increases with j, and f(j) decreases with j (since a possible stategy is to reject the (j + 1)th
prize also). Therefore there exists J such that j/n < f(j) if and only if 7 < J. This confirms the
optimal stategy as having the following form: reject the first J prizes out of hand, and accept the
subsequent prize which is the best of those viewed so far. If there is no such prize, we pick the last
prize presented.

Let I; be the probability of achieving the best prize by following the above strategy. Let Az be
the event that you pick the kth prize, and B the event that the prize picked is the best. Then,

n 3 P(B | Ax)P(Ay) 3 (=) ( J 7) J > }
l= K)P(Ag) = —}{—.--}=-— —.,
k=J+1 pert 77 \EO1 MAF ko!

and we choose the integer J which maximizes this expresion.

When n is large, we have the asymptotic relation Ty ~ (J/n)log(n/J). The maximum of the
function hy(x) = (x/n) log(n/x) occurs at x = n/e, and we deduce that J ~ n/e. [A version of this
problem was posed by Cayley in 1875. Our solution is due to Lindley (1961).]

36. The joint density function of (X, Y, Z) is

fQ,y,2) = exp{—$(r? — 2ax — Quy — 2vz+4* +? +-v?)}

1
(2x07)3/2

where r2 = x* + y* + z*. The conditional density of X, Y, Z given R = r is therefore proportional
to exp{Ax + zy + vz}. Now choosing spherical polar coordinates with axis in the direction (A, 2, v),

we obtain a density function proportional to exp(acos@) sin@, where a = r\/A24+ p24 v2. The
constant is chosen in such a way that the total probability is unity.

37. (a) ¢'(x) = —x¢(x), so Hi (x) = x. Differentiate the equation for Hy, to obtain H,41 (x) =
xHpy(x) — H/(x), and use induction to deduce that Hy, is a polynomial of degree n as required.
Integrating by parts gives, when m <n,

OO oO
/ Hn (*) Hn (x)(x) dx = (-1)" / Hn (x) (x) dx
—cO —0O
OO
= (1! / HE (x)6°-Y (x) dx
—-cO
fo)
=.= (-yn / HE (x60 (x) dex,
—-cO

221

[4.14.38]-[4.14.39] Solutions Continuous random variables

and the claim follows by the fact that 42”) (x) = m!.
(b) By Taylor’s theorem and the first part,

_4\n
Toe  Hn() = Lo $x) =x),

whence
[o.<)

rt”
y ain) = = exp{- 5 (x —1)*+ 5x7} = exp(xt — 31”),
n=0”

38. The polynomials of Problem (4.14.37) are orthogonal, and there are unique expansions (subject
to mild conditions) of the form u(x) = 37729 ar Hy(x) and v(x) = 7°25 by Hy (x). Without loss of
generality, we may assume that E(U) = E(V) = 0, whence, by Problem (4.14.37), ag = bo = 0.
By (4.14.37a) again,

oe) [o@)
var(U) = E(u(X)*) = Soa?r!, var(V) = > bert.

r=1

By (4.14.37b),

fo ¢) mo n
e(> Fm(X)s > Fn (¥)t )= E(exp(sX — 4s? +1¥ — 427}) = eS?

! !
m=O mt n=0 nt

By considering the coefficient of st”,

n ‘ —_
p'n! ifm=n,

E(Hm(X)An(¥)) = { 0 ifm £n

and so

[o.<) [o@) [o.<)
cov(U, V) = e(> am Hm (X) > bn n(¥)) = 2 anbapint

m=1 n=1

lallxnei anne" int] Sy |anbn|n!
Vren nt De, ban! Vue agn! Sona an!

where we have used the Cauchy—Schwarz inequality at the last stage.

lo, VY = < lel,

39. (a) Let ¥, = Xq@) — X(r—1) with the convention that X(@) = 0 and X(41) = 1. By Problem
(4.14.21) and a change of variables, we may see that Y,, Y2,..., Yn41 have the distribution of a point
chosen uniformly at random in the simplex of non-negative vectors y = (yj, y2,--.. ¥n41) With sum
1. [This may also be derived using a Poisson process representation and Theorem (6.12.7).] Conse-
quently, the Y; are identically distributed, and their joint distribution is invariant under permutations

of the indices of the Y;. Now pias Y, = 1 and, by taking expectations, (n + 1)E(%1) = 1, whence
E(X()) = rE(%1) = rin +1).
(b) We have that

2

1
2) 2 _ y\n-l _—
Bp) = [ x*n(1 — x) dx = @aba+2’

ntl 2
l= B|(> rr) = (n+ DEY?) +.n(n + DE( ¥2),
r=1

222
Problems Solutions [4.14.40]-[4.14.42]

implying that ;

BOND e+ D

and also
. r(is+)

= 2 _ -
E(X vy X(s)) = rEQy) +r — DEM Y2) = @+tbm+y

The required covariance follows.

40. (a) By paragraph (4.4.6), X 2 is r(4 ; 5) and ¥? + Z? is ri , 1). Now use the results of Exercise
(4.7.14).

(b) Since the distribution of X*/R? is independent of the value of R? = X* + Y? + Z?, it is valid
also if the three points are picked independently and uniformly within the sphere.

41, (a) Immediate, because the N(0, 1) distribution is symmetric.
(b) We have that

io, @) oO
/ 26 (x) P(Ax) dx = / {P(x) P(x) + O(@)[1 — &(-Ax)]} dx
oO —OO

OO OO
= / G(x) dx + / $(x)[@ (x) — ©(-Ax)]dx = 1,
00 00

because the second integrand is odd. Using the symmetry of ¢, the density function of [Y| is

b(x) + 6%) {OAx) — &(—Ax)} + b(—x) + G(—x){P(-Ax) — O(Ax)} = 2G().

(c) Finally, make the change of variables W = |Y|, Z = (X +A|Y[))/V1+ 22, with inverse |y| = w,

x =zV1+A2 —Aw, and Jacobian 1+ 42. Then

fw,z(w, 2) = VI+A2 fx jy) (2V 1 +A? — Aw, w)
= V1+A2-¢(zV1+4+A2 — Aw) - 26(w), w>0, xeR.

The result follows by integrating over w and using the fact that

_ $@)OQz)

OO
2_} d —
[ o(z 1+A w)o(w) w 32

42. The required probability equals
P({X3 — 4X1 + X2)}? + (85 — $071 + Yay}? = H(K1 — Xo)? + 4% - F)”)
= P(U? + UZ < VP +. VP)

where U1, U2 are N(0, 3), Vi, V2 are N(O, 4), and U,, U2, V;, V2 are independent. The answer is
therefore

p =P(3(N? + NZ) < 4(N3 +.NZ)) where the N; are independent (0, 1)

= P(K] < 4K) where the K; are independent chi-squared x(2)
Ky 1 1] 1]
_ <-/=P(B<5)=
(ate <4) Bsa=4
[4.14.43]-[4.14.48] Solutions Continuous random variables

where we have used the result of Exercise (4.7.14), and B is a beta-distributed random variable with
parameters 1, 1.

43. The argument of Problem (4.14.42) leads to the expression

P(U? + US +. US < VP +. V3 +. V2) = P(K, < 4K2) where the K; are x7(3)
1 3

(BS 4) 3 4x’

where B is beta-distributed with parameters 3, 3.

44. (a) Simply expand thus: E[(X — »)?] = E[X? — 3X2 + 3Xp2 — 3] where u = E(X).

(b) var(Sy) = no” and E[(S, — nw)?] = nE[(X, — 14)°] plus terms which equal zero because
E(X; — ) =0.

(c) If Y is Bernoulli with parameter p, then skw(Y) = (1 — 2p)/./pq, and the claim follows by (b).
(d) my =A, m2 =A4+A2, m3 =A? 4+.3d2 +A, and the claim follows by (a).

(e) Since AX is (1, t), we may as well assume that A = 1. Itis immediate that E(X”) = '(t4+-n)/T),
whence

t¢+D¢4+2)—-3t-t¢+1)4+20 2
sway = EEDERD 3 EE DEI 2

45. We find as above that kur(X) = (m4 — 4m3m, + 6mym{ — 3m})/o4 where mg = E(X*).
(a) m4 = 30% for the N(0, 07) distribution, whence kur(X) = 304 /o4,

(b) my = r!/A", and the result follows.

(c) In this case, mg = 44 + 643 + 702 +A, m3 =AP7 4302 +A, mg = AZ +A, and my =A.
(d) (var Sy)? = n2o4 and E[(Sp — nm 1)*] = nE[(X1 — mj)*] + 3n(n — 104.

46. We have as n — oc that
e-*\" —x
P(X (n) <x + logn) = {1 — e 4108)" — ¢ - —) >e ee,”
n

By the lack-of-memory property,

1 1 1
E(X() =>, E(X@) => +> sees

whence, by Lemma (4.3.4),

ad — 1 1
—eé * ‘ = 1 —_— ene _— =
[ {l-e }dx = jim E(X(n) — logn) = im ( + i 4+---4+1 logn) y.

47. By the argument presented in Section 4.11, conditional on acceptance, X has density function
fs. You might use this method when fs is itself particularly tiresome or expensive to calculate. If
a(x) and b(x) are easy to calculate and are close to fs, much computational effort may be saved.

48. M = max{Uj, U2,..., Uy} satisfies

Y e-1
P(M <1) =E(t?) = ——.
e-1

224

Problems Solutions [4.14.49]-[4.14.51]

Thus,
P(Z > z) =P(X > [z}+2)+ P(X = lz] 41, ¥ < lz] +1-2z)
—[zJ—-2 +1-
= ee eee ert EE ah owe,
l-—e7 e-1

49. (a) Y has density function e~ for y > 0, and X has density function fy (x) = ae~® for x > 0.
Now Y > 5(X — a)” if and only if

ze 2 2 _ly2

~ax € / [2 _

ae eX fF cf =e 2%,
a Va 1

1,2
which is to say that aVfy(X) < f(X), where a = a 1e2% /2/n. Recalling the argument of
Example (4.11.5), we conclude that, conditional on this event occurring, X has density function /.

(b) The number of rejections is geometrically distributed with mean a—', so the optimal value of a is
that which minimizes ae 3” /n/2, that is, ¢ = 1.
(c) Setting
z= { +X with probability 4 5
—-X with probability 4 5
we obtain a random variable Z with the N(0, 1) distribution.

1
50. (a) E(X) -[ Vl-uedu=n/4.

4 conditional on Y > 5(X —a)?,

1
(b) E(Y) = = [ 2” sin 6 do = 2/n.

51. You are asked to calculate the mean distance of a randomly chosen pebble from the nearest
collection point. Running through the cases, where we suppose the circle has radius a and we write
P for the position of the pebble,

25
)) E[OP| = _ aan r 2 ar do = 24
ma
2 ta 2a cos @ 32a
(ii) E|AP| = —5 "| r? dr do = ——
ma on
qn asec 2a cos
(iii) E(|AP| A |BP|) = 41) I r? dr dO ah [ r? dr ao]
47
4a
=— log(1 — x 1.13.
ue (16 tat logit + V5) = 2 > x
(iv)

6 i x 50 2a cos
E({AP| A [BP] A |CP]) = "| f [ r? dr dO + |, [ r? dr ao}
maz | Jo 0 zx JO
where x =a sin(47) cosec( $7 —6)
2a 5m 1 Xu 5m
-={/ -3/3 cosec? (F +0) ao+ 8 cos? 6 dé
x\Jo 8 3 4Q

2a { 16 33 | 3 2a
= Tse > ae 67.
at ait 6 we | 3X0
[4.14,52]-[4.14.55] Solutions Continuous random variables

52. By Problem (4.14.4), the displacement of R relative to P is the sum of two independent Cauchy
random variables. By Exercise (4.8.2), this sum has also a Cauchy distribution, and inverting the
transformation shows that © is uniformly distributed.

53. We may assume without loss of generality that R has length 1. Note that A occurs if and only if
the sum of any two parts exceeds the length of the third part.

(a) If the breaks are at X, Y, where 0 < X < Y < 1, then A occurs if and only if 2Y > 1, and
2(¥ — X) < land 2X < 1. These inequalities are satisfied with probability i.

(b) The length X of the shorter piece has density function fy(x) = 2 for0 <x < 5. The other pieces
are of length (1 — X)Y and (1 — X)(1 — Y), where Y is uniform on (0, 1). The event A occurs if and
only if2Y <2XY+1landX+Y-XY> 5 and this has probability

1

2 1 1 —2x
2 | {ma —x) 2d a dx = log(4/e).

(c) The three lengths are X, 31 — x), x1 — X), where X is uniform on (0, 1). The event A occurs
if and only if X < 5.

(d) This triangle is obtuse if and only if

which is to say that X > /2 — 1. Hence,

P(V2-1<X <}4) ~ 3 2wWa.

P(obtuse | A) =
( 1A) P(X < 4)

54. The shorter piece has density function fy (x) = 2 forO <x < 5. Hence,

PR <r)=P x < 2r
r= r =
~ 1-x — 14+r’

with density function fr(r) = 2/1 —- r)* for 0 <r < 1. Therefore,

1 lye
E(R) =| P(R > rar = | i ” dr =2Iog2- 1,
0 0

+r

1 lord -
BR) = [ 2rP(R > rar = [ ar =") ay = 3 — 41082,
0 0 l+r

and var(R) = 2 — (2log2)?.

55. With an obvious notation,
E(R?) = E[(X1 — X)7] + E((% — ¥2)*] = 4E(X2) — 4{E(X1)}* = 4- $a? — 4(4a)? = ba?.

By a natural re-scaling, we may assume that a = 1. Now, X, — X> and Y; — Y> have the
same triangular density symmetric on (—1, 1), whence (X; — Xp)" and (Y; — Yo)? have distribution

226

Problems Solutions [4.14.56]-[4.14.59]

1
function F(z) = 2,/z — z and density function fz(z) = z 2 — 1, for 0 <z < 1. Therefore R has

the density f given by
[L(G 1) ( ! 1) 4 if0<r<1
f~)= 1

1 1
—-l ——— - 1} dz ifl<r<2.
[, (= ) (z —Z )
The claim follows since

711 dz=2 sin [2 sin“! ,/* forr0<a<b<1
— dz = -- - <a<o<l.
a Vivr—Zz r r

56. We use an argument similar to that used for Buffon’s needle. Dropping the paper at random
amounts to dropping the lattice at random on the paper. The mean number of points of the lattice in a
small element of area dA is dA. By the additivity of expectations, the mean number of points on the
paper is A. There must therefore exist a position for the paper in which it covers at least [A] points.

57. Consider a small element of surface dS. Positioning the rock at random amounts to shining light
at this element from a randomly chosen direction. On averaging over all possible directions, we see
that the mean area of the shadow cast by dS is proportional to the area of dS. We now integrate over
the surface of the rock, and use the additivity of expectation, to deduce that the area A of the random
shadow satisfies E(A) = cS for some constant c which is independent of the shape of the rock. By
considering the special case of the sphere, we find c = i It follows that at least one orientation of

the rock gives a shadow of area at least iS.
58. (a) We have from Problem (4.14.11b) that Y¥, = X;,/(X1 +---+ X,) is independent of X; +

----+X,, and therefore of the variables X;41, X-42,°-+ ,Xk41,X1 +--+: + Xx41. Therefore Y, is
independent of {¥;45 : s => 1}, and the claim follows.
(b) Let S = X, +--+ + Xg41. The inverse transformation x1 = z15, x2 = 7225, ..., Xk = ZS,
Xe4] = S — ZS — 228 — +++ — ZS has Jacobian

Ss 0 O --- Z1

0 Ss O -:- 22

0 0 O .:-:: Zk

The joint density function of X;, X2,..., Xx, S is therefore (with o = yet Br),

ll Br (z,§)brVe—Rers MPe+I {5(1 — zy — +++ — zp) yPeti be As 21 — ze)
r=1 Pr) P(Br+1)
k
r=]

where /f is a function of the given variables. The result follows by integrating over s.

59. Let C = (crs) be an orthogonal » x n matrix with c,; = 1/./n for 1 <i <n. Let Y;, =
an XjsCrs, and note that the vectors Y, = (Yj;, Yo,,.-., Yur), 1 <r <n, are multivariate
normal. Clearly EY;, = 0, and

E(Yir Ys) = So crtesuE(Xis Xju) = So crtcsudturij = So crtestvij = drs Vij,

fu tu t

227

[4.14.60]-[4.14.60] Solutions Continuous random variables

where 6;, is the Kronecker delta, since C is orthogonal. It follows that the set of vectors Y, has the
same joint distribution as the set of X;. Since C is orthogonal, X;, = ~?_, csr Yis, and therefore

1 1
Sig = Vo esreirYis Yin - > Xir Y) Xin = D bot Yis Vin — Te | Kine D | Xin
r r $,t r r

r,5,t
n—-1
s s=l

This has the same distribution as 7;; because the Y, and the X; are identically distributed.

60. We sketch this. Let E[PQR| = m(a), and use Crofton’s method. A point randomly dropped in
S(a + da) lies in S(a) with probability

2
a 2da
=| - — da).
(*;) a + o(da)

Hence
dm _6m_| 6mp

daa a’

where m,(a) is the conditional mean of |PQR| given that P is constrained to lie on the boundary of
S(a). Let b(x) be the conditional mean of |PQR| given that P lies a distance x down one vertical
edge.

x Ri qT

Ty

Ro

By conditioning on whether Q and R lie above or beneath P we find, in an obvious notation, that

x\2 a—x\? 2x(a — x)
bx) = (=) may +( ; ) my + > ~mr Rp:

By Exercise (4.13.6) (see also Exercise (4.13.7), mr,,R, = 4(4a)(5a) = }a?. In order to find
m r,, we condition on whether Q and R lie in the triangles 7; or 72, and use an obvious notation.

Recalling Example (4.13.6), we have that m7, = mp, = H . jax. Next, arguing as we did in
that example,
M7, ,T) = 5 : g {ax - 4ax - dax - fax}.

Hence, by conditional expectation,

_1 4 1 1 4 1 1 4 3
MR = 4°97 FOX +4 AX +7 O° Bax

We replace x by a — x to find mz,, whence in total

a) 108 108 ae 8 108° 108 108

b(x) = (2) Wax (‘ =#)’ I3a(a—x)  2x(a—x) a? 13 9 1dax | 12x?

228

Problems Solutions [4.14.61]}-[4.14.63]

Since the height of P is uniformly distributed on [0, a], we have that

11a?
108 °

1 a
mp(a) = -{f b(x) dx =

We substitute this into the differential equation to obtain the solution m(a) = Tha.

Turning to the last part, by making an affine transformation, we may without loss of generality
take the parallelogram to be a square. The points form a convex quadrilateral when no point lies

inside the triangle formed by the other three, and the required probability is therefore 1 — 4m(a)/a? =
1-7 = %.

61. Choose four points P, Q, R, S uniformly at random inside C, and let 7 be the event that their
convex hull is a triangle. By considering which of the four points lies in the interior of the convex
hull of the other three, we see that P(T) = 4P(S € PQR) = 4E|PQR|/|C|. Having chosen P, Q, R,
the four points form a triangle if and only if S lies in either the triangle PQR or the shaded region A.
Thus, P(7) = {|A| + E|PQR]}/|C|, and the claim follows on solving for P(7).

62. Since X has zero means and covariance matrix I, we have that E(Z) = ye + ECX)L = ym, and the
covariance matrix of Z is E(L’X’XL) = L/IL = V.
63. Let D = (dj) = AB — C. The claim is trivial if D = 0, and so we assume the converse. Choose
i, k such that dj, ¢ 0, and write yj; = Vi=l djjxj = S + digxy. Now P(yj = 0) = E(P(xx =
—S/dix | S)). For any given S, there is probability at least 5 that x, 4 —S/dj,, and the second claim
follows.

Let x1, X2,..-,Xm be independent random vectors with the given distribution. If D 4 0, the

probability that Dx; = 0 for 1 < 5s < mis at most 4)”, which may be made as small as required by
choosing m sufficiently large.

229

5

Generating functions and their applications

5.1 Solutions. Generating functions

1. (a) I€|s| <a — p)“},

CO -1 n
Gs) = sane )ra-nn= {2}

m=0

Therefore the mean is G’(1) = n(1 — p)/p. The variance is G’(1)+G’(1) —G’(1)? = n(1 — p)/p?.
(b) If |s| < 1,
a 1 1 l-s
G(s) = mp. J = —— ji —s).
(y= Sos (- wat) 1+( - ) og(1 — 5)
m=1
Therefore G’(1) = 00, and there exist no moments of order 1 or greater.

(c) If p < |s| < p7},

oo
_ m({1—P\ im _1-P SP Pls
G(s) = > 5 (<2) = ti p+ PS}

m=—CO

The mean is G’(1) = 0, and the variance is G’(1) = 2p(1 — p)~?.

2. (i) Either hack it out, or use indicator functions I thus:

oo oo X-1 x
1- 1- G(s)
T(s) = D> s"P(X > n) =2(Ss"e<n) =2(5") =x ( ras ) ~  I=s *

n=0

(ii) It follows that

G's)

= lim = G'(1) = E(X)

T(1) =lim {° — eo}
stl st]

1-s

by L’H6pital’s rule. Also,

"ay 4s —(1~—s)G’(s) +1- G(s)
= tg

= 46"(1) = 4 {var(X) — G'(1) + G’(1)?}

230

Generating functions Solutions [5.1.3]-[5.1.5]

whence the claim is immediate.

3. (i) We have that Gy y(s,t) = E(s*t¥), whence Gyx,y(, 1) = Gx(s) andGy y(1, t) = Gy(t).
(ii) If E|XY| < oo then

B(XY) = EB (x¥s%"Us?1) ae (s, t)
~ s=t=1 dsar ** si=l
4. We write G(s, t) for the joint generating function.
0° j . . .
(a) G(s,t) = S> sik — a) (6 — aad peI-}
j=0k=0
j _ _ _ j+1
_ “¥ (4) a “ “1 ae if Blt| <1
_ 0180) | 1 set \ es) <1
d= poe \i-@s/B) T-axf “ph!
(1 — a)(B — @)

~ (1 —ast)(B — as)

(the condition a|st| < 1 is implied by the other two conditions on s and t). The marginal generating
functions are
(1 — a)(B — @)

l-a@

and the covariance is easily calculated by the conclusion of Exercise (5.1.3) as a(1 — a)~2,
(b) Arguing similarly, we obtain G(s, t) = (e — 1)/{e(1 — tes —2y) if |tle* —2 < 1, with marginals

l-el 1—e7!
Pra, i G 1,t = TTT:
1 — es-2 (1,7) 1—te-!

Gs, 1) =
and covariance e(e — 172.
(c) Once again,

log{1 —tp(1 — p+sp)}

OOD = log(1 — p)

if |tp(. — p+sp)| <1.

The marginal generating functions are

log{1 — p+ p*(1—s)} log(1 — tp)
, Gd,t)=——,
log(1 — p) (8 log(1 — p)

G(s, 1) =

and the covariance is 9
__p*{p +log( — p)}

(1 — p)*{log(1 — p)}?

5. (i) We have that

E(x" y") = Sixty mk (7) pk — p)"* = (px +qy)"
t=O

231

[5.1.6]-[5.2.1] Solutions Generating functions and their applications

where p+q =1.

(ii) More generally, if each toss results in one of ¢ possible outcomes, the ith of which has probability
pi, then the corresponding quantity is a function of ¢ variables, x,,x2,...,x,, and is found to be
(pix, + poxe +-++ + prxt)”.

6. We have that

1— stl

’

1
B6%) = (es* |) = ft tus — Dy)" du =
0 n+1 l-s
the probability generating function of the uniform distribution. See also Exercise (4.6.5).
7. We have that

Gy y,z%, y,z7) =G,y,z,D = (xyz txey+yz+ezxtx+y4+z4+1)
= 3x + D504 D564) =GCx)Gy(Gz@,

whence X, Y, Z are independent. The same conclusion holds for any other set of exactly three random
variables. However, G(x, y, z, w) # Gy(x)Gy(y)Gz(z)Gw(w).

8. (a) We have by differentiating that E(X 2n) — 0, whence P(X = 0) = 1. This is not a moment
generating function.

(b) This is a moment generating function if and only if 5°, p- = 1, in which case it is that of a random
variable X with P(X = a;) = pr.

9. The coefficients of s” in both combinations of G;, G2 are non-negative and sum to 1. They are
therefore probability generating functions, as is G(as)/G(q) for the same reasons.

5.2 Solutions. Some applications
1. Let G(s) = E(s*) and Gs(s) = Lj=0 si S;. By the result of Exercise (5.1.2),

s(1-—G(s)) _ 1-sG)
l-s l-s °

[o@) [o.<)
T(s)= Do s™P(X =m) =14+5 > >s*P(X > WH =14

Now,

. m Xx ~ m x x
Gs(s) = So s"E =E{ Sos = E{(1+s)*} =G(1 4s)
m=0 m m=0 m

T(s)-T@) _ Gs(s — 1) ~ Gs(0)
Ss s—1
where we have used the fact that T(0) = Gs (0) = 1. Therefore

n n
Sos P(x > i) = Sos - 17155.
i=l j=l

so that

Equating coefficients of s‘—!, we obtain as required that
n j-1 .
P(X >i= S; -1)/7, l<i<n.
(X > i) (/71) ) Sin

232
Some applications Solutions [5.2.2]-[5.2.4]

Similarly,
Gs) — Gs) _ Td +s)-—T()

Ss l+s

whence the second formula follows.

2. Let A; be the event that the ith person is chosen by nobody, and let X be the number of events
Aj, Ag,..., An which occur. Clearly

n—j\i (n—-j-1\"7
P(A; M Aig N=. Ay) = ( t) ( J )

n-1 n-1

if i, #i2 #--- # i;, since this event requires each of ij, ..., i; to choose from a set of n — j people,
and each of the others to choose from a set of size n — j — 1. Using Waring’s Theorem (Problem
(1.8.13) or equation (5.2.14)),

P(X =k = yew \ (2) 5;

Using the result of Exercise (5.2.1),

n .
; -1
P(X > k) = (-1v-* (; Z :) si l<k<n,
a

where

while P(X > 0) = 1.
3. (a)

E(x*t¥) = E{EQ*tY [Y)}= E{x%e?@-D} = E{(xe*1)¥} = exp{y(xe*} —D}.

(b) The probability generating function of X is

GG) = 3 {s(1— p)}* _ log{1 — s(1— p)}
~ £y klog(l/p) logp

Using the ‘compounding theorem’ (5.1.25),

Pp )" log p

_ = e(G(s)-1) _
Gy(s) = Gy(G(s)) =e = (; sp

4, Clearly,

1 ly 1 x 1 h 1-—q"t!

where g = | — p. In the limit,

1 \_ 1-(-A/nyrt} 1-e
E(ix)- hath 7)

233

[5.2.5]-[5.3.1] Solutions Generating functions and their applications

the corresponding moment of the Poisson distribution with parameter 2.

5. Conditioning on the outcome of the first toss, we obtain hy = ghy,—1 + p( — hy_}) forn = 1,
where g = 1 — p and hg = 1. Multiply throughout by s” and sum to find that H(s) = "729 s"hn
satisfies H(s) — 1 = (q — p)sH(s) + ps/(1 —s), and so

1-—@qs { 1 1 \
H = = .
= G-50-@-ps} 2 ios TT-@=ps

6. Byconsidering the event that HTH does not appear in n tosses and then appears in the next three,
we find that P(X > n)p*q =P(X =n+)pq+P(X =n +3). We multiply by s"+3 and sum over

n to obtain x
1-E(s
Pas? = pqs E(s*) + E(s*),
which may be solved as required. Let Z be the time at which THT first appears, so Y = min{X, Z}.
By a similar argument,

P(Y > n)p’g =P(X =Y¥ =n+ 1)pq + P(X =¥ =n+3)+P(Z=V =n+2)p,
P(Y > n)q?p =P(Z =Y =n4+1)pqt+P(Z=Y =n+3) 4+ P(X =¥ =n42)q.

We multipy by s"+1 sum over n, and use the fact that P(Y =n) = P(X = Y =n) +P(Z =Y =n).

7. Suppose there are n + 1 matching letter/envelope pairs, numbered accordingly. Imagine the
envelopes lined up in order, and the letters dropped at random onto these envelopes. Assume that
exactly j + 1 letters land on their correct envelopes. The removal of any one of these j + 1 letters,
together with the corresponding envelope, results after re-ordering in a sequence of length n in which
exactly j letters are correctly placed. It is not difficult to see that, for each resulting sequence of length
n, there are exactly j + 1 originating sequences of length n + 1. The first result follows. We multiply
by s/ and sum over j to obtain the second. It is evident that Gj(s) = s. Either use induction, or
integrate repeatedly, to find that G,(s) = 079 (s — 1)" /rl.

8. We have for |s| < 4 + 1 that

xX xX A(s—1) Lad a, s ‘
BOs") = IEG" | A) = Ele = gap aL (aa)

9. Since the waiting times for new objects are geometric and independent,

E(s?) =s (=) (=) (a2):

Using partial fractions, the coefficient of s* is 3, {5(4)*-4 — 4(5)*-4 + Gy}, for k > 4.

5.3 Solutions. Random walk
1. Let Ax be the event that the walk ever reaches the point kK. Then Ay > Aj41 if k > 0, so that
r-1
P(M > r) = P(Ar) = P(Ao) | [ P(Agsi | Ag) = (p/9)", r= 0,
k=0

234

Random walk Solutions [5.3.2]-[5.3.4]

since P(Ag41 | Ay) = P(A | Ao) = p/g for k = 0 by Corollary (5.3.6).
2. (a) We have by Theorem (5.3.1c) that

oo oo
So sk 2k fo (2k) = 5 Fo(s) = = s*Po(s) = S_ s”* po (2k — 2),

<2
k=l V1—s? k=l

and the claim follows by equating the coefficients of gk.
(b) It is the case that a, = P(S1S2--- Son % 0) satisfies

lore)
m= >> folk,
k=2n42
k even

with the convention that a = 1. We have used the fact that ultimate return to 0 occurs with probability
1. This sequence has generating function given by

1

[o.@) [o.@) oe)
ys" YS f= YS fork) SS s™
k=2 n=0

n=0 = k=2n42 =
k even k even

_ 1- Fos) _ 1
~ las? y= 52

by Theorem (5.3.1c)

oo
= Po(s) = > 5?" P(S2n = 0).

n=0

Now equate the coefficients of s2”. (Alternatively, use Exercise (5.1.2) to obtain the generating
function of the a, directly.)

3. Draw a diagram of the square with the letters ABCD in clockwise order. Clearly paa(m) = 0 if
m is odd. The walk is at A after 2n steps if and only if the numbers of leftward and rightward steps are
equal and the numbers of upward and downward steps are equal. The number of ways of choosing

2k horizontal steps out of 2n is (Gn). Hence

2k

Paa(2n) = >> (72) ork pn—2k — Sia + By" + (a — By" } = 3 {1+(@-p)"}
=0

with generating function

Gals) = So Qn) =5{ 14 : \
ANTS LoS PAKS DT Tse — BPS

Writing Fa(s) for the probability generating function of the time T of first return, we use the
argument which leads to Theorem (5.3.1a) to find that Ga(s) = 1+ Fa(s)Ga(s), and therefore
Fa(s) =1~Ga(sy"}.

4. Write (Xn, Yn) for the position of the particle at time n. It is an elementary calculation to show
that the relations Un, = Xn + Yn, Vn = Xn — Yn define independent simple symmetric random walks
U and V. Now T = min{n : Un, = m)}, and therefore Gr(s) = {s—!(1 — /1—s2)}” for |s| <1
by Theorem (5.3.5).

235
[5.3.5]-[5.3.6] Solutions Generating functions and their applications

Now X — Y = Vy, so that

-1\f
gx) tee" 1m) =| ($43 ) | arene)

where we have used the independence of U and V. This converges if and only if |4(s + s7ly <1,
which is to say that s = +1. Note that G(s) converges in a non-trivial region of the complex plane.

5. Let T be the time of the first return of the walk S to its starting point 0. During the time-interval
(0, T), the walk is equally likely to be to the left or to the right of 0, and therefore

TR+L’ ifT <2n,
Lon = .
2nR if T > 2n,

where R is Bernoulli with parameter 1 L’ has the distribution of 2,7, and R and L’ are independent.
It follows that G,,(s) = E(s/2") satisfies

n

Gon(s) = 5-3. +87*)Gon—ae(s) FQ) + D5 + 5") F2K)
k=1 k>n

where f(2k) = P(T = 2k). (Remember that Lo, and T are even numbers.) Let H(s,t) =
Sno 12"G>,,(s). Multiply through the above equation by t2” and sum over 2, to find that

H(s,t) = 4H(s,t) {F(t) + F(st)} + 5 (J + J(st)}
where F(x) = S°P2.p x?* (2k) and
< 1
JQ) = pap» [QW = Fs <1.

by the calculation in the solution to Exercise (5.3.2). Using the fact that F(x) = 1 — /1—x2, we
deduce that H(s, t) = 1/./(1 — #2)(1 — s2t2). The coefficient of 212" is

-1 -1
P(Lon = 2k) = (, 4) (“1 ( peo
_ (2k) (2n-2k\ (1\™ ere,
=|, )]\ pk (5) = P(S2x = O)P(S2n—2k = 9).

6. We show that all three terms have the same generating function, using various results established
for simple symmetric random walk. First, in the usual notation,

252

ie.)
—0)52? — —

m=0

Secondly, by Exercise (5.3.2a, b),

E(T A 2m) = 2mP(T > 2m) + S~ 2kfo(2k) = 2mP(Sam = 0) + S~ P(Srk-2 = 0).
k=1 k=1

236

Random walk Solutions [5.3.7]-[5.3.8]

Hence,

2 2
5° Po(s) bey 2s
p—52 750) = Gap

CO
S- s°E(T A 2m) =
m=0

Finally, using the hitting time theorem (3.10.14), (5.3.7), and some algebra at the last stage,

CO o@) m CO m
5 57" 2E|Som| = 4 > 5°" > 2kP(Sam = 2k) = 45> 5°" S> 2m fog (2m)

m=0 m=0 k=1 m=0 k=1
dQ me d F,(s)? 2s?
$y 8" D faxlOm) = As 7 Fis? (1 — 52372

m=0 k=1

7. Let In be the indicator of the event {S, = 0}, so that S,41 = Sn + Xn41 + In. In equilibrium,
E(Sg) = E(Sp) + E(X1) + Eo), which implies that P(Sg = 0) = E(U/g) = —E(X;1) and entails
E(X,) < 0. Furthermore, it is impossible that P(Sg = 0) = 0 since this entails P(Sg = a) = 0 for all
a < oo. Hence E(X1) < Oif S is in equilibrium. Next, in equilibrium,

E(z°0) = E(eSnt1) = E(2#tXn4i tif] — In) + In}).
Now,
E{z5ntXn+i tin] — J,)} = E(z™ | Sp > O)E(Z*!)P(Sp > 0)
Ege tXn+i tlh J,) = 2E(2*1)P(Sp = 0).

Hence
E(z°0) = E(z*1)[{E(2°0) — P(Sp = 0)} + zP(Sp = 0)]
which yields the appropriate choice for E(z50),
8. The hitting time theorem (3.10.14), (5.3.7), states that P(To, =) = (|b|/n)P(Sp = 6), whence

b
E(Top | Top < 00) = P(Top < 00) S>P(Sn = 5).
n

The walk is transient if and only if p 4 4, and therefore E(Tp, | Top < co) < oc if and only if
pF 3. Suppose henceforth that p 4 5.
The required conditional mean may be found by conditioning on the first step, or alternatively

as follows. Assume first that p < g, so that P(Tpp < 00) = (p/q)? by Corollary (5.3.6). Then
>, P(Sn = 8) is the mean of the number N of visits of the walk to b. Now

p\?
P(N =r)= (2) p”\(1-p), r>1,

where p = P(S, = 0 for some n > 1) = 1 — |p — q|. Therefore E(N) = (p/q)?/\p — q| and

b  (p/q)?
(p/q’ |p—al

E(Top | Top < 0) =

We have when p > q that P(To, < 00) = 1, and E(71) = (p — qa. The result follows from
the fact that E(7o») = bE(Z1).

237

[5.4.1]-[5.4.4] Solutions Generating functions and their applications

5.4 Solutions. Branching processes

1. Clearly E(Z, | Zm) = Zmu"—™ since, given Zm, Zy is the sum of the numbers of (n — m)th
generation descendants of Zm progenitors. Hence E(ZmZp | Zm) = Zz p™—™ and E(ZmZn) =
E{E(Zm Zn | Zm)} = E(Z2,)u"—™. Hence

cov(Zm, Zn) = 2" -™E(Z2,) — E(Zm)E(Zn) = uw" var(Zm),

and, by Lemma (5.4.2),

—m_ [Vat Zm { Jur —w™)/A—p") ifu 1,
Z ,Z — ,2-m Fs
pZm, Zn) = \) var Zn Jmin if = 1.

2. Suppose 0 <r <n, and that everything is known about the process up to time r. Conditional on
this information, and using a symmetry argument, a randomly chosen individual in the nth generation
has probability 1/Z, of having as rth generation ancestor any given member of the rth generation.
The chance that two individuals from the nth generation, chosen randomly and independently of each
other, have the same rth generation ancestor is therefore 1/Z,. Therefore

P(L <r) =E{P(L <r | Z,)} =E(1— Z,!)

and so

P(L=r)=P(L <r+1)-PWL <r) =K(Z,!)-E(Z},), 9 Osr<n.

If0 < P(Z; = 0) < 1, then almost the same argument proves that PL = 7 | Z, > 0) =
Nr — H+) forO <r <n, where yy = E(Z;! | Z, > 0).

3. The number Z, of nth generation decendants satisfies

n
if p=
P(Zn = 0) = Gn ntl eee
Zn=)=GnO =) gipn—gry
prt — grt ifp#q,
whence, for n > 1,
1
ee ifp=
nn +1) if p =4,
P(T =n) = P(Zn = 0) — P(Zp—1 = 0) = platy — gy?
if p #q.

(p" — q”)(p"t1 — gn +1)
It follows that E(T) < 00 if and only if p < q.
4, (a) As usual,

Go(s) = G(G(s)) =1—afa(l — s)8}8 = 1a! +81 — 5)6”.
This suggests that G,(s) = 1 — gl tb +6" — 5)” for n > 1; this formula may be proved
easily by induction, using the fact that G(s) = G(Gy_1(s)).
(b) As in the above part (a),
Gas) = f (PF TPO) = fF PPO) = F(a)

238

Age-dependent branching processes Solutions [5.4.5]-[5.5.1]

where P)(s) = P(P(s)). Similarly Ga(s) = f~!(Py(f(s))) for n = 1, where Pn(s) = P(P,—1(s)).

(c) With P(s) = as /{1 — (1 — @)s} where a = y7, it is an easy exercise to prove, by induction, that
Pn(s) = &"s/{1 — (1 — @”")s} for n > 1, implying that

nm 1/m
_ myl/m _. es
Gn(s) = Pais”) ~ 1 ad wm} ,

5. Let Z, be the number of members of the nth generation. The (m + 1)th generation has size
Cn41 + In4i1 where C,41 is the number of natural offspring of the previous generation, and I,4+1 is
the number of immigrants. Therefore by the independence,

E(s2"+1 | Zn) = E(s@r+! | Zn) H(s) = G(s)2" H(s),

whence
Gn4i(s) = E(s"+1) = E{G(s)”"}H(s) = Gn(G(s)) H(s).

6. By Example (5.4.3),

n—-(n-l)s n-Il 1
E Zn) — = > 0.
(™) n+l—ns n + 20 4¢n-1—s)’ ”

Differentiate and set s = 0 to find that

E(V,) = SP Zn =1)= 3 or = bn’.
n=0 n=0
Similarly,
oe | |

oo
_ no _yr_t 12 1
BU) = Ge asi? eee La

n=0 n=0 n=0 n=0

fora] 2 fore) 2 fore)

n (n4+1)*-2@41)4+1 1.2, 1.4 1
E(V¥3) = = =in~+en'—2 _
(V3) ray » (n + 1)4 6 90 X

n=0

The conclusion is obtained by eliminating 57, (n + 1)73.

5.5 Solutions. Age-dependent branching processes
1. (i) The usual renewal argument shows as in Theorem (5.5.1) that
f io,@)
Gis) = [ GGueyfropdu+ f° sfrwdu.
t

Differentiate with respect to t, to obtain

0 az)
97 Ot) = Go) fr O +f on {G(Gr—u(s))} fr(u) du — sfr@).

Now Go(s) = s, and p 5
ar {G(Gr—-u(s))} = ——— (G(G:-u(s))},
t ou

239

[5.5.2]-{5.5.2] Solutions Generating functions and their applications

so that, using the fact that fr (u) = 4e~*” if u > 0,

t FB) t t
[ 5 (G(Gr—-u(s))} fru) du = -[GG1-w(s)) fr], - A [ G(G1—u(s)) fru) du,
having integrated by parts. Hence

2 G(s) = G(syae™ + {-G(sre™ + AGGi(s))} —4 {G6 - / ” sfr(w) au} — sre
at ‘
= A{G(G,(s)) — Gi(s)}.

(ii) Substitute G(s) = s? into the last equation to obtain

aG: 2
— =A(G7 -G
at ( t t)

with boundary condition Go(s) = s. Integrate to obtain At + c(s) = log{1 — Gy for some function
c(s). Using the boundary condition at t = 0, we find that c(s) = log{1 — Go} = log{l — s7}, and
hence G;(s) = se~**/{1 — s(1 — e~*“)}. Expand in powers of s to find that Z(t) has the geometric
distribution P(Z(t) = k) = (1 — e~*)F-1e—t for k > 1.

2. The equation becomes

aG
Gp = + GP) - Gi

with boundary condition Go(s) = s. This differential equation is easily solved with the result

2s+t(L—s) _ 4/t 2-t

Gels) = 24+t(1—s) 2441-5) ¢

We pick out the coefficient of s” to obtain

4 t \?r
PZO =" = ary (=) / nel

and therefore

4 t \" 2/7 ¢t \k
PZ) = =a (55) =? (55) , kL.

n=k

It follows that, for x > 0 and in the limit as t —> 00,

Lxt]-1 1—[xt]
P(Z(t) > xt | Z(t) > 0) = rane = (5 <3) = ( + *) os oo,

240

Characteristic functions Solutions [5.6.1]-[5.7.2]

5.6 Solutions. Expectation revisited

1. Set a = E(X) to find that u(X) > u(EX) + A(X — EX) for some fixed 4. Take expectations to
obtain the result.

2. Certainly Z, = ant X; and Z = yaa |X;| are such that |Z,| < Z, and the result follows by
dominated convergence.

3. Apply Fatou’s lemma to the sequence {—X, : > 1} to find that

(lim sup Xn) = -E(lim inf —Xn) > —liminf E(—Xn) = lim sup E(Xn).
n->0oO

noo
4. Suppose that E|X"| < oo where r > 0. We have that, if x > 0,

xPUX|>») < | u’dF(u)y>0 asx,
[x,00)
where F is the distribution function of |X|.

Conversely suppose that x”P(|X{ > x) > O where r > 0, and letO0 < 5s < r. Now E|X*| =
limy+oo fu us dF (u) and, by integration by parts,

[ u' dF(u) = [-#' (1 - Fw)|e + [eX — F(u)) du.

The first term on the right-hand side is negative. The integrand in the second term satisfies su®~!P(|X| >
u) < sus—!.y-" for all large u. Therefore the integral is bounded uniformly in M, as required.

5. Suppose first that, for all e > 0, there exists 6 = 8(€) > 0, such that E(|X|Z4) < ¢€ for all A
satisfying P(A) < 6. Fix € > 0, and find x (> 0) such that P(|X| > x) < 5(€). Then, for y > x,

x

y x
/ lu|dFy(u) < / lu|dFx(u) +E (\Xllyxi>x) < / luldFy(u) +e.
x

-y —x _
Hence J, |u| dFy(u) converges as y —> 00, whence E|X| < 00.

Conversely suppose that E|X| < oo. It follows that E (IXlZqx|>y}) —> Oas y > oo. Lete > 0,

and find y such that E (|X|J,)x|>y}) < 5e. For any event A, [4 < Tange +Jp where B = {|X| > y}.
Hence
E(\X|14) < E(\X|Jange) + E(\X|Ip) < yP(A) + de.

Writing 6 = €/(2y), we have that E(|X|I4) < € if P(A) < 6.

5.7 Solutions. Characteristic functions
1. Let X have the Cauchy distribution, with characteristic function ¢(s) = esl, Setting Y = X,
we have that dy+y(t) = d(2t) = e 2ltl = dy(t)dy (t). However, X and ¥ are certainly dependent.
2. (i) It is the case that Re{@(t)} = E(costX), so that, in the obvious notation,
lo @] lo @]
Re{1 — 6(2r)} = / {1 — cos(2tx)} dF (x) = 2 | {1 — cos(tx)}{1 + cos(tx)} dF (x)
—OO —OO

< af” {1 — cos(tx)} dF(x) = 4Re{1 — 6(t)}.

241
[5.7.3}-[5.7.6] Solutions Generating functions and their applications

(ii) Note first that, if X and Y are independent with common characteristic function ¢, then X — Y
has characteristic function

v(t) = E(e!* Ee") = 6) (1) = (OO = OOP.

Apply the result of part (i) to the function % to obtain that 1 — |p (2t)|2 < 40 - |f(t)|?). However
|@(@)| < 1, so that

1 — 19 (28)| < 1-|@ Qe)? < 40 - 1@@)) < 80 - 1G).

3. (a) With m, = E(X*), we have that

[o.<)
E(eo*) =1+ 5) —mpo* =14 S06),
k=1

1
kt
say, and therefore, for sufficiently small values of 0,
(14!

Kx@)=)>_

r=]

S(6)”.

Expand S(6)" in powers of 9, and equate the coefficients of 6, 6, 63, in turn, to find that ky (X) =m,
ka(X) = mz — mi, k3(X) = m3 — 3mm + 2m}.
(b) If X and Y are independent, Ky,y(6) = log{E(e°*)E(e°”)} = Kx(@) + Ky(@), whence the
claim is immediate.

192
4. The N(O, 1) variable X has moment generating function E(e?* )= e2? , so that Ky (0) = 407,

5. (a) Suppose X takes values in L(a, b). Then

|x (227/b)| = = |e27!a/dy

So 2m ix/ p(x =x) So ett P(X =a+bm)| =1
x m

since only numbers of the form x = a + bm make non-zero contributions to the sum.
Suppose in addition that X has span b, and that |@x(T)| = 1 for some T € (0, 27/b). Then
ox (T) = e'© for some c € R. Now

E(cos(T'X —c)) = dE(e!T XH 4 eiT tic) — 1,

using the fact that E(e7*) = $x(T) = e7!¢. However cosx < 1 for all x, with equality if and
only if x is a multiple of 27. It follows that TX — c is a multiple of 27, with probability 1, and hence
that X takes values in the set L(c/T, 27/T). However 22 /T > b, which contradicts the maximality
of the span b. We deduce that no such T exists.

(b) This follows by the argument above.

6. Thisisaform of the ‘Riemann—Lebesgue lemma’. It is a standard result of analysis that, fore > 0,
there exists a step function g¢ such that [°>, | f (x) — ge(x)| dx < €. Let de(t) = f%, ef ge(x) dx.
Then

ao | 0°
bx ~ 001 =| / e''* (f(x) — ge(x)) dx| < / If) — ge(x)| dx <e.
oO

—OO

If we can prove that, for each €, |@¢(t)| - 0ast — too, then it will follow that |¢x (¢)| < 2e for all
large t, and the claim then follows.

242

Characteristic functions Solutions [5.7.7]-[5.7.9]

Now g(x) is a finite linear combination of functions of the form cl, (x) for reals c and intervals
A, that is ge(x) = a cxTa, &); elementary integration yields

eitbe _ pitay

bet) = ae

where ay and by are the endpoints of A,. Therefore

etn <2 a0, ast —> oo.
k=1

7. If X is N(u, 1), then the moment generating function of X 2 is

2 OO 2 1 Ll gy_y)2 1 ps
My2(s) = E(e®* -| sx" OM) gy — ex ;
x28) ( ) 00. Vin * 1 — 2s P 1—2s

ifs < 4, by completing the square in the exponent. It follows that

= I “js 1 $0
My(s) = |] J = .
r= | spo (51% aaa? (75)

j=l

It is tempting to substitute s = it to obtain the answer. This procedure may be justified in this case
using the theory of analytic continuation.

8. (a) T2 = X2/(Y/n), where X2 is x2(1; 42) by Exercise (5.7.7), and Y is x2(n). Hence T? is
F(1,n; w?).
(b) F has the same distribution function as

_ (A? +.B)/m
~ Vin

where A, B, V are independent, A being N (/6, 1), B being x2(m — 1), and V being x2(n). Therefore

5 B/(m —1)
E(Z) = ~ {ECA )E (=) +(m — DE (Ae-”)}

i _ n _ n(m+8)
= {d+ O25 tlm yp }- ner.

where we have used the fact (see Exercise (4.10.2)) that the F(r, s) distribution has mean s/(s — 2)
ifs > 2.

9. Let X be independent of X with the same distribution. Then |@|2 is the characteristic function of
X — X and, by the inversion theorem,

l “ —itx 9°
=|. \p@)?e" dt = fy x(x) =f. f(y) f(x + y) dy.

Now set x = 0. We require that the density function of X — X be differentiable at 0.

243

[5.7.10]-[5.8.2] Solutions Generating functions and their applications

10. By definition,
. wo ,
9 gx(y) = / BOD F(x) dz.

Now multiply by fy (y), integrate over y € R, and change the order of integration with an appeal to
Fubini’s theorem.

11. (a) We adopt the usual convention that integrals of the form ihe g(y) dF (y) include any atom of
the distribution function F at the upper endpoint v but not at the lower endpoint u. It is a consequence
that F, is right-continuous, and it is immediate that F; increases from 0 to 1. Therefore F; is a
distribution function. The corresponding moment generating function is

M(t4+r)

_— °° tx _— 1 °° tx+Tx _—
Me(t) =f e*“ dF. a) = MO [oe dF) = .

—cO
(b) The required moment generating function is

Mxsyt+t) _ Mx(¢+2)My(t +2)
Mx+r() Mx (My

the product of the moment generating functions of the individual tilted distributions.

5.8 Solutions. Examples of characteristic functions

1. (i) We have that A(t) = E(e#X) = E(e“#%) = g_x(t).

(ii) If X; and X> are independent random variables with common characteristic function @, then
0%, 4+X7 (1) = $x, OGx, (1) = 60".

(iii) Similarly, @x,-x, ©) = dx, OO-x,() = 68D = 16 OP.

(iv) Let X have characteristic function @, and let Z be equal to X with probability 5 and to —X
otherwise. The characteristic function of Z is given by

oz(t) = 4(E(e™*) + E(e#*)) = 3 (6) + OO) = Re),

where we have used the argument of part (i) above.
(v) If X is Bernoulli with parameter 7 then its characteristic function is @(¢) = 5 + zel . Suppose Y

is a random variable with characteristic function w(t) = |@(t)|. Then w(t)? = (t)¢(—t). Written
in terms of random variables this asserts that Y; + Y> has the same distribution as X; — X 2, where
the Y; are independent with characteristic function y, and the X; are independent with characteristic
function ¢. Now X; € {0, 1}, so that X; — X2 € {—1, 0, 1}, and therefore Y; € {-4, 4}. Write
a = P(Y; = 4). Then

P(Y| + ¥o = 1) =a? = P(X] — X= 1) = G,
PY, + Yo =—1) =(1-a)? = P(X — X2 =-1) = Gj,
implying that «2 = (1 — a)? so that a = 7 contradicting the fact that ¢? = 5. We deduce that no
such variable Y exists.

2. Fort > 0,
P(X > x) = P(e!® > e!*) < ce Ee!*),

Now minimize over t > 0.

244

Examples of characteristic functions Solutions [5.8.3]-[5.8.5]

3. The moment generating function of Z is

Mz(t) =E {B(e'*? | y)} =E(Mxcry) =B{ (; +) \

[ a m yt lay _ yyn—n—l F
o \A-ty B(n,m —n)

Substitute v = 1/y and integrate by parts to obtain that

foe) —] m—n—-1

(Av —1)™
satisfies
i 1 (went) m-n-1, (m.n. 291
=\|— _ =¢ _
m Mm —1) v—nm! aA(m—1) 7b" m,n, A}m—1n

for some c(m,n, A). We iterate this to obtain

dv c! 1

oO
L = "I, = | ee Se
mn =C Anta = fo Gy anetl ~ aa an

for some c’ depending on m, n, A. Therefore Mz(t) = c’”(A — t)—” for some c” depending on m,n, A.
However Mz(0) = 1, and hence c” = A”, giving that Z is (A, n). Throughout these calculations
we have assumed that t is sufficiently small and positive. Alternatively, we could have set ¢ = is and
used characteristic functions. See also Problem (4.14.12).

4. We have that
y2 Oy? 1 (x — )*
E(e#*") = / el!* exp | — dx
( ) —0o V2n02 207
2
00 1 [x —-pi- 207it)-1| ity?
= _ —_——_— | d.
I. Imo 2o2(1— 202i) | *P\ To 252i |

_ 1 ex i tp?
~ SA. — 26254 P\ T= 202i } °
The integral is evaluated by using Cauchy’s theorem when integrating around a sector in the complex
plane. It is highly suggestive to observe that the integrand differs only by a multiplicative constant

from a hypothetical normal density function with (complex) mean u(1 — 207it)71 and (complex)
variance o(1 — 202it)7},

1
5. (a) Use the result of Exercise (5.8.4) with « = 0 and o* =1: Py2(t) = (1 —2it) 2, the
1
characteristic function of the x72) distribution.
1
(b) From (a), the sum S has characteristic function s(t) = (1 — 2it)” 2”, the characteristic function
of the y2(n) distribution.
(c) We have that
w(el!X1/X2) — Ef B(elX1/%2 | X>)} = E (px, (t/X2)) = E(exp{—427/X5}).

245

[5.8.6]-[5.8.6] Solutions Generating functions and their applications

Now

E(exp{—427/X3}) = [. l exp | — a ~ x dx

Pe fog Jin P\ 2 0)

There are various ways of evaluating this integral. Using the result of Problem (5.12.18c), we find
that the answer is e~!‘!, whence Xj / X>2 has the Cauchy distribution.

(d) We have that

ete) = (Rte | Xa)} = B(x, (Xa) = E(e"2°%3)
1

-[ = ae r{- PUT EY} dx = Tams,

oo / 2.
1
on observing that the integrand differs from the N(0, (1 + 12)~2) density function only by a multi-
plicative constant. Now, examination of a standard work of reference, such as Abramowitz and Stegun
(1965, Section 9.6.21), reveals that

© cos(xt)
dt = Ko(x),
[ t = Kola)

V1422

where Ko(x) is the second kind of modified Bessel function. Hence the required density, by the
inversion theorem, is f(x) = Ko(|x|)/a. Note that, for small x, Kg(x) ~ —logx, and for large
positive x, Ko(x) ~ e7* fax /2.

As a matter of interest, note that we may also invert the more general characteristic function
é(t) =U —it 70 + it)F. Setting 1 — it = —z/x in the integral gives

1 love) ea its e7Fxt-1  poxtixoo edz
2m Joo (1 — it)? (1 + it) 2P2ni J—x-ixoo (—z)® 1 +2/(2x))

e* (2x) 28-2)
=—F@  Vbe-s),40-0-~)

where W is a confluent hypergeometric function. When a = § this becomes

@/a"3
I@ Tava

where K is a Bessel function of the second kind.

(e) Using (d), we find that the required characteristic function is PX, Xp (bx, xO =A+ 2y71,
In order to invert this, either use the inversion theorem for the Cauchy distribution to find the required
density to be f(x) = del for -oo < x < o, or alternatively express (1 + 17)! as partial
fractions, (1 + ¢2)7! = At — it) + (1 +is)7}), and recall that (1 — it)! is the characteristic
function of an exponential distribution.

f@)= -1@)

6. The joint characteristic function of K = (X1, X2,..., Xn) satisfies ¢x(}) = E(e!tX’) = Ee!)

where t = (f1, 9, ...,%) € R” and Y = tX! =) X1 +---+%Xn. Now Y is normal with mean and
variance
n n
E(Y) =) yE(Xj)=tw’, —-var(Y) = }> tjmcov(X;, X;) = te,
j=l jpk=l

where yz is the mean vector of X, and V is the covariance matrix of X. Therefore ¢x(t) = dy(1) =
exp(ity’ — 4tVt’) by (5.8.5).

246

Inversion and continuity theorems Solutions [5.8.7]-[5.9.2]

Let Z = X — w. It is easy to check that the vector Z has joint characteristic function ¢7(t) =
1
eT atVe which we recognize by (5.8.6) as being that of the (0, V) distribution.

1,2 1,2
7. We have that E(Z) = 0, E(Z”) = 1, and E(e') = E{E(e!@ | U, V)} = E(e2") = 2°.
If X and Y have the bivariate normal distribution with correlation o, then the random variable Z =

(UX + VY)//U2 + 2pUV + V? is N(O, 1).

8. By definition, E(e!!*) = E(cos(tX)) + iE(sin(tX)). By integrating by parts,

ft (tx)ae** d x x yre** g Mt
cos(tx)Ae x=—=s—, sin(tx)Ae x= —s—,
0 2412? Jo +P
and

+iat A

42422 — Ait

9. (a) We have that e~*! = e-* Ip>0} + e* [ty <0}, whence the required characteristic function is

1 1 1 1
n= = .
o@) 5 (ati) 1422

(b) By a similar argument applied to the (1, 2) distribution, we have in this case that

1 1 1 1—??
tH=x = .
0O= 35 (a ain? * asim) a+
10. Suppose X has moment generating function M(t). The proposed equation gives
1 t
2 1 2

M(t) = Mut) du = ; Mv) dv.

0 0

Differentiate to obtain tM’ + M = M2, with solution M(t) = A/(A +1). Thus the exponential
distribution has the stated property.

11. We have that oo,
ox,y(s,t) =E(eh* 4") = bc x 4:7 (1).
Now sX +tY is N(O, sta? 4 2stoto + 12) where o2 = var(X), r= var(Y), e = corr(X, Y), and
therefore
ox,y(s,t) = exp{—4(62o? 4 2statp + 1717)},

The fact that @y y may be expressed in terms of the characteristic function of a single normal variable
is sometimes referred to as the Cramér—Wold device.

5.9 Solutions. Inversion and continuity theorems

1. Clearly, for 0 < y < 1, P(Xn < ny) =n7!|ny| > yasn > oo.

2. (a) The derivative of F, is f,(x) = 1 —cos(2nrx), for 0 < x < 1. It is easy to see that f, is
non-negative and fe Fn(x) dx = 1. Therefore F;, is a distribution function with density function fy.

(b) Asn > ox,
1

—— > 0,
= Onn

sin(2n7x) <

2ni

247
[5.9.3]-|5.9.6] Solutions Generating functions and their applications

and so F,(x) > x for 0 < x < 1. On the other hand, cos(2nz x) does not converge unless x € {0, 1},
and therefore f,(x) does not converge on (0, 1).

3. We may express N as the sum N = 7; + T, + ---+ J; of independent variables each having the
geometric distribution P(T; =r) = pq” —! for r > 1, where p+q=1. Therefore

ir yk
mno=onot={ Pe .

get

implying that Z = 2Np has characteristic function

perit \ ~ {242g +00)

k
pen — 25t)-*
1—(1 — p)e2Pit pd — 2it + 0(1)) \ > C= 218)

$z(t) = on (2pt) =

as p |. 0, the characteristic function of the rd, k) distribution. The result follows by the continuity
theorem (5.9.5).

4. All you need to know is the fact, easily proved, that ym (t) = e!!” satisfies

7 _ fam ifj+k=0,
[ woma={ 5 if {+k £0,

for integers j and k.
Now, $(t) = D2_, ef P(X = J), so that

i entk a) dr = 3 px af “jw Lod = 2. PX =k 20
Qn Jn an #4, -n 7 2n

If X is arithmetic with span A, then X/A is integer valued, whence

x mfr .
P(X = kA) = 5 [ pe Ox dt.
IT

5. Let X be uniformly distributed on [—a, a], Y be uniformly distributed on [—b, b], and let X and
Y be independent. Then X has characteristic function sin(at)/(at), and Y has characteristic function
sin(bt)/(bt). We apply the inversion theorem (5.9.1) to the characteristic function of X + Y to find

that
© sin(at) sin(bt) aAb
se [oxen ds = of OO at = furs) = SX*.

6. Itis elementary that
oO OO
[ exp{ fn(x)} dx = [ x"e* dx =T(n+ 1) =n!.
0 0

In addition, a =n, f/’(a) = —n~!, and

°° 1 2 gn - a (x — ny? ~
[ exp{ fn(a) + 3 ~ a) fy (a)} dx =n"e "f exp < — dx ~n"e"./2nn,
0 0

2n
and Stirling’s formula follows.

248
Two limit theorems Solutions [5.9.7]-{5.10.1]

7. The vector X has joint characteristic function @(t) = exp(— Seve ). By the multidimensional
version of the inversion theorem (5.9.1), the joint density function of X is

1
f®= sx | exp(—itx’ — 5tvt’) dt
7 R"

Therefore, ifi 4 j,

af 1 [ ve’ — levi af
=. tit; —itx — 5tVt)dt=
avujy (2) Jpn! exp(—itx’ — 3tVt) 8x; x;"
and similarly when i = j. Wheni + j,
a of
—-P Xp <uj= Lee
[2 fit g(r aw = 0
Q ino! xj=xj= 7— >

where {” - dx’ is an integral over the variables x, for k # i, j.
Therefore, P(max, X% < u) increases in every parameter v;;, and is therefore greater than its
value when v;; = Ofori # j, namely [], P(X, < u).

8. By a two-dimensional version of the inversion theorem (5.9.1) applied to E(eitX’), t= (4, %),

* x, > 0, > H =~ it : I exp(—itx’ seve) atl dx
~ px, > =~ _. ity’ — 1

zal exp(— 3tVt') 3tVe’) it
“ie R2 (ity)(it2) |

2nV/|V—-} 1
== = | exp(— 5tVt’) dt= dd WN | = .
4n2 J Jp2 4a 2nV/1— p2

We integrate with respect to ¢ to find that, in agreement with Exercise (4.7.5),

1 1
P(X; >0, X2>0) = at zz sin’.

5.10 Solutions. Two limit theorems

1. (a) Let {X; : i > 1} beacollection of independent Bernoulli random variables with parameter 5.
Then S, = i X; is binomially distributed as bin(n, 5). Hence, by the central limit theorem,

_ n [Sn — 3a [ 1 1

2" = P| ——+— <x] > (x) -— ®(-x) = e 2” dy,

7 (7) ( Tye <7) 7PM PCO™ fee
Ik-An|<4xJ/n

where ® is the N(0, 1) distribution function.

249

[5.10.2}-[5.10.4] Solutions Generating functions and their applications

(b) Let {X; : i => 1} be a collection of independent Poisson random variables, each with parameter 1.
Then Sp = 5-7 X; is Poisson with parameter n, and by the central limit theorem

ki
k—n|<x./a

k _—
e" » =P Gia s x) — $(x)— O(-x), as above.
n

2. Asuperficiaily plausible argument asserts that, if all babies look the same, then the number X of
correct answers in 7 trials is a random variable with the bin(n, 5) distribution. Then, for large n,

by the central limit theorem. For the given values of n and X,

X—jn 910-750 _
Lyn 5/15

Now we might say that the event {X — in > 3/n } is sufficiently unlikely that its occurrence casts
doubt on the original supposition that babies look the same.

A statistician would level a good many objections at drawing such a clear cut decision from such
murky data, but this is beyond our scope to elaborate.

3. Clearly

_ itY _ it _ _ 1 oo 1 \°
by () = E{E(e®” | X)} = Efexp(X(e — ))} = (; - 5) = & =) .
It follows that 1
E(Y) = 540) =, E(Y?) = —o-(0) = s? 42s,

whence var(Y) = 2s. Therefore the characteristic function of the normalized variable Z = (Y —
EY)/./var(Y) is
z(t) = et VS/* by (t/V25).

Now,
log { dy (t/W2s)} = —s log (2 — eit/V2s) =s (eit/V2s -1)+ 1 5 (eit/V2s = 1)° +o(1)
= ity/ ts — de? — bt? + 0c),
where the o(1) terms are as s — oo. Hence log{¢z(t)} > —3t
by the continuity theorem (5.9.5).
Let P,, P2, ... be an infinite sequence of independent Poisson variables with parameter 1. Then
Sn = Pi + Po +-+-+ Pn is Poisson with parameter n. Now Y has the Poisson distribution with
parameter X, and so Y is distributed as Sy. Also, X has the same distribution as the sum of s
independent exponential variables, implying that X —> oo as s — oo, with probability 1. This

suggests by the central limit theorem that Sy (and hence Y also) is approximately normal in the limit
as 5 —> 00. We have neglected the facts that s and X are not generally integer valued.

2ass > oo, and the result follows

4, Since X, is non-arithmetic, there exist integers n,,2,...,"% with greatest common divisor 1
and such that P(X; = n;) > Ofor1 <i < k. There exists N such that, for alln > N, there exist non-
negative integers a, a2,..., @% such thatn = ajn,+---+ any. If x is a non-negative integer, write

250
Two limit theorems Solutions [5.10.5]-[5.10.5]

N = Bin, +---+ Beng, N+x = yjny +:--++ yeny for non-negative integers B1,..., Bk, Y1--+. Ve-
Now Sy = X1 +---+ Xp is such that

k
P(Sg = N) > P(Xj =n; for Bj_y < j < Bj, 1<i <k) =] [P(X =2))4 > 0
i=l

where Bp = 0, Bi = 61 + fo +-:- + 6;, B = By. Similarly P(Sg = N +x) > O where
G=y +yv2+--:+y,. Therefore

P(Sg — Sg,p+G = x) = P(SgG =N+x)P(Sp = N) > 0

where Sg, p+G = were X;. Also, P(Sg — Sp,34G = —*) > 0 as required.
5. Let X1, X2,... be independent integer-valued random variables with mean 0, variance 1, span 1,

as : . 1,2
and common characteristic function ¢. We are required to prove that ./nP(U, = x) > e 2” //2n

as n — oo where
1 — XytXo4+-:-+Xn

/n
and x is any number of the form k/./n for integral k. The case of general jz and o7 is easily derived
from this.

By the result of Exercise (5.9.4), for any such x,

1 ” —itx
_ t) dt,
an./n afi, Pun)

since U,, is arithmetic. Arguing as in the proof of the local limit theorem (6),

P(U, =x) =

2x |/nP(Un = x) — f(x)| < In t+ In
where f is the N(O, 1) density function, and

1,2

ln . 12 .
In -| je“ (bu, () —e 2 }| at, Jn -| Jew#X 938" ay,
—ayn |t|>a./n

Now Jn = 22x (1 — @(./n)) > Oasn — oo, where & is the N(0, 1) distribution function. As
for In, pick 6 € (0, 7). Then

‘ve yt? 1,2
In ni i-5t d n _i, at
< [> (loti e 2" | a |+e72" Vat

1,2
The final term involving e 2" is dealt with as was Jy. By Exercise (5.7.5a), there exists A € (0, 1)
such that |@(t)| < A if 6 < |t| <2. This implies that

/ |o(t//n)"| dt < (@ —8)a"/n > 0,
b./n<|tl|<a.J/n

and it remains only to show that
5./n 1,2
/ |(t//n)” — e 2! |dt > 0 as n —> 00.
—b./n

251

[5.10.6]-[5.10.7] Solutions Generating functions and their applications

The proof of this is considerably simpler if we make the extra (though unnecessary) assumption
that m3 = E|X | < 00, and we assume this henceforth. It is a consequence of Taylor’s theorem (see

1,2, 43
Theorem (5.7.4)) that (t) = 1— 41? —1it3m3+0(t3) ast —> 0. It follows that (t) = e 2" *7 OO
for some finite 0 (t). Now |e* — 1| < |x|e!*!, and therefore

|o(t//n)" — e2"| = 272" lexp(t3n720(tn~2)) — 1

1 1
\f?0(tn~2)| |30(tn2)| 1»
< exp —ut .
Jn Jn 2

Let Ks = sup{|@(u)| : |u| < 5}, noting that Ks < oo, and pick 6 sufficiently small that 0 < 6 <7
and 8K < 4. For |t| < 5,/n,

\p(t//n)” — en2" | < Kye exp(2oKy —h\< xy te de

and therefore

bJn 1 Ks pova !
/ \o(t/Va)" —e72" | dt < = | (Pe74" dt > 0 asn— oo
Ja Vn J-3Jn

as required.

6. The second moment of the X; is

et 2 -1 ,2u
2 [ _* dx = / < du
0 -2x(logx)? oo
(substitute x = e”), a finite integral. Therefore the X’s have finite mean and variance. The density

function is symmetric about 0, and so the mean is 0.

By the convolution formula, if 0 < x < e7!,

-1

A= fofe-nay> [ rote—ydy= fe | foray,

since f(x — y), viewed as a function of y, is increasing on [0, x]. Hence

fo) 1
PO) > Feet] ~ Alx|dog lp?

for 0 < x < e~!. Continuing this procedure, we obtain

O<x<e!,

Sn

nh
> ee
) = Fidog lapel’

for some positive constant ky. Therefore f,(x) — oo asx — O, and in particular the density function
of (X, +---+ Xn)/./n does not converge to the appropriate normal density at the origin.

7. Wehave for s > 0 that

otis) = oo I * exp(—(2x)7! — xs)x73/? dx

1 foe)
= [In [ exp(—3y7 - sy )2dy by substituting x = y?

252
Large deviations Solutions [5.10.8]-[5.11.3]

by the result of Problem (5.12.18c), or by consulting a table of integrals. The required conclusion
follows by analytic continuation in the upper half-plane. See Moran 1968, p. 271.

8. (a) The sum Sp = 57)_, X, has characteristic function E(eitSn) = @(t)" = b(tn”), whence
Un = Sn/n has characteristic function ¢(tn) = E(e#"*1). Therefore,

P(Sn <¢) = P(X) <c)=P(X1 <=) +0 as n —> 00.

(b) E(e"7n) = b(t) = Ee**1).

9. (a) Yes, because X, is the sum of independent identically distributed random variables with
non-zero variance.

(b) It cannot in general obey what we have called the central limit theorem, because var(Xn) =
(n2 — n) var(@) + nE(©)(1 — E()) and n var(X;) = nE(©)(1 — E(@)) are different whenever
var(@) + 0. Indeed the right ‘normalization’ involves dividing by n rather than ./n. It may be shown
when var(@) # 0 that the distribution of X,/n converges to that of the random variable ©.

5.11 Solutions. Large deviations

1. We may write S, = >>] X; where the X; have moment generating function M(t) = 5(e +e-*),
Applying the large deviation theorem (5.11.4), we obtain that, for 0 < a < 1, P(S, > an)!/" >
inf,o{g(¢)} where g(t) = e~% M(t). Now g has a minimum when e’ = ./(1 +. a)/(1 — a), where
it takes the value 1/./(1 + a)!+4(1 — a)!~4 as required. If a > 1, then P(S, > an) = 0 for all n.

2. (i) Let Y, have the binomial distribution with parameters n and 5. Then 2Y, — n has the same
distribution as the random variable S, in Exercise (5.11.1). Therefore, if 0 < a < 1,

1

Pn —- jn > dan)'/” = P(S, > an)!/" 5 Jaratad sia’
a —a

and similarly for PY, — 5n <- 5an), by symmetry. Hence

4
Vd tayad ayia

tin = {2"P(|¥n — 5n| > dan)}1/" a

(ii) This time let S, = X,+--++Xy, the sum of independent Poisson variables with parameter 1. Then
T, = e"P(Sp, > n(1 + a)). The moment generating function of X; — 1 is M(t) = exp(e’ — 1 — 2),

and the large deviation theorem gives that tT; in, einf;,9{g(t)} where g(t) = e~% M(t). Now
g’(t) = (ef — a — 1) exp(e" — at — t — 1) whence g has a minimum at t = log(a + 1). Therefore
l/” _s eg(log(1 +.a)) = {e/(a + 1)}441.

3. Suppose that M(t) = E(e!*) is finite on the interval [—6, 6]. Now, fora > 0, M(6) > 6°? P(X >
a), so that P(X > a) < M(5)e~®¢. Similarly, P(X < —a) < M(—é)e~*4.
Suppose conversely that such A, jz exist. Then

M(t) < Eel) = | ell® d F(x)
[0,00)

where F is the distribution function of |X|. Integrate by parts to obtain
oO
M(t) <1+ [-el*[1 — F@)]]9° +f lle” [1 — F(x) dx
0

253
(5.11.4]-[5.12.2] Solutions Generating functions and their applications

(the term ‘1’ takes care of possible atoms at 0). However 1 — F(x) < pe~*, so that M(t) < 00 if
|t| is sufficiently small.

4. The characteristic function of S;,/n is {e~!4/"ly" = el, and hence S,/n is Cauchy. Hence

oO dx 1 u —1

5.12 Solutions to problems

1. The probability generating function of the sum is

1 6 510 1 10 1—56 10 1 10
fede} = (5) | = (5) (1 — 10s +---)(1 +105 +--+).
The coefficient of s27 is

i=]
1\!° { (10\ (14 10\ (20 26
(5) 2N\sJ}7\a haa) tha] ft:
2. (a) The initial sequences T, HT, HHT, HHH induce a partition of the sample space. By conditioning
on this initial sequence, we obtain f(k) = gf(k — 1) + paf(k — 2) + pq f(k — 3) fork > 3,
where p+q = 1. Also f(l) = fQ) = 0, f@G3) = p>. In principle, this difference equation
may be solved in the usual way (see Appendix J). An alternative is to use generating functions.
Set G(s) = re sk Ff (k), multiply throughout the difference equation by s* and sum, to find that
G(s) = ps? /{l-—qs - pqs — p?qs?}. To find the coefficient of s*, factorize the denominator,
expand in partial fractions, and use the binomial series.

Another equation for f (k) is obtained by observing that X = k if and only if X > k — 4 and the
last four tosses were THHH. Hence

k-4
fk) = ap°(1 - + fo), k>3.
i=l

Applying the first argument to the mean, we find that 44 = E(X) satisfies wu = g(1+)+ pq(2+
14) + p?g3 + 1) + 3p? and hence u = (1+ p + p?)/p?.

As for HTH, consider the event that HTH does not occur in n tosses, and in addition the next
three tosses give HTH. The number Y until the first occurrence of HTH satisfies

P(Y > n)p*g =P(Y =n + lpg +P(Y =n +3), n> 2.

Sum over n to obtain E(Y) = (pq + 1)/(p?q).
(b) Gy (s) = (gq + ps)", in the obvious notation.

(i) PQ divides N) = 3{G NC) + Ga(—D)}, since only the coefficients of the even powers of s
contribute to this probability.

(ii) Let w be a complex cube root of unity. Then the coefficient of P(X = k) in 3{G NC) + Gy(o) +
Gy(w*)} is

s{l+o%+o%}=1, ifk =3r,

H{l+o+0"}=0, ifk=3r+1,

A{1l+o*+o}=0,  ifk=3r +2,

254

Problems Solutions [5.12.3]-[5.12.6]

Ly

for integers r. Hence 3{Gy() + Gy(w) + Gy(w’)} = me am 9 P(N = 3r), the probability that N

is a multiple of 3. Generalize this conclusion.

3. We have that 7 = & if no run of n heads appears in the first k — n — 1 throws, then there is a
tail, and then a run of n heads. Therefore P(T = k) = P(T > k —n — l)qp” fork > n+ 1 where
p+q=1. Finally P(T =n) = p”. Multiply by s* and sum to obtain a formula for the probability
generating function G of T:

at+j
G(s) — p"s” = gp" ss SS PT =j)=4p" Lrr=n > st
k=n+1  j>k—n—-1 k=n+1
nontl © n a
= Soe = pa -si)= 4 *_a- ee).
j=l

Therefore
p's _ prtlsntl

G — ane .
(s) 1—s+qp%st1

4. The required generating function is

Gis) = ye (E )era = pr = (-F-)

where p +q = 1. The mean is G’(1) = r/p and the variance is G” (1) + G’(1) — {G’(1)}* = rq/p?.

5. It is standard (5.3.3) that po(2n) = 7") (pq)". Using Stirling’s formula,

2 n+} —2n /2 4 n
po(2n) ~ SNE (pgyt = SP
{n"*2 2. /In}? mn

The generating function Fo(s) for the first return time is given by Fo(s) = 1 — Pots)! where
Pots) = on s*" p9(2n). Therefore the probability of ultimate return is Fo(1) = 1 — 171 where, by

Abel’s theorem, 1
A= 2n 2
2 Pot <0o ifp#g.
Hence Fo(1) = 1 if and only if p = 4.
6 (a) Rp = x? + y?2 satisfies
E(Rn41 — Rn) = E{(Xh41 — Xn) + Wo — YF
= 26(X2,) — X2)= oe B(x, — x? | X,)}
= 2E{4[(Xn +1)? — X32) + 4[(Xn — 1)? — X2)} = 1.

Hence Ry =n + Ro =n.

(b) The quick way is to argue as in the solution to Exercise (5.3.4). Let Un = Xn+Yn, Vn = Xn—Yn.
Then U and V are simple symmetric random walks, and furthermore they are independent. Therefore

1\2" fon °
po(2n) = P(U2q = 0, Voy = 0) = PU2y = O)P(V2q = 0) = (5) (’ \

255

[5.12.7]-[5.12.8] Solutions Generating functions and their applications

by (5.3.3). Using Stirling’s formula, pp(2n) ~ (nx)~', and therefore Yn Po(2n) = oo, implying
that the chance of eventual return is 1.

A longer method is as follows. The walk is at the origin at time 0 if and only if it has taken equal
numbers of leftward and rightward steps, and also equal numbers of upward and downward steps.

Therefore 3
five (2n)! _ (1\*" (2n
pan = (7) 2 Gai — mn? (5) (rp:

7, Let e;; be the probability the walk ever reaches j having started from i. Clearly e¢g9 =
€a,a—1€a—1,a—2°** 10, Since a passage to 0 from a requires a passage to a — 1, then a passage
to a — 2, and so on. By homogeneity, ego = (€10)°.

By conditioning on the value of the first step, we find that ej9 = pe3q + geo = pedo +q. The
cubic equation x = px +q has roots x = 1, c, d, where

ea Pave +4pa P+ pt + 40g
2p , 2p ,

Now |c| > 1, and |d| > 1 if and only if p* +4pq > 9p* which is to say that p < 3. It follows that
ego=lifp< 3, 80 that ego =lifp< $.
When p > 3 we have that d < 1, and it is actually the case that e179 = d, and hence

a
_ (-p+vp? +4pq 1
€a0 = Dp if p > 3.

In order to prove this, it suffices to prove that egg < 1 for all large a; this is a minor but necessary
chore. Write T, = Sn — So = >-7_, Xi, where X; is the value of the ith step. Then

€go = P(T, < —a for somen > 1) = P(np — Th > nu +a for some n > 1)

oo
< SO P@y-—T =n +a)

n=1
where 2. = E(X,) = 2p — g > 0. As in the theory of large deviations, for ¢ > 0,
P(np — Tn > np +a) < et Ht (pel HX)"
where X is a typical step. Now E(e’—*)) = 1 + o(t) ast | 0, and therefore we may pick t > 0

such that @(t) = e#E(e"#-%)) < 1. It follows that egg < S~O2, e7'40(t)" which is less than 1
for all large a, as required.

8. We have that
E(s*e¥ | X+¥=n)= i tn *(; ) ar = (ps + qt)",

where p + q = 1. Hence Gy y(s,t) = G(ps + qt) where G is the probability generating function
of X + Y. Now X and FY are independent, so that

G(ps + qt) = Gx(s)Gy(@) = Gy y(s, DGx,y (1, ft) = G(ps + q)G(p + 4t).

256

Problems Solutions [5.12.9]-[5.12.11]

Write fu) = G0 +4),x =s—1, y =t —1, to obtain f(px + qy) = f (px) f (gy), a functional
equation valid at least when —2 < x, y < 0. Now f is continuous within its disc of convergence,
and also f(0) = 1; the usual argument (see Problem (4.14.5)) implies that f(x) = e for some
A, and therefore G(s) = f(s — 1) = e*“—)_ Therefore X + Y has the Poisson distribution with
parameter 4. Furthermore, Gx(s) = G(ps +q) =e? (s~l) whence X has the Poisson distribution
with parameter Ap. Similarly Y has the Poisson distribution with parameter Aq.

9. In the usual notation, G,41(s) = Gn(G(s)). It follows that Gri) = G"(1)6'(1)? +
G!,(1)G” (1) so that, after some work, var(Zp4.1) = #2 var(Zn) + "0. Iterate to obtain

2,,n n+1
o 1-
mH yo. 4 any THEE)

> > 0,
1-p n=

var(Zn41) = o7(u" +b

for the case pw # 1. If ~ = 1, then var(Z,41) = o2(n +1).

10. (a) Since the coin is unbiased, we may assume that each player, having won a round, continues
to back the same face (heads or tails) until losing. The duration D of the game equals k if and only
if k is the first time at which there has been either a run of r — 1 heads or a run of r — 1 tails; the
probability of this may be evaluated in a routine way. Alternatively, argue as follows. We record S
(for ‘same’) each time a coin shows the same face as its predecessor, and we record C (for ‘change’)
otherwise; start with a C. It is easy to see that each symbol in the resulting sequence is independent
of earlier symbols and is equally likely to be S or C. Now D = k if and only if the first run of r — 2
S’s is completed at time k. It is immediate from the result of Problem (5.12.3) that

(gsy'2(1 — 55)

G = .
p= TT, + (3571

(b) The probability that A; wins is

oO
m = >_> P(D =n - 1) +k-1).

n=]

Let @ be a complex (r — 1)th root of unity, and set

1 1 1

$e tt Gp o"s)
@r-2Dk-D 7P

It may be seen (as for Problem (5.12.2)) that the coefficient of st in W,;(s) is P(D = i) if i is of the
form n(r — 1) + (k — 1) for some n, and is 0 otherwise. Therefore P(A, wins) = W;(1).

(c) The pool contains £D when it is won. The required mean is therefore

E(Dia, wins}) _ Ww.)

E(D | Ay wins) = =
(D | Ax wins) = ra wins) W,()

(d) Using the result of Exercise (5.1.2), the generating function of the sequence P(D > k), k > 0, is
T(s) =(1 — Gp(s))/(1 — 5). The required probability is the coefficient of s” in T(s).

11. Let T, be the total number of people in the first n generations. By considering the size Z; of the

first generation, we see that
Zi

Tm =14+ 5 >T1@

i=1

257

[5.12.12]-[5.12.14] Solutions Generating functions and their applications

where T,,—1 (1), T,—-1 (2), .. . are independent random variables, each being distributed as T, 1. Using
the compounding formula (5.1.25), Hn(s) = sG(Hy_1(s)).

12. We have that

P(Zn > N, Zm = 0)

P(Zn > N | Zm =0) =

P(Zm = 0)
=> hm =0| Zn =N+r)P(Zn =N +r)
~ a P(Zm = 0)
= BGEnon = ON Pn = N47)
~ P(Zm = 0)
P(Zm = O)N+! _ _ aN _ N
s “PE = 0) 2 Pen =N+r)<P(Zm =0)" = Gn(0)”.

13. (a) We have that Gw(s) = Gy(G(s)) = eM(G)-)) Also, Gy(s)l/" = e(G()—D/n. the
same probability generating function as Gy but with A replaced by A/n.

(b) We can suppose that H(0) < 1, since if H(0) = 1 then H(s) = 1 for all s, and we may take A = 0
and G(s) = 1. We may suppose also that H(0) > 0. To see this, suppose instead that H(0) = 0
so that H(s) = s" Dix sJhj+r for some sequence (hz) and some r > 1 such that hy > 0. Finda

positive integer n such that r/n is non-integral; then H(s)!/" is not a power series, which contradicts
the assumption that H is infinitely divisible.

Thus we take 0 < H(0) < 1, andso0 < 1 — A(s) < 1 forO <s <1. Therefore
log H(s) = log(1 — {1 — H(s)}) =A(-1+ A(s))

where 4 = — log H(0) and A(s) is a power series with A(O) = 0, A(1) = 1. Writing A(s) =
D721 ajs/, we have that

di a
—{H(sje*}/"| = tay ton!)
ds/ n
s=0
asn —> oo. Now A(s)!/* is a probability generating function, so that each such expression is non-

negative. Therefore aj; > 0 for all j, implying that A(s) is a probability generating function, as
required.

14. It is clear from the definition of infinite divisibility that a distribution has this property if and only
if, for each n, there exists a characteristic function W, such that @(f) = w,(t)" for all ¢.

(a) The characteristic functions in question are
tt
N(w, 07): ot) =e
Poisson (A): (ft) = ere 1)

x “
TA,4): @@)= (=a) .

In these respective cases, the ‘nth root’ % of @ is the characteristic function of the N(u/n, o2 /n),
Poisson (A/n), and (A, 2/n) distributions.

258

Problems Solutions [5.12.15]-[5.12.16]

(b) Suppose that ¢ is the characteristic function of an infinitely divisible distribution, and let wy, be a
characteristic function such that #(t) = wn(t)". Now |@(f)| < 1 for all £, so that

1 if || 49,
_ l/n
oH|\= t ->
lYa(t)| = 1 { 0 if|o)| <0.
For any value of ¢ such that ¢(t) # 0, it is the case that W(t) > lasn > OO, To see this, suppose
instead that there exists 6 satisfying 0 < 9 < 2m such that y(t) > e° along some subsequence.
Then y(t)” does not converge along this subsequence, a contradiction. It follows that

. 1 if d(t) 49,
* t)= lim t=
(#) vO) = lim vn {6 fp) <0.

Now ¢ is a characteristic function, so that ¢(+) 4 0 on some neighbourhood of the origin. Hence
w(t) = 1 on some neighbourhood of the origin, so that y is continuous at the origin. Applying the
continuity theorem (5.9.5), we deduce that w is itself a characteristic function. In particular, y is
continuous, and hence y(t) = 1 for all t, by (x). We deduce that ¢(+) # 0 for all r.

15. We have that

_ ay PS=n|N=n)P(N=n) —_ p*P(N =n)
PIN =n | S=N)= YEP(SH=kKIN=HPN=KH YO, PEPIN =k)"

Hence E(x” | S = N) = G(px)/G(p).
If N is Poisson with parameter A, then

eM(px—l)

apaty = OP = GY.

EG’ |S=N)=
Conversely, suppose that E(x® | § = N) = G(x)?. Then G(px) = G(p)G(x)?, valid for |x} < 1,
0 < p <1. Therefore f(x) = log G(X) satisfies f (px) = f(p) + pf (x), and in addition f has a
power series expansion which is convergent at least for0 < x < 1. Substituting this expansion into the

above functional equation for f , and equating coefficients of p'x/, we obtain that f(x) = —A(1 — x)
for some A > 0. It follows that N has a Poisson distribution.

16. Certainly

1 — (py + p2)

1— (py *p)\!
1 — po — pys ,

n
) , Gyt)=Gyy(t= (
1 — pj — pat

1 — (py + p2) )
1 — (py + p2)s

Gx(s) = Gx y(s, 1) = (
Gxty(s) = Gx y(s,5) = (

giving that X, Y, and X + Y have distributions similar to the negative binomial distribution. More
specifically,

WX =h= ( ‘ata -a)", PY=hH= ( ‘at — B)",

P(X+Y¥=b= ("Eo tao

for k > 0, where @ = p;/(1 — p2), B = p2/(1 — pi), y = pi t+ po.

259

[5.12.17]-[5.12.19] Solutions Generating functions and their applications
Now
PY=y)  B
where A is the coefficient of t” in Gy y(s, t) and B is the coefficient of t” in Gy (t). Therefore

1— pi —p2\" y 1—pi—po\" y
BO% IY =y)= ( al 22) ( P2 )/XC P\ ?2) ( P2 )}
1— pis 1 — pis 1— py l— py
1-p, n+y
= (7-74)

17, As in the previous solution,

E(s* |Y=y)=

Gx(s) = e@tyys—D Gy(s) = eBtyG-) Gxyy(s) = e(@t+BYs—Dey(s2=1)_
18. (a) Substitute u = y/a to obtain
[o.@)
I(a,b)= [ exp(—y = a*b?y~*)a7! dy= aI, ab).
0

(b) Differentiating through the integral sign,

lee)
ie = [ {-3 exp(—a7u? - vu} du
0 u

[o.<)
=— [ 2exp(—a*b*y~? — y*) dy = —21 (1, ab),
0

by the substitution u = b/y.
(c) Hence 91 /8b = —2al, whence I = c(a)e~24” where

c(a) = 1(a,0) = f ena du= ve
0

a

(d) We have that
oo d
Ee") = i et _¢-¢/8-85 dy = 2dI(./g #1, Ve)
0 Vx

by the substitution x = y?.

(e) Similarly
ad 1 2 1
E(e?*) = | et eg VO) gy = 4f—1 (= vi)
0 V2 x3 x \J2

by substituting x = y~?.

19. (a) We have that

E(eit¥) = E{E(e*/ | ) = Elox(t/¥)} = Ble2"/?}
_ [% 42? y? -/2 (; 4) _ cit
= [age w= Ft (aa)

260

Problems Solutions [5.12.20]-[5.12.22]

in the notation of Problem (5.12.18). Hence U has the Cauchy distribution.

(b) Similarly
oo 2 1 1,2 2 1
E(eY -| e e 2* dx=4/—I (=~) — eo V2t
( ) 60 om Vz a

for t > 0. Using the result of Problem (5.12.18e), V has density function

f@)= a eT/Qx) x >0.

V2rx3

(c) We have that W~2 = X~2 4 y~2 + Z~2. Therefore, using (b),
E(etW) =e 3V2t = eV 18 = Bie 9)

for t > 0. It follows that W~? has the same distribution as 9V = 9X~2, and so W” has the same
distribution as aX 2 Therefore, using the fact that both X and W are symmetric random variables, W

has the same distribution as 3 X, that is N(0, §).

20. It follows from the inversion theorem that

Fax +h) — F(x) _ 1 jim N 1 —e7ith -ltxg() i
h 2% N>co JN it ,

Since |@| is integrable, we may use the dominated convergence theorem to take the limit ash | 0
within the integral:

fe ~ Om N->00 _N- ¢ ‘

The condition that ¢ be absolutely integrable is stronger than necessary; note that the characteristic
function of the exponential distribution fails this condition, in reflection of the fact that its density
function has a discontinuity at the origin.

21. Let G, denote the probability generating function of Z,. The (conditional) characteristic function
of Z,/p" is

, it/u™)_G_ ©
itZn/u” = Gn(elti#) Gn(0)
E(e | Zn > 0) 16,0)

It is a standard exercise (or see Example (5.4.3)) that

we” —1— psu"! —1)
G = ;
n(s) wrt) —1— ps(u" — 1)

whence by an elementary calculation

u-l

itZn/ pe”
E(e n | Zn > 0) >

asn —> 00,

the characteristic function of the exponential distribution with parameter 1 — u~!.

22. The imaginary part of @x(t) satisfies
5${x(t) — dx} = 3{6x(1) — ox(-)} = 4 {Ee#*) — Ee} = 0

261

[5.12.23]-[5.12.24] Solutions Generating functions and their applications

for all t, if and only if X and —X have the same characteristic function, or equivalently the same
distribution.

23. (a) U = X + Y and V = X - ¥ are independent, so that ¢y4y = $y ¢y, which is to say that
g2x = bx+yex-_y, Or

$(2t) = {P(t}? Hb@o(—t)} = 76-2).
Write w(t) = o(t)/o(—t). Then
b2t) — o)3o(—t) _

2t) = = = 2
4) = Gn one
Therefore ,
v@=v¥40 = wiint =.= wee)" forn > 0.
However, as h — 0,
_ 1,2 2
y(n) = 2 8 a FO) M1 os,

o(-h) 1 — 43h? + 0(h?)

so that Y(t) = {1 + o(t?/22ny}?" — lasn — oo, whence y(t) = 1 for all ¢, giving that
$(-t) = (t). It follows that

g(t) = oh o(—4n = on = 90/2") — forn = 1

g2n
1 2? _1,2
= {1-5 se toe} > e 2! asn—> oO,

so that X and Y are N(O, 1).
(b) With U = X + Y and V = X — Y, we have that #(s, t) = E(eSUt+"V) satisfies

(*) e(s, t) = Eel tOX4iS DY) — gs 4 t)o(s — 1).

Using what is given,

a = -E(V7 iY) = —EfeYE(V? | U)} = —EQe!Y) = —24(5)?.
t=0
However, by (*),
a*y " hogy
Sr) = 2d") - 6'65)"},
 |r=0

yielding the required differential equation, which may be written as
d
qe! ¢)=-1.
s

_1.2
Hence log (s) =a +bs — 587 for constants a, b, whence ¢(s) =e 2° .

24. (a) Using characteristic functions, z(t) = dx (t/n)" = eT,
(b) E|X;| = 00.

262

Problems Solutions [5.12.25]-[5.12.27]

25. (a) See the solution to Problem (5.12.24).
(b) This is much longer. Having established the hint, the rest follows thus:

OO
fray) / Fa) fy —x)dx
00

OO
== / {F@) +f —x)} dx 4 Jey) =
00

(4+ y2) +780)

2
m(4+y?)

where
o.¢)
r= | {xf(x) + (vy —x) f(y —x)} dx
—ooO

- jj i 2) _ ayy _
= yi FE {log(1 + x“) — log(1 + (y — x) | =0.

Finally,

z=2 2z) = ————.
fz(@) = 2fx+y Q2z) nde
26. (a) Xj + Xg+---+ Xn.
(b) X; —X - where X, and X 1 are independent and identically distributed.

(c) Xn, where N is a random variable with P(N = j) = pj; for 1 < j < a, independent of
X1, X2,...,Xn-

(d) we Zj where Z), Z2,... are independent and distributed as X,, and M is independent of the
Z; with P(M = m) = (3)+! for m > 0.
(e) YX, where Y is independent of X, with the exponential distribution parameter 1.

itx
o(t) = [ 28 ay.

00 etx + etx

27. (a) We require

First method. Consider the contour integral

2 e tz
k= | ee
where C is a rectangular contour with vertices at +K,+K +i. The integrand has a simple pole at
1
Z= 5i , with residue e~ 2° /(iz). Hence, by Cauchy’s theorem,
1
Qe 2! 1

> >= i as K — oo.
l+e cosh(5t)

Ik

Second method. Expand the denominator to obtain

1

co
a _1)k _
cosh(7x) 2 1)* exp{—(2k + Iz}.

Multiply by e!* and integrate term by term.

263

[5.12.28]-[5.12.30] Solutions Generating functions and their applications
(b) Define ¢(t) = 1 — |t| for |¢| < 1, and ¢(t) = 0 otherwise. Then
aco dt = — Af e*( — |e) at
Qn

= " (1 —t) cos(tx) dt = oa — cos x).
x Jo UX

Using the inversion theorem, ¢ is the required characteristic function.

(c) In this case,
eo | _ oO .
/ eX eX dy = | y “ey dy =T(1 —it)
—00 0

where I is the gamma function.
(d) Similarly,

oO. oO.

{| tear + | ete ar}
0 0

lft, ty

—2\l-it 1 4itf 7 1422"

-r'(

1). Now, Euler’s product for the gamma function states

(e) We have that E(X) = —i¢’(0) =

that
nin®

ali, z(z+1):--(z@+n)
where the convergence is uniform on a neighbourhood of the point z = 1. By differentiation,

n 1 1
(1) = li —— [losn—-1—-_—-—.---— — \ba_y
” di, (a (tose 2 wi)} y

28. (a) See Problem (5.12.27b).

(b) Suppose ¢ is the characteristic function of X. Since ¢’(0) = ¢”(0) = ¢’” (0) = 0, we have that
E(X) = var(X) = 0, so that P(X = 0) = 1, and hence ¢(t) = 1, a contradiction. Hence ¢ is not a
characteristic function.

(c) As for (b).

(d) We have that cost = 5 (ett +e) whence ¢ is the characteristic function of a random variable

T@)=

taking values +1 each with probability 3.
(e) By the same working as in the solution to Problem (5.12.27b), @ is the characteristic function of
the density function

1— |x| if |x| <1,
x)=
F@) { 0 otherwise.

29. We have that

[1 — o(t)| < Ell — e**| = EV — eitXy(] — e-itXy
= Ey/2{1 — cos(tX)} < E|rX|

since 2(1 — cosx) < x? for all x.

30. This is a consequence of Taylor’s theorem for functions of two variables:

865.1) = > ~~ dbmn(0, 0) + Ruw(s. 1)

m<M
n=<N

264

Problems Solutions [5.12.31]-[5.12.33]

where ¢mn is the derivative of ¢ in question, and Ryyy is the remainder. However, subject to appro-
priate conditions,

os,t)= >> COO wx") + ofsMe N)

m<M
n<N
whence the claim follows.
31. (a) We have that
x? < x2 x4 <]
3 Ss a _ at <1-—cosx

if |x| < 1, and hence
| (tx)* dF (x) < [ 3{1 — cos(tx)} dF (x)
(-t-1 yey [—-t-1 2-1]
< 3 f° {1 —cos(tx)} dF (x) = 3{1 —Re d()}.
o@)

(b) Using Fubini’s theorem,

1 t [o.@) 1 t
“i {1 — Re d(v)} dv -| “| {1 — cos(vx)} dud F(x)
t Jo =-oo f Jy=0
=-[- (1- =) dF (x)
00 tx
> / . (1 _ =) dF (x)
|tx|>1 ix

since 1 — (tx)! sin(tx) > Oif |tx| < 1. Also, sin(tx) < (tx)sin 1 for |tx| > 1, whence the last
integral is at least

| x (—sin1) dF(x) = }pqx| 217.
It

x|>1

32. It is easily seen that, if y > 0 and n is large,

PCa Ma) > ») = (Mn < 1-7) = [] (xi <1-2)

Il
ms
=

|
ss |<<
Ne
=

bss)

I

4

33. (a) The characteristic function of Y,, is

W(t) = Efexp(it(X — ay/va)} = exp{a(eit/V> — 1) - itvVA} = = exp{—4 Lye 4 o(1)}

as A. —> co. Now use the continuity theorem.

(b) In this case,
. i A
th) = ent ( - “z) ;

so that, as 1 —> oo,

tt.

re

; . 2
log Wit) = —it VA — Alog ( - =) = —itVE 4A (5-3 -£ soa) + -

265

[5.12.34]-[5.12.36] Solutions Generating functions and their applications

(c) Let Z, be Poisson with parameter n. By part (a),

Zn —n
P( Ti <0) + 90)=}

where ® is the N(O, 1) distribution function. The left hand side equals P(Zn <n) = "fo eink /Kt.

34. If you are in possession of r — 1 different types, the waiting time for the acquisition of the next
new type is geometric with probability generating function

(n—r-+l)s

Therefore the characteristic function of Uy, = (T, — nlogn)/n is

n n . |;
“ita” TT Gye! “TT (i —r + VDelt/n natn!
Un(t) =e itlogn Gr(elt/") =n it
r=1

ae n—(r — leit/n ~ Tita) (neit/n _ r)

The denominator satisfies

n—1 n—l
[[ @e*”" -7) = 400) [[@-it-+)
r=0 r=0

as n —> 00, by expanding the exponential function, and hence
-it n!
lim yn(t) = lim a Sr =T(1 —in),
n-> 00 n->0o IVa (n—it—r)
where we have used Euler’s product for the gamma function:

nin®

Tro +r)

the convergence being uniform on any region of the complex plane containing no singularity of T.
The claim now follows by the result of Problem (5.12.27c).

35. Let X, be uniform on [—n, n], with characteristic function

> TZ) an>w

sin(at)
nq..,
dn(t) = / aye dx =) nt itr £0,
—n on 1 ift =0.

It follows that, as n —> 00, dy (t) —> dp;, the Kronecker delta. The limit function is discontinuous at
t = 0 and is therefore not itself a characteristic function.

36. Let G;(s) be the probability generating function of the number shown by the ith die, and suppose
that
5 2 (l-s Il )

12
G1 (s)G2(s) = Ss; is = ,
Ga" 11 —s)

so that 1 —s41 = 1101 - 5)H, (s)H2(s) where H;(s) = s!G; (s) is a real polynomial of degree 5.
However

5
1—s''=(1—s) |] @ -s)\@-s)
k=1

266

Problems Solutions [5.12.37]-[5.12.38]

where 1, @1,...,@5, @5 are the ten complex eleventh roots of unity. The wz come in conjugate
pairs, and therefore no five of the ten terms in The (wx — 5)(@_ — 5) have a product which is a real
polynomial. This is a contradiction.

37. (a) Let H and T be the numbers of heads and tails. The joint probability generating function of
H and T is

Gu,r(s,t) = E(s™ 17) = Es" tN—#) = E{E((s/t)# 2% |N)} =E {e” (4 + my"

where p = | — q is the probability of heads on each throw. Hence
Gu,r(s,t) = Gn(qt + ps) = exp {A(gt + ps — 1)}.
It follows that
Gu(s)=Gurs,=eP8Y), Gp) = Gyr, = e4EY,

so that Gy 7(s,t) = Gy(s)Gr(t), whence H and T are independent.
(b) Suppose conversely that H and T are independent, and write G for the probability generating
function of N. From the above calculation, Gy ,7(s, t) = G(gt + ps), whence Gy (s) = G(q + ps)
and Gr(t) = G(qt + p), so that G(qt + ps) = G(q + ps)G(qt + p) for all appropriate s, +. Write
f(@) = G(1 — x) to obtain fx + y) = f(x) f(y), valid at least for allO < x,y < min{p, q}.
The only continuous solutions to this functional equation which satisfy f(0) = 1 are of the form
f(x) = e for some yz, whence it is immediate that G(x) = e*@—1) where A = — LL.
38. The number of such paths x containing exactly n nodes is 2"—!, and each such z satisfies
P(B(x) = k) = P(Sn > k) where Sp = Y, + Yo +---+ Yy is the sum of n independent Bernoulli
variables having parameter p (= 1 — q). Therefore E{X,(k)} = 2°-1P(S, > k). We set k = nf,
and need to estimate P(S, > nf). It is a consequence of the large deviation theorem (5.11.4) that, if
psB<1,

P(Sy > nB)!/" —> inf {ePuay}

t>0

where M(t) = E(e*¥1) = (g + pe’). With the aid of a little calculus, we find that

B _ 1-g .
P(Sp > np)'/" > (3) (3) . ps<Bpe<i.

B 1-8
Hence 0 ify) <1
Wy <i,
BUM) > { if y(B) > 1,
where

B 1-B
P 1—p
y(B) =2 (3) ((=3)
B 1—B
is a decreasing function of 8. If p < 5s there is a unique B. € [p, 1) such that y(6,) = 1; if p > 5

then 7 (8) > 1 for all 6 € [p, 1) so that we may take 6, = 1.
Turning to the final part,

P(Xn(Bn) = 1) < E{X,(8n)} > 0 if B > Be.
As for the other case, we shall make use of the inequality

E(N)*

() PN £0) > Soy

267

[5.12.38]-[5.12.38] Solutions Generating functions and their applications

for any N taking values in the non-negative integers. This is easily proved: certainly
var(N | N #0) = E(N? | N #0)—E(N | N #0)" 20,

whence
E(N2) . EWN)?

PIN £0) ~ P(N £0)2°

We have that E{X, (Bn)*} = Vex,p E(/7 Jp) where the sum is over all such paths z, p, and I, is the
indicator function of the event {B(z) > Bn}. Hence

E{Xn(Bn)?} = S$) Ex) + 5) EUnlp) = E{Xn(Bn)} +2"7! S> EU Ip)
7 wep p#L

where L is the path which always takes the left fork (there are 2”—! choices for 2, and by symmetry
each provides the same contribution to the sum). We divide up the last sum according to the number

of nodes in common to p and L, obtaining a 2n—m—l RI, Ij.) where M is a path having exactly
m nodes in common with L. Now

EUzIu) = EUy | In = DEUL) < PCh—-m = Bn — m)JECU)

where Ty—m has the bin(n — m, p) distribution (the ‘most value’ to [jy of the event {77 = 1} is
obtained when all m nodes in L M M are black). However

Edy) = Pn = Bn) > p™P(Th—-m = Bn —m)

so that EU, Iy) < p~™ EU, )ECUy). It follows that N = Xy,(Bn) satisfies

nal n-1 m
E(N?) < E(N) +2771 S* aneml a BAIL IEC) = E(N) + $E(N)? S> (;.)

m=1 m=1

whence, by (+),
1

E(N)~! +5 Oy 2p)
If 6 < B- then E(N) — oo asn —> oo. It is immediately evident that P(N # 0) > lif p < 4.
Suppose finally that p > 5 and 8 < B-. By the above inequality,

P(N #0) >

(4%) P(Xn(Br) > 0) > c(B) for all n
where c(f) is some positive constant. Find « > 0 such that 6B +€ < £,. Fix a positive integer m, and

let Pm be a collection of 2” disjoint paths each of length n — m starting from depth m in the tree.
Now

P(Xn(Bn) = 0) < P(B(v) < Ba forall v € Pm) =P(B() < Bn)”
where v € Pm. However
P(B() < fn) < P(B(W) < (B + €)(n — m))
if Bn < (B + €)(n — m), which is to say that n > (6 + €)m/e. Hence, for all large n,
P(Xn(Bn) =0) < {(1-c(B +0)"

268

Problems Solutions [5.12.39]-[5.12.42]

by (4); we let n — oo and m — oo in that order, to obtain P(X, (86n) = 0) — Oasn > oo.

39. (a) The characteristic function of X» satisfies
. Ra REN? a: n .
E(el’Xn) — ( —=+ * ei) = ( + —[e* - 11) —> exp (Afe”’ — 1),
n n n

the characteristic function of the Poisson distribution.
(b) Similarly,

p e t/n x
1-—(1— p)eit/n Rit

E(el!¥n/n) _

as n —> 00, the limit being the characteristic function of the exponential distribution.

40. If you cannot follow the hints, take a look at one or more of the following: Moran 1968 (p. 389),
Breiman 1968 (p. 186), Loéve 1977 (p. 287), Laha and Rohatgi 1979 (p. 288).

41. With ¥, = kX,, we have that E(%,) = 0, var(Y,) = k*, E|Y¥~| = k3. Note that S, =
Y, + Yo +---+ Yn is such that

1 n 3 n4
War(Sp)372 2 Blt |~ cg > 0

as n —> oo, where c is a positive constant. Applying the central limit theorem ((5.10.5) or Problem

(5.12.40)), we find that
Sn

J/var Sp

3

», N(O,1), asn —> oo,

where var Sy = S~t_, k? ~ an asn > 0.

42. We may suppose that 4 = 0 and o = 1; if this is not so, then replace X; by ¥; = (X; — w)/o.
Let t = (to, ti, f2,.--, tn) € Ret and set # = n7! vje1 t;. The joint characteristic function of the

n+ 1 variables X, Z1, Z2,.-., Zn is

g(t) = neta e{ [loo (: \2 +t =i] x;)}

by independence. Hence
p(t) = ex 1 ys [% + (t} -f , = ex 9 ly (t; 1?
~ P93 » nd ~ PL On 32 “i

where we have used the fact that vj=1 (t; — f) = 0. Therefore

o(t) = E(e*)E (eof Sag - xi}) = E (ei JE ye (eof s12}).

whence X is independent of the collection Z|, Z2,..., Zn. It follows that X is independent of
S=m-Nt ye, Zi. Compare with Exercise (4.10.5).

269

[5.12.43]-[5.12.47] Solutions Generating functions and their applications

43. (i) Clearly, P(Y < y) = P(X < log y) = ®dlog y) for y > 0, where ® is the N(0, 1) distribution
function. The density function of Y follows by differentiating.
(ii) We have that fa(x) > Oif |a| < 1, and

xJ/20 -o V2n

since sine is an odd function. Therefore f noe fax) dx = 1, so that each such fg is a density function.

o 1 L dog x)2 oO 1 1,2
asin(2z log x) e 2 (logx) ax= | asin(Qay)e 2% dy=0
i aa y y

For any positive integer k, the kth moment of f; is LS xk f(x) dx + Igtk) where

ln / ed a sinQryye- 2” ay =0
= S1. é€ =
a = ons y y

since the integrand is an odd function of y — k. It follows that each fy has the same moments as f.

44. Here is one way of proving this. Let X,, X2,... be the steps of the walk, and let S, be the
position of the walk after the nth step. Suppose 4 = E(X}1) satisfies uw < 0, and let e, = P(S, =
0 for some n > 1 | Sg = —m) where m > 0. Then em < yy P(T, > m) where T, = X1 + X24+
+++ +X, = Sy — So. Now, fort > 0,

PCT, > m) = P(T, —ny > m—np) < e7 fm—ni) Re! Tn~n)) =e fm fe" M(t) }"
where M(t) = E(e'(%1-)), Now M(t) = 1+ O(t2) ast —> 0, and therefore there exists ¢ (> 0) such
that 0(t) = e’# M(t) < 1 (remember that jz < 0). With this choice of t, ¢m < S772) e"6(1)" > 0
as m — oo, whence there exists K such that ey, < 5 form > K.

Finally, there exist 5, € > 0 such that P(X; < —é) > e, implying that P(Sy < —K | Sg =0) >
é where N = [K/65], and therefore

P(S, # 0 for alln > 1| Sp =0) > (1—-ex)e% > de;

therefore the walk is transient. This proof may be shortened by using the Borel—Cantelli lemma.

45. Obviously,
a if X, >a,

={ yal if X, <a,

where I. has the same distribution as L. Therefore,

E(s") = s?P(X, > a) + So sE(s) P(X] =r).

r=1
46. We have that
Ww { W,-1+1 with probability p,
"| Wy-1+1+ Wa with probability g,

where Wr is independent of W,,; and has the same distribution as W,,. Hence G,(s) = psGy_1(s)+
gsGp—(s)Gy(s). Now Go(s) = 1, and the recurrence relation may be solved by induction. (Alter-
natively use Problem (5.12.45) with appropriate X;.)

47. Let W, be the number of flips until you first see r consecutive heads, so that P(Ln <r) =

P(W, > n). Hence,

1—-E(s")
1-s

[o@) [o.<)
1+ So s*P(Ln <r= >> s"P(W >n=

n=1 n=0

’

270

Problems Solutions [5.12.48]-[5.12.52]

where E(s¥) = G;(s) is given in Problem (5.12.46).

48. We have that
3X n with probability 5,
Xx n+1 =

5Xn+Yn with probability 4.

Hence the characteristic functions satisfy

‘ , a
Gari (t) = EC *n41) = 5430) + 34n(Q0 —
1; 1, oA
A — ait A— git A —it2-" x
1 2 1 4 —n
= at = _1(st — 12
ona gn IGN > $1 ( ) hit narearr

asn —> oo. The limiting distribution is exponential with parameter A.

49. We have that
1
1
0 +

1 1 got
[ G(s)ds =E i s*ds\) =E
0 0 X+1

(a)(l-e~*)/a, (6) -(p/q?)(q+log p), (c)—9"*!)/[@+1)p], @) —[1+(p/9) log p]/log p.
(e) Not if P(X + 1 > 0) = 1, by Jensen’s inequality (see Exercise (5.6.1)) and the strict concavity of
the function f(x) = 1/x. If X + 1 is permitted to be negative, consider the case when P(X +1 =
-) =P(X+1=1) =}.

50. By compounding, as in Theorem (5.1.25), the sum has characteristic function

péx(t) __P
1—q¢x(t)  Ap—it’

Gy(¢x(t)) =

whence the sum is exponentially distributed with parameter Ap.

51. Consider the function G(x) = {E(X*)}~! f*,, y? dF(y). This function is right-continuous and
increases from 0 to 1, and is therefore a distribution function. Its characteristic function is

OO gitx 2 2
I. EK” dF(x) = ~ ER?) G2?

52. By integration, fy(x) = fy(y) = 4, |x| < 1, ly < 1. Since f(x, y) # fx(x) fy(y), X and Y
are not independent. Now,
1(¢42) if -2<z<0,

1
= , — d =
fxty@) I, FQ, 2 — x) dx iio if0<z <2,

the ‘triangular’ density function on (—2, 2). This is the density function of the sum of two independent
random variables uniform on (—1, 1).

271

6

Markov chains

6.1 Solutions. Markov processes
1. The sequence X 1, X,... of independent random variables satisfies
PUXng1 = J | X1 =i1,...,Xa = in) = PXng1 = S),

whence the sequence is a Markov chain. The chain is homogeneous if the X; are identically distributed.

2. (a) With Y,, the outcome of the nth throw, X,41 = max{Xn, Y;,+41}, so that

0 ifj<i
py =4 gi iff=i
i iff >i,

for 1 <i, j <6. Similarly,

(n) 0 ifj <i
s(n) =
Pij (gi)” if fai.

If 7 > i, then pjj (1) = P(Z, = j), where Zp = max{Yj, Y2,..., Yn}, and an elementary calculation

yields
‘\n - n
py() = (2) -() , i<j <6

(b) Nn+1 — Nn is independent of N,, N2,..., Nn, so that N is Markovian with

if j=i+1,

Pi = if j =i,

Oo an ae

otherwise.

(c) The evolution of C is given by

0 if the die shows 6,
Cr+ = :
C,+1~ otherwise,
whence C is Markovian with
Pij = j=it+l,
otherwise.

Oo awn ane

272
Markov processes Solutions [6.1.3]-[6.1.4]

(d) This time,
B,—-1 if B, > 0,
Br+1 = :
Y; if B, = 0,
where Y; is a geometrically distributed random variable with parameter 1. independent of the sequence
Bo, Bo,--- , Bry. Hence B is Markovian with

1 if j=i-120,
PU) (Sit) iti =0, f= 1.

3. (i) If X, =i, then X,41 € {i — 1,i +1}. Now, fori > 1,
(*)

P(Xngy, =i t1 | Xn =i, BY = P(Xyn4] =i +1 | Sx =i, BYP(Sp =i | Xn =i, B)

+ PUXn41 =i +1 | Sp =i, BPS, = —i | Xn =i, B)
where B = {X; =i, forO <r <n} and iop,i;,...,i,—1 are integers. Clearly
P(Xng1 =itl|S,=i,By=p, P(Xn41 =i+1|S, = -i, B)=4@,

where p (= 1 — q) is the chance of a rightward step. Let / be the time of the last visit to 0 prior to
the time n, / = max{r : iy = 0}. During the time-interval (/, n], the path lies entirely in either the
positive integers or the negative integers. If the former, it is required to follow the route prescribed by
the event B 1 {S, = i}, and if the latter by the event B 9 {S, = —i}. The absolute probabilities of
these two routes are

pret) gin)

1 : 1 :
T= pied g2@-t-i | 1 =

whence
m pt
m+. p+q@

P(Sp =i | Xn =i, BY = =1-P(S, = —-i | Xp =i, B).

Substitute into («) to obtain
pit! + git}

P(Xn41 =i+1| Xn =i, B)= Tag

=1-P(X,4) =i-1|Xn =i, B).

Finally P(X,41 =1| Xn =0, B) =1.

(ii) If Y, > 0, then Y, — Y,,41 equals the (n + 1)th step, a random variable which is independent of
the past history of the process. If Y, = 0 then S, = Mn, so that Y,,4.1 takes the values 0 and 1 with
respective probabilities p and g, independently of the past history. Therefore Y is a Markov chain
with transition probabilities

if jazi-l if; =0
fori > 0, py = 4? _ ; poj = 4? ‘
q ifj=it+, q ifj=1.
The sequence Y is a random walk with a retaining barrier at 0.
4. For any sequence ig, ij, ... of states,
P(Xn, =is forO<s <k +1)
P(Xn, =is forO < s < k)

PV ear = ike. | Yr = ir forO<r <k) =

k
= Fe
[=o Pis.is44 @s+1 — Ms)
= Pig iggy Me+1 — Mk) = Peg. = tet | Ye = ik),

273

[6.1.5]-[6.1.9] Solutions Markov chains

where pj;; (7) denotes the appropriate n-step transition probability of X.
(a) With the usual notation, the transition matrix of Y is

pe iff=it+2,
Nip = \ 2pq if j =i,
qg ifj=i-2.
(b) With the usual notation, the transition probability 7;; is the coefficient of s/ in G(G(s))!.
5. Writing KX = (X 1, X2,..., Xn), we have that

P(F, 1(X) = 1, Xn =i)
P(1(X) = 1, Xn =i)

P(F | 1(X) = 1, X, =i) =

where F is any event defined in terms of X,, X,41,.... Let A be the set of all sequences x
(x1, %2,.--,X,—1, 2) of states such that J (x) = 1. Then

P(F, 1(X) =1,X, =i) = $0 P(F,X =x) = PCF | Xn =i) So PK =)

xéA xeA

by the Markov property. Divide through by the final summation to obtain P(F | 1(X) = 1, Xn
i) =P(F | X, =i).

6. Let A, = {Xz = x, forO <k <n, X, =i}. The required probability may be written as

P(Hr) P(r)

Now P(X74m = j | Ar, T =n) = P(Xntim = j | An, T =n). Let I be the indicator function of
the event H, 1{T = n}, an event which depends only upon the values of X1, X2,..., Xn. Using the
result of Exercise (6.1.5),

P(Xntm = j | Hn, T =n) = P(Xn4m =f | Xn =i) = py (m).
Hence

pij(m) >>, Pn, T =n)
P(Ay)

P(Xr4m = j| Hr) = = pij(m).

7. Clearly
PO n41 = J | ¥ =i, forO <r <n) = P(X, 41 = 0 | X,y = a, forO <r <n)

where b = h-(), ar = h-1G,); the claim follows by the Markov property of X.

It is easy to find an example in which / is not one-one, for which X is a Markov chain but Y is
not. The first part of Exercise (6.1.3) describes such a case if Sp # 0.

8. Not necessarily! Take as example the chains S and Y of Exercise (6.1.3). The sum is S, + ¥, =
Mn, which is not a Markov chain.

9. Allof them. (a) Using the Markov property of X,
P(Xm+r =k | Xm =im, ---, Xm+r-1 = im+tr-1) = PXm+r =k | Xmtr-1 = imtr—-1)-

274
Classification of states Solutions [6.1.10]-[6.2.1]
(b) Let {even} = (Xo, = in, forO <r < m} and {odd} = {X2,41 = i2,4) forO <r < m— I}.
Then,

P(Xoms2 =k, Xom+1 = iam41, even, odd)
P(X2m+2 = k | even) = >’ ma Pleven) =

_ Soy am =k, Xom41 = iam4i | Xam = i2m)P(even, odd)
P(even)

= P(Xom42 =k | Xam = im),
where the sum is taken over all possible values of is for odd s.
(c) With Y, = (Xn, Xn+1);
P(Yng1 = (k,1)| Yo = (io, i1), ---. Yn = Gn K)) = Pata = kD) | Xn =)
= P(¥n41 = (kD) | Yn = Gin, ),
by the Markov property of X.
10. We have by Lemma (6.1.8) that, with wo = P(X; = j),
(1) _ _ (1)
Mxy Pxyxg °° * Px,_1,k Pk,x,44 Pxy_1%n Mxy Px,_1,k Pkyxp41
LHS = =H

1 1
HD === Dry yxpq12)-°* Prin ttn US? Pay 1xp41 (2)

= RHS.

11. (a) Since S,41 = S, + X,41, a sum of independent random variables, § is a Markov chain.
(b) We have that

Png =k | ¥; = xj +2j-1 for] <i <n) = PO,41 =k | Xn = xn)

by the Markov property of X. However, conditioning on X,, is not generally equivalent to conditioning
on Y, = Xn + X,_1, so Y does not generally constitute a Markov chain.

(Cc) Z, =nX1+(n— 1)X24+---+ Xn, so Z,41 is the sum of X,,4 1 and a certain linear combination
of Z;, Z2,... , Zn, and so cannot be Markovian.

(d) Since Sy41 = Sn + Xn41, Zn41 = Zn + Sn + Xn41, and X,+41 is independent of X1,..., Xn,
this is a Markov chain.

12. With 1 arow vector of 1’s, a matrix P is stochastic (respectively, doubly stochastic, sub-stochastic)
if P1’ = 1 (respectively, 1P = 1, P1’ < 1, with inequalities interpreted coordinatewise). By recursion,
P satisfies any of these equations if and only if P” satisfies the same equation.

6.2 Solutions. Classification of states

1. Let Ag be the event that the last visit to i, prior to n, took place at time k. Suppose that Xo = i,

so that Ag, Aj,..., An—1 forma partition of the sample space. It follows, by conditioning on the Aj,
that
n-1
Pig) = >> pi (Oliy @ — &)
k=0

for i # j. Multiply by s” and sum over n (> 1) to obtain Pjj(s) = Pij(s)Lij() fori 4 j. Now
Pi (s) = Fjj(s) Pjj(s) if i ¥ j, so that Fyj(s) = Li; (s) whenever P;;(s) = P;;(s).

As examples of chains for which P;;(s) does not depend on i, consider a simple random walk on
the integers, or a symmetric random walk on a complete graph.

275

[6.2.2]-[6.3.1] Solutions Markov chains

2. Leti (4 5) bea state of the chain, and define n; = min{n : pjs() > 0}. If Xo =i and Xn, =s
then, with probability one, X makes no visit to i during the intervening period [1, n; — 1]; this follows
from the minimality of nj. Now s is absorbing, and hence

P(mo return toi | Xp =i) = P(Xn, =5 | XQ =!) > O.

3. Let J, be the indicator function of the event {X; = i}, so that N = 0 J, is the number of
Visits to i. Then

[o.<) [o@)
E(N) = )> E(k) = 9° pi)
k=0 k=0

which diverges if and only if i is persistent. There is another argument which we shall encounter in
some detail when solving Problem (6.15.5).

4. Wewrite P;(-) = P(- | Xo =). One way is as follows, another is via the calculation of Problem
(6.15.5). Note that P}(V; > 1) = P,(7j < 00).
(a) We have that

P,(V; > 2) =P,(V; > 2| Vi = DPJ; = 1) =P (VY; = 1)?

by the strong Markov property (Exercise (6.1.6)) applied at the stopping time 7;. By iteration, P;(V; >
n) =P, (V; = 1)”, and allowing n — oo gives the result.
(b) Suppose i # j. Form > 1,

P)(Vj = m) =P; (Vj > m | Tj < co)Pi(Tj < 00) = P)(Vj > m — 1)P\(Tj < 00)

by the strong Markov property. Now let m —> oo, and use the result of (a).

5. LetO =P(Tj < Tj | X» =i) = PCG < Tj | Xo = J), and let N be the number of visits to j
before visiting i. Then

P(N > 1| Xo =i) =P(T; < T; | Xo =i) = 9.

Likewise, P(N > k | Xp =i) =@(1 - gyk-1 for k > 1, whence

[o.@)
E(N | Xo =i) = 5 91-0)! =1.
k=1

6.3 Solutions. Classification of chains

1. Ifr = 1, then state i is absorbing for i > 1; also, 0 is transient unless ag = 1.

Assume r < 1 andlet J = sup{j : a; > 0}. The states 0, 1, ..., J form an irreducible persistent
class; they are aperiodic if r > 0. All other states are transient. For 0 < i < J, the recurrence time
T; of i satisfies P(T; = 1) =r. If T; > 1 then 7; may be expressed as the sum of

7) ‘= time to reach 0, given that the first step is leftwards,
7 := time spent in excursions from 0 not reaching /,

7?) := time taken to reach i in final excursion.

276
Classification of chains Solutions [6.3.2]-[6.3.3]

It is easy to see that E(T’?) = 1+ — 1)/U —r) if i > 1, since the waiting time at each

intermediate point has mean (1 — r)—!. The number N of such ‘small’ excursions has mass function
P(N =n) = a;(1 — aj)", n > 0, where aj = Vjai aj; hence E(N) = (1 — @;)/a;. Each such small
excursion has mean duration

i-1 : i-1
J aj
So +1) “= *L aac

j=0

and therefore

i-l .
@)_ } . J4j
E(T; )= 2 fa-a+ yr Ze},
j=0
By a similar argument,

E(7,°) = — 4.
)=2 (14 fe) a

Combining this information, we obtain that
[o.<)
ET) =r +(1—neE(T +7. + 7.9) = z(t -r +) ia), i>,
a; ‘

and a similar argument yields E(7p) = 1+ 5° jp 10; /(i—r). The apparent simplicity of these formulae
suggests the possibility of an easier derivation; see Exercise (6.4.2). Clearly E(T}) < oo fori < J
whenever )> j jaj < 00, a condition which certainly holds if J < oo.

2. Assume that 0 < p < 1. The mean jump-size is 3p — 1, whence the chain is persistent if and
only if p = 4; see Theorem (5.10.17).

3. (a) All states are absorbing if p = 0. Assume henceforth that p 4 0. Diagonalize P to obtain
P = BAB! where

1 1 1

prot wal(t 2 A
B={1 0 -1], Bl=/4 0 -4],

1 -1 1 1 _1 1

4 2 4

1 0 0
A=|0 1-2p 0 .
0 0 1-4p

1 0 0
BA"B-'=B[0 (1—2p)" 0 Bo!
0 0 (1— 4p)"

Therefore

whence pj; (11) is easily found.
In particular,

pin) = 4+ 41 — 2p)" +401 - 4p)”, pag) = 5 + 41 - 4p)”,
and p33(n) = pi1(1) by symmetry.

277

[6.3.4]-[6.3.5] Solutions Markov chains

Now F;;(s) = 1 — P(s)7!, where

1 1 1
Pu) = Ps) = Gq) + sa —2p) * ars 4p)’
1

1

After a little work one obtains the mean recurrence times uw; = Fi, QQ): 4, = 43 = 4, bn =2.

(b) The chain has period 2 (if p 4 0), and all states are non-null and persistent. By symmetry, the
mean recurrence times j4; are equal. One way of calculating their common value (we shall encounter
an easier way in Section 6.4) is to observe that the sequence of visits to any given state j is a renewal
process (see Example (5.2.15)). Suppose for simplicity that p # 0. The times between successive
visits to 7 must be even, and therefore we work on a new time-scale in which one new unit equals two
old units. Using the renewal theorem (5.2.24), we obtain

2 2
pij Qn) > — if |j —iliseven, pjj(2n +1) > — if |j —i| is odd;
by Hy

note that the mean recurrence time of j in the new time-scale is 5 fj. Now > j Pij (m) = 1 for all m,
and so, letting m = 2n — ov, we find that 4/j2 = 1 where yz is a typical mean recurrence time.

There is insufficient space here to calculate p;;(n). One way is to diagonalize the transition
matrix. Another is to write down a family of difference equations of the form pj2(n) = p- poo(n —
1) + (1 — p)- paz(n — 1), and solve them.

4. (a) By symmetry, all states have the same mean-recurrence time. Using the renewal-process
argument of the last solution, the common value equals 8, being the number of vertices of the cube.
Hence py = 8.

Alternatively, let s be a neighbour of v, and let t be a neighbour of s other than v. In the obvious
notation, by symmetry,

by = 1+ 3ysv, sv =14 fusy + fur,
Mey = 1+ ZHsv + qlley + GPs My = 1+ getwy + Fltro,

a system of equations which may be solved to obtain zy = 8.
(b) Using the above equations, Lwy = 2. whence fiyw = 2 by symmetry.
(c) The required number X satisfies P(X =n) = 0”~1(1 — 6)? for n > 1, where @ is the probability

that the first return of the walk to its starting point precedes its first visit to the diametrically opposed

vertex. Therefore
[o.@)

E(X) = Sono" 11-0)? = 1.

n=l
5. (a) Let P;(-) = PC | Xo = i). Since i is persistent,
1=P,(Vj = 00) = Pi (Vj =0, Vi = 00) + P;(Vj > 0, Vj = 00)

< PV; =0) + P(T; < 00, Vj = 00)
<1-P,G < 00) +P; < c)P;(V; = 00),
by the strong Markov property. Since i + j, we have that P;(Vj = 00) > 1, which implies nj; = 1.

Also, P;(Tj < oo) = 1, and hence j — i and j is persistent. This implies n;; = 1.
(b) This is an immediate consequence of Exercise (6.2.4b).

278

Classification of chains Solutions [6.3.6]-[6.3.9]

6. Let Pi) = PC | Xo = 2). It is trivial that n; = 1 for j <¢ A. For j ¢ A, condition on the first
step and use the Markov property to obtain

nj = >> pjeP(Ta < 00 | X1 = 1) = © vyene.
kes k

If x = (x; : j € S) is any non-negative solution of these equations, then x; = 1 > nj for j ¢ A. For
JA,

xj = > Pik = > Pik + S> PjkXk =Pi(Ta =D 4+ S> DjkXk

keS keA k@A k@A
=Pj)Ts =D) + S> Pie, Pri + > puixi} =P\(T4 <2) + S> Pik > PkiXi-
k@A icA idA kdA id A

We obtain by iteration that, for j ¢ A,

xy =P(Ta Sat So Pik Phy ky °° * Pkt —\skn¥kn 2 P(Ta <n),

where the sum is over all kj, ko,...,kn ¢ A. We let n — oo to find that x; > Pj(T4 < 00) =7nj;.

7, The first part follows as in Exercise (6.3.6). Suppose x = (x; : j € S) is a non-negative solution
to the equations. As above, for j ¢ A,

xj= tt So jee =P)(T4 >= 1+ > Pik ¢ + > puis)
k kA idA

=P(T4 2 1)+Pj)(Ts 22) 4+---+ Pi (Tg = 1) + > jk, Pkyka + * Pky_ykn*kn

> $5 (Ta =m),

m=1

where the penultimate sum is over all paths of length n that do not visit A. We let n — oo to obtain
that x; = Ej(T4) = 9;.

8. Yes, because the 5; and 7; are stopping times whenever they are finite. Whether or not the exit
times are stopping times depends on their exact definition. The times U; = min{k > U,_1 : Xu, €
A, Xy,+1 ¢ A} are not stopping times, but the times U; + 1 are stopping times.

9. (a) Using the aperiodicity of j, there exist integers r1,r2,..., 7s having highest common factor
1 and such that p; jE) > Ofor 1 < k <s. There exists a positive integer M such that, ifr > M, then
r = Y=) 4ere for some sequence a1, a2,..., as of non-negative integers. Now, by the Chapman—

Kolmogorov equations,

S
pir) = ]] pyre > 0,
k=1

so that pjj(r) > O for allr > M.
Finally, find m such that p;;(m) > 0. Then

PyT+m)= pij(m)pjy7)>0  ifr>= M.

(b) Since there are only finitely many pairs i, j, the maximum R(P) = max{M(i, j) : i, 7 € S}is
finite. Now R(P) depends only on the positions of the non-negative entries in the transition matrix P.

279

[6.3.10]-[6.3.10] Solutions Markov chains

There are only finitely many subsets of entries of P, and so there exists f(n) such that R(P) < f(n)
for all relevant n x n transition matrices P.

(c) Consider the two chains with diagrams in the figure beneath. In the case on the left, we have that
P11(5) = 9, and in the case on the right, we may apply the postage stamp lemma with a = n and

b=n-1.
3 3 4
‘ 2
2 4
( )
1 n

10. Let X, be the number of green balls after n steps. Let e; be the probability that X, is ever zero
when Xp = j. By conditioning on the first removal,

j+2

J ;
= : ; 1s > 1,
I agaeynittag¢pi JF
with eg = 1. Solving recursively gives
(x) ej = 1-1-2) 148 4...4 ER GA ,
Pi P1P2°** Pj-1
where ; ;
p=. gg =i
Tage)? 4% 2agt
It is easy to see that
j-l
Qi 2
y) Set ig gg as j > 00.
xp PIP2°** Pj-1 j+1

By the result of Exercise (6.3.6), we seek the minimal non-negative solution (e;) to (+), which is
attained when 2(1 — e;) = 1, that is, ey = 4. Hence

;—1
1 gg: 9j-1 __!}
2p Pip2;Pj-1 i +1

For the second part, let d; be the expected time until j — 1 green balls remain, having started with j
green balls and j + 2 red. We condition as above to obtain

j
d; = 1+ — —f{d; d;}.
HT Ge pts}

We set ej = dj — (2j + 1) to find that (j + 2)e; = jej41, whence e; = 5iG + 1)e;. The expected
time to remove all the green balls is

n n n
Sod; = fe, +24 —D} = at 2 +e, 4G FD.
j=l j=l j=l
The minimal non-negative solution is found by setting e; = 0, and the conclusion follows by Exercise
(6.3.7).

280

Stationary distributions and the limit theorem Solutions [6.4.1]-[6.4.2]

6.4 Solutions. Stationary distributions and the limit theorem

1. Let Y, be the number of new errors introduced at the nth stage, and let G be the common probability
generating function of the Y,. Now Xn41 = Sn + Y¥p41 where Sy has the binomial distribution with
parameters X, and g (= 1 — p). Thus the probability generating function Gy of Xn satisfies

Gn4i(s) = G(s)E(s*") = G(s)E{E(s™ | Xn)} = G(s)E{(p + gs)*"}
= G(s)Gn(p + 95) = G(s)Gn(1 — q(1—s)).

Therefore, for s < 1,

Gn(s) = G(s)G(1 — q —s))Gn_2(1 — q?(1 — s)) =

n—1 oo
= Go(1-q"(1—s)) [] GU - 9’ -s)) > [[ GU -¢"d-5))
r=0 r=0

as n — oo, assuming g < 1. This infinite product is therefore the probability generating function of
the stationary distribution whenever this exists. If G(s) = e*(s—]), then

ca Co
[[ ¢Q-9’a-s)) = exp{ Ms => a} = eX(s-1)/P,
r=0 0

so that the stationary distribution is Poisson with parameter A/p.

2, (6.3.1): Assume for simplicity that sup{j : a; > 0} = oo. The chain is irreducible ifr < 1.
Look for a stationary distribution 2 with probability generating function G. We have that

Io =aptgon + (l—r)m, mj =ayngo tray t+ (1 —1r)aj4) fori > 1.

Hence
sG(s) = mgs A(s) +rs(G(s) — 29) + 1 —1r) (G(s) — 29)

where A(s) = >» F=0 a jst , and therefore

G(s) =m (“9 -( FF)

d-rj(s-D
Taking the limit as s ¢ 1, we obtain by L’H6pital’s rule that

A) +1-
G(l) = (SH).

There exists a stationary distribution if and only ifr < 1 and A’(1) < 00, in which case

sA(s) -(—r-+sr)
(s —1)(A’(1) +.1-r)

G(s) =

Hence the chain is non-null persistent if and only ifr < 1 and A’(1) < oo. The mean recurrence time
4; is found by expanding G and setting uw; = 1/7;.

(6.3.2): Assume that 0 < p < 1, and suppose first that p 4 i. Look for a solution {y; : 7 4 0} of
the equations

(*) y= S> Pij Yj. i #0,
i#0

281

[6.4.3]-[6.4.3] Solutions Markov chains

as in (6.4.10). Away from the origin, this equation is y; = gy;-1 + py;.2 where p +q = 1, with
auxiliary equation po? —O+q=0. Now po? —O+4q = p(@ — 1) —a)(@ — B) where

—p-Vp?+4pq _ B= —p+vVp? +479 |,

a=
2p 2p

Note that 0 < 6 < lifp> 5, while £ > lifp< 1.
For p > 4, set
_fAt+Bp ifizt,
ws { C+Dai ifi<-t,
the constants A, B, C, D being chosen in such a manner as to ‘patch over’ the omission of 0 in the
equations (+):

(+) Y-2=4y-3, Y-1=4Y-2+ PY1, Yi = Py.
The result is a bounded non-zero solution {y;} to (x), and it follows that the chain is transient.

For p < 7 follow the same route with

_ { 0 ifi>1,
c+ Dol +EB ifi<-1,
the constants being chosen such that y_» = gy_3, y-1 = gy_2.-
Finally suppose that p = 7 so that @ = —2 and 8 = 1. The general solution to (+) is

1,

A+Bi+Ca' ifi
yi= -]

>
D+Ei+Fa! ifi<

subject to (#*). Any bounded solution has B = E = C = 0, and («*) implies that A = D = F = 0.
Therefore the only bounded solution to («) is the zero solution, whence the chain is persistent. The
equation x = xP is satisfied by the vector x of 1’s; by an appeal to (6.4.6), the walk is null.

(6.3.3): (a) Solve the equation x = xP to find a stationary distribution 7 = ( a 5, i) when p # 0.
Hence the chain is non-null and persistent, with 4) = 77) _ 4, and similarly wz = 2, w3 = 4.

(b) Similarly, x = (4, 4, 4, 4) isa stationary distribution, and uj = 2! = 4.

(6.3.4): (a) The stationary distribution may be found to be z; = } for all i, so that wy = 8.

3. The quantities X;, X2,..., Xn depend only on the initial contents of the reservoir and the rainfalls
Yo, ¥1,..., ¥,-1- The contents on day n + 1 depend only on the value X,, of the previous contents
and the rainfall Y,. Since Y, is independent of all earlier rainfalls, the process X is a Markov chain.
Its state space is § = {0,1,2,..., K — 1} and it has transition matrix
8o0t+81 82 83 -* 8x-1 GK
80 81 82 -'* 8K-2 GK-1
P= 0 80 81": 8K-3 GK-2
0 0 0 |: 20 G,

where g; = P(Y, = i) and G; = )OP2; gj. The equation x = xP is as follows:

mo = 0(80 + 81) + 7180,
Tr = HOSr+i + W18r +++ + + 4180; O<r<K-—l,
UK-1 =MGK +0,Gx_j+::-+7K_1G}.

282
Stationary distributions and the limit theorem Solutions [6.4.4]-[6.4.5]

The final equation is a consequence of the previous ones, since a O x; = 1. Suppose then that
v = (1, v2, ...) is an infinite vector satisfying

vo = vo(go + 81) + 4180, Yr = VO8rg1 + Y18r +--+: +4180 forr > 0.

Multiply through the equation for v; by s” +1 and sum over r to find (after a little work) that

[o.<) . [o@)
N(s) = So ys‘, G(s) = S> gis’
i=0 i=0

satisfy sN(s) = N(s)G(s) + vogo(s — 1), and hence

1 -—1

+ ns) — 806 =)

vo s— G(s)
The probability generating function of the 7; is therefore a constant multiplied by the coefficients of
sysl...jsk-lin go(s — 1)/(s — G(s)), the constant being chosen in such a way that a n=l.

When G(s) = p(1 — gs)~!, then gg = p and

sols—1) _ pl —49s) —p q
s— G(s) p-—4qs 1—(qs/p)

The coefficient of 5° is 1, and of s! is git} /p' if i > 1. The stationary distribution is therefore given
by 2; = gxo(q/p)' fori = 1, where
_ 1 _ p-4

140% gq/pyi P49 + 4?(1— G/p)*-)

TO

if p #q,and mp = 2/(K + lif p=q = t.

4. The transition matrices

Oo ONeNIE

i]
fa
ll
“oonN
oe
- Oo
Ne
ia]
i)
II
Oo ONeBNIE
NENIE O ©
NEN O ©

have respective stationary distributions 7; = (p,1— p) anda2 = (3 P, 5 P, (1 — p), 5(1 — p))
for any 0 < p < 1.
5. (a) Set i = 1, and find an increasing sequence n1(1),1(2),... along which x;(m) converges.
Now set i = 2, and find a subsequence of (n1(j) : j = 1) along which x2(n) converges; denote
this subsequence by n2(1), n2(2),.... Continue inductively to obtain, for each i, a sequence nj =
(nj(J) : j = 1), noting that:

(i) m;41 is a subsequence of n;, and

Gi) limy—s oo xj (nj (r)) exists for all i.
Finally, define m; = n;,(k). For eachi > 1, the sequence m;, mj+1, ... is a subsequence of n;, and
therefore lim; oo x; (my) exists.
(b) Let S be the state space of the irreducible Markov chain X. There are countably many pairs i, j
of states, and part (a) may be applied to show that there exists a sequence (n, : r > 1) and a family
(aij : i, 7 € S), not all zero, such that pj; (ar) > ajj asr > oo.

283

[6.4.6]-[6.4.10] Solutions Markov chains

Now X is persistent, since otherwise p;;(n) — 0 for alli, 7. The coupling argument in the proof
of the ergodic theorem (6.4.17) is valid, so that pgj(n) — ppj(n) > 0 as n — on, implying that
Oa; = ap; for alla, d, j.

6. Just check that m satisfies x = xP and >, a, = 1.

7. Let Xp be the Markov chain which takes the value r if the walk is at any of the 2” nodes at level
r. Then X, executes a simple random walk with retaining barrier having p = 1 — g = Z, and it is
thus transient by Example (6.4.15).

8. Assume that X, includes particles present just after the entry of the fresh batch Y,. We may write

Xn
Xn41 = >_ Bin + Yn
i=l

where the B; , are independent Bernoulli variables with parameter 1 — p. Therefore X is a Markov
chain. It follows also that

Gn41(s) = E(s*"+1) = Gn(p +. qs)ero-),

In equilibrium, Gni, = Gn = G, where G(s) = G(p+ gs)e*s —). There is a unique stationary
distribution, and it is easy to see that G(s) = e*6 —)/P must therefore be the solution. The answer is
the Poisson distribution with parameter )/ p.

9. The Markov chain X has a uniform transition distribution pj, = 1/() + 2),O0<k < ft.
Therefore,

E(Xn) = E(E(Xn | Xn—1)) = 9 (1+ E(Xn-1)) =
=1-(5)" + "Xo.

The equilibrium probability generating function satisfies

G(s) = E(s*") = E(E(s* |X 1)) =—E _ ast?
i (1—s)(Xn +2) f’

whence d
qe 1 — s)G(s)} = —sG(s),

subject to G(1) = 1. The solution is G(s) = e*~!, which is the probability generating function of
the Poisson distribution with parameter 1.

10. This is the claim of Theorem (6.4.13). Without loss of generality we may take s = 0 and the y; to
be non-negative (since if the y; solve the equations, then so do y; + c for any constant c). Let T be the
matrix obtained from P by deleting the row and column labelled 0, and write T” = (¢; y(n): i, j #0).
Then T” includes all the n-step probabilities of paths that never visit zero.

We claim first that, for all i, j it is the case that t;;(n) > Oasn — oo. The quantity #;;(m) may
be thought of as the n-step transition probability from i to j in an altered chain in which s has been
made absorbing. Since the original chain is assumed irreducible, all states communicate with s, and
therefore all states other than s are transient in the altered chain, implying by the summability of t;; ()
(Corollary (6.2.4)) that tj; (2) — 0 as required.

Iterating the inequality y > Ty yields y > T” y, which is to say that

[o.@) [o.@)
wz day = minOr+s) QD) wj@), 121.
j=l ~ j=rtl

284

Stationary distributions and the limit theorem Solutions [6.4.11]-[6.4.12]

Let An = {X, # Ofork <n}. Fori > 1,

oo
P(Aoo | Xo =i) = lim P(An | Xo =i) = 4 @)
j=l

< jim, La

mins>| inal

Let € > 0, and pick R such that
Ji

ee <KE,.
ming>1{yR+s}

Take r = R and let n — oo, implying that P(Aoo | Xp = 1) = 0. It follows that 0 is persistent, and
by irreducibility that all states are persistent.

11. By Exercise (6.4.6), the stationary distribution is 7a = 7B = 2p =7p = é c= 4.

(a) By Theorem (6.4.3), the answer is “A = 1/7a = 6.

(b) By the argument around Lemma (6.4.5), the answer is pp(A) = 7pU¥a = 1p/7A = 1.

(c) Using the same argument, the answer is pc(A) = 1¢/7,A = 2.

(d) Let P;(.) = P(- | Xo = 4), let 7; be the time of the first passage to state j, andlet vy; = P;(T4 < Tg).
By conditioning on the first step,

1
vB =5+5 5%; veo=4+qpt+q 4°D, VA = 3B +5 3 VCs vp = 7%;

with solution v4 = 3, v= i, ve = 5: vp = i
A typical conditional transition probability t;3; = P;(X1 = j | Ta < Tg) is calculated as follows:

Pa(X1 =B)Pa(Ta < Te) vp 3
Tap = Pa(X; =B| Ty < Tg) = = > =-2

Pa(Ta < Tp) 2va 5’
and similarly,
2 2 1 1 3 1
Tac = §, TRA = 3> TBC = 3 TCA= J, TU z = 3% tc=!1

We now compute the conditional expectations 4; = E;(Ta | T4 < Tp) by conditioning on the first
step of the conditioned process. This yields equations of the form ji, = 1+ 2 fis + Ziic, whose
solution gives ia = ig

(e) Either use the stationary distribution of the conditional transition matrix T, or condition on the first
step as follows. With N the number of visits to D, and yn; = E;(N | Ta < Tp), we obtain

na=3nnt+2nc, m=0+4nc, nmc=O+3nnt+ gta), mD=N70,

whence in particular 74 = 0:

12. By Exercise (6.4.6), the stationary distribution has 74 = ip m= i. Using the argument
around Lemma (6.4.5), the answer is pp(A) = mpHa = 1B/TA = 2.

285

(6.5.1]-[6.5.6] Solutions Markov chains

6.5 Solutions. Reversibility

1. Look for a solution to the equations 2; pj; = 7; p;i- The only non-trivial cases of interest are
those with j = i+ 1, and therefore Aja; = j417j;41 for 0 <i < b, with solution

BYEZ py

an empty product being interpreted as 1. The constant zg is chosen in order that the z; sum to 1, and
the chain is therefore time-reversible.

2. Leta be the stationary distribution of X, and suppose X is reversible. We have that 7; pj; = Pit;
for alli, j, and furthermore z; > 0 for all i. Hence

Uj Pij Pik Pki = Pji%lj PjkPki = Pji Pkj Mk Pki = Pji Pkj PikTi

as required when n = 3. A similar calculation is valid when n > 3.

Suppose conversely that the given display holds for all finite sequences of states. Sum over all
values of the subsequence j2,..., jn—1 to deduce that p;j(n — 1) pji = pij pji(n — 1), where i = jy,
J = jn. Take the limit as n — oo to obtain 2; pj; = pjj 7; as required for time-reversibility.

3. With a the stationary distribution of X, look for a stationary distribution v of Y of the form

{ chu; ifi ZC,
y=

cm fi EC.

There are four cases.

(a) i€C, jf €C: vigij = cui Ppiy = CBN; Pji = Y;Q;i;

(b) i, f EC: viqij = CN; Pij = CN; Pji = VjQji.

(c) 1g C, J EC: vigy = cB Piy = CBX; Pji = VjQji,

(d) i,j €C: vigij = cB pij = CB; Dji = Vj Qji-

Hence the modified chain is reversible in equilibrium with stationary distribution v, when

fe om +Du} =1
i¢C ieC
In the limit as 6 | 0, the chain Y never leaves the set C once it has arrived in it.

4. Only if the period is 2, because of the detailed balance equations.

5. With Yn = Xn — 4m,
E(Y¥n) = E(Qn—1) + E(Xn — Xn-1)

= 8, +8 { (1 _ Xn=1

m

Xn 2
) - “1 = E(¥n—1) — —E(%n_1)-
m Mm

Now iterate.

6. (a) The distribution 7; = 8/(@ + 8), m2 = a/(a + B) satisfies the detailed balance equations,
so this chain is reversible.
(b) By symmetry, the stationary distribution is 7 = G, 3 3), which satisfies the detailed balance

equations if and only if p = 3:

(c) This chain is reversible if and only if p = 5.

286

Chains with finitely many states Solutions [6.5.7]-[6.6.2]

7. A simple random walk which moves rightwards with probability p has a stationary measure
ttn = A(p/q)", in the sense that m is a vector satisfying x = mP. It is not necessarily the case that
this 7 has finite sum. It may then be checked that the recipe given in the solution to Exercise (6.5.3)
yields x(i, j) = pi p3 / 07,s)eC P| P2 as stationary distribution for the given process, where C is the
relevant region of the plane, and p; = p;/gq; and p; (= 1 — q;) is the chance that the ith walk moves
rightwards on any given step.

8. Since the chain is irreducible with a finite state space, we have that z; > 0 for all i. Assume the
chain is reversible. The balance equations 7; pjj = 7; Pji give pi; = 7; Pji/nji. Let D be the matrix
with entries 1/7; on the diagonal, and § the matrix (7; p;;), and check that P = DS.

Conversely, if P = DS, then d;} Pij = a7! Pji, whence 7; = d;} / de | satisfies the detailed

balance equations.
1 Tj
ij = =, | — Dij./T;-
Pij Jai \ 7 Pij fT;

Note that
If the chain is reversible in equilibrium, the matrix M = (, [1 |; Di i) is symmetric, and therefore M,
and, by the above, P, has real eigenvalues. An example of the failure of the converse is the transition
matrix

1 0 0

which has real eigenvalues 1, and -4 (twice), and stationary distribution z = g, g, 5)- However,

m1 pi3=O0F 5 = 703 p31, so that such a chain is not reversible.

9. Simply check the detailed balance equations 7; pjj = 7; Pji-

6.6 Solutions. Chains with finitely many states

1. Let P = (pj : 1 < i, j < n) bea stochastic matrix and let C be the subset of R” containing
all vectors x = (x1, %2,...,Xn) satisfying x; > O for all i and we x; = 1; for x e€ C, let
[|x|] = max, {x;}. Define the linear mapping T : C > R” by T(x) = xP. Let us check that T is a
continuous function from C into C. First,

IT()|| = max 9 Pi} < a||x|

L

where
a = max > pi}
i
hence ||7(x) — T(y)|| < @ |x — yl]. Secondly, T(x); = 0 for all j, and

S > T(); = Soo ivi = Som dS pi =1.
j ji i J

Applying the given theorem, there exists a point w in C such that T (@) = 2, which is to say that
n=aHP.

2. Let P bea stochastic m x m matrix and let T be the m x (m + 1) matrix with (i, j)th entry

f= { piy — 9: if j <m,
4 1 ifj=m+],

287

[6.6.3]-[6.6.4] Solutions Markov chains

where 6;; is the Kronecker delta. Let v = (0,0,...,0,1) € R”+1. If statement (ii) of the question
is valid, there exists y = (1, y2,.-., Y¥m+1) such that

m
ymt1 <0 => (pij — 8i)¥j + ym41 = O for 1 <i <m;
j=l

this implies that

m
So py) ZV —Ym4i > yi — forall,
j=l
and hence the impossibility that Ljel Pij yj > max;{y;}. It follows that statement (i) holds, which
is to say that there exists a non-negative vector x = (x1,%2,.-..,m) such that x(P — I) = 0 and

>. xi = 1; such an x is the required eigenvector.

3. Thinking of x,41 as the amount you may be sure of winning, you seek a betting scheme x such
that x,41 1s maximized subject to the inequalities

n
Xn41 = So xitiy forl <j <m.
i=1

Writing aj; = —tj; for 1 <i <n and a,41,; = 1, we obtain the linear program:
atl
maximize X,+41 subject to So xii <0 forl<j<m.
i=]
The dual linear program is:

m
minimize 0 subject to So aij yj =0 forl <i<n,
j=l

m
Se an41,j)% =1, y2=0 forl<j<m.
j=l

Re-expressing the a;; in terms of the t;; as above, the dual program takes the form:

m
minimize 0 subject to So tj Pj =0 forl <i<a,
j=l
m
pj=1, pj=90 forl<j<m.
j=l

The vector x = 0 is a feasible solution of the primal program. The dual program has a feasible
solution if and only if statement (a) holds. Therefore, if (a) holds, the dual program has minimal value
0, whence by the duality theorem of linear programming, the maximal value of the primal program is
0, in contradiction of statement (b). On the other hand, if (a) does not hold, the dual has no feasible
solution, and therefore the primal program has no optimal solution. That is, the objective function of
the primal is unbounded, and therefore (b) holds. [This was proved by De Finetti in 1937.]

4. Use induction, the claim being evidently true when n = 1. Suppose it is true for n = m. Certainly
p+! is of the correct form, and the equation P’”+1x/ = P(P” x’) with x = (1, @, w”) yields in its
first row

1 m+1 + 42,m41@ + 43, m410 = (1 — pt po)” (Px’); = (1 — p + poy" t

288

Branching processes revisited Solutions [6.6.5]-[6.7.2]

as required.

5. The first part follows from the fact that 21/ = 1 if and only if rU = 1. The second part follows
from the fact that 7; > 0 for alli if P is finite and irreducible, since this implies the invertibility of
I-P+U.

6. The chessboard corresponds to a graph with 8 x 8 = 64 vertices, pairs of which are connected
by edges when the corresponding move is legitimate for the piece in question. By Exercises (6.4.6),
(6.5.9), we need only check that the graph is connected, and to calculate the degree of a corner vertex.

(a) For the king there are 4 vertices of degree 3, 24 of degree 5, 36 of degree 8. Hence, the number of
edges is 210 and the degree of a corner is 3. Therefore (king) = 420/3 = 140.

(b) ju(queen) = (28 x 21+ 20 x 23 +12 x 2544 x 27)/21 = 208/3.

(c) We restrict ourselves to the set of 32 vertices accessible from a given corner. Then jz(bishop) =
(14x 7+10x9+6x 1142 x 13)/7 = 40.

(d) w(knight) = (4x 24+8x3+4+20x4+ 16 x 6+ 16 x 8)/2 = 168.
(e) (rook) = 64 x 14/14 = 64.

7. They are walking on a product space of 8 x 16 vertices. Of these, 6 x 16 have degree 6 x 3 and
16 x 2 have degree 6 x 5. Hence

w(C) = (6 x 16x 6x3+16x 2 x 6 x 5)/18 = 448/3.

8 |P-Alj=QA-)DQAt+ 5) + ‘). Tedious computation yields the eigenvectors, and thus
1 1 1 8 -4 -4 0 oO 0
PP=2/1 1 t)4+QCh"{ 2 -1 -1])4+9Ch"{-2 3-1}.
111 -10 5 5 2 -3 1

6.7 Solutions. Branching processes revisited

|

1. We have (using Theorem (5.4.3), or the fact that G,z41(s) = G(Gn(s))) that the probability
generating function of Zy is

n—(n—1)s
Gnl(s) =" ev .

PZ, =k) = n \kt1 n-1 h kT nk-!
me N at n+1/\n+1 ~ (a + 1/41

for k > 1. Therefore, for y > 0, as n > oo,

so that

1 [2yn]k-1 1 \—l2yn] 2
= s- =]- = — ev
PZn = 2yn| Zn >= 7G) f Gane =} (145) >1l-e™”*.,
=1

2. Using the independence of different lines of descent,

fo ¢)
E(s2” | extinction) = >
k=0

s*P(Zp =k, extinction) _ 3 s*P(Zn = kn

1
= G :
P(extinction) n n(sn)

k=0 "

where Gy is the probability generating function of Zp.

289

[6.7.3]-[6.8.1] Solutions Markov chains
3. We have that 7 = G(n). In this case G(s) = (V1 — ps), and therefore 7 = q/p. Hence

Pate" — 4" ~ p(sq/p)(p"" — a")
q prt! — grt! — p(sq/p)(p" — 4")
_ plq” — p" — qs(q""" — p""")}

~ grt _ peti — qs(q" — p")

1
—Gp(sn) =
n

which is Gp(s) with p and g interchanged.
4. (a) Using the fact that var(X | X > 0) > 0,

E(X2) = E(X? | X > 0)P(X > 0) > E(X | X > 0)*P(X > 0) = E(X)E(X | X > 0).

(b) Hence
n E(Zn) ar y2
E(Zn/u™ | Zn > 0) < ME(Zn) =E(W,)

where W, = Z,/E(Zn). By an easy calculation (see Lemma (5.4.2)),

2 l1—un7? 2 2
Ew?) = 2 Mats ~— +12 —P-
we — pb ue — pe p-@q
where o? = var(Z1) = p/q.
(c) Doing the calculation exactly,
E(Zn/u") 1 1

E(Zn/pn” | Zn > 0) =

= >
P(Zn > 0) 1 -—G,(0) 1-7

where 7 = P(ultimate extinction) = g/p.

6.8 Solutions. Birth processes and the Poisson process
1. Let F and W be the incoming Poisson processes, and let N(t) = F(t)+-W(t). Certainly N(0) = 0
and N is non-decreasing. Arrivals of flies during [0, s] are independent of arrivals during (s, 7], if
s < t; similarly for wasps. Therefore the aggregated arrival process during [0, s] is independent of
the aggregated process during (s, t]. Now
P(N(E +h) =n4+1|N(@) =n) =P(AAB)
where
A = {one fly arrives during (t,t +h]}, B = {one wasp arrives during (t, t + AJ}.
We have that

P(A A B) = P(A) + P(B) — P(ANB)
= Ah + ph — (AR)(uh) + o(h) = (A+ wh + o(h).

Finally
P(N(t +h) >n+1|N(@) =n) < P(ANB)+P(CUD),

290

Birth processes and the Poisson process Solutions - [6.8.2]-[6.8.4]

where C = {two or more flies arrive in (¢, f + h]} and D = {two or more wasps arrive in (t, t + h]}.
This probability is no greater than (Ah)(uh) + o(h) = ofA).

2. Let I be the incoming Poisson process, and let G be the process of arrivals of green insects.
Matters of independence are dealt with as above. Finally,

P(G(t +h) =n+1|GQ@ =n) = pP(IG +h) =n 4+1| I(t) =n) + 04) = pah + Off),
P(G(t +h) >n+1|GQ@) =n) < PU +h) > n4+1/1@ =n) =o(h).

3. Conditioning on 7; and using the time-homogeneity of the process,

P(Et-—u)>x) ifus<t,
P(E(@) > x|Tj =u) = 0 ift<u<t4+x,
1 ifu>t+x,

(draw a diagram to help you see this). Therefore
oo
P(E(t) > x) = [ P(E(t) > x| Tj =u)ae™ du
0

t [oe]
= [ P(E(t —u) > x)Ae* du +f re du.
0 t+x

You may solve the integral equation using Laplace transforms. Alternately you may guess the
answer and then check that it works. The answer is P(E(t) < x) = 1 — e~*, the exponential
distribution. Actually this answer is obvious since E(t) > x if and only if there is no arrival in
[t, t +x], an event having probability e~**.

4. The forward equation is
Pi) =AG-VpPij-10-—AipPAO, i Si,

with boundary conditions p;;(0) = 6;;, the Kronecker delta. We write G;(s,t) = 0 i si pij (t), the
probability generating function of B(¢) conditional on B(Q) = i. Multiply through the differential
equation by s/ and sum over j:

aGi _ 4,281 _ , aGr

— =) ;
ary, as

a partial differential equation with boundary condition G;(s, 0) = s’. This may be solved in the usual
way to obtain G;(s, t) = ete d- s7)) for some function g. Using the boundary condition, we
find that g(1 — s~!) = 5! and so g(u) = (1 —u)~, yielding

1 (sey

Gis.) = aig —s=)yi ~ [Tosa eS)

The coefficient of s/ is, by the binomial series,
-at{i-1 —Myj—i oe
(*) pit) =e 5-1 d-e “yi, j zi,

as required.

291
[6.8.5]-[6.8.6] Solutions Markov chains

Alternatively use induction. Set j = i to obtain Pi; (t) = —Aip;(t) Gemember p; ;_1(¢) = 0),
and therefore p;;(t) = e~™!_ Rewrite the differential equation as

ay (Pi (te) = AG — Dp j-1 Me".
Set j =i +1 and solve to obtain p; ;41(t) =i et (i —e ). Hence (+) holds, by induction.

The mean is

= Te,
sal

a
E(Bt)) = 5 CL t)

by an easy calculation. Similarly var(B(t)) = A + E(B(d) —- E(B())2 where

2

a

sal

Alternatively, note that B(¢) has the negative binomial distribution with parameters e~™ and J.

5. The forward equations are
Ph(t) = An—1Pn—1(t) —AnPat), 2 29,

where A; = iA + v. The process is honest, and therefore m(t) = ©, nPn(t) satisfies

oo oo

m'(t) = So n[(n —1)A +0] Pn) — Do nna + v) pnt)

n=] n=0

oo
= So {Al@t Dn — 17] + vf + 1) — 2) } nl)

n=0

[o@)
= So (an + v)pn(t) = Am(t) + v.

n=0

Solve subject to m(0) = 0 to obtain m(t) = v(ert —1)/d.

6. Using the fact that the time to the nth arrival is the sum of exponential interarrival times (or using
equation (6.8.15)), we have that

oO
in (6) = [ 68 pa(t) dt
0

is given by

~ Loy i
0) = — | | ——

Pn(@) an it +6

which may be expressed, using partial fractions, as

~ 1 QV aja;
Pn) = — >. —

An 20 Ai +8
where
ayo=
‘ IT Aj — Ai
j=0
J#i

Continuous-time Markov chains Solutions [6.8.7]-[6.9.1]

so long as A; # Aj whenever i # j. The Laplace transform Py, may now be inverted as
l< —Ajt
Pn(t) = — So ajaje yy
An 0

See also Exercise (4.8.4).

7. Let Ty be the time of the nth arrival, and let T = limp oo Tr = sup{t : N(t) < oo}. Now, as in
Exercise (6.8.6),

dnPn(O) = |] —— =Ee@-o™
nPn(?) es orerw, (e-""")
i=0
since Ty = Xp +X1+---+Xn where Xx is the (k + 1)th interarrival time, a random variable which is
exponentially distributed with parameter A,. Using the continuity theorem, E(e—?7) —> E(e7®") as
n —> 00, whence An Pn(O) > E(e—°T) as n —> 00, which may be inverted to obtain An pn(t) > f (1)
as n —> co where / is the density function of T. Now

E(N(t) | N(t) < 00) = Yon Pn(t)

which converges or diverges according to whether or not >, apn(t) converges. However pn(t) ~
aw FO as n —> 00, so that 5°, npn(t) < oo if and only if >, naz} < oO.
When Ay = (n+ 5°, we have that

ire) -l
E(e*T) = Il t + ad = sech (x V0).
2

n=0

Inverting the Laplace transform (or consulting a table of such transforms) we find that

1 8
10 =-= 5 01(3| t/n?) -

where @ is the first Jacobi theta function.

6.9 Solutions. Continuous-time Markov chains
1. (a) We have that

Pip =P +API2, Poo = —Ap22 + uP2I,

where p12 = 1 — py, P21 = 1 — p22. Solve these subject to p;;(t) = 4;;, the Kronecker delta, to
obtain that the matrix P; = (pj; (t)) is given by

1 ( A+ pe OtM He) |

Pe a He Ot tre Ow

(b) There are many ways of calculating G”; let us use generating functions. Note first that G° =I,

the identity matrix. Write
G" = (“ in), n>0,
Cn dn

293
[6.9.2]-[6.9.4] Solutions Markov chains

and use the equation G+! = G - G" to find that
Ont] = —Han t+ ben,  Cn41 = AGn — ACh.

Hence ay4.1 = —(u/A)cn+1 forn > 0, and the first difference equation becomes a,4, = —(A+)an,
n > 1, which, subject to aj] = —y, has solution an = (—1)"“( + pytt, n > 1. Therefore
Cn = (-D" haa + py?! for n > 1, and one may see similarly that by = —ay, dy = —cy for
n > 1. Using the facts that ag = dy = 1 and by = co = 0, we deduce that )>P0 q(t” /n!)G" = P;
where P; is given in part (a).

(c) With x = (71, 72), we have that —yuzry + Amy = O and yum] — Am. = 0, whence 2, = (A/p) 72.
In addition, m7] + 22 = lif, =A/A+ mW =1—7.

2. (a) The required probability is

P(X(t) = 2, Xt) =1| XO) =1) _ pia)p2 24)
P(X (Bt) =1| X(0) = 1) p11 Gt)

using the Markov property and the homogeneity of the process.
(b) Likewise, the required probability is

Pi2(t) poi (2t) p11)
P11 3t) Pi)

the same as in part (a).

3. The interarrival times and runtimes are independent and exponentially distributed. It is the lack-
of-memory property which guarantees that X has the Markov property.

The state space is § = {0, 1,2, ...} and the generator is

—r h 0 0
Bw —-At+p) A 0
h

G=]| 0 bh -Q+H

Solutions of the equation 7G = 0 satisfy
Ang + em =0, Anjy—-Q+p4)aj + erj41 = Ofor 7 > 1,

with solution 7; = mg(a/p)!. We have in addition that 53; 7; = 1ifa < wanda = {1- a/pyol.
4. One may use the strong Markov property. Alternatively, by the Markov property,

Pao. =31|¥e =i, Te =t, B) =PVay1 = 1 ¥n =i, In =D)
for any event B defined in terms of {X(s) : s < T,}. Hence
oO
PO =F 1¥a= 8B) = [POs =F Ya =i Te = Or, (0 at
=PUn41 = 31% =),
so that Y is a Markov chain. Now q;; = P(¥n41 = J | Yn = #) is given by

°° a
dij - | pij@)ae™ dt,

294
Continuous-time Markov chains Solutions [6.9.5]-[6.9.8]

by conditioning on the (n + 1)th interarrival time of N; here, as usual, pj; (¢) is a transition probability
of X. Now

ea) 00
So midi; - | (Smipye)re™ dt =| mae dt=7j.
i i

5. The jump chain Z = {Z, : n > O} has transition probabilities hj; = g;;/g;, i # j. The chance
that Z ever reaches A from j is also nj, and nj = Me hjxnx for j ¢ A, by Exercise (6.3.6). Hence
—8jnj = don Bk Nk, as required.

6. Let 7; = inf{t : X(t) 4 X(O)}, and more generally let Tj, be the time of the mth change in value
of X. For j ¢ A,

Mj =Ej(T1) + D> hjewe,
kj

where Ej denotes expectation conditional on Xp = j. Now Ej (71) = 8 vy, and the given equations
follow. Suppose next that (a, : k € S) is another non-negative solution of these equations. With
U; = Tj41 — T; and R = min{n > 1: Zn € A}, we have for j ¢ A that

1 1 |
aj = — + Dhak = — + Shel + hann
8] kga Si kga 8k mga
= Ej;(Uo) + Ej (U1 [tr>1)) + Ej (Got r>2)) + +--+ EjUnltrony) + 2,
where © is a sum of non-negative terms. It follows that

a; = Ej(Uo) + Ej Ui iret) +--+» + Ej Unlrsny)
n
=E; (x U-le>1)) = Ej (min{T, Ha}) > Ej(Ha)
r=0
as n —> 00, by monotone convergence. Therefore, yz is the minimal non-negative solution.

7. First note that i is persistent if and only if it is also a persistent state in the jump chain Z. The
integrand being positive, we can write

oe) OO
[ putde =e] [ Ux (p=i at

where {Tn : n > 1} are the times of the jumps of X. The right side equals

oo
X(0) = | = | Yat — Tn), =i} | Yo = |

n=0

oo 1 oo
SOE | XO) = DA) = a do hi)
n=0

n=0
where H = (h;;) is the transition matrix of Z. The sum diverges if and only if i is persistent for Z.

8. Since the imbedded jump walk is persistent, so is X. The probability of visiting m during an
excursion is @ = (2m)~!, since such a visit requires an initial step to the right, followed by a visit to
m before 0, cf. Example (3.9.6). Having arrived at m, the chance of returning to m before visiting 0
is 1 — a, by the same argument with 0 and m interchanged. In this way one sees that the number NV
of visits to m during an excursion from 0 has distribution given by P(N > k) = a(1 — aj] k>1.
The ‘total jump rate’ from any state is A, whence T may be expressed as an V; where the V; are
exponential with parameter A. Therefore,

Ete") = Gy ( )= 0-040 an

1-90 an-—6

295

[6.9.9]-[6.9.11] Solutions Markov chains

The distribution of T is a mixture of an atom at 0 and the exponential distribution with parameter aA.

9. The number N of sojourns in i has a geometric distribution P(N = k) = fea -—f),k>1,
for some f < 1. The length of each of these sojourns has the exponential distribution with some
parameter g;. By the independence of these lengths, the total time T in state i has moment generating

function A
(oT lq _ (85 ) __si—f)
)= vy -A (35) =sq-yp70

The distribution of T is exponential with parameter g;(1 — f).

10. The jump chain is the simple random walk with probabilities 4. /(A + 2) and z/(A + 2), and with
Poi = 1. By Corollary (5.3.6), the chance of ever hitting 0 having started at 1 is 4/4, whence the
probability of returning to 0 having started there is f = 4/A. By the result of Exercise (6.9.9),

A— pb

E(e9%) =
(e""°) are,

as required. Having started at 0, the walk visits the state r > 1 with probability 1. The probability of
returning to r having started there is

a ee Qu
fr= 3

+
Atm Atm A+ yp’

and each sojourn is exponentially distributed with parameter gy = A + yw. Now g-(1 — fr) =A-— Bp,
whence, as above,
ac
Bef) =
A-p—@
The probability of ever reaching 0 from X (0) is ({z/ )* ©), and the time spent there subsequently
is exponential with parameter 4 — yz. Therefore, the mean total time spent at 0 is

n (HAE) - Sa

A-—p Apo

11. (a) The imbedded chain has transition probabilities

hej = { 8kj/8k KF j,
J 0 ifk = j,

where g, = —gxg. Therefore, for any state /,

m key TSK (BK; /8k) 1; 8j ~
So ghey = = = Fj,
yi 8: Doi Ti8i

where we have used the fact that G = 0. Also i, > Oand >>, 7 = 1, and therefore # isa stationary
distribution of Y.

Clearly #% = xy for all k if and only if g, = >>; 7:8; for all k, which is to say that g; = gx for
all pairs i, k. This requires that the ‘holding times’ have the same distribution.
(b) Let T, be the time of the nth change of value of X, with To = 0, and let Un, = T,41 — Tn. Fixa
state k, and let H = min{n > 1: Z, = kj}. Let y;(&) be the mean time spent in state i between two
consecutive visits to k, and let 7;(k) be the mean number of visits to i by the jump chain in between two

296

Birth-death processes and imbedding Solutions [6.9.12]-[6.11.2]

visits to k (so that, in particular, 7, (k) = 9, | and yk) = 1). With E j and P; denoting expectation
and probability conditional on X (0) = j, we have that

OO oo
yitk) = Ex (> Un Zy =i, un) = So Ex(Un | LiZ_=i})Pe(Zn =i, H > n)

n=O n=0

Ss 1 1.
= So = Pk(Zn =i, H > n) = —7i@).
n=0 i 8i
The vector 7(k) = (7;(k) : i € S) satisfies P(K)H = 7(k), by Lemma (6.4.5), where H is the
transition matrix of the jump chain Z. That is to say,
Yo 1@® =y)  forjes,
iiZj 8i

whence 5°; ¥(k)gij = O for all j € S. If ux = 57; 4K) < 00, the vector (y; (k)/ px) is a stationary
distribution for X, whence 2; = y;(k)/x for alli. Setting i = k we deduce that zy = 1/(gz tg).

Finally, if $7; 278; < 00, then

~ — 1 i m8i
fig = = SE = So 81.
Tk = WRB ;

12, Define the generator G by gj; = —vj, gij = vj4;;, so that the imbedded chain has transition
matrix H. A root of the equation xG = 90 satisfies

O= So migiy = —ajyj t+ SO) Guvhiy
i ii j

whence the vector ¢ = (2;v; : j € S) satisfies ¢ = €H. Therefore = av, which is to say that
jv; = av;,for some constant a. Now v; > Ofor all j, sothat 7; = a, which implies that yj my #1.
Therefore the continuous-time chain X with generator G has no stationary distribution.

6.11 Solutions. Birth—death processes and imbedding

1. The jump chain is a walk {Z,} on the set S = {0, 1, 2,...} satisfying, fori > 1,
. . i ifj=it+l,
Pn = 1Zn=)={ P a
l-—p, ifj=i-1,
where pj = A;/(Aj + ui). Also P(Zp41 = 1 | Zn = 0) = 1.
2. The transition matrix H = (h;;) of Z is given by

ip

- if j=i-1,
A+ip
ifj=it+l.
A+ ip wysit

To find the stationary distribution of Y, either solve the equation # = 2Q, or look for a solution of
the detailed balance equations mh; ;41 = 2i41hj+1,;. Following the latter route, we have that

hovhio «+ hin:
nj = x ua ii i>,
hi j—1-+- 421419

297

[6.11.3]-[6.11.4] Solutions Markov chains

whence 2; = mo'(1 + i/p)/i! fori > 1. Choosing zg accordingly, we obtain the result.

It is a standard calculation that X has stationary distribution v given by v; = p!e~?/i! fori > 0.
The difference between 7 and v arises from the fact that the holding-times of X have distributions
which depend on the current state.

3. We have, by conditioning on X (4), that

n(t +h) =E{P(X(@ +h) =0| X(h))}
= h-1+(1— Ah — ph)n(t) + AhE(E) + o(h)

where &(t) = P(X(¢) = 0 | X(O) = 2). The process X may be thought of as a collection of particles
each of which dies at rate 4: and divides at rate A, different particles enjoying a certain independence;
this is a consequence of the linearity of A, and jn. Hence &(t) = n(t)2, since each of the initial pair
is required to have no descendants at time t. Therefore

n't) =u — (A+ w)n@) + Ang)?

subject to 7(0) = 0.
Rewrite the equation as
"i y
(1—n)(—An)

and solve using partial fractions to obtain

At .
At+1 ith =m,
n(t) = pd — eb)

Finally, if0 <t <u,

P(X(t) =0) _ n(t)

P(X(t) =0| X@) = 0) = P(X(u) =0| XM = %) 5G) = 0) = Tw)

4. Therandom variable X(t) has generating function

_ UL =s) — (e—AsyetO
C6.) = Fa) — i oases OH)

as usual, The generating function of X(t), conditional on {X(t} > 0}, is therefore

SS nPM(X® =n) _ Gs, t) - GO,t)
ds PX®>0)  1-GO,n

n=1

Substitute for G and take the limit as t > 00 to obtain as limit

H(s) = (Ns — Sis"
ue — hs n=] ,

where, with p = A/, we have that pn = ie p)forn > 1.

298

Special processes Solutions [6.11.5]-[6.12.1]

5. Extinction is certain if 7. < yu, and in this case, by Theorem (6.11.10),

fore) [o.@)
E(T) =| (LT > t)dt - | {1 -E(s*)],_o} at
0 0

Jo —aeQ-“)t ~ 48 w-Al-

IfA > wthen P(T < 00) = p/A, so

oo d oO (A pet ] ny
E(T|T _ 1— (Es* bas aay t= a1 —).
(T | T <0) [ { ee YI 50 yh pew wo \i—p

In the case 1 = ph, P(T < co) = 1 and E(T) = oo.

6. By considering the imbedded random walk, we find that the probability of ever returning to 1
is max{A, w}/(A + yw), so that the number of visits is geometric with parameter min{A, }/(A + y).
Each visit has an exponentially distributed duration with parameter 2 + yz, and a short calculation
using moment generating functions shows that V] (oo) is exponential with parameter min{i, jw}.

Next, by a change of variables, Theorem (6.11.10), and some calculation,

Dos Er (0) = 2X [ s"hxw)=r) au) =E ([ sx) au)

J t 1 ad — s) — (un — ds)e~ Ot
= [ E(s*) du- MT 8 ( S)— (yu sje
0 Xr x h—p

1 {1 _ As(eP? — 1)

=—-+lo terms not involving s,
Xr 8 pePt — j, \ + e

where p = fs — A. We take the limit as t > oo and we pick out the coefficient of s”.

7. If = yw then, by Theorem (6.11.10),

Atl —s)4+s l-s
R(s*O) — DA —
(e" = Gd ay 41 md—s)+1

and

t 1
[ E(s*) du =t — = logtard — 8) +1)
0

dog £1 — S| 4 terms not involvin
=-——i10 _ TMs no VOLV! AY
x °8 1+at 8

Letting ¢ —> oo and picking out the coefficient of s” gives E(V,;(c0o)) = (rd)~1. An alternative
method utilizes the imbedded simple random walk and the exponentiality of the sojourn times.

6.12 Solutions. Special processes

1. The jump chain is simple random walk with step probabilities 4/(A + ) and w/(A + yz). The
expected time {19 to pass from 1 to 0 satisfies

x
419 = 1+ —— (Ha + #19) = 14+

299

[6.12.2]-[6.12.4] Solutions Markov chains

whence j4i9 = (u% + A)/(u — A). Since each sojourn is exponentially distributed with parameter
iu +A, the result follows by an easy calculation. See also Theorem (11.3.17).

2. We apply the method of Theorem (6.12.11) with

seh

Gy(s,u) = i <n

—s+se

the probability generating function of the population size at time u in a simple birth process. In the
absence of disasters, the probability generating function of the ensuing population size at time v is

A(s,v) = exp (» [ tens. u) — 11du) ={s+(1 = ser,
0

The individuals alive at time ¢ arose subsequent to the most recent disaster at time t — D, where D
has density function be *, x > 0. Therefore,

t be79* oe ¥t dx ewvt
E(s*) = E(H(s, D - | ~
(s ) ( (s, )) 0 ral —st+ seyv/d +e (1 —s + se Myv/d

3. The mean number of descendants after time t of a single progenitor at time 0 is e@~)".. The
expected number due to the arrival of a single individual at a uniformly distributed time in the interval

on [0, x] is therefore
Q-“)x _
1 [ [OW gy = Oe
x Jo (A — w)x

The aggregate effect at time x of N earlier arrivals is the same, by Theorem (6.12.7), as that of N
arrivals at independent times which are uniformly distributed on [0, x]. Since E(V) = vx, the mean
population size at time x is v[eA-#)* — 1]/(A — w). The most recent disaster occurred at time t — D,
where D has density function be7o* , x > 0, and it follows that

v feQ-)* — WY dx + eX feA—mt _ 47.

pe A pL

E(X(t)) = i bev es

This is bounded as t > oo if and only if6 > A — w.

4. Let N be the number of clients who arrive during the interval [0, t]. Conditional on the event
{N = n}, the arrival times have, by Theorem (6.12.7), the same joint distribution as n independent
variables chosen uniformly from [0, t]. The probability that an arrival at a uniform time in [0, t] is
still in service at time f is B = fol — G(t —x)]t7! dx, whence, conditional on {N = 7}, the total
number M still in service is bin(x, 8). Therefore,

B (eM) = B(B(e™ | N)) = B((Be? +1—f)") = Gy Be? +1 - py =e,

whence M has the Poisson distribution with parameter ABt = A fo — G(x)Jdx. Note that this
parameter approaches XE(S) as t — oo.

300

Spatial Poisson processes Solutions [6.13.1]-[6.13.3]

6.13 Solutions. Spatial Poisson processes

1. It is easy to check from the axioms that the combined process N(t) = B(t) + G(t) is a Poisson
process with intensity B + y.

(a) The time S$ (respectively, T) until the arrival of the first brown (respectively, grizzly) bear is
exponentially distributed with parameter B (respectively, y), and these times are independent. Now,

B

PIS <T)= °° e Ps e—V5 ds =
is<n=[ "6 ray NN

(b) Using (a), and the lack-of-memory of the process, the required probability is

(ais) wey
Bty/] Bty

(c) Using Theorem (6.12.7),

1 ~l+e”
E(min{S, 7}| BQ) = 1) = Eames} =" = ,

2. Let B, be the ball with centre 0 and radius r, and let N- = |T] MN B;|. We have by Theorem
(6.13.11) that S, = xelins, g(x) satisfies

A(u)

BOS 1M) = Nr ff 8) de,

where A(B) = Sye p(y) dy. Therefore, E(S,) = f By g(w)A(u) du, implying by monotone conver-
gence that E(S) = faa g(u)A(u) du. Similarly,

2
nt io =(| Ss; i)
xelINBy

-»( S- ro) > s@eay)

xe€lNNBy xy
x, yENNBy

AQ) A(W)A(¥)
= N, 2 d 7 (Np — —_ 5
i g(u) AB) u+ N,( 1) Threw g(u)g(v) ACB, dudv

whence 5
E(S2) = | g(w)?2(w) du + ( | g(w).(u) da)
By By

By monotone convergence,
E(S?) = | g(u)2A(u) du + E(S)2,
By

and the formula for the variance follows.

3. If B,, Bo,..., By, are disjoint regions of the disc, then the numbers of projected points therein
are Poisson-distributed and independent, since they originate from disjoint regions of the sphere. By

301

[6.13.4]-[6.13.8] Solutions Markov chains
elementary coordinate geometry, the intensity function in plane polar coordinates is 24/11 — r?,
O<r<1,0<0 <2z.

4, The same argument is valid with resulting intensity function 24./1 — r2.

5. The Mercator projection represents the spherical coordinates (@, 6) as Cartesian coordinates in
the range 0 < @ < 27, 0 < 0 <7. (Recall that 0 is the angle made with the axis through the north
pole.) Therefore a uniform intensity on the globe corresponds to an intensity function 1 sin @ on the
map. Likewise, a uniform intensity on the map corresponds to an intensity 4./ sin 6 on the globe.

6. Let the X; have characteristic function @. Conditional on the value of N(¢), the corresponding
arrival times have the same distribution as N(t) independent variables with the uniform distribution,
whence

E(e?S@) _ E{EeSO | N(t))} _ B{E(e@ Xe NO}

cay t
= exp{ar(B(ei@Xe*") — 1)} = exo{ {¢(0e%) — 1} au},
0
where U is uniformly distributed on [0, t]. By differentiation,

d

E(S(t)) = —i6§@)) = ged — e™),

RE(X2)
2a

E(S(t)?) = — 6%) = ES)? + (1 — et),

Now, fors < t, S(t) = S(s)e7#E-9) 4 XG —s) where S(t — s) is independent of S(s) with the same
distribution as S(t — s). Hence, fors < t,

2 2
MEX) _ e7 208) g—a(t—s) + AE(X") (aw

_— —a(i—s) _.
cov(S(s), S(t)) = var(S(s))e = =

as s — oo with v = t — s fixed. Therefore, p(S(s), S(s + v)) ~ e-%" ass > ow.

7. The first two arrival times 7), Ty satisfy
XL xX
P(T, <x, m%—-—T,>y) -/ Aue AM e-—AUtY)-A®) gy -/ Aue" AWTY) dy.
0 0

Differentiate with respect to x and y to obtain the joint density function A(x)A(x + y)e7A@),
x, y > 0. Since this does not generally factorize as the product of a function of x and a function of y,
T, and T> are dependent in general.

8. Let X; be the time of the first arrival in the process N;. Then

PU =1, T >t) =P(t < Xj < inf{X2, X3,...})

© ols =n AL oat
-/ P(inf{X2, X3,...} > x)Aje** dx = Te
t

302

Markov chain Monte Carlo Solutions [6.14.1]-[6.14.4]
6.14 Solutions. Markov chain Monte Carlo

1. If P is reversible then
RHS = H(e pays ) vi = SP mipiyxiyi = > my pyivixy = Dmx) (x piivi) = LHS.
i Sj i,j i,j j i

Suppose conversely that (x, Py) = (Px, y) for all x, y €/ 2 (sr). Choose x, y to be unit vectors with 1
in the ith and jth place respectively, to obtain the detailed balance equations x; pjj = 7; pji-

2. Just check that 0 < 5;; < 1 and that the p;;j = gj; bj; satisfy the detailed balance equations
(6.14.3).

3. Itis immediate that pj, = |Ajx|, the Lebesgue measure of Aj. This is a method for simulating
a Markov chain with a given transition matrix.

4. (a) Note first from equation (4.12.7) that d(U) = 5 SUP; dry (u;., uj.), where uj. is the mass
function u;;, t € T. The required inequality may be hacked out, but instead we will use the maximal
coupling of Exercises (4.12.4, 5); see also Problem (7.11.16). Thus requires a little notation. For
i,j € S,i # j, we find a pair (X;, X;) of random variables taking values in T according to the

marginal mass functions u;., 4j., and such that P(X; # Xj) = sary (uj., uj.). The existence of
such a pair was proved in Exercise (4.12.5). Note that the value of X; depends on j, but this fact
has been suppressed from the notation for ease of reading. Having found (X;, X;), we find a pair
(Y(X;), ¥(X;)) taking values in U according to the marginal mass functions v Xj UX;5 and such that

P(Y (Xi) A Y(X;) | Xi, Xj) = S4tv (vx; Ux;.)- Now, taking a further liberty with the notation,

P(Y(X;) # ¥(Xj)) =\ 5 PU =r, Xj = SPO £4 YO))
ses
xs

= > P(X =r, Xj =5)Zdtv(rr., vs.)
rses

rxés
< {3 sup ary (rs vs.) }P(X; # Xj),
rFS

whence
d(UV) = sup P(Y (Xi) #Y(X;)) < {3 sup dry r vs.) } {sup P(X; # X;)}
ifj rs i,j

and the claim follows.
(b) Write S = {1, 2,..., m}, and take

ya {PXo=)D PXo=2) --- P(Xo =m)
~\ PY =1) P(¥p =2) --- PY =m) /°

The claim follows by repeated application of the result of part (a).
It may be shown with the aid of a little matrix theory that the second largest eigenvalue of a finite
stochastic matrix P is no larger in modulus that d(P); cf. the equation prior to Theorem (6.14.9).

303

[6.15.1]-[6.15.6] Solutions Markov chains

6.15 Solutions to problems

1. (a) The state 4 is absorbing. The state 3 communicates with 4, and is therefore transient. The set
{1, 2} is finite, closed, and aperiodic, and hence ergodic. We have that f34(7) = qr! J so that
faa = Dn fag(n) = §.

(b) The chain is irreducible with period 2. All states are non-null persistent. Solve the equation
x = 7P to find the stationary distribution x = (2. <: > z) whence the mean recurrence times are
g 16 IS, 8, in order.

2. (a) Let P be the transition matrix, assumed to be doubly stochastic. Then

> pi) = 2 > pik - Dy = o(S Pik (n — ») Pkj
ik k i

t

whence, by induction, the -step transition matrix P” is doubly stochastic for all n > 1.

If j is not non-null persistent, then p;;(n) > Oasn — oo, foralli, implying that >>; pi; (2) > 0,
a contradiction. Therefore all states are non-null persistent.

If in addition the chain is irreducible and aperiodic then p;;(n) —> 2;, where m is the unique

stationary distribution. However, it is easy to check that r = (N~!, N—!,..., N7!) is a stationary
distribution if P is doubly stochastic.

(b) Suppose the chain is persistent. In this case there exists a positive root of the equation x = xP, this
root being unique up to a multiplicative constant (see Theorem (6.4.6) and the forthcoming Problem
(7)). Since the transition matrix is doubly stochastic, we may take x = 1, the vector of 1’s. By the
above uniqueness of x, there can exist no stationary distribution, and therefore the chain is null. We
deduce that the chain cannot be non-null persistent.

3. By the Chapman—Kolmogorov equations,
Pulm +r+n)> py) pM pi), m,r,n > 0.
Choose two states i and j, and pick m and n such that a = pj; (m)p;;(n) > 0. Then
Dig(m +r +n) > aps; (r).

Set r = 0 to find that p;;(m +n) > 0, and sod(@) | (m+n). Ifd(@i) {r then pjj(m +r +n) =0, so
that p;;(r) = 0; therefore d@) | d(j). Similarly d(j) | d(@), giving that d@) = d(j).
4. (a) See the solution to Exercise (6.3.9a).
(b) Let i, j, 7, s € S, and choose N(i, r) and N(j, s) according to part (a). Then
P(Zn = (7,5) | Zo = @, J) = Pir ™)pjs(n) > 0

ifn > max{N(i,r), N(j, s)}, so that the chain is irreducible and aperiodic.
(c) Suppose S = {1, 2} and
_f0 1
P= (} 0)

In this case {{1, 1}, {2, 2}} and {{1, 2}, {2, 1}} are closed sets of states for the bivariate chain.

5. Clearly P(N = 0) = 1 — fi;, while, by conditioning on the time of the nth visit to 7, we
have that P(N > n+1|N =n) = fj; forn > 1, whence the answer is immediate. Now
P(N = co) = 1 — R29 P(N =n) which equals 1 if and only if fj; = fj; = 1.

6 Fixi # j and let m = min{n : pjj(n) > 0}. If Xo =i and X» = j then there can be no
intermediate visit to i (with probability one), since such a visit would contradict the minimality of m.

304

Problems Solutions [6.15.7]-[6.15.8]

Suppose Xo = i, and note that (1 — f;;)pjj(m) < 1 — fiz, since if the chain visits j at time m
and subsequently does not return to i, then no return to i takes place at all. However fj; = 1 ifi is
persistent, so that fj; = 1.

7. (a) We may take S = {0, 1,2, ...}. Note that g;;(”) > 0, and
[o.@)
Sai =1, gat) => auDayo,
j 1=0

whence Q = (qj; (1)) is the transition matrix of a Markov chain, and Q” = (qj; (#)). This chain is
persistent since

So ai) => pi) =00 forall,
n n
and irreducible since i communicates with j in the new chain whenever j communicates with i in the
original chain.
That
xj : .
(*) gij(n) = 5 iM: ifj,n>l,
i

is evident when n = 1 since both sides are qj; (1). Suppose it is true for n = m where m > 1. Now

imt+)= SO Ue), tb 4A,

kik#éj
so that x
i(m+l= YS seman), iF),
%i kikeshj
which equals g;; (mm + 1) as required.
(b) Sum () over n to obtain that
xj, oy
(4+) 1=~p;(j), if j,
xi

where 0;(j) is the mean number of visits to i between two visits to 7; we have used the fact that
yen 81j (2) = 1, since the chain is persistent (see Problem (6.15.6)). It follows that x; = xo; (0) for
all i, and therefore x is unique up to a multiplicative constant.

(c) The claim is trivial when i = j, and we assume therefore that i 4 j. Let N;(j) be the number
of visits to i before reaching j for the first time, and write Py and Ex for probability and expectation
conditional on Xo = k. Clearly, Pj(Nj (Vj) 2) = Aji — Ay! for r > 1, whence

oe)
hii
pi(j) = Bj (Ni (A) = DOP =) =

r=1 y

The claim follows by (««),

8. (a) If such a Markov chain exists, then
Hn
tn=)_ filn-i. n= 1,
i=l

305
[6.15.9]-[6.15.9] Solutions Markov chains

where f; is the probability that the first return of X to its persistent starting point s takes place at time
i. Certainly ug = 1.

Conversely, suppose u is a renewal sequence with respect to the collection (fi, : m > 1). Let X
be a Markov chain on the state space § = {0, 1, 2, ...} with transition matrix

jo {Pozitaipeled if j =i +1,
PU \ 1 _pr >i42|T>it)) iff =0,

where T is a random variable having mass function fy, = P(T = m). With Xo = 0, the chance that
the first return to O takes place at time n is

n—-1

P(X = 0, Il X; #0 | Xo = 0) = P0O1P12°** Pn—2,n—1Pn—1,0
1

G+ ») "=! GG 41)
={1-
( cm J Ua

=G(n)-Ga+)= fn

where G(m) = P(T > m) = -7°_, fn. Now un = P(Xy = 0| Xo = 0) satisfies
n
vo = 1, tn = - fitn-i forn > 1,
i=l

whence vy = uy, for all n.

(b) Let X and Y be the two Markov chains which are associated (respectively) with u and v in the
above sense. We shall assume that X and Y are independent. The product (u;vp : n > 1) is now the
renewal sequence associated with the bivariate Markov chain (Xn, Yn).

9. Of the first 2n steps, let there be i rightwards, 7 upwards, and k inwards. Now X2, = 0 if
and only if there are also i leftwards, 7 downwards, and k outwards. The number of such possible
combinations is (2n)!/{(i! j! k!)*}, and each such combination has probability (2)?¢+/+) = (4)2”.
The first equality follows, and the second is immediate.

Now

1\7" (2n n!
© Pn =0) < (5) (*)u DRI jE
i+j+k=n

where

M = max {or i,j,k > 0, i+ j+k=ah.
It is not difficult to see that the maximum M is attained when i, j, and k are all closest to 5n, so that

n!
< —
~ 3(L an]!

Furthermore the summation in () equals 1, since the summand is the probability that, in allocating n
balls randomly to three urns, the urns contain respectively i, j, and k balls. It follows that

(2n)!

P(Xy, = 0) < —
Man = 9) = mnt inlys

306
Problems Solutions [6.15.10]-[6.15.13]

3
which, by an application of Stirling’s formula, is no bigger than Cn 2 for some constant C. Hence
yn P(Kan = 9) < 00, so that the origin is transient.

10. No. The line of ancestors of any cell-state is a random walk in three dimensions. The difference
between two such lines of ancestors is also a type of random walk, which in three dimensions is
transient.

11. There are one or two absorbing states according as whether one or both of @ and 8 equal zero. If
aB # 0, the chain is irreducible and persistent. It is periodic if and only if a = 6 = 1, in which case

it has period 2.
( B a } f )

If0 < af < | then
is the stationary distribution. There are various ways of calculating P”; see Exercise (6.3.3) for
example. In this case the answer is given by

n_i__y,_pynf & —a@ Ba).
(a + p)P" =(1-a—f) (%, r+ a“)

proof by induction. Hence

(a + B)P” > ¢ “) asin —> 00.

The chain is reversible in equilibrium if and only if 21 pj2 = 72,21, which is to say that aB = Ba!

12. The transition matrix is given by
+\ 2

N-

( ") if j=i4+1,
-\2 “\2
i N-i ip:

~\2

i

7 if j=i-1,

(x) et

for 0 < i < N. This process is a birth-death process in discrete time, and by Exercise (6.5.1) is
reversible in equilibrium. Its stationary distribution satisfies the detailed balance equation 7; p;,j41 =

2
mj41Pi+1,i forO <i < N, whence 7; = mo() for 0 <i < N, where

13. (a) The chain X is irreducible; all states are therefore of the same type. The state 0 is aperiodic,
and so therefore is every other state. Suppose that Xp = 0, and let T be the time of the first return to
0. Then P(T > n) = aga) ---a,—1 = bn for n > 1, so that 0 is persistent if and only if b, —> 0 as
n— OO.

(b) The mean of T is

oO CO
E(T) = So PIT >n)= Son.
=0 =0

307

[6.15.14]-[6.15.14] Solutions Markov chains

The stationary distribution 7 satisfies
po = So me( — ag), Tn =Npy,-14,-1 forn > 1.
k=0

Hence mn = mobn and 79 | = +2 bn if this sum is finite.
(c) Suppose a; has the stated form fori > I. Then
n-1
bn =by [[U- Ai), n=.
i=]

Hence by, —> 0 if and only if $0; Ai ~B = oo, which is to say that 8 < 1. The chain is therefore
persistent if and only if 6 < 1.

(d) We have that 1 — x < e~* for x > 0, and therefore

fore) oo n—-1 oo
So bn < by Yew{-4 oi} <b; > exp {-An n~P <oo if6 <1.
n=I n=l i=] n=l

(e) If 8 = 1 and A > 1, there is a constant c; such that

[oe] fo 6) n—-1 1 oo oO
So bn <b; Yexp{-4 i} <cy S— exp {—Alogn} =c] SinA < 00,
n=l n=l i=]

n=l n=l

giving that the chain is non-null.
(f) If 8 = land A <1,

aml (Seni (5

=) = by I-1
n—-1/)°
Therefore 5°, by = 00, and the chain is null.

14, Using the Chapman—Kolmogorov equations,

< (1- paih)) py + > vie(h)
kf

lpi +4) — py Ol = — Sik) Pej ©)

< (1— puh)) + (1 — pis) > 0

ash | 0, if the semigroup is standard.

Now log x is continuous for 0 < x < 1, and therefore g is continuous. Certainly g(0) = 0. In
addition p;j(s +t) > pj (s) pit) for s,t > 0, whence g(s + t) < g(s) + g(t), 5,t > 0.

For the last part

1 g(t) pitt) —1

~ (pi) — 1) = —— -

j (PHO =O ogi — = ps)
ast | 0, since x/log(1 —x) — —lasx | 0.

308

Problems Solutions [6.15.15]-[6.15.16]

15. Leti and j be distinct states, and suppose that p;;(¢) > 0 for some t. Now

cana |
pi) = DI Gai,

n=0 ~
implying that (G”);; > 0 for some n, which is to say that
(*) Bi,ky Sky sky Bln, j FO

for some sequence kj, kz, ..., kn of states.

Suppose conversely that (*) holds for some sequence of states, and choose the minimal such value
of n. Then i, ky, kz,..., kn, j are distinct states, since otherwise n is not minimal. It follows that
(G");; > 0, while (G”);; = 0 for 0 < m < n. Therefore

oy ke
k
py) =0" 5 a (G")ij
k=n

is strictly positive for all sufficiently small positive values of t. Therefore i communicates with j.
16. (a) Suppose X is reversible, and let i and j be distinct states. Now

P(X(0) =i, X() = j) = P(X) =i, X() = j),
which is to say that 7; p;;(¢) = 7; p;i(t). Divide by ¢ and take the limit as ¢ | 0 to obtain that

MBij = Tj 8ji-
Suppose now that the chain is uniform, and X (0) has distribution w. If ¢ > 0, then

PXO =) =o mp = 7,

so that X(t) has distribution 7 also. Now let t < 0, and suppose that X(t) has distribution w. The
distribution of X (s) for s > 0 is wPs—; = x, a polynomial identity in the variable s — t, valid for all
s > 0. Such an identity must be valid for all s, and particularly for s = ¢, implying that w = x.

Suppose in addition that 7; 2;; = 1; 9;; for alli, 7. For any sequence k1, k2,..., kn of states,
Wi 8i,ky 8k, ky °° Skin, j = Sky i ky 8ky kp Bhan jf 0 8k i8ko ky Bi kn MH -
Sum this expression over all sequences k,, ko, ..., ky of length n, to obtain
mG"); = 1j(G"*!)j;, n> 0.

It follows, by the fact that P, = eG, that

00 io.@]
1 1
nm pit) =m > ait (Gi = Hi > it (Gi = APIO
m=0 ~ m=0 ~

for all i, j,f. Fort; < tg <--++< ft,

P(X(t)) = 11, X(t) = in, .... X(t) =in)
= Mi; Piz in (t2 — 11) * ++ Pinyin Un — tn-1)
= Pin, iy (2 — C1) Fin Pin, iz (3 — 12) +++ Pin) ,in Gn — 1) =
= Pini (12 — 11) +++ Pinig_1 Gn — 2-1) Bin
= P(¥(@) =i, Y(t2) =i2,...,¥ Gn) = in),

309

[6.15.17]-[6.15.19] Solutions Markov chains

giving that the chain is reversible.
(b) Let S = {1, 2} and

o=(G 4)

where af > 0. The chain is uniform with stationary distribution

(c) Let X be a birth-death process with birth rates 4; and death rates j4;. The stationary distribution
satisfies

1 — AQMO =O, Wet MeL — ThA = RAR — WR—-1AK-1 fork > 1.

Therefore x4.) 4g4-1 = Ax for k = O, the conditions for reversibility.

17. Consider the continuous-time chain with generator

G= (;’ B )
y —¥Y
It is a standard calculation (Exercise (6.9.1)) that the associated semigroup satisfies

_( y+Bhe) BO-AW)
(B+ Y)Pr = (ra —h@) B+yhit) )

where h(t) = e+”), Now Py = P if and only if y + Bh(1) = B + yh(1) = a(B + y), which is
to say that B = y = -4 log(2a — 1), a solution which requires that a > 5.

18. The forward equations for py, (t) = P(X (ft) = n) are

Po(t) = up — Apo,
Ppt) =App-1— A+nu) prt uat ppg, n21.

6 _ 6 (ag— 2%
ra BOs

In the usual way,

with boundary condition G(s,0) = s/. The characteristics are given by

_ ds _ dG
~ pws—1l A(s— DG’

dt

and therefore G = e? S~)) ¢ ((s —1)e~#*), forsome function f, determined by the boundary condition
to satisfy e® S—) ¢(s — 1) = s!. The claim follows.
Ast > 00, G(s, t) > e? S—)), the generating function of the Poisson distribution, parameter p.

19. (a) The forward equations are

ra)
5, Pit (s,t) =—-A@ pi, 0),

a . .
9p Pil +4) = —AC) pi (St) +A pi, 7-10), i<j.

310

Problems Solutions [6.15.20]-[6.15.20]

Assume N(s) =i ands < t. In the usual way,

oe)
G(s,t;x) = 5 x/P(N@ = j|N(s) =i)
j=i

satisfies

9G _ a NG
van (t)@ — 1)G.

We integrate subject to the boundary condition to obtain

. t
G(s,t;x) =x' exp {( - » f ay du ,

whence pj; (t) is found to be the probability that A = j — i where A has the Poisson distribution with
parameter f Au) du.
The backward equations are

ft)
95 Pil (s,t) =A) pi41,;, 0) — AG) pi 1);

using the fact that p;+1,;(t) = pi,;-1(), we are led to

9G As) — DG.
as

The solution is the same as above.
(b) We have that

t
PT > t) = poolt) = exp | - [ AY) au},

so that ;
fra) =1~@ exp - [ Cu) au}, t>0.

In the case A(t) = c/(1 +2),

oo OO du
T= R(T dt=
BO) | (T> Hae [ d+?

which is finite if and only if c > 1.

20. Let s > 0. Each offer has probability 1 — F(s) of exceeding s, and therefore the first offer
exceeding s is the Mth offer overall, where P(M = m) = F (sy"—![1 — F(s)], m = 1. Conditional
on {M = m}, the value of X yy is independent of the values of X1, X2,..., Xyg_1, with

1-F@)

O<s <u,

and X1, X2,..., Xyg_1 have shared (conditional) distribution function

_ Fu) <u<s

311

(6.15.21]-[6.15.21] Solutions Markov chains

For any event B defined in terms of X1, X2,..., Xy—1, we have that

oe)
P(Xy >u,B)= > P(Xy > u,B| M=m)P(M =m)
m=1
[o.<)
= 0 P(Xy > u|M=m)P(B | M =m)P(M =m)
mz=z1

[o@)
=P(Xy >u) >> P(B| M=m)P(M =m)
m=1

=P(Xy > u)P(B), O<s <u,

where we have used the fact that P(X yy > u | M = m) is independent of m. It follows that the first
record value exceeding s is independent of all record values not exceeding s. By a similar argument
(or an iteration of the above) all record values exceeding s are independent of all record values not
exceeding s.

The chance of a record value in (s,s + A] is

F(s+h)—F(s) ff (s)h
1— F(s) ~ 1 ~ F(s)

Ps <Xy<s+h)= + o(h).

A very similar argument works for the runners-up. Let Xy,,X,,.-. be the values, in order,
of offers exceeding s. It may be seen that this sequence is independent of the sequence of offers
not exceeding s, whence it follows that the sequence of runners-up is a non-homogeneous Poisson
process. There is a runner-up in (s, s + A] if (neglecting terms of order o(/)) the first offer exceeding
s is larger than s + h, and the second is in (s, s + 4]. The probability of this is

(44°) See Fsyh
1— F(s) 1 - F(s) 1 - F(s)

) +o(4) = + of).

21. Let F;(x) = P(N*(t) < x), and let A be the event that N has a arrival during (¢,f +). Then
Fin (t) = AAP(N*(¢ +h) <x| A) + (1 — Ah) F(x) +018)

where
foe)

P(N*(¢+h) <x|A)= / F(x — y) f(y) dy.
OO

Hence 3 oo
ap ft) = —AF;(x) +a Fi(x — y) fQ) dy.

Take Fourier transforms to find that ¢;(@) = E(e!#4* ©) satisfies

a
om = —Adt + AOrd,

an equation which may be solved subject to ¢9(0) = 1 to obtain ¢;(6) = et G@)-1,
Alternatively, using conditional expectation,

$1(0) = E{E(e!°N"© | N@))} = E{O@)N}
where N(¢t) is Poisson with parameter At.

312

Problems Solutions [6.15.22]-[6.15.25]

22. We have that

whence E(N(t)) = 4 (Ay + Ag)t and var(N(t)) = 4(Ay + Ag)t + FAq — Ap)? 2?.
23. Conditional on {X(t) = i}, the next arrival in the birth process takes place at rate Ai.
24. The forward equations for py (t) = P(X (t) = n) are

1+pa-1) 14+ pn
’ (t) = ————p, 102) - t), > 0,
Pr®) itp Pn—-1(t) iter n>

with the convention that p_;(t) = 0. Multiply by s” and sum to deduce that

d+ ut)” = 56 + ys?22 50S

as Gg — us
acer ary ary
as required.

Differentiate with respect to s and take the limit as s ¢ 1. If E(X (t)*) < 00, then

aG
m(t) = E(X(@)) = Bs

s=1

satisfies (1 + ut)m/(t) = 1 + wm(t) subject to m(0) = I. Solving this in the usual way, we obtain
m@t)=I+d4+ypbt.
Differentiate again to find that
a2G

n(t) = E(X(t)(X(t) — 1) = 5

s=1

satisfies (1 + pt)n’(t) = 2(m (tf) + um(t) + un(t)) subject to (0) = I ([ — 1). The solution is
at) =10 -1) +270 +uDt+dtuDdd tut yene?.

The variance of X(t) is n(t) + m(t) — m(t)?.
25. (a) Condition on the value of the first step:
x .

J .
Aj t uy

Bj
: . . + ne
nj Nj+1 hj + jj

as required. Set x; = ni+.1 — nj to obtain A;x; = ujx;_1 for j = 1, so that

It follows that ;
j J
nj+i=not > xe =1+(m—-) doe.
k=0 k=0
The n; are probabilities, and lie in [0, 1]. If “? ex = co then we must have n; = 1, which implies
that 7; = 1 for all j.

313

[6.15.26]-[6.15.28] Solutions Markov chains
(b) By conditioning on the first step, the probability n;, of visiting 0 having started from j, satisfies

n= (G+ D2 nya + 7? nj-1
’ P+G+)?

Hence, (j + 1)?(nj41 — 1j) = i7(nj — nj -1), giving (7 + 1)?(nj-41 — 2) = 11 — no. Therefore,

j
1
1-nj41=Ud-—m) >> —,;
oo FFD

> d- man? as j —> 00.

By Exercise (6.3.6), we seek the minimal non-negative solution, which is achieved when 4; = 1 —
(6/2).

26. We may suppose that X(0) = 0. Let TJ, = inf{t : X(t) = n}. Suppose TJ, = T, and let
Y = T,41 — T. Condition on all possible occurrences during the interval (T, T + /) to find that
E(Y) = Anh)A + unh(h + EY’) + (1 — Anh — enh) (h + E(Y)) + off),

where Y’ is the mean time which elapses before reaching n + 1 from n — 1. Set mn = E(Ty41 — Tn)
to obtain that
My = Unh(my—1 + mn) + my +h{l — An + Un) mp} + o(h).

Divide by / and take the limit ash | 0 to find that A,my, = 1+ unm,—1,n > 1. Therefore
1 pn 1 Ln 4, Bnbn-1"*" BA

" An An nt An AnAn-l1 AnAn—1°+*4A0

since mg = Ag | The process is dishonest if 0 Mn < 00, Since in this case Tog = lim T;, has
finite mean, so that P(Too < 00) = 1.

On the other hand, the process grows no faster than a birth process with birth rates A;, which is
honest if }7°° 9 1/An = 00. Can you find a better condition?

27. We know that, conditional on X (0) = J, X (#) has generating function

at(1 =o +s)!

G6. = (a —s)41

so that

I
P(T <x |X) =D) =P(X(x) =0| X0) =) =G0,x) = (* :)

It follows that, in the limit as x > ov,

per =x) => ( Ax ) Pao) =) =Gx0)( Ax jaa
WO ep Nae $1 ) \Ox +1

For the final part, the required probability is {x7/(«7 + DY = {1+ @D7!}-!, which tends to
e7!/* as I -> 00.

28. Let Y be an immigration—death process without disasters, with Y (0) = 0. We have from Problem
(6.15.18) that Y(t) has generating function G(s, t) = exp{o(s — 1)(1 — e~#*)} where p = A/p. As
seen earlier, and as easily verified by taking the limit as t — oo, Y has a stationary distribution.

314

Problems Solutions [6.15.29]-[6.15.31]

From the process Y we may generate the process X in the following way. At the epoch of each
disaster, we paint every member of the population grey. At any given time, the unpainted individuals
constitute X, and the aggregate population constitutes Y. When constructed in this way, it is the case
that Y(t) < X(t), so that Y is a Markov chain which is dominated by a chain having a stationary
distribution. It follows that X has a stationary distribution 7 (the state 0 is persistent for X, and
therefore persistent for Y also).

Suppose X is in equilibrium. The times of disasters form a Poisson process with intensity 6. At
any given time f, the elapsed time T since the last disaster is exponentially distributed with parameter
5. At the time of this disaster, the value of X(t) is reduced to 0 whatever its previous value.

It follows by averaging over the value of T that the generating function H(s) = )>7o.ps"zn of
X(t) is given by

1
H(s)= [ de “G(s, u) du = mere) | x6/M)—-le~es-Dx gy
0 ub 0

by the substitution x = e~“”. The mean of X(t) is

H'(l) = L Se" E(Y (u)) du = [ se! p (1 —e##) du = PH = *
0 0 é+u S+ph

29. Let G(|B|, s) be the generating function of X(B). If BNC = @, then X(BUC) = X(B)4+X(C),
so that G(a+ 6, s) = G(a, s)G(f, s) for |s| < 1,a, 6 = 0. The only solutions to this equation which
are monotone in & are of the form G(a, s) = e®“) for |s| < 1, and for some function A(s). Now any
interval may be divided into n equal sub-intervals, and therefore G(q, s) is the generating function of
an infinitely divisible distribution. Using the result of Problem (5.12.13b), A(s) may be written in the
form A(s) = (A(s) — 1)A for some A and some probability generating function A(s) = iw ajs* . We
now use (iii): if |B| = a,
P(X(B)=>1) 1 -—e%@o-)
= >
P(X(B)=1) — ardaye**@0-D)

asa | 0. Therefore ag + a; = 1, and hence A(s) = ag + (1 — ag)s, and X(B) has a Poisson
distribution with parameter proportional to |B].

1

30. (a) Let M(r, s) be the number of points of the resulting process on R+ lying in the interval (7, s].
Since disjoint intervals correspond to disjoint annuli of the plane, the process M has independent
increments in the sense that M(r1, 51), M(r2, 52), ..., M(rn, sn) are independent whenever rj <
Sy <Q <-+: <1 < Sp. Furthermore, forr <s andk > 0,

x _ k j~An(s—r)
P(M(r,s) =k) = P(N has k points in the corresponding annulus) = tAx(s as é

(b) We have similarly that

oo (Amrx2)f eax?
P(Ra < x) = PCN has least & points in circle of radius x) = S> ——_—
r!
rk

and the claim follows by differentiating, and utilizing the successive cancellation.

31. The number X(S) of points within the sphere with volume S and centre at the origin has the
Poisson distribution with parameter AS. Hence P(X (S$) = 0) = e745, implying that the volume V of
the largest such empty ball has the exponential distribution with parameter A.

315

[6.15.32]-[6.15.33] Solutions Markov chains

It follows that P(R > r) = P(V > er”) = e7€" ” for r > 0, where c is the volume of the unit
ball in x dimensions. Therefore

a — A
fr(r) = Ancr®™ eer r>0.

Finally, E(R) = fo? eer" dr, and we set v = Acr”.

32. The time between the kth and (k + 1)th infection has mean Ags whence

N
E(T) porn

Fl

Now

ul 1 1 {ota 1 \
SKN +1-K N41 Gk GN+1-k

~2 yl. a {logN +y +0(N71)}
Tok NFILOEN TY

ray

It may be shown with more work (as in the solution to Problem (5.12.34)) that the moment
generating function of A(N + 1)T — 2log N converges as N > 00, the limit being {T'(1 — 6)}?.

33. (a) The forward equations for pn (t) = P(V(t) =n + 4) are
Prt) = (@ + V)pngi@) — 2nt+ Dpn@) +n, 229,

with the convention that p_;(¢) = 0. It follows as usual that

aG _ 9G _(,,3G 9G

as required. The general solution is

1 1
Gon= oF (r+ i)

for some function f. The boundary condition is G(s, 0) = 1, and the solution is as given.

(b) Clearly
T T
my(T) =E | In, dt -[ Ent) at
0 0

by Fubini’s theorem, where I; is the indicator function of the event that V(t) = n + h.
As for the second part,

l-s

ce r logf1 + (1 —s)T
$= s"mp(T) =| G(s, t)dt = logil + — s)T]
_ 0

so that, in the limit as T —> 0,

oe)
2 8" (mn(T) — log T) = = log (4 2) > wet 2) 5) > an
n=0

316

Problems Solutions [6.15.34]-[6.15.35]

1

if |s| < 1, where an = 7%_, i—*, as required.

(c) The mean velocity at time ¢ is

2° as|eny 2
02) = | “27
A cme.
(2, 1) _o7
(0, 1) > > zt_Le_.
(a, 1) 1, 3)
A
(3,1)
(1, 0) ~~ = so77
d, 2) ~~
A Sort

34. It is clear that Y is a Markov chain, and its possible transitions are illustrated in the above
diagram. Let x and y be the probabilities of ever reaching (1, 1) from (1, 2) and (2, 1), respectively.
By conditioning on the first step and using the translational symmetry, we see that x = ay + 5x7
and y = 4 + bxy. Hence x? — 4x2 4+.4x —1=0, an equation with roots x = 1, 33 + /5). Since
x is a probability, it must be that either x = 1 or x = 33 — /5), with the corresponding values of
y=landy= 4/5 — 1). Starting from any state to the right of (1, 1) in the above diagram, we
see by recursion that the chance of ever visiting (1, 1) is of the form x%y* for some non-negative
integers a, 8. The minimal non-negative solution is therefore achieved when x = 33 — 4/5) and

y= 5/5 — 1). Since x < 1, the chain is transient.

35. We write A, 1, 2, 3, 4, 5 for the vertices of the hexagon in clockwise order. Let T; = min{n >
1: X, =i} and P;(.) = PC | Xo =).
(a) By symmetry, the probabilities p; = P;(Ta < Tc) satisfy

1,1
PA= 3p, Pl=3t+35P2, p2a= 5P1 + 53, P3= 3 po,

whence pa = H-

(b) By Exercise (6.4.6), the stationary distribution is zc = f, = 3 fori # C, whence wa =
ay) = 8.

(c) By the argument leading to Lemma (6.4.5), this equals uamc = 2.

(d) We condition on the event EF = {Ta < Tc} as in the solution to Exercise (6.2.7). The probabilities
b; = P;(&) satisfy

bp =444b2, by = 4b, +4b3, 3 = Fb,

317
[6.15.36]-[6.15.37] Solutions Markov chains

yielding b) = by = é , b3 . The transition probabilities conditional on E are now found by

equations of the f form
— PoA)pi2_ b2 _ 1

P\(E) 3b,.—«7”

and similarly t2) = 5, 723 = 5 732 = 4, TA = $. Hence, with the obvious notation,

Hox =14 Surat §usa, w3a=1tu2a, MaA=14+Fu2<,

giving “1A = wo , and the required answer is 1+ wja = 1 + =.
6. (a) We have that
Bim — i)? a(i + 1)?
Piitl = me”? Pi+1,i = ry

Look for a solution to the detailed balance equations

B(m —i)* a(i +1)?
Kj =Ti+1—— 9

m2

to find the stationary distribution

2 m 2
Tj = (") (B/o)'xp, where mp = »(") (B/a)'

i=0

(b) In this case,
B(m —i) ad +1)

Piit1 = m > Pi+tli= m

Look for a solution to the detailed balance equations

B(m —i) ag +1)
Aj = Ni+1 ,
m

m

yielding the stationary distribution

m —-1 n

37. We have that

d(s+nHy= So mee (5+?)
k 1k

= > KC aj (S)PiKE) Fi. by the Chapman—Kolmogorov equations
j Tj Ik

. . t .
> SS Ik SS Mj PjRD) ails) by the concavity of c
i j Mk Ty

= Yims (22) = 6s),

318
Problems Solutions [6.15.38]-[6.15.41]

where we have used the fact that F mj pjk(t) = me. Now a;(s) —> mj ass — oo, and therefore
d(t) > c(1).

38. By the Chapman—Kolmogorov equations and the reversibility,

ug (2t) = 5 P(X (2t) = 0| X(t) = f)P(X@) = j | XO =0)
j

2
=> 2P(xen = j|X@ =0)uj@) =20 So 7; (u2) .
jd j

aj

2

The function c(x) = —x~“ is concave, and the claim follows by the result of the previous problem.

39. This may be done in a variety of ways, by breaking up the distribution of a typical displacement and
using the superposition theorem (6.13.5), by the colouring theorem (6.13.14), or by Rényi’s theorem
(6.13.17) as follows. Let B be a closed bounded region of R?, We colour a point of Tl atx ¢ R@
black with probability P(x + X € B), where X is a typical displacement. By the colouring theorem,
the number of black points has a Poisson distribution with parameter

[ AP@+X¢ Bydx=3 | ay [ P(X € dy —x)
Rd yeB xeR?

= / ay | P(X Edy) =A|BI,
yeB verR?d

by the change of variables v = y — x. Therefore the probability that no displaced point lies in B is
e~4181, and the claim follows by Rényi’s theorem.

40. Conditional on the number N(s) of points originally in the interval (0, s), the positions of these
points are jointly distributed as uniform random variables, so the mean number of these points which
lie in (—c<, a) after the perturbation satisfies

Ss ] OO
as [ <PX+u<a)du i [ Fy(a —u)du = E(R,) as s —> Oo,
os 0

where X is a typical displacement. Likewise, E(Lp) = 4 Sor [1 — Fy (a + u)] du. Equality is valid
if and only if

OO a
[u-Feonde= [ Freyas,
a —C
which is equivalent to a = E(X), by Exercise (4.3.5).
The last part follows immediately on setting X; = V-t, where V;, is the velocity of the rth car.

41. Conditional on the number N(*) of arrivals by time ft, the arrival times of these ants are distributed
as independent random variables with the uniform distribution. Let U be a typical arrival time, so
that U is uniformly distributed on (0, ¢). The arriving ant is in the pantry at time t with probability
nx =P(U +X > 2), or in the sink with probability 9 = P(U + X <t <U +X +), or departed
with probability 1 — o — a. Thus,

E(x4)y8O) _ E{E(x40 yBO | N(t)) }

=E{(ax+pyt+tl—a—- pyNOr = ftA-NeheY-D_

Thus A(t) and B(t) are independent Poisson-distributed random variables. If the ants arrive in pairs
and then separate,

E(x4M y3O | N(t)) = {1r2x? + Inpxy + py” + 2ymx +2ypy + y2}NO

319

[6.15.42]-[6.15.45] Solutions Markov chains
where y = 1 — x ~ p. Hence,
EXAM yBO) = exp{atrx + py +y)" — Y},

whence A(t) and B(¢) are not independent in this case.

42. The sequence {X;} generates a Poisson process N(t) = max{n : Sy, < t}. The statement that
Sn = t is equivalent to saying that there are n — | arrivals in (0, ¢), and in addition an arrival at t. By
Theorem (6.12.7) or Theorem (6.13.11), the first n — 1 arrival times have the required distribution.

Part (b) follows similarly, on noting that fy(w) depends on u = (w1, u2,..., un) only through
the constraints on the u;.

43. Let Y be a Markov chain independent of X, having the same transition matrix and such that Yo

has the stationary distribution 7. Let T = min{n > 1: X, = Y,} and suppose Xp =i. As in the
proof of Theorem (6.4.17),

|pij(n) — 71 = om (pij (2) — pej ))| < D0 meP(T > 2) =P(T > 2).
k k

Now,
P(T >r+1|T>r)<1-€? for r > 0,

where € = min;;{pj;} > 0. The claim follows with A = 1 — é2.

44. Let J; (n) be the indicator function of a visit to k at time n, so that EU, (1)) = P(Xyn = k) = ag(n),
say. By Problem (6.15.43), |ag(n) — 2%| < A". Now,

2 1 n-l 2
= 5E prize) - ni)
n r=0

1
= 5 EU) — mim) — m1)}.

# ([Eiw@—x
n

Let s = min{m,r} and t = |m — r|. The last summation equals
1
=z Dail) pis ©) — a5 (7m — a5 (m) 5; + 17}
r m
1
= 3 Gi) — MPO — m1) + mi(PUO — 2)
r m

+ 7; (aj (s) — mj) — mi (ai(r) — 1) — my (aj(m) — x) }

1 S+t t s r m
<3) ae +Ab4aS 4a" 40)
r m
An
sz70 asn — oOo,
n

where 0 < A < oo. For the last part, use the fact that ar ff Xr) = ies FOV; (). The result
is obtained by Minkowski’s inequality (Problem (4.14.27b)) and the first part.

45. We have by the Markov property that f(X,41 | Xn, Xn—1,--- , Xo) = f(Xn41 | Xn), whence
E(log f(Xn41 | Xn, Xn—1,--- Xo) |Xn»--. Xo) =E(log f(Xn41 | Xn) | Xn).

320

Problems Solutions [6.15.46]-{6.15.48]
Taking the expectation of each side gives the result. Furthermore,

H(Xn4i | Xn) = — 0 (pi log pij)P(Xn = i).
ij
Now X has a unique stationary distribution 7, so that P(X; =i) > 2; as n — oo. The state space

is finite, and the claim follows.

46. Let T = inf{t : X; = Y;}. Since X and Y are persistent, and since each process moves by
distance 1 at continuously distributed times, it is the case that P(T < oo) = 1. We define

{ xX; ift <T,
Z= .
Y; ift >T,

noting that the processes X and Z have the same distributions.
(a) By the above remarks,

(P(X; = k) — PCY; = &)| = |P(Zp = k) — PCY; =k)
<|P(Z, =k, T <t)+P(Zp =k, T >t) - PM =k, T <1) -P% =k, T> |
<P(X; =k, T>) +P; =k, T> Dd.

We sum over k € A, and let tf + oo.

(b) We have in this case that Z; < Y; for all t. The claim follows from the fact that X and Z are
processes with the same distributions.

47. We reformulate the problem in the following way. Suppose there are two containers, W and N,
containing n particles in all. During the time interval (¢, ¢ + dt), any particle in W moves to N with
probability dt + o(dt), and any particle in N moves to W with probability Adt + o(dt). The particles
move independently of one another. The number Z(¢) of particles in W has the same rules of evolution
as the process X in the original problem. Now, Z(t) may be expressed as the sum of two independent
random variables U and V, where U is bin(r, @;), V is bin(n — 7, wy), and 6; is the probability that a
particle starting in W is in W at time ¢, y is the probability that a particle starting in N at 0 is in W
at t. By considering the two-state Markov chain of Exercise (6.9.1),

A+ pe Atuyt a — re ~Atwt

o
A+ pu

and therefore
E(s*) = B(s¥)yE(s”) = (56; +1—5)" (5 +1—5)"7".

Also, E(X(#)) = r@; + (nm — r) yy and var(X(t)) = r@;. — 8;) + — rr). — 1). In the limit as
n — ©0, the distribution of X (¢) approaches the bin(n, A/(A + «)) distribution.

48. Solving the equations

HO = 4101 + prm2, 1 = 422 + Porto, Sox; =1,
i

gives the first claim. We have that y = 7; (pi — qi)2;, and the formula for y follows.
Considering the three walks in order, we have that:
A. m= 1 for each i, and ya = —2a < 0.

B. Substitution in the férmula for yg gives the numerator as 3{ _ Aga +o(a) } , which is negative
for small a whereas the denominator is positive.

321

[6.15.49]-[6.15.51] Solutions Markov chains

C. The transition probabilities are the averages of those for A and B, namely, pp = (tp -
ay+ (4 -aj= 3 —a, and so on. The numerator in the formula for yc equals 30 +o(1),
which is positive for small a.

49. Calla car green if it satisfies the given condition. The chance that a green car arrives on the scene
during the time interval (u, u + h) is ARP(V < x/(¢ — u)) for u < t. Therefore, the arrival process
of green cars is an inhomogeneous Poisson process with rate function

Au) = { AP(V <x/(t—u)) if u <t,
0 ifu>t.

Hence the required number has the Poisson distribution with mean

t t
rf e(v< +.) du=2 | P(v <=) du
0 t—u 0 u

t
= af EU (vuex}) du = AE(V—! min{x, V#}).
0

50. The answer is the probability of exactly one arrival in the interval (s, f), which equals g(s) =
AC —- sje7At-s), By differentiation, g has its maximum at s = max{0, tf — a7, and g(s) = et
when t > acl,

51. We measure money in millions and time in hours. The number of available houses has the
Poisson distribution with parameter 301, whence the number A of affordable houses has the Poisson
distribution with parameter é +301 = 5A (cf. Exercise (3.5.2)). Since each viewing time T has moment

generating function Ee®) = (78 - e’) /@, the answer is

Ga(E(e*")) = exp{5a(e7? — e? — 6)/0}.

322

7

Convergence of random variables

7.1 Solutions. Introduction

1. (a) El(eX)"| = |cl” + {Xl}.

(b) This is Minkowski’s inequality.

(c) Let e > 0. Certainly |X| > Je where J, is the indicator function of the event {|X| > ¢}. Hence
E|X"| > EjlZ| = P(\X| > ©), implying that P(|X| > €) = 0 for all e > 0. The converse is trivial.
2. (a) E({aX + bY}Z) = aE(XZ) + DE(YZ).

(b) E(X + Y}?) + E({X — ¥}?) = 2E(X?) + 2E(¥?).

(c) Clearly

n 2 n
E {5} ) = S>E(X?) +25 - E(X;X)).
i=1 i=]

i<j

3. Let fu) = Ze, g(u) = 0, h(u) = —4e, for all u. Then de(f, g) + de(g,h) = 0 whereas
de(f,h) = 1.

4. Either argue directly, or as follows. With any distribution function F, we may associate a graph
F obtained by adding to the graph of F vertical line segments connecting the two endpoints at each
discontinuity of F. By drawing a picture, you may see that /2d(F, G) equals the maximum distance
between F and G measured along lines of slope —1. It is now clear that d(F, G) = 0 if and only if
F = G, and that d(F, G) = d(G, F). Finally, by the triangle inequality for real numbers, we have
that d(F, H) < d(F,G)+d(G, H).

5. Take X to be any random variable satisfying E(X 2) = oo, and define Xp = X for all n.

7.2 Solutions. Modes of convergence
1. (a) By Minkowski’s inequality,
{B|X" |} < {BX — XI} + CELXh I;
let n -> 00 to obtain lim infy—o9 E|X},| > E|X’"|. By another application of Minkowski’s inequality,
{E[X5|}/" < {EUXn — XI} + {B1X"

whence lim sup, _, 59 E|X},| < E|X"|.

323

[7.2.2]-[7.2.4] Solutions Convergence of random variables

(b) We have that
|E(Xn) — E(X)| = |E(Xn — X)| < E|Xn — X| > 0

as n —> oo. The converse is clearly false. If each X, takes the values +1, each with probability 5
then E(X,,) = 0, but E|X, — 0| = 1.

(c) By part (a), E(X2) + E(X2). Now Xn —> X by Theorem (7.2.3), and therefore E(Xn) > E(X)
by part (b). Therefore var(X;,) = E(X?) — E(Xn)? > var(X).

2. Assume that Xp a X. Since |X,| < Z for all n, it is the case that |X| < Z as. Therefore
Zn = |Xn — X| satisfies Z, < 2Z a.s. In addition, if ¢ > 0,

E|Zn| = E (Zalizp<e}) +E (Znl{Z_>e}) S € + 2E (ZI Z_>e}) -

As n — 00, P(|Zn| > €) — 0, and therefore the last term tends to 0; to see this, use the fact that
E(Z) < oo, together with the result of Exercise (5.6.5). Now let € | 0 to obtain that E]Z,| > 0 as
noo.

3. We have that X —n~! < X, < X, so that E(Xn) — E(X), and similarly E(Y,) > E(Y). By
the independence of X» and Yp,

E(Xn Yn) = E(Xn)JEWn) > EQOQE(Y).

Finally, (X —n7!)(¥ —n7!) < XnYn < XY, and
B{ (x- *) (¥- “} =Exy) — PCOFEM | —> E(XY)
n n n n

as n — oo, so that E(XnYn) > E(XY).

4. Let F), Fy, ... be distribution functions. As in Section 5.9, we write F, > F if Fn(x) > F(x)

for all x at which F is continuous. We are required to prove that F,, > F if and only ifd(Fn, F) > 0.
Suppose that d(F,, F) — 0. Then, for e > 0, there exists N such that

F(x-—©)-—€< Fya(xs) < Fat+eyte for all x.

Take the limits as n — oo and € — 0 in that order, to find that F,(x) — F(x) whenever F is
continuous at x.

Suppose that F, — F. Let e > 0, and find real numbers a = x1 < x2 <--- < xn = b, each
being points of continuity of F, such that

(i) F;(a) < € foralli, F(b) > 1—e,

Gi) |xj41. —2;| < €forl <i<n.
In order to pick a such that F;(a) < € for all i, first choose a’ such that F(a’) < xe and F is
continuous at a’, then find M such that | Fyn (a’) — F(a’)| < ze form > M, and lastly find a continuity
point a of F such that a < a! and Fi,(a) < € forl <m < M.

There are finitely many points x;, and therefore there exists N such that | Fimn(x;) — F(xj)| < €
for alli and m > N. Now, if m > N and x; < x < x41,

Fn(*) S Fm Qj41) < F@i4i) +65 F@t+e) +e,

and similarly
Fm(x) = Fm) > FQ) -—€ = F@—€)-€.

Similar inequalities hold if x < a or x > b, and it follows that d(Fin, F) < ¢ ifm > N. Therefore
d(Fy, F) > Oasm > ow.

324

Modes of convergence Solutions [7.2.5]-[7.2.7]

5. (a) Suppose c > 0 and pick 6 such that 0 < 6 < c. Find N such that P(|Y, — c| > 8) < 6 for
n> N. Now, for x > 0,

P(XnYn <x) <P(XnYn < x, |¥n — | <5) + P(|¥n — | > 8) < P(X <— 5) +6,
—_
and similarly
P(XnYn > x) <P(Xn¥n > x, [¥n —c| <8) +5 <P (x, > esa) +6.
c

Taking the limits as n — oo and 6 | 0, we find that P(Xn Yn < x) > P(X < x/c) if x/c is a point
of continuity of the distribution function of X. A similar argument holds if x < 0, and we conclude

that XnYn » cX if c > 0. No extra difficulty arises if c < 0, and the case c = 0 is similar.

For the second part, it suffices to prove that ¥-! - eclit Yn Je (~ 0). This is immediate
from the fact that ly! —e7!| < €/{le|(c] — =} if [Yn — ce] < € (< Icl).
(b) Let € > 0. There exists N such that
P(\Xnl>6)<€, P(\¥n-Yl>e)<e, ifn>N,

and in addition P(|Y| > N) < €. By an elementary argument, g is uniformly continuous at points of
the form (0, y) for |y| < N. Therefore there exists 6 (> 0) such that

Ig’, y)-—gO,yl<e — if|x’| <6, ly’ -y| <5.
If |Xn| < 9, |¥n — Y| < 6, and |Y| < N, then |g(Xn, Yn) — g(O, Y)| < €, so that
P(Ig(Xn; Ya) — 80, Y)| > €) < PUXnl > 8) + P(I¥n — 1 > 8) + PUY] > N) < 3¢,

forn > N. Therefore g(Xn, Yn) a g(0, Y) asn > oo.
6. The subset A of the sample space (2 may be expressed thus:

oO CO ©

A=()U [) {lXntm — Xn] <k7"},

k=1n=1m=1
a countable sequence of intersections and unions of events.
For the last part, define

limn>e Xn(w) ifweA
0 ifw ¢é A.
The function X is ¥-measurable since A € F.
7. (a) If Xn(@) > X() then cn Xn(w) > cX(@).
(b) We have by Minkowski’s inequality that, as n > oo,
E(|enXn — cX|") < |cn|"E(|Xn — XI") + len — cl" E|X"| > 0.

X(o) = {

(c) If c = 0, the claim is nearly obvious. Otherwise c 4 0, and we may assume that c > 0. For
0 < € < c, there exists N such that |cy — c| < € whenever n > N. By the triangle inequality,
lenXn — cX| < |en(Xn — X)| + |(Cn — c)X|, So that, forn > N,

P(lenXn — eX| > €) < P(cn|Xn — X| > 4€) + P(len —cl- |X| > 4e)

€ €
<P! |X, —X ——_ Pi |X —_
= ( " I> sero) + ( er)

— 0 asn — oo.

325

(7.2.8]-[7.3.1] Solutions Convergence of random variables

(d) A neat way is to use the Skorokhod representation (7.2.14). If Xn ae X, find random variables

Yn, Y with the same distributions such that Y, “Ss y. Then cnYn “3 cy , so that cn Yn », cy,
implying the same conclusion for the X’s.

8. If X is nota.s. constant, there exist real numbers c and € such that 0 < € < 5 and P(X < c) > 2e,

P(X >c+e) > 2e. Since Xy LA X, there exists N such that
P(Xn <c)>e, P(X, >c+O>e, ifneN.

Also, by the triangle inequality, |X, — Xs| < |X; — X| + |Xs — X|; therefore there exists M such
that P(|X r— Xs| > €) < €? for r,s > M. Assume now that the X, are independent. Then, for
r,s >max{M,N},r 4s,

e > P([X; — Xs| > €) => P(X, <c, Xs >c+€) = P(X; < 0)P(Xs > +0) > 2,

a contradiction.

9. Either use the fact (Exercise (4.12.3)) that convergence in total variation implies convergence in
distribution, together with Theorem (7.2.19), or argue directly thus. Since |u(-)| < K < o%,

|E@u(Xn)) — E(u(X))| = So wef - fe <K > \fs® — f®|> 0.
k k

10. The partial sum Sy, = >7}_.; Xr is Poisson-distributed with parameter on = 7), Ar. For fixed
x, the event {S, < x} is decreasing in n, whence by Lemma (1.3.5), if o, ~ o < co and x isa
non-negative integer,

x

[o@) . e%q/
P(x <x) = jim P(Sn <x) => jy
r= y=

Hence if o < 00, aay X,y converges to a Poisson random variable. On the other hand, if o, — oo,
then e~ yj =0 of /j! — 0, giving that Pope Xy > x) = 1 for all x, and therefore the sum
diverges with probability 1, as required.

7.3 Solutions. Some ancillary results
1. (a If |Xn — Xm| > € then either |X, — X| > iG or |Xm — X| > 56, so that
P(|Xn — Xml > €) < P(|Xn —X| > 3€) +P(|\Xm — X|> ze) 30

asn,m —> oo, fore > 0.

Conversely, suppose that {X,} is Cauchy convergent in probability. For each positive integer k,
there exists ny, such that

P(IXn—Xml>=27-*) <2* — forn,m > ng.

The sequence (nx) may not be increasing, and we work instead with the sequence defined by Nj = 71,
Ne+1 = max{Nz + 1, ng41}. We have that

So P(X Nes —Xnjl2 2-*) < 00,
k

326

Some ancillary results Solutions  [7.3.2]-[7.3.3]

whence, by the first Borel—Cantelli lemma, a.s. only finitely many of the events {|X jy, 417 Xn, | =
2-*) occur. Therefore, the expression

[o.<)
X= Xwy, + 0X — Xny)
k=l

converges absolutely on an event C having probability one. Define X (w) accordingly for w € C, and

X(@) = 0 for w ¢ C. We have, by the definition of X, that Xy, “+ X as k > oo. Finally, we ‘fill
in the gaps’. As before, for € > 0,

P(|\Xn —X| > €) < P(\Xn — Xn, 1 > 3€) + P(IXy, — X| > ge) > 0

as n, k —> oo, where we are using the assumption that {X,} is Cauchy convergent in probability.

(b) Since Xp, bx , the sequence {X,,} is Cauchy convergent in probability. Hence
P(I¥n — Ym| > €) =P(IXn —Xm|>€) 70 asn,m— oo,

for € > 0. Therefore {Y,} is Cauchy convergent also, and the sequence converges in probability to

some limit Y. Finally, X, “3 X and Y, > X, so that X and Y have the same distribution.

2. Since An © UC, Am, we have that

OO wo

lim. supP(An) < im, U Am) = P( im, U An) = P(Ap i.0.),
mn m=n

where we have used the continuity of P. Alternatively, apply Fatou’s lemma to the sequence Tac of

indicator functions.

3. (a) Suppose X29, = 1, X2n41 = —1, forn > 1. Then {S, = 01.0.} occurs if X; = —1, and not
if X; = 1. The event is therefore not in the tail o-field of the X’s.

(b) Here is a way. As usual, P(S2, = 0) = 7")(pd — p)}", so that

DoPSmn =0) < co ifp #9,
n

implying by the first Borel-Cantelli lemma that P(S, = 0i.0.) = 0.
(c) Changing the values of any finite collection of the steps has no effect on J = liminf J, and J =
lim sup Tp, since such changes are extinguished in the limit by the denominator ‘,/n’. Hence J and J
are tail functions, and are measurable with respect to the tail o-field. In particular, {7 < —x}N{J > x}
lies in the o-field.

Take x = 1, say. Then, PZ < —1) = P(J > 1) by symmetry; using Exercise (7.3.2) and the
central limit theorem,

PJ > 1) > P(Sp > J/n io.) > limsupP(S, > /n) = 1— ®(1) > 0,
n->oo

where ® is the N(O, 1) distribution function. Since {J > 1} is a tail event of an independent sequence,
it has probability either 0 or 1, and therefore PU. < —1) = P(VJ = 1) = 1, andalsoPU < —-1,J >
1) = 1. That is, on an event having probability one, each visit of the walk to the left of —./n is
followed by a visit of the walk to the right of ./n, and vice versa. It follows that the walk visits 0
infinitely often, with probability one.

327
[7.3.4]-[7.3.8] Solutions Convergence of random variables

4. Let A be exchangeable. Since A is defined in terms of the X;, it follows by a standard result of
measure theory that, foreach n, there exists an event An € 0(X 1, X2,..., Xn), suchthatP(AA An) >
0 as n — oo. We may express A, and A in the form

An = {Xn € Bn}, A={X eB},
where Xp, = (X1, X2,..., Xn), and B, and B are appropriate subsets of R” and R®. Let
Al = {K€ By}, A’ = {X’ € B},

where xi, = (Xn41, Xn42,---, X2n) and xX’ = (Xn41, Xn42,---, Xn, X1, X2,---, Xn, X2n41,
X2n42,--+-):
Now P(An M A),) = P(An)P(Aj,), by independence. Also, P(An) = P(Aj,), and therefore

(*) P(A M Al) = P(An)? > P(A)? asn > 00.

By the exchangeability of A, we have that P(A A A),) = P(A’ A A‘), which in turn equals
P(A A Aj), using the fact that the X; are independent and identically distributed. Therefore,

[P(An 1. Aj,,) — P(A)| < P(A A An) + P(AA AL) OO asn > ow.

Combining this with (+), we obtain that P(A) = P(A)2, and hence P(A) equals 0 or 1.

5. The value of S, does not depend on the order of the first n steps, but only on their sum. If S, = 0
i.o., then Sy, = 01.0. for all walks {5/,} obtained from {S,} by permutations of finitely many steps.

6. Since f is continuous on a closed interval, it is bounded: | f(y)| < c for all y € [0, 1] for some c.
Furthermore f is uniformly continuous on [0, 1], which is to say that, if ¢ > 0, there exists 6 (> 0),
such that | f(y) — f(z)| < € if |y — z| < 6. With this choice of €, 5, we have that |E(ZI4c)| < €, and

1 _
IE(ZI4)| < 2cP(A) < 20-205
né2

by Chebyshov’s inequality. Therefore

2c

E(Z ,
IE(Z)| <€ +75

which is less than 2 for values of n exceeding 2c/ (e382).

7. If {Xn} converges completely to X then, by the first Borel-Cantelli lemma, |X, — X| > € only
finitely often with probability one, for all ¢ > 0. This implies that X, 5; X; see Theorem (7.2.4).

Suppose conversely that {X,} is a sequence of independent variables which converges almost
surely to X. By Exercise (7.2.8), X is almost surely constant, and we may therefore suppose that

Xn =; ¢ where c € R. It follows that, for e > 0, only finitely many of the (independent) events
{|Xn — c| > €} occur, with probability one. Using the second Borel—Cantelli lemma,

SS P(IXn —cl>€) <0.
n

8. Of the various ways of doing this, here is one. We have that

n\— yx - 2 Iss,’ 1 os
2 » , ited i) -n@o) i

l<i<j<n i=l i=1

328

Some ancillary results Solutions  [7.3.9]-[7.3.12]

Now n=! 3%" X; ae LL, by the law of large numbers (5.10.2); hence n—! 57” X; a jt (use Theorem

(7.2.4a)). It follows that (n7} Hi X;)* 4 2; to see this, either argue directly or use Problem
(7.11.3). Now use Exercise (7.2.7) to find that

Arguing similarly,
1 n 2 P
oe X? 0,
n(n — 1) 2d i”
and the result follows by the fact (Theorem (7.3.9)) that the sum of these two expressions converges
in probability to the sum of their limits.

9. Evidently,

Xn 1

By the Borel-Cantelli lemmas, the events An = {X,/logn > 1 + €} occur as. infinitely often for
—1 <e <0, and as. only finitely often for « > 0.

10. (a) Mills’s ratio (Exercise (4.4.8) or Problem (4.14.1c)) informs us that 1 — d(x) ~ x! b(x) as
x — oo. Therefore,

P(|Xn| = V/2logn(1 +€)) ~

1
Jin logn(l + e)n(ite)? |

The sum over n of these terms converges if and only if « > 0, and the Borel—-Cantelli lemmas imply
the claim.

(b) This is an easy implication of the Borel—Cantelli lemmas.

11. Let X be uniformly distributed on the interval [—1, 1], and define X_ = Ipy<(_1)n /ny- The dis-
tribution of X, approaches the Bernoulli distribution which takes the values +1 with equal probability
3 The median of X, is 1 ifm is even and —1 if n is odd.

12. (i) We have that

CO CO
yor (M+) = or (Har) = ASP <0
_ r ral x x

r=1

for x > 0. The result follows by the second Borel—Cantelli lemma.
(ii) (a) The stationary distribution z is found in the usual way to satisfy

k-1 2
= ——_JIp_ See SS
Tk ey eo} kk +1)

Lb k>2.

Hence my = {k(k + 1)}7! for k > 1, a distribution with mean J°%.,(k + 1)7! = 00.
(b) By construction, P(Xn < Xo +n) = 1 for all n, whence

x
P (1imsup “* < 1) =1.

n>o fn

It may in fact be shown that P(lim supy_, 99 Xn/n = 0) = 1.

329
[7.3.13}-[7.4.3] Solutions Convergence of random variables

13. We divide the numerator and denominator by ./no. By the central limit theorem, the former
converges in distribution to the N(0, 1) distribution. We expand the new denominator, squared, as

1 < 7 _ n i.
no2 dr _ pu)? _ no? —p) Sok, —pyt mies _ yu).

r=]

By the weak law of large numbers (Theorem (5.10.2), combined with Theorem (7.2.3)), the first term
converges in probability to 1, and the other terms to 0. Their sum converges to 1, by Theorem (7.3.9),
and the result follows by Slutsky’s theorem, Exercise (7.2.5).

7.4 Solutions. Laws of large numbers
1. Let Sp, =X, +X2+---+ Xn. Then

n . 2
E(S2) = 4° — <

and therefore S,/n 5, 0. On the other hand, >>; P(|X;| = i) = 1, so that |X;| > i i.o., with
probability one, by the second Borel—Cantelli lemma. For such a value of i, we have that |$; — $;~1| >
i, implying that S,/n does not converge, with probability one.

2. Let the Xy satisfy
1 3 1
P(Xn = —-n)=1-—5, P(X, =n “n=:
n n

whence they have zero mean. However,
Xn 1
P(—+¢-l]= += < ©,
vere) dae

implying by the first Borel—-Cantelli lemma that P(X,/n — —1) = 1. It is an elementary result of
real analysis that no! ed Xn — —lif x, — —1, and the claim follows.

3. The random variable N(S) has mean and variance 4|S| = cr?, where c is a constant depending
only on d. By Chebyshov’s inequality,

S| ~ ) ~ eS) Ve) crt

By the first Borel—Cantelli lemma,

where S; is the sphere of radius k. It follows that N(S;)/|Sx| =, ask —> oo. The same conclusion
holds as k — oo through the reals, since N(S) is non-decreasing in the radius of S.

[Sgl7!N (Sz) — Al > € for only finitely many integers k, a.s.,

330

Martingales Solutions [7.5.1]-[7.7.1]

7.5 Solutions. The strong law

1. Let J;; be the indicator function of the event that X; lies in the ith interval. Then

n nom m
log Rm = 5 Zm(é) log pi = D> S> Uy log pi = 5° Y;
j=l

i=] i=1]j=1

where, for 1 < j < m, Yj = a Qj log p; is the sum of independent identically distributed
variables with mean h
E(¥) = )_ pj log pj = —h.
i=l
By the strong law, m~! log Rin Ss —h.
2. The following two observations are clear:
(a) N(t) < nif and only if T, > t,

(b) Tra) <t < Tya)41-
If E(X1) < oo, then E(T,) < 00, so that P(T, > t) — O ast — oo. Therefore, by (a),

P(V(t) <n) =P(T, >t) > 0 ast> &,

implying that N(¢) ="; co ast > 0.
Secondly, by (b),
Ty) <_ Ty()41

—l
N® <N® ~ Natit CTNO

Take the limit as f — oo, using the fact that T,/n aS, E(X1) by the strong law, to deduce that
as.
t/N(t) — E(X)).

3. By the strong law, S,/n —-> E(X,) # 0. In particular, with probability 1, S,, = 0 only finitely
often.

7.6 Solution. The law of the iterated logarithm

1. The sum S, is approximately N(0, n), so that

—ha

n
P(Sn > V/anlogn) = 1— ®(,/alogn) < Jalosa

for all large n, by the tail estimate of Exercise (4.4.8) or Problem (4.14.1c) for the normal distribution.
This is summable if a > 2, and the claim follows by an application of the first Borel—Cantelli lemma.

7.7 Solutions. Martingales
1. Supposei < j. Then
B(X;X;) = E{E[(S; - S)-1) Xi | So, S1,..-5 5j—1]}
= E{ X; [E(S; | So, S1,---, S7-1) -— s\-1)} =0

331

[(7.7.2]-[7.8.2] Solutions Convergence of random variables

by the martingale property.
2. Clearly E|S,| < 00 for all n. Also, for n > 0,

1 1— prt
E(Sy41 | Zo, Z1,--+, Zn) = until E(Zn41 | Zo,.--, Zn) —m “T-p

1 1— prt

3. Certainly E|S,| < co for all n. Secondly, for n > 1,

E(Sn41 | Xo, X1,---, Xn) = ME(Xp41 | X0,.--, Xn) + Xn
= (aa +1)Xn +0bXyz_1,

which equals S, if a = (1 — a),

4. The gambler stakes Z; = f;-|(X1,..., Xj-1) onthe ith play, ata return of X; per unit. Therefore
Si; = Sj-, + X;Z; fori > 2, with S$; = X,Y. Secondly,

E(Sp41 — Sn | Xi, -.- Xn) = Znt1E(Xn41 | Xy,...,Xn) = 0,

where we have used the fact that Z,4 depends only on X1, X2,..., Xn.

7.8 Solutions. Martingale convergence theorem

1. It is easily checked that S, defines a martingale with respect to itself, and the claim follows from
the Doob-Kolmogorov inequality, using the fact that

E(S3) = 5_ var(X;).
j=l

2. It would be easy but somewhat perverse to use the martingale convergence theorem, and so we
give a direct proof based on Kolmogorov’s inequality of Exercise (7.8.1). Applying this inequality to
the sequence Zm, Zm41,... where Z; = (X; — EX;)/i, we obtain that S, = Z] + Zo +---+Zn

satisfies, for « > 0,
r

1
P (mas, |Sn — Sm| > c) < 2 SS var(Zn).
n=m+1

We take the limit as r —> 00, using the continuity of P, to obtain

1a 1
P( sup ISn~ Sul =e) <3 S> a2 Var(Xn).

nzm n=m+1
Now let m — oo to obtain (after a small step)
P (jim, sup IS ~ Sn <€) =1 for alle > 0.
Any real sequence (x,,) satisfying
lim sup |x, — xm| < € for all € > 0,

M—>O p>m

332

Prediction and conditional expectation Solutions [7.8.3]-[7.9.4]
is Cauchy convergent, and hence convergent. It follows that 5, converges a.s. to some limit Y.
The last part is an immediate consequence, using Kronecker’s lemma.

3. By the martingale convergence theorem, S = limp-+oo Sn exists a.s., and Sy aS 5S. Using
Exercise (7.2.1c), var(S,) — var(S), and therefore var(S) = 0.

7.9 Solutions. Prediction and conditional expectation

1. (a) Clearly the best predictors are E(X | Y) = Y?, E(Y | X) =0.
(b) We have, after expansion, that

E{(X — aY — b)*} = var(¥”) + a? E(¥”) + {b - E(Y”)P,

since E(Y) = E(Y3) = 0. This is a minimum when b = E(Y?) = 3 and a = 0. The best linear
predictor of X given Y is therefore 4.

Note that E(Y | X) = Ois a linear function of X; it is therefore the best linear predictor of Y
given X.
2. By the result of Problem (4.14.13), E(Y | X) = 2 + po2(X — 1)/o}, in the natural notation.

3. Write
n
g(a) =) a;X; = aX’,
i=1

and
v(a) = E{(Y — g(a))?} = E(Y*) — 2aE(YX’) + ava’.

Let a be a vector satisfying Va’ = E(YX’). Then

v(a) — v@) = aVa’ — 2aE(YX’) + 2aE(YX’) — aVa’
= aVa’ — 2aVa’ + aVa’ = (a — a)V(a — a)’ > 0,

since V is non-negative definite. Hence v(a) is a minimum when a = 4, and the answer is g(a). If V
is non-singular, a = E(YX)V~!.

4, Recall that Z = E(Y | G) is the (‘almost’) unique %-measurable random variable with finite
mean and satisfying E{(Y — Z)Ig} = 0 for all G € G.

(i) Q € G, and hence E{E(Y | 9)/Q} = E(Z/g) = E(VIg).

(ii) U = aE(Y | 3) + BE(Z | G) satisfies

E(U Ig) = aE{E(Y | 9)Ig} + BE{E(Z | $)Ig}
= @E(Y Ig) + BE(ZIg) = E{(aY + BZ)Ig}, Ge §.

Also, U is $-measurable.

(iii) Suppose there exists m (> 0) such that G = {E(Y | 9) < —m} has strictly positive probability.
Then G é€ G, and so E(Y/g) = E{E(Y | $%)Ig}. However Yig > 0, whereas E(Y | $)Ig < —m.
We obtain a contradiction on taking expectations.

(iv) Just check the definition of conditional expectation.

(v) If Y is independent of 3, then E(Y Ig) = E(Y)P(G) for G € G. Hence E{(Y — E(Y))I[g} = 0 for

G é §, as required.
ll

333

[(7.9.5]-[7.10.1] Solutions Convergence of random variables

(vi) If g is convex then, for all a € R, there exists A(a) such that
a(y) = g(a) + (y — aA);

furthermore 2 may be chosen to be a measurable function of a. Set a = E(Y | Y) and y = Y, to
obtain

g(Y) = g{E(Y | 9)} + {¥ — EY | 9) } {EY | 9)}.

Take expectations conditional on G, and use the fact that E(Y | 4) is $-measurable.
(vii) We have that
En | $) — EY | 9)| < E{I¥n — Y1|G} < Vn

where Vr = E{supy>n |¥m — Y||9}. Now {Vq : n > 1} is non-increasing and bounded below.
Hence V = limy-+o0 Vn exists and satisfies V > 0. Also

E(V) < E(Vn) =E{ sup [Yn — vi}.

m>n

which tends to 0 as m — oo, by the dominated convergence theorem. Therefore E(V) = 0, and hence
P(V = 0) = 1. The claim follows.

5. EY | X)=X.

6. (a) Let {X, : n > 1} be a sequence of members of H which is Cauchy convergent in mean
square, that is, E{|Xn — Xm|7} — Oasm,n — oo. By Chebyshov’s inequality, {X, : n > 1} is
Cauchy convergent in probability, and therefore converges in probability to some limit X (see Exercise

(7.3.1). It follows that there exists a subsequence {Xy, : k > 1} which converges to X almost surely.
Since each Xp, is G-measurable, we may assume that X is 3-measurable. Now, as n ~> 00,

E{|Xn —X??}=E {im inf |Xn — Xn Pb < liminf E{|Xn — Xn,|7} > 0,
k->00 k>oo

where we have used Fatou’s lemma and Cauchy convergence in mean square. Therefore Xp mS x.
That E(x 2) < oO is a consequence of Exercise (7.2.1a).

(b) That (i) implies (ii) is obvious, since 1g € H. Suppose that (ii) holds. Any Z (€ H) may be
written as the limit, as n -> oo, of random variables of the form

m(n)

Zn = > ai(n)lein)

i=l

for reals aj(n) and events G;(n) in 3; furthermore we may assume that |Zy| < |Z]. It is easy to see
that E{(Y — M)Z,} = 0 for all n. By dominated convergence, E{(Y — M)Zn} > E{(Y — M)Z},
and the claim follows.

7.10 Solutions. Uniform integrability
1. It is easily checked by considering whether |x| < a@ or |y| < a that, for a > 0,
Ix + yl getyl>2a} S 2 (lel xieay + lygyiza}) -
Now substitute x = Xy and y = Yj, and take expectations.

334
Uniform integrability Solutions [7.10.2]-[7.10.6]

2. (a) Let e > 0. There exists N such that E(JX, — X|") < e ifn > N. Now E|X"| < ow, by
Exercise (7.2.1a), and therefore there exists 6 (> 0) such that
E(\X|la)<e, E(\Xn|"I4)<¢€ forl<n<N,
for all events A such that P(A) < 6. By Minkowski’s inequality,
{E(Xn\"L4)}'/" < {BUXn — XI"Ta)}'/” + {BCX Tay}
if P(A) < 6. Therefore {|X,|" : n => 1} is uniformly integrable.

Wr ec aelr ifn>N

If r is an integer then {X/, : n > 1} is uniformly integrable also. Also X7, 4, xX" since X n a 4
(use the result of Problem (7.11.3)). Therefore E(X},) > E(X") as required.

(b) Suppose now that the collection {|X,|" : n => 1} is uniformly integrable and Xp, +, X. We show
first that E|X"| < oo, as follows. There exists a subsequence {Xn, : k > 1} which converges to X
almost surely. By Fatou’s lemma,

E|X’|=E (imine 1X1") < lim inf E|X”, | < supE|X%| < 00.
k-> 00 k->00 k n

If € > 0, there exists 6 (> 0) such that
E(\X"|f4) <€, EQXh|Ia) <€ for all n,

whenever A is such that P(A) < 6. There exists N such that B,(e) = {|Xn — X| > €} satisfies
P(Br(e)) < 6 for n > N. Consequently

E(\Xn — X|")<e" +E(|Xn—X\'Ipyey), n>N,
of which the final term satisfies
{E (Xn —XI"IBye)}/” < {E (IX lBae) }/” + (E(IX" ayo) }” <2".

Therefore, X;, 4X.

3. Fix € > 0, and find a real number a such that g(x) > x/e ifx >a. Ifb >a,

E ([Xnllqxn|>0)) < €E{g(Xnl)} < € sup E{g(|Xn[)},

whence the left side approaches 0, uniformly in n, as b + oo.

4. Hereisa quick way. Extinction is (almost) certain for such a branching process, so that Zp a8 0,

and hence Zp, - 0. If {Z, : n > 0} were uniformly integrable, it would follow that E(Z,) > 0 as
n — oo; however E(Z,) = 1 for all n.

5. We may suppose that X,, Y,, and Z, have finite means, for all n. We have that 0 < Y, — Xn <
Zn — Xn where, by Theorem (7.3.9c), Zn — Xn a Z—X. Also

E|Zn — Xn| = E(Zn — Xn) > E(Z — X) = BIZ — X|,
so that {Z, — X, : n > 1} is uniformly integrable, by Theorem (7.10.3). It follows that {Y, — Xp :

n > 1} is uniformly integrable. Also Yn — Xn & Y — X, and therefore by Theorem (7.10.3),

E|¥, —Xn| > E|Y — X|, whichis to say that E(Y,) -E(X,) > E(Y)—E(X); hence E(Y,) > E(Y).
It is not necessary to use uniform integrability; try doing it using the ‘more primitive’ Fatou’s

lemma.

6. For any event A, E(|Xn|{4) < E(ZI,4) where Z = sup, |Xn|. The uniform integrability follows

by the assumption that E(Z) < oo.

335
[7.11.1]-[7.11.2] Solutions Convergence of random variables

7.11 Solutions to problems

1. E|X/,| = co forr > 1, so there is no convergence in any mean. However, if € > 0,

2
P(|Xn| > €)=1—— tan !(ne) > 0 asn—> ©Oo,
a

P
so that X, —> 0.
You have insufficient information to decide whether or not X, converges almost surely:

(a) Let X be Cauchy, and let X, = X/n. Then X,, has the required density function, and Xp, +50.
(b) Let the X,, be independent with the specified density functions. For € > 0,

2 . —!
P(|X,| > €) = — sin
a

1 2
J/1+n2e2 mene’
so that 5°, P(|Xn| > €) = oo. By the second Borel—Cantelli lemma, |Xn| > € i.o. with probability

one, implying that X, does not converge a.s. to 0.

2. (i) Assume all the random variables are defined on the same probability space; otherwise it is
meaningless to add them together.
(a) Clearly X,(@) + Y¥n(@) > X(w) + Y(@) whenever X,n(@) — X(@) and ¥,z(@) > Y().
Therefore

{Xn + Yn» X+Y¥} S {Xn ~ X}UL{Yn ~ Y},

a union of events having zero probability.
(b) Use Minkowski’s inequality to obtain that

{E(\Xn+¥n —X—Y|")}!" < {EdXn — XP)}/" + (EU¥n - YI}.

(c) If € > 0, we have that
{\Xn+¥n —X—¥|>e} S{|Xn—X|> Fe} U {Ita — YI > ze},

and the probability of the right side tends to 0 asin > ow.

(d) If X, » X and the X, are symmetric, then —Xy, » X. However Xy, + (—Xn) ae 0, which
generally differs from 2X in distribution.
(ii) (e) Almost-sure convergence follows as in (a) above.

(f) The corresponding statement for convergence in rth mean is false in general. Find a random
variable Z such that E|Z”| < oo but E|Z2"| = oo, and define X, = Yn = Z for all n.

(g) Suppose X, 4 X and ¥;, Ft Y. Let e > 0. Then

P(IXnY¥n — X¥| > €) =P(|(Xn — X)Wn — Y) + Kn — XY + Xen — ¥)| > €)
< P(|Xn — X|-|¥n —¥| > 4€) +P(IXn — XI -1¥| > 4)
+ P(IX|-|¥n — Y| > 3€).

Now, for 6 > 0,
P(|Xn — X|-|¥| > 4) < P(|Xn — X| > €/(38)) + P(|Y| > 8),

336

Problems Solutions [7.11.3]-[7.11.5]

which tends to 0 in the limit as n > oo and 6 — oo in that order. Together with two similar facts, we
. P
obtain that X,Y, — XY.

(h) The example of (d) above indicates that the corresponding statement is false for convergence in
distribution.

3. Let e > 0. We may pick M such that P(|X| > M) < €. The continuous function g is uniformly
continuous on the bounded interval [—M, M]. There exists 6 > 0 such that

lex)-gQ)| se if|x —y| < Sand |x| < M.
If |g(Xn) — g(X)| > €, then either |X, — X| > 6 or |X| => M. Therefore
P(|g(Xn) — g(X)| > €) < P(|Xn — X| > 6) +P(|X| => M) > P(IX| > M) <e,

in the limit as n —> oo. It follows that g(Xn) 4 g(X).
4. Clearly

itX ‘ it, /10/ a ee ee
tiAny _ j = —
E(el!Xn) = [[Ee 5/107) I] 16° canto)
j=l j=l
1—e!# 1-el?

~ Tord — elt/10") ir

asn — oo. The limit is the characteristic function of the uniform distribution on [0, 1].
Now Xn < Xy41 < 1 for all n, so that Y(w) = limp oo Xn(@) exists for all w. Therefore

Xn 2s Y; hence X;, as Y, whence Y has the uniform distribution.

5. (a) Suppose s < ¢. Then
E(N(s)N(t)) = E(N(s)?) + E{N(s)(N(t) — N(s))} = E(N(8)*) + E(N(8))E(N() — N(8)),
since N has independent increments. Therefore

cov(N(s), N(t)) = E(N(s)N(t)) — EWN(s))E(N@®)
= (As)? +As +As{A(t — 5)} — (As)(At) = As.

In general, cov(N(s), N(t)) = A min{s, ¢}.
(b) N(t +h) — N(t) has the same distribution as N(h), if h > 0. Hence

E ({N(@ +h) — N@)}”) = BVH?) = Gh)? + ah

which tends to 0.as h — 0.
(c) By Markov’s inequality,

1
P(ING +h) —NO|>«) < zE({NE +m) —NO}’),

which tends to 0 ash —> 0, if € > 0.
(d) Lete > 0. ForO <h < el,

p(Jae

; | > c) = P(M(t +h) — N(t) = 1) =Ah + o(h),

337

[7.11.6]-[7.11.10] Solutions Convergence of random variables

which tends to 0 ash > 0.

On the other hand,
N@+A)-N@)\7\_ 1 2
n ({Meepate }") = Jato + an}

which tends to co ash | 0.
6. By Markov’s inequality, S, = 577. Xj satisfies

E(S4)
(ne)4

P(|Spl > ne) Ss

for € > 0. Using the properties of the X’s,

4
E(S4) = nE(X4) +4 (;) E(X3X>) + (2) (;) E(X2X3)

+3 (2) (5) E(X2X2X3) +4! (;) E(XX>X3X4)
4
= nE(X4) + (;) (3) E(X2X2),

since E(X;) = 0 for all i. Therefore there exists a constant C such that
S>P(n' Sal >e)< > £ < 0,
n n n

implying (via the first Borel—Cantelli lemma) that n—!5, 7“ 0.
7. We have by Markov’s inequality that
E{|X, — X|"
S>P(iXn — X| >€) < yy Mea < 00
n n
for € > 0, so that X, —-> X (via the first Borel—Cantelli lemma).

8. Either use the Skorokhod representation or characteristic functions. Following the latter route,
the characteristic function of aX, + b is

E(el! @Xn+0)) _ elfhg,, (at) = et oay (at) _ Eel! @X+))
where ¢», is the characteristic function of X,. The result follows by the continuity theorem.
9. For any positive reals c, t,

E{(X +c)?}

PX >H=PX+ce>t+oe< G+o2

Set c = o”/t to obtain the required inequality.
10. Note that g(u) = u/(1 +4) is an increasing function on [0, oo). Therefore, for € > 0,

\Xnl € } l+e ( |Xn| )
P(|Xn| > €-) =P > < -E
(IXnl > €) (Sie 1te)/~ e€ 1+ |Xnl

338

Problems Solutions [7.11.11]-[7.11.14]

by Markov’s inequality. If this expectation tends to 0 then Xn 50.

Suppose conversely that X, *, 0. Then

IXn| ) €
E < -P(\Xn| < 1- P(X
(We <The (UXnl <6) + (| nl >) > 7

as n ~—> 00, for € > 0. However € is arbitrary, and hence the expectation has limit 0.

11. (@) The argument of the solution to Exercise (7.9.6a) shows that {X,} converges in mean square

if it is mean-square Cauchy convergent. Conversely, suppose that Xp, aS x. By Minkowski’s
inequality,

{E((Xm — Xn)2)}? < (6((Xm — X)2)}/? + {((Xn — XY) }? + 0

as m,n —> oo, so that {X,,} is mean-square Cauchy convergent.

(ii) The corresponding result is valid for convergence almost surely, in rth mean, and in probability. For
a.s. convergence, it is self-evident by the properties of Cauchy-convergent sequences of real numbers.
For convergence in probability, see Exercise (7.3.1). For convergence in rth mean (r > 1), just adapt
the argument of (i) above.

12. If var(X;) < M for all i, the variance of n—! S77_, X; is
1< M
— So var(Xi) <—-0 asn- ow.
n n
i=]
13. (a) We have that
P(My < anx) = F(anx)" > H(x) asn— oo.

If x < O then F(a,x)" — 0, so that H(x) = 0. Suppose that x > 0. Then
—log H(x) = — lim {nlog[1 — (1 — F@nx))]} = lim {a — F(@nx))}

since —y7l log(1 — y) > lasy | 0. Setting x = 1, we obtain n(1 — F(an)) > —log H(1), and
the second limit follows.

(b) This is immediate from the fact that it is valid for all sequences {ay}.

(c) We have that

1— F(te*t) _1- F(te*tY) 1 — F(te*) . log H(e”) log H(e*)
1-F@)  1—F(te*) 1-F() log H(1) log H(1)

as t —> oo. Therefore g(x + y) = g(x)g(y). Now g is non-increasing with g(0) = 1. Therefore
g(x) = e— 6% for some B, and hence H(u) = exp(—au~P) for u > 0, where a = — log H(1).

14, Either use the result of Problem (7.11.13) or do the calculations directly thus. We have that

1 1. _ysxny)” 1 _yfay)”
P(Mn < xn/1) = {3+5a (=)} = {1- 5 fan (=)
if x > 0, by elementary trigonometry. Now tan—! y = y + o(y) as y > 0, and therefore

1 n
P(M, <xn/x) = (1 -— +007) > e l/* asn —> oo.
xn

339

[7.11.15]-[7.11.16] Solutions Convergence of random variables

15. The characteristic function of the average satisfies
n ipt -1)\" _, pint
ot/ny” =(1+—+07)} - as n —> 00.
n

By the continuity theorem, the average converges in distribution to the constant wu, and hence in
probability also.

16. (a) With uy, = u(xn), we have that

|E@(X)) — EU(Y))| < So lunl fa — gnl < Sofa — gal

if ||ulloo < 1. There is equality if u, equals the sign of f, — gn. The second equality holds as in
Problem (2.7.13) and Exercise (4.12.3).

(b) Similarly, if ||u\]oo < 1,

oO OO
|E@(X)) — E@(Y))| < / lu(x)| - lf) — g@)| dx < / | f(x) — g(x)| dx
oo oo

with equality if u(x) is the sign of f(x) — g(x). Secondly, we have that
|P(X € A) — PY € A)| = 3 |E(u(X)) — E@U(Y))| < 54rv(X, ¥),

where

1 ifx € A,
u(x) = ;
-1 ifx¢A.
Equality holds when A = {x € R: f(x) => g(x)}.
(c) Suppose dty (Xn, X) — 0. Fix a € R, and let u be the indicator function of the interval (—00, a].
Then |E(u(Xn)) — E(u(X))| = |P(Xn < a) — P(X < a)|, and the claim follows.

On the other hand, if X, = n—! with probability one, then Xy ans 0. However, by part (a),
apy (Xn, 0) = 2 for all n.
(d) This is tricky without a knowledge of Radon—Nikodym derivatives, and we therefore restrict
ourselves to the case when X and Y are discrete. (The continuous case is analogous.) As in the
solution to Exercise (4.12.4), P(X #4 Y) = zary (X, Y). That equality is possible was proved for
Exercise (4.12.5), and we rephrase that solution here. Let w»z = min{ fn, gn} and uw = >>, un, and
note that

dry(X,Y) = Sofa — anl = (fn + 8n — un} = 201 - pW).
n n
It is easy to see that
1 ifu=O,
0 ifp=1,

and therefore we may assume that 0 < yp < 1. Let U, V, W be random variables with mass functions

pary(X, ¥) =P(X #¥) = {

— gn,0 — mi — gn,0
PU = xp) = L&, PCV = xp) = Le 8 py a yy = — a = fn OF
bh l-p l-p
and let Z be a Bernoulli variable with parameter jz, independent of (U, V, W). We now choose the
pair X’, Y’ by
(U,U) ifZ=1,

(XY) = {
(V,W) ifZ=0.

340

Problems Solutions [7.11.17]-[7.11.19]

It may be checked that X’ and Y’ have the same distributions as X and Y, and furthermore, P(X’ 4
Y’) =P(Z = 0) =1—p= hdtv (X,Y).

(e) By part (d), we may find independent pairs (X;, Y/), 1 < i < n, having the same marginals as
(X;, ¥;), respectively, and such that P(X! # ¥/) = $drv(Xi, ¥;). Now,

n

n n n
av(> Xi, vi) = dry (> xi) v1)
i=l i=1 1

i=1 i= i=

n n n n

< (>> X#D) ¥) < 257 P(X; # Yi) =25 ldrv%i, Mi).
i=1 i=1 i=1 i=1

17. If X,, X2,... are independent variables having the Poisson distribution with parameter A, then

Sn = Xi, +X2+---+Xy has the Poisson distribution with parameter An. Now nS, » A, so that
E(g(n7! Sn)) — g(A) for all bounded continuous g. The result follows.

18. The characteristic function Wyn of
(Xn — 1) — Ym — m)

J/m+n

Umn =

satisfies

i n -i n (m — n)it
tog vin (t) = n(elt/YF™ — 1) + m(etI/mm — 1) 4 TR > 5

as m,n —> 00, implying by the continuity theorem that Umpy x N(O, 1). Now Xp + ¥y is Poisson-
distributed with parameter m + n, and therefore

Xn+Y¥n P
Vnn = 4/———— > l asm,n —> 0O
m+n

by the law of large numbers and Problem (3). It follows by Slutsky’s theorem (7.2.5a) that Umn / Vinn 4
N(O, 1) as required.

12

19. (a) The characteristic function of Xn is dn(t) = expf{ipnt — 50217} where j4n and o2 are

1,2
the mean and variance of X,. Now, limp_soo @n(1) exists. However ¢,(1) has modulus e~ 2°,
and therefore 07 = limy—oo of, exists. The remaining component e'#*' of @,(t) converges as
n -> 00, say e'#n! —» O(t) as n -> 00 where @(t) lies on the unit circle of the complex plane. Now

ont) > a (ten 27? which is required to be a characteristic function; therefore @ is a continuous
function of t. Of the various ways of showing that 6(t) = e!#* for some y, here is one. The sequence
Wnt) = ellnt isa sequence of characteristic functions whose limit @(f) is continuous at t = 0.
Therefore 0 is a characteristic function. However y;, is the characteristic function of the constant py,
which must converge in distribution as n —> 00; it follows that the real sequence {,} converges to
some limit 2, and 6(t) = e!#? as required.
This proves that @n(t) > expf{ipt — 50°87}, and therefore the limit X is N(u, oa).

(b) Each linear combination s X, +tY, converges in probability, and hence in distribution, tos X +7rY.
Now sXy +tY, has anormal distribution, implying by part (a) that sX + ¢Y is normal. Therefore the
joint characteristic function of X and Y satisfies

x,y(8,t) = bsx4sy (1) = exp{iE(sX +t¥) — 5 var(sX +tY)}

= exp{i (sux +tyy) — 5 (s70% + 2stoxyoxoy + Pop)}

341
[(7.11.20]-[7.11.21] Solutions Convergence of random variables

in the natural notation. Viewed as a function of s and f, this is the joint characteristic function of a
bivariate normal distribution.

When working in such a context, the technique of using linear combinations of X, and Yy is
sometimes called the ‘Cramér—Wold device’.

20. (i) Write ¥; = X; — E(X;) and T, = S77, ¥j. It suffices to show that n—!7, “s 0. Now, as
n> o,
2 2 1 n 2 ne
E(T2/n?) = =z 2 var(Xi) + > cov(X;, Xj) < “2 > 0:
i=1 lsi<j<n

(ii) Let ¢ > 0. There exists J such that |o(X;, X;)| < € if |i — j| = 7. Now

n
S> cov(X;, Xj) < > cov(X;, X;) + SS cov(X;, X;) < 2nIc+n'ec,

ij=l li—j|<t li-jl>I
lsi,jsn 1<i,j<n
since cov(X;, X;) < |o(Xi, Xj) |4/var(X;) - var(X;). Therefore,
2Ic
R(T? /n?) <—+€c €c asn — ©Oo.
n

This is valid for all positive €, and the result follows.

21. The integral
eo oe
| dx
2 xlog|x|

diverges, and therefore E(X) does not exist.
The characteristic function ¢ of X; may be expressed as

60 = 2 [ax

x? log x

whence

ot) — $0) _ © 1 —cos(tx)
2c 7 -[f x2 log x dx

Now 0 < 1 —cos@ < min{2, 67}, and therefore

_ 1/t ,2 (oe)
we #0) < [ t art | ax, ifts 0.
2c 2 1

log x jt x7 logx
Now
1 ff" dx
— — +0 asu —> 00,
u j2 logx
and 0° 2 1 oo 2 2
/ 5 dx < | =z ax = , &>l.
u x“ logx logu Jy x ulogu
Therefore 0
t —
eo = o(t) ast | 0.
2c

Now ¢ is an even function, and hence ¢’(0) exists and equals 0. Use the result of Problem (7.11.15)
to deduce that n—! ><] X; converges in distribution to 0, and therefore in probability also, since 0 is
constant. The X; do not obey the strong law since they have no mean.

342

Problems Solutions [7.11.22]-[7.11.24]

22. If the two points are U and V then

1 ,l
a -wyir= [P| w—waudv =f,

and therefore

1 1X pl
pen UW & asn —> oO,

by the independence of the components. It follows that X,/./n 4 1/6 either by the result of
Problem (7.11.3) or by the fact that

x2 1

n 6

Xn _ 3)
vn V6

He yt), tte 1
Jn Jbl” V6

n 4/6
23. The characteristic function of ¥j = X71 is

cos y
y2

1. . 1 0°
of) = sf (elt/* 4 eHt/*) dy = [ cos(t/x) dx = iif dy
1) 0 lt}

by the substitution x = |¢|/y. Therefore
[o.<) 1 _
o(t) =1- ii f dy =1-It|+ ole) ast + 0,
|t| y

where, integrating by parts,

1l-c © si
r= | Pay = f sme dua =.
0 y Oo uu 2

It follows that J, = n~! viel x! has characteristic function

a\t|

o(t/n)" = (1 -—— +0007) ea Ldd
2n

as t ~ oo, whence 27, / is asymptotically Cauchy-distributed. In particular,

2 [© du 1
P(|27, 1 = =x t .
((2%/nl> 1) >= [y= 5 ast 00

24. Let my, be a non-decreasing sequence of integers satisfying 1 < m, <n, my, — oo, and define

_ Xi if |Xp| < mtn
mk | sign(Xe) if [Xgl > mtn,

noting that Y,,, takes the value +1 each with probability 5 whenever m, < k <n. Let Z, =

n n
1
Pn # Zn) < S>PUXel > mn) < >> G79 ano,
k=1 k=myn

343

(7.11.25]-[7.11.26] Solutions Convergence of random variables

from which it follows that U,/./n & N(O, 1) if and only if Z,/./n » N(O, 1). Now

mn

Zn = S_ Yak + Bn—mn
k=1

where By—m, is the sum of n — my independent summands each of which takes the values +1, each
possibility having probability 3: Furthermore

which tends to 0 if my, is chosen to be mz, = [ni/ 5]; with this choice for m,, we have that
n—|Bn—mn 8 N(O, 1), and the result follows.
Finally,

so that

25. (i) Let dp, and ¢ be the characteristic functions of X, and X. The characteristic function wv, of
xX Ny is

[o.<)
Wilt) = >> oj OPN: = 3)

j=l
whence

oo
wet) — ()| < 3514; — 6OIPC: = J).

j=l
Lete > 0. We have that $;(¢) > ¢(¢) as j > oo, andhence for any T > 0, there exists J (7) such that
Idj(t) — 6@)| < € if j = F(T) and |t| < T. Finally, there exists K (7) such that P(N, < J(T)) <€
ifk > K(T). It follows that

\We(t) — P(t)| < 2P(Ne < J(T)) + €P(Ng > J(T)) < 3€

if |t] < T andk > K(T); therefore Y(t) > (ft) ask > 00

(ii) Let Yr = supy>n |Xm — X|. Fore > 0,n > 1,

P(|Xy, — X| > €) < P(N, Sn) +P([Xn, — X| > €, NK > 20)
< PING <n) +P%n > €) > Pn > ©) as k —> oo.

Now take the limit as n —> oo and use the fact that Y, aac §)
26. (a) We have that

a(n — k,n) n)

a(n + 1,n) -I(! 1-7) se(- »:!).

=0 i=0
344

Problems Solutions [7.11.27}-[7.11.28]

(b) The expectation is

rid

_ jun neg
n= Dus (ae) a
where the sum is over all j satisfying n — M./n < j <n. For sucha value of j,

@ - *) nie — e™ [nit ni

8) J He | ost >

Jn ji Jn \ jt G-D)!

whence E, has the form given.

(c) Now g is continuous on the interval [— M, 0], and it follows by the central limit theorem that

1442
0 1 1,2 M 1,2 L—e 7M
En f g(x) e 2” dx= | * e2* dx =~
-M 0

Vin Jin J 20
Also,
enn enn e—n—k? /(2n)
En < yan the < En + paln — k,n) < Bn + Fan + Ln)

where k = |M./n]. Take the limits as » > oo and M — oo in that order to obtain

_ nitden -_!
V2n ~ noo n!} ~ Jon

27. Clearly
Rn

E(Ra+1 | Ro, Ri, ..., Rn) = Rat 5

since a red ball is added with probability Ry, /(n + 2). Hence
E(Sn41 | Ro, Ri,---, Rn) = Sn,

and also 0 < S, < 1. Using the martingale convergence theorem, S = limpn—oo Sp exists almost
surely and in mean square.

28. Let 0 <e < 4, and let
k(t) = |0t], mt) = [0 — &)kG)], nt) = LA +A)

and let Imn(t) be the indicator function of the event {m(t) < M(t) < n(t)}. Since M(t)/t 4 6, we
may find T such that EUmn(t)) > 1—€ fort > T.

We may approximate Sjy() by the random variable S,q) as follows. With A; = {|S; — Sxl >

e VEO},

P(Amq@)) < P(Am@), Imn(t) = 1) + P(Am@), Inn) = 0)

k(t)-1 n(t)—-1
< P( U Aj) +P( U Aj) + P(Imn(t) = 0)
j=m(t) j=k()
{k(t) — m(t)}o2 — {n(t) — k(t) }o?
<KO oun.

<€(1+20%), ift > T,

345

(7.11.29]-[7.11.32] Solutions Convergence of random variables

by Kolmogorov’s inequality (Exercise (7.8.1) and Problem (7.11.29)). Send t — oo to find that

Ss -—S§,
Dz = ES KO an 0 ast > Oo.

Now Sx¢r) / Jkt) 4 N(O, o*) as t —> 00, by the usual central limit theorem. Therefore

SM() _ Sk) Pe, 62)

Jkt) CRE)
which implies the first claim, since k(t)/(@t) — 1 (see Exercise (7.2.7)). The second part follows by
Slutsky’s theorem (7.2.5a).

29. We have that S, = S,; + (Sn — S,;), and so, for n > k,

E(S214,) = E(S214,) + 2E{ Se(Sn — Se)Ia, } +E{ (Sn — Sx)? 1A, }-

Now S21 At = 71 Ax; the second term on the right side is 0, by the independence of the X’s, and the
third term is non-negative. The first inequality of the question follows. Summing over k, we obtain
E(S2) > c?P(Mn > c) as required.

30. (i) With S, = 0¥_) Xi, we have by Kolmogorov’s inequality that

m+n

1 2
P{ max |S, —Sml>eée}<s E(X
(mex m+k — Sm ) <3 pe (Xf)

for € > 0. Take the limit as m,n —> oo to obtain in the usual way that {S,; : r > 0} is a.s. Cauchy
convergent, and therefore a.s. convergent, if “Tv E(X?2) < oo. It is shorter to use the martingale
convergence theorem, noting that S, is a martingale with uniformly bounded second moments.

Gi) Apply part (i) to the sequence Y; = X;/b, to deduce that wey Xx/bx converges a.s. The claim
now follows by Kronecker’s lemma (see Exercise (7.8.2)).

31. (a) This is immediate by the observation that
N:;
i,j /

(b) Clearly vi Pi; = 1 for each i, and we introduce Lagrange multipliers {uj : i ¢ S} and write
V=AP)4+0; 4 > j Pij- Differentiating V with respect to each pj; yields a stationary (maximum)
value when (Nj; /pij) + wi = 0. Hence >, Nix = —g, and

(c) We have that Nj; = yt Nik 1, where I, is the indicator function of the event that the rth transition
out of i is to j. By the Markov property, the J, are independent with constant mean p;;. Using the

strong law of large numbers and the fact that °, Nix =‘; 00 asin -> 00, Pij “4 EW) = Pij-

32. (a) If X is transient then Vj(n) < 00 a.s., and j4; = 00, whence V;(n)/n “S$ 0 = p71. If X is
persistent, then without loss of generality we may assume Xo = i. Let T(r) be the duration of the rth

346

Problems Solutions [7.11.33]-[7.11.34]

excursion fromi. By the strong Markov property, the T (7) are independent and identically distributed
with mean j;. Furthermore,

, “@-! V; (n)
Vo » Tr) < Ta » T(r).

By the strong law of large numbers and the fact that V; (7) =: 00 as n —> 00, the two outer terms
sandwich the central term, and the result follows.

(b) Note that 7d f(X,) = ies f@DVi(n). With Q a finite subset of S, and z; = y; 1, the

unique stationary distribution,
Vj (n) 1 ;
» (M2 - 2) so)
n bi

n-1 :
f (Xr) f@
y ey Mle)
Vie)
+(e +o) Mf oo

r=0 i Mi
<{y |v 2
i¢Q

Li
where || f lloo = sup{| f(@)| : i €¢ S}. The sum overi € Q converges a.s. to 0 as n — o, by part (a).

The other sum satisfies
Viln) Vil)
yA tg) Er +s)

i€Q

which approaches 0 a.s., in the limits as n —> oo and Q ¢ S.

33. (a) Since the chain is persistent, we may assume without loss of generality that Xp = j. Define
the times R,, R2,... of return to j, the sojourn lengths $;, S2,... in j, and the times Vj, V2,...
between visits to 7. By the Markov property and the strong law of large numbers,

i 1
-S*s, = —, ram eS
n=l 8j

Also, Rn/Rn+1 —> 1, since 4; = E(R1) < 00. If Rn <t < Ry41, then

Ry Dora Sr 15r <l a Ratt wrt Der=1 St 1 Sr
I =j\ds <
Ratt yaa v= {X(s)=j} Rn ply,

Let n —> oo to obtain the result.

(b) Note by Theorem (6.9.21) that pj;;(¢) > 1; ast — 00. We take expectations of the integral in
part (a), and the claim follows as in Corollary (6.4.22).

(c) Use the fact that
[ f(X(s)) ds = vf Kx (s)=j} ds
jes

together with the method of solution of Problem (7.11.32b).

34, (a) By the first Borel—Cantelli lemma, X, = Y, for all but finitely many values of n, almost
surely. Off an event of probability zero, the sequences are identical for all large n.

(b) This follows immediately from part (a), since X, — Y, = 0 for all large n, almost surely.
(c) By the above, a7! $7, (X, — Yr) = 0, which implies the claim.

347
[7.11.35]-[7.11.37] Solutions Convergence of random variables
35. Let Yn = XnIx,|<a}. Then,

do PRn # Yn) = $I P(Xn| > a) < 00

by assumption (a), whence {X;,} and {Y,} are tail-equivalent (see Problem (7.11.34)). By assumption
(b) and the martingale convergence theorem (7.8.1) applied to the partial sums ye (Yn — E(Y,)),
the infinite sum )77°.| (Yn — E(Yn)) converges almost surely. Finally, }“7C., E(Yn) converges by
assumption (c), and therefore yea Yn, and hence Ry Xn, converges a.s.

36. (a) Let ny < nz <--- < ny =n. Since the J; take only two values, it suffices to show that

r
P(UIn, =lforl<s<r)= [] Pda = 1).

s=l1

Since F is continuous, the X; take distinct values with probability 1, and furthermore the ranking of
X 1, X2,..-, Xn is equally likely to be any of the n! available. Let x,,x2,..., xn be distinct reals,
and write A = {X; = x; for 1 <i <n}. Now,

PUn, = 1forl <s <r| A)

= i {(" 7 " (n-1- not} {(": 7 ') (ns_1 — 1 - nah “-+(m, —1)!
n! Ns—] Ns—2

1 1 1

Ns Ms] My

and the claim follows on averaging over the x;.

(b) We have that E(I,) = PU, = 1) = k7! and var(I,) = k~!(1—k7!), whence >, var(Ix/ log k) <
oo. By the independence of the J; and the martingale convergence theorem (7.8.1), S772, Uk —
k-ly/ log k converges a.s. Therefore, by Kronecker’s lemma (see Exercise (7.8.2)),

1 1\ as.
i-7z)— 0 asn— oo.
logn <> J

1

The result follows on recalling that }(7_) j—" ~ logn asin — oo.

37. By an application of the three series theorem of Problem (7.11.35), the series converges almost
surely.

348

8

Random processes

8.2 Solutions. Stationary processes

1. With aj(n) = P(Xn = 71), we have that
cov(Xm, Xmin) = P(Xmtn = 1| Xm = DP(Xm = 1) — P(Xm4n = 1)P(Xm = 1)
= ay(m) p11 (n) — a) (m)a,(m +n),
and therefore,

a\(m) pi (1) — ay (m)a,(m +n)
Jay(m)(1 — ay (m))a, (m Fn) — ay(m Fn)

Now, a,(m) > a/(a + B) as m — oo, and

pP(Xm, Xm+n) =

_ a B _y_ gyn
Pu) = a tay a — B)",

whence (Xm, Xm+n) > (1 —a@ — 8)” as m -> ov. Finally,

a
at p

1 n
lim —S° P(X, =1) =
r=1

n—>OOn

The process is strictly stationary if and only if Xo has the stationary distribution.

2. We have that E(T(t)) = 0 and var(T(t)) = var(To) = 1. Hence:
(a) p(T (s), T(s +1) = E(T(s)T(s +1) = E[(-I)NOF9 NO] = oP
(b) Evidently, E(X (t)) = 0, and

E[X(t)?] = e(f [ T(u)T(v) du av)

t v
=2 [ E(T(u)T(v)) dudv =2 / / e UW) du dy
O<u<ve<t v=0 J/u=0

1 1
=r (gt gem) ~3 ast—> ©.

3. We show first the existence of the limit A = lim, 1 g(¢)/t, where g(t) = P(N(t) > 0). Clearly,

g(x +y) =P(NG@ + y) > 0)
= P(N(x) > 0) + P({N(x) = 0} {NG@ + y) — N(x) > 0})
<g(x)+e0) ~~ forx,y>=0.

349
[8.3.1]-[8.3.4] Solutions Random processes

Such a function g is called subadditive, and the existence of 1 follows by the subadditive limit theorem
discussed in Problem (6.15.14). Note that 4 = 00 is a possibility.

Next, we partition the interval (0, 1] into n equal sub-intervals, and let J,(r) be the indicator
function of the event that at least one arrival lies in ((” — 1)/n,r/n], 1 <r <n. Then 7?_, In(r) t
N(1) as n — o0, with probability 1. By stationarity and monotone convergence,

E(N(1)) = e( im, ymin) = lim, ed intr) = lim ng(n7') =2.

8.3 Solutions. Renewal processes

1. See Problem (6.15.8).

2. With X acertain inter-event time, independent of the chain so far,

F -{x- if Bn = 0,
ml | B,—1. if By > 0.

Therefore, B is a Markov chain with transition probabilities p;;_, = 1 fori > 0, and poj = fj41
for j = 0, where f, = P(X =n). The stationary distribution satisfies x; = Tj4+1 +70 Fits jJ2=9,
with solution x; = P(X > j)/E(X), provided E(X) is finite.

The transition probabilities of B when reversed in equilibrium are

Ti+] _ PX >it 1) ~ Fi+t

Dade =_ ; = — fori > 0.
Put = PXY>ip)” PO“ pXsH r=

These are the transition probabilities of the chain U of Exercise (8.3.1) with the f; as given.

3. We have that p"un = S77) p"*un_xp* fe, whence vn = p"un defines a renewal sequence
provided p > Oand }°,, p" f, = 1. By Exercise (8.3.1), there exists a Markov chain U and a state s
such that up» = P(Un = s) > ms, asin —> 00, as required.

4. Noting that N(0) = 0

SS EW())s” =S- ras" = =S Sos

r=0 r=1k=1 k=l r=k
a ugs® Us) - 1 _ F(s)U(s)
tcl l-s l-s 1l—s

Let Sm = > 7%. Xx and So = 0. Then P(N(*) = n) = P(Sn <r) — P(Sy41 <7), and

o@)
ys 'E e|("™*") =ystS eal P(Sn <t) — P(Sn41 <2)
t=0 n=0
CO
= Sos! bay (“ry ‘ets, <9),
t=0 1 1
Now,
F(s)"
Ses <= SR =e =e = sis vs

350

Queues Solutions [8.3.5]-[8.4.4]

whence, by the negative binomial theorem,

Ste | (NO +K\| 1 _ Us)
Sos E = =
= k (1—s)(— F(s))F 1s"

5. This is an immediate consequence of the fact that the interarrival times of a Poisson process are
exponentially distributed, since this specifies the distribution of the process.

8.4 Solutions. Queues

1. We use the lack-of-memory property repeatedly, together with the fact that, if X and Y are
independent exponential variables with respective parameters 4 and yi, then P(X < Y) =A/(A+ 4p).

(a) In this case,

1 aA wb bh 1 ub Xr aA 1 2h
P=5 : + 5 : + =5 +7277:
2lAtMu AtuM AtU 2\Atu Atm Atu 2 (A+t+p)
uw \2
(b) If A < yz, and you pick the quicker server, p = 1 — (4).
Bb
2AL
(A + yu)?"

2. The given event occurs if the time X to the next arrival is less than ¢, and also less than the time
Y of service of the customer present. Now,

(c) And finally, p =

t a
P(X <1, X<Y)= I nee P* dy = (1 — e OHH),
0 A+ UL

3. By conditioning on the time of passage of the first vehicle,
a
E(T) = I (x +E(T))Ae* dx + ae,
0

and the result follows. If it takes a time b to cross the other lane, and so a + b to cross both, then, with
an obvious notation,

ah_ yo eb _ |}

é
(a) E(Ta) + ET) = —— +
Lb
eatb)A+u) _ 4
b E(Tq45) = ———-——_
(b) (Ta+b) an

The latter must be the greater, by a consideration of the problem, or by turgid calculation.
4. Look for a solution of the detailed balance equations

A(n + 1)

; n>0.
n+2 Jin 2

Mint =
tofind that z, = p"zo/(n+1) isastationary distribution if < 1, in whichcase zg = —p/log(1—p).
Hence 7, 2%n = Amg/(u—A), and by the lack-of-memory property the mean time spent waiting for
service is p79/(js — 4). An arriving customer joins the queue with probability

oO

yt _ pttlog(l — p)
n+2”"  plog(1—p) *

n=0

351

[8.4.5]-[8.5.5] Solutions Random processes

5. Byconsidering possible transitions during the interval (¢, ¢ +), the probability p;(¢) that exactly
i demonstrators are busy at time ¢ satisfies:

pott +h) = py(t)2h + pot) — 2h) + off),
pitt +h) = pot)2h + py @)Cl — A) — 2h) + po(t)2h + off),
Pott +h) = po(t)( — 2h) + pi @h + off).
Hence,
Patt) = 2p) (t)—2po(t), py (t) = 2po(t) —3p1@) +2p2(t), pot) = —2polt) + pr (),

and therefore p(t) = a + be~2! + ce—* for some constants a, b, c. By considering the values of p2
and its derivatives at = 0, the boundary conditions are found to be a + b +c = 0, —2b — 5c = 0,
4b + 25c = 4, and the claim follows.

8.5 Solutions. The Wiener process

1. We might as well assume that W is standard, in that o* = 1. Because the joint distribution is
multivariate normal, we may use Exercise (4.7.5) for the first part, and Exercise (4.9.8) for the second,

giving the answer
~ + — ¢sin —+sin™ 4/—+sin™" 4/— >.
8 4x t u u

2. Writing W(s) = /sX, Wt) = JtZ, and W(u) = ./uY, we obtain random variables X,
Y, Z with the standard trivariate normal distribution, with correlations pj = ./s/u, p2 = Jt/u,
3 = ./s/t. By the solution to Exercise (4.9.9),

(u—t)@t —s)

Z|X =
var(Z | X, Y) tu —s)
yielding var(W(t) | W(s), W(u)) as required. Also,

(u—t)W(s)+ ¢ —s)W(u)
u—s

E(W@)W(u)| W(s), W(v)) =E { | W(u) | W(s), we} ,

which yields the conditional correlation after some algebra.

3. Whenever a” + b? = 1.
4. Let Aj) = Wj + Dt/n) — W(jt/n). By the independence of these increments,

nal 20 nl to? to
E Ain? —o2t) = tay? — (o” py2) = §O-
( jay -o ') 2 (se * because E(Aj(n)*) = —
j=0 j=0
n-1 24 24 24 24
3t 2t t 3t
= ( 5-4 } because E(Aj(n)*) = ——>
‘ n n n n
J=0
21204
= —-0 asn —> OO.
n

5. They all have mean zero and variance f, but only (a) has independent normally distributed incre-
ments.

352

Problems Solutions [8.7.1]-[8.7.3]

8.7 Solutions to problems

1. E(¥n) = 0, and cov(Y¥m, Yntn) = ao n+; for m,n > 0, with the convention that oa, = 0
for k > r. The covariance does not depend on m, and therefore the sequence is stationary.

2. Wehave, by iteration, that ¥, = S,(m) +amtly 4 where $;,(m) = =o al Zn—j- There
are various ways of showing that the sequence {S,(m) : m > 1} converges in mean square and almost
surely, and the shortest is as follows. We have thata”t!y, _,_; > Oinm.s. and as. asm — 00;
to see this, use the facts that var(at!Y, in) = «2+ var(¥), and

2(m+Dgy2

a

SP Yn m1 >e< > ew < 0, e>0.
€

m m

It follows that S,(m) = Y, —a’"+1y, _,, 1 converges in m.s. and a.s. as m —> oo. A longer route to
the same conclusion is as follows. Forr < s,

a2”

B(|Su(6) ~ S09?) <8} (> Zn i) = ac,

j=r+l j=rtl

whence {S,(m) : m > 1} is Cauchy convergent in mean square, and therefore converges in mean
square. In order to show the almost sure convergence of S,(m), one may argue as follows. Certainly

m m oO oO
e(> x!Z,-j1) = DPElo! Zn—j| > Do lalE|Zn—j| < Dj lel! < 00,

j=0 j=0 j=0 j=0

whence ~ Feo at Zp ; is as. absolutely convergent, and therefore a.s. convergent also. We may
express limm—oo Sn(m) as oe! Zn—j. Also, a™tly 1 —> 0 in mean square and a.s. as
m —> oo, and we may therefore express Y, as

lo.@)
Yn = Seal LZn—j a.s.

It follows that E(Y,) = limm—oo E(Sn(m)) = 0. Finally, forr > 0, the autocovariance function
is given by
e(r) = cov(Yn, Yn—r) = E{(@Y,-1 + Zn)¥n—r } =ac(r — 1),

whence
a
— a?

e(r) = a!"le@) = r=...,-1,0,1,...,

since c(O) = var(Yn).

3. Iftisanon-negative integer, N(t) is the number of 0’s and 1’s preceding the (¢ + 1)th 1. Therefore
N(t) +1 has the negative binomial distribution with mass function

f= (Sr ota, k>t+l.

If ¢ is not an integer, then N(¢t) = N([t]).

353
[8.7.4]-[8.7.6] Solutions Random processes

4, We have that

Ah + oth) if j=i+,
s(av-+8)= j 000 =1)={ pih-etn if j=i—1,
L-Q+ypih+o(h) ifj =i,

an immigration—death process with constant birth rate 1. and death rates 4; = ip.

Either calculate the stationary distribution in the usual way, or use the fact that birth-death
processes are reversible in equilibrium. Hence Ax; = wi + 1)2j+.1 fori > 0, whence

1 fai
i! \p

5. We have that X (t) = Rcos(V) cos(6t) — Rsin(¥) sin(6t). Consider the transformation u =
rcos wv, v = —r sin wy, which maps [0, 00) x [0, 277) to R2. The Jacobian is

du su
Or Ow] _ _r
dv avi| ”?
dr aw
whence U = Rcos WV, V = —Rsin W have joint density function satisfying

rfu,v(r cosy, —rsiny) = friw(r, v).
: —7W?+0?) :
Substitute fy,y(u,v) =e 2 /(2x), to obtain
1 _1,2
frew(r, ¥) = 5—re ar, r>0,0<y <2n.
1
Thus R and W are independent, the latter being uniform on [0, 27).

6. Accustomer arriving at time u is designated green if he is in state A at time ¢, an event having
probability p(u, t —u). By the colouring theorem (6.13.14), the arrival times of green customers form
a non-homogeneous Poisson process with intensity function 4(u)p(u, t — u), and the claim follows.

354

9

Stationary processes

9.1 Solutions. Introduction

1. We examine sequences W,, of the form

oe)
(*) Wn = > agZn—k
k=0

for the real sequence {ax : k > 0}. Substitute, to obtain ag = 1, aj = a, ay = aa;_1| + Ba;—2,7r > 2,
with solution
(l+r)aq if a? +46 =0,
= r+l r+1
or Ay Ag

otherwise,
Ay — AQ

where 4. and Az are the (possibly complex) roots of the quadratic x? — ax — B = 0 (these roots are
distinct if and only if a2 + 48 # 0).
Using the method in the solution to Problem (8.7.2), the sum in (*) converges in mean square and

almost surely if |A;| < 1 and |A2| < 1. Assuming this holds, we have from (+) that E(W,) = 0 and
the autocovariance function is

c(m) = E(Wn Wa-m) = ac(m —1)+ Be(m—2), mol,

by the independence of the Z,. Therefore W is weakly stationary, and the autocovariance function
may be expressed in terms of a and £.

2. We adopt the convention that, if the binary expansion of U is non-unique, then we take the
(unique) non-terminating such expansion. It is clear that X; takes values in {0, 1}, and

P(Xn41 = 1|X; =x; for1 <i <n)=5
for all xj, x2,...,%n; therefore the X’s are independent Bernoulli random variables. For any se-
quence ky < ky < --- < k,, the joint distribution of Vj, , Vy,,..., Ve, depends only on that of
Xi +1> Xky 42. +++ Since this distribution is the same as the distribution of X;, X2,..., we have that
(Vk, > Vig» --+> Vk) has the same distribution as (Vo, Vi—k,,---» Vk-—k,)- Therefore V is strongly
stationary.

Clearly E(Vn) = E(Vo) = 5 and, by the independence of the X;,

oo
cov(Vo, Vn) = 5-272" var(Xi) = ay(4)".

i=]

355
[9.1.3]-[9.2.1] Solutions Stationary processes

3. (i) For mean-square convergence, we show that S; = an an Xn is mean-square Cauchy con-
vergent as k — oo. We have that, forr < s,

Ss

§ 2
E{(Ss—S/}= >> aiajeli ~ j= OY » al}

ij=rtl i=r+1

since |c(m)| < c(0) for all m, by the Cauchy—Schwarz inequality. The last sum tends to 0 asr, s — 00
if 5°; |aj| < oo. Hence S; converges in mean square as k —> 00.

Secondly,
n n n
e(> la.Xz!) < SS lagl -E|Xel < \/E(X3) D> |ael
k=1 k=1 k=1

which converges as n —> oo if the |a,| are summable. It follows that }7Z.; |a;X;| converges
absolutely (almost surely), and hence )y..; a Xx converges a.s.
(ii) Each sum converges a.s. and in mean square, by part (i). Now

oe)
cy(m)= > ajaye(m +k — j)
j.k=0

whence

00 2
ley (n)| < «(0){ lai} < OO.

4. Clearly X, has distribution w for all n, so that {f(Xn) : n > m} has fdds which do not depend
on the value of m. Therefore the sequence is strongly stationary.

9.2 Solutions. Linear prediction
1. (i) We have that
(x) E{(Xn41 —@Xn)?} = (1 +.@7)c(0) — 2ae(1),

which is minimized by setting a = c(1)/c(0). Hence Xnsi = c(1)Xn/c(0).
(ii) Similarly
(#4) E{(Xn41 — BXn — yXn-1)"} = 1+ B? + ye) + 2B(y — De(1) — 2yeQ),

an expression which is minimized by the choice

e(1)(c(0) — ¢(2)) c(O)e(2) — c(1)?_

P=—Toz ey?’ > e@2 eye

Xn+1 is given accordingly.
Gii) Substitute a, 8, y into (*) and («*), and subtract to obtain, after some manipulation,

__ fe(1)? — e(O)e(2)}*
~ e(0){e(0)2 — ¢(1)2}"

356

Autocovariances and spectra Solutions [9.2.2]-[9.3.2]

(a) In this case c(0) = 4, and c(1) = c(2) = 0. Therefore X41 = Xn41 = 0, and D = 0.
(b) In this case D = 0 also.
In both (a) and (b), little of substance is gained by using X,,+1 in place of X41.
2. Let {Z, :n =...,—1,0,1,...} be independent random variables with zero means and unit
variances, and define the moving-average process
_ Zn + aZy-1

Xn =
(*) Tae

It is easily checked that X has the required autocovariance function.

By the projection theorem, X;, — Xn is orthogonal to the collection {Xn_- : r > 1}, so that
E{(Xn — Xn)Xn—r} = 0,7 > 1. Set Xn = OP, bs Xn—s to obtain that

a=b)+boa, 0=b,_ja+bs+be4 10 for s > 2,

where @ = a/(1 + a”). The unique bounded solution to the above difference equation is bs =

(—1)+1a5, and therefore
oe)

Xn = C1) ta’ Xp.
s=1
The mean squared error of prediction is
s 2 2c 2 1 2 1
= —a)ys _ = ——
E{(Xn — Xn) y=2{ (So a) ns) \. Tae Sn) = lta’

Clearly E(X,) = 0 and

oe)
cov(Xn, Xn—m) = > brbsc(m +r—s), m > 0,

rs=l

so that X is weakly stationary.

9.3 Solutions. Autocovariances and spectra
1. Itis clear that E(X,,) = 0 and var(X,) = 1. Also
cov(Xm, Xm+n) = cos(mdA) cos{(m + n)A} + sin(mA) sin{(m + n)A} = cos(nd),

so that X is stationary, and the spectrum of X is the singleton {A}.
2. Certainly dy (t) = (e#" — e~#™)/ (Ait), so that E(Xn) = dy (dy (n) = 0. Also

cov(Xim, Xmtn) = E(XmXm+n) = E (eV Vm UF V mt) — gy (n),
whence X is stationary. Finally, the autocovariance function is
cin) = dyin) = far),
whence F is the spectral distribution function.

357

[9.3.3]-[9.3.4] Solutions Stationary processes
3. The characteristic functions of these distributions are

. 1
(i) p(t)=e 2

1 + 1 _ 1
l—it  1+it/) 14727

(ii) ptt) =

4. (i) We have that

var(# 9x) _t 3 cov(X;, X, y=
n J} 72 Je Oke 2

dup) dF(A).
jp k=l

(—7, 7] (>

The integrand is

2 eink _ eink _ _ L=cos(nd)
~\ nq etA_1 J L-cosa ’

lay) sin(nd/2) \?
(78) = c(0) _ (Sao) dF(A).

It is easily seen that | sin @| < |@|, and therefore the integrand is no larger than

n
j=l

whence

Az \?

Asn ~—> oo, the integrand converges to the function which is zero everywhere except at the origin,
where (by continuity) we may assign it the value 1. It may be seen, using the dominated convergence
theorem, that the integral converges to F (0) — F(0—), the size of the discontinuity of F at the origin,
and therefore the variance tends to 0 if and only if F(0) — F(O—) = 0.

Using a similar argument,
1 n—1 0 n-l
2S (y= ( ei) drQ)=00) | endydFay
” i=0 mn Y(-m,0]\jZ9 (—1,7]

where

1 ifA =0,
gn(A) = | ein -—1

nik 1) BAF

is a bounded sequence of functions which converges as before to the Kronecker delta function 5,9.

Therefore
n—-1

1 .
- 2H) > c(0)(F()— F(0-)) — asn > ow.

358

The ergodic theorem Solutions [9.4.1]-[9.5.2]

9.4 Solutions. Stochastic integration and the spectral representation

1. Let Hy be the space of all linear combinations of the X;, and let Hx be the closure of this space,
that is, Hy together with the limits of all mean-square Cauchy-convergent sequences in Hy. All
members of Hy have zero mean, and therefore all members of Hy also. Now S(A) € Hy forall A,
whence E(S(A) — S(j)) = 0 for all A and yu.

2. First, each Yj, lies in the space Hy containing all linear combinations of the X, and all limits of
mean-square Cauchy-convergent sequences of the same form. As in the solution to Exercise (9.4.1),
all members of Hy have zero mean, and therefore E(Y¥) = 0 for all m. Secondly,

_ gid g—ink
E(Y¥mY »= | ~___ f(A) dA = 8mn.
mY n (um) oF) fA) mn
As for the last part,
= ijdh enh ind
ajYn—j; =f (> aje_ i) os asc) = | er’ dSQ)=
2 Pet Ienz) de") Te mF) (<7)

This proves that such a sequence X, may be expressed as a moving average of an orthonormal
sequence.

3. Let Hy be the space of all linear combinations of the X,, together with all limits of (mean-
square) Cauchy-convergent sequences of such combinations. Using the result of Problem (7.11.19),
all elements in Hy are normally distributed. In particular, all increments of the spectral process are
normal. Similarly, all pairs in Hy are jointly normally distributed, and therefore two members of
Hy are independent if and only if they are uncorrelated. Increments of the spectral process have
zero means (by Exercise (9.4.1)) and are orthogonal. Therefore they are uncorrelated, and hence
independent.

9.5 Solutions. The ergodic theorem

“1. With the usual shift operator tr, it is obvious that tg =@, so that @ € J. Secondly, if A € 2,
then t~!(A°) = (r~!A)® = A®, whence A° € £. Thirdly, suppose Ay, Az,... € £. Then

oo lo. @) lo. @)
YU Ai) =(Jrta =U A.
i=l i=l i=l

so that 7° A; € J.

2. The left-hand side is the sum of covariances, c(0) appearing n times, and c(i) appearing 2(n — i)
times for 0 < i <n, in agreement with the right-hand side.

Lete > 0. Ifé(j) = j7! i c(i) > o7 as j —> 00, there exists J such that |¢(j) —o2| <
when j > J. Now

2S a =f e+ 3 i(o? +o} +0? +e
j=l j=l j=J+l

asn — oo. A related lower bound is proved similarly, and the claim follows since € (> 0) is arbitrary.

359

[9.5.3]-[9.6.3] Solutions Stationary processes

3. It is easily seen that Sm = 57729 aj Xn+i Constitutes a martingale with respect to the X’s, and

m CO
E(Sp,) = > oF E(Xn 41) S > oe,
i=0 i=0

whence S,, converges a.s. and in mean square as m —> 00.

Since the X, are independent and identically distributed, the sequence Y, is strongly stationary;
also E(Y,) = 0, and so nl a Y; — Z a.s. and in mean, for some random variable Z with mean
zero. For any fixed m (> 1), the contribution of X;, X2,..., Xm towards an Y; is, for large n, no
larger than

m
Cn = S>@o + ay +--+ +aj.1)X;).

j=l

Now n-!Cm — 0 as n — oo, so that Z is defined in terms of the subsequence X41, Xm+2,---
for all m, which is to say that Z is a tail function of a sequence of independent random variables.
Therefore Z is a.s. constant, and so Z = Oa.s.

9.6 Solutions. Gaussian processes
1. The quick way is to observe that c is the autocovariance function of a Poisson process with
intensity 1. Alternatively, argue as follows. The sum is unchanged by taking complex conjugates, and

hence is real. Therefore it equals

2

n n n n n
Da (igP +5 So uty > x)= oy oe
1 k j=l

j= =jf+l k=j+l1 k=j

= KC; — t-1)
j=l

2 n
-| Sx
k=jtl
2

n

Doz

k=j

where to = 0.

2. Fors,t > 0, X(s) and X(s + ¢) have a bivariate normal distribution with zero means, unit
variances, and covariance c(t). It is standard (see Problem (4.14.13)) that E(X(s +t) | X(s)) =
c(t)X(s). Now

o(s +1) = E(XO)X(s + 1)) = E{E(X(OX(s +1)| XO, X(s))}
= E(X(O)c(t)X(s)) = e(s)e(t)

by the Markov property. Therefore c satisfies c(s + t) = c(s)c(t), c(O) = 1, whence c(s) = c(1)s! =
pls! Using the inversion formula, the spectral density function is
oo 1 p2

1 . _
aAy= —ish _ —, l<cx.
f= 5 oo Omllaperp «AIS *

Note that X has the same autocovariance function as a certain autoregressive process. Indeed,
stationary Gaussian Markov processes have such a representation.

3. If X is Gaussian and strongly stationary, then it is weakly stationary since it has a finite variance.
Conversely suppose X is Gaussian and weakly stationary. Then c(s, f) = cov(X(s), X(#)) depends

360

Problems Solutions [9.6.4}-{9.7.2]

ont — s only. The joint distribution of X(t,), X (t2), ..., X (én) depends only on the common mean
and the covariances c(¢;, t;). Now c(#;, tj) depends on t; — 4; only, whence X(t), X(t2), ..., X(tn)
have the same joint distribution as X(s + f,), X(s + t2),...,X(s +t). Therefore X is strongly
stationary.

4. (a)Ifs,t > 0, we have from Problem (4.14.13) that
E(X(s + 1)? | X(s)) = Xe? +1 — ett)’,
whence
cov(X(s)*, X(s +1)”) = E(X(s)*X(s +1)?) —1
= E{X(sPE(X( +1)? | X(s)) $= 1
= ¢(t)E(X(s)*) + (1 — c(t)? )E(X(s)*) — 1 = 2c(t)?

by an elementary calculation.
(b) Likewise cov(X(s)?, X(s + £)°) = 3G + 2c(t)*)e(t).

9.7 Solutions to problems

1. Itis easily seen that Y, = Xn + (a2 — B)X,_1 + BY,—1, whence the autocovariance function c
of Y is given by
1+a?— p?
1 — p2
e(k) = ; 5
ikl! {* +a — B*)

ifk =0,

1 — p?

Set Yat = 79 a; ¥,_; and find the a; for which it is the case that E{(¥,41 — Yn41)¥n—k} =0
for k > 0. These equations yield

ifk £0.

[o@)
ck+1)=Scajek-i), k>0,
i=0

which have solution a; = «(8 — a)! fori > 0.

2. The autocorrelation functions of X and Y satisfy

;
oxpx(n) = oF » ajagpy(n +k — j).
j,k=0

Therefore

2 [o,6) r
os -i .
of f(A) = xe > eink > ajay py(n +k — j)
oo
_%5 ajazel®-D* Se WOH D y(n $k = j)
21 J

= of |Gale™)|? fy (a).

361

[9.7.3}-[9.7.5] Solutions Stationary processes

In the case of exponential smoothing, Gg (e!*) =(1-p)/d- pel), so that

_ y\2
fy) = < BY fy) Alem,

—2ucosa + p2’

where c = of /o% is a constant chosen to make this a density function.

3. Consider the sequence {X,,} defined by
Xn = Yn —- Y, = Yn — @Y,_1 — BYn-2.

Now X;, is orthogonal to {Y,_,; : k > 1}, so that the X, are uncorrelated random variables with
spectral density function fy (A) = Qn)7}, 4 € (—2,27). By the result of Problem (9.7.2),

og fx (A) = of|1 — ae" — pe? fra),
whence 22
ox/o¥

{1 _ aeir _ Berir|2’

fra = on —m <i <7.

4. Let {X/, : 2 > 1} be the interarrival times of such a process counted from a time at which a
meteorite falls. Then X}, X5,... are independent and distributed as Xz. Let Y;, be the indicator
function of the event {X/,, = n for some m}. Then

E(¥m Yn-+n) = P(Yn = 1, Yntn = 1)
= PYmtn =1( Ym = DPYn = 1D) = POY, = De

where a = P(Yn = 1). The autocovariance function of Y is therefore c(n) = a{P(¥), = 1) — a},
n > 0, and Y is stationary.
The spectral density function of Y satisfies

fy(a _1 3 ~im _€) __p, 1 3 imecyy — 2
Y= a7 2.8 a(dl—a) mal — a) A ° a an

Now
oO oT
Soe ¥, _ So etn
n=0 n=0

where T, = X{ + X4, +---+ Xj; just check the non-zero terms. Therefore

co a / a? a a?
ink iXT,
= aE — = ;
de cn) =a { se n [ek = 7-0) Toe

when e!* + 1, where ¢ is the characteristic function of X. It follows that

j= u Re{ 1 a \ 2 rq
WO= Tae) ° 1-¢) 1-ei an’ |A| < x.

5. We have that

E(cos(nU)) = /

Kh

an | a |
Fy Cos(nu) du=0, E(cos*(nU)) -| Fy cos (nu) du= 3

Hh

362

Problems Solutions [9.7.6]-[9.7.7]
forn > |. Also
5(cos(mU) cos(nU)) = Ef 3 (cos[(m + n)U] + cosl(m — n)U])} = 0

ifm #n. Hence X is stationary with autocorrelation function p(k) = dz0, and spectral density
function f(A) = (27)! for |A| < 2. Finally

E{cos(mU) cos(nU) cos(rU)} = SEY (cos{(m +n)U] + cos[(m — n)U]) cos(ru)}
= 4{o(m+n—-r)+p(m—n—r)}

which takes different values in the two cases (m,n, r) = (1, 2,3), (2, 3,4).

6. (a) The increments of N during any collection of intervals {(u;, vj) : 1 <i <n} have the same
fdds if all the intervals are shifted by the same constant. Therefore X is strongly stationary. Certainly
E(X (f)) = Aa for all t, and the autocovariance function is

0 if ’

Therefore the autocorrelation function is

0 if |t| > a,
p= .
1—|t/o| if |t] <a,

which we recognize as the characteristic function of the spectral density f(A) = {1—cos(a@A)}/ (a2);
see Problems (5.12.27b, 28a).

(b) We have that E(X(t)) = 0; furthermore, for s < t, the correlation of X(s) and X(t) is

e0v(X6), X(t) = cov W(s) — Ws - 1), Wt) — We - 1))
=s-—min{fs,t-1}-(-l+6-1)
1 ifs <t-1,
ae ift-l<s<t.

This depends on t — s only, and therefore X is stationary; X is Gaussian and therefore strongly
stationary also.
The autocorrelation function is

0 if |h| > 1,

h) =
pth) oat if |h| <1,

which we recognize as the characteristic function of the density function f(A) = (1 — cosa)/ (2).

7. We have from Problem (8.7.1) that the general moving-average process of part (b) is stationary
with autocovariance function c(k) = j=0 oj c.4;,k = O, with the convention that as = Oif s < 0
ors >r.

(a) In this case, the autocorrelation function is

1 ifk =0,
a
pk) 1+ a if |k|
0 if |k] > 1,

363
[9.7.8]-[9.7.13] Solutions Stationary processes

whence the spectral density function is

1 2 Xr
fa = = (0 (0) +e p(l) +e p(— 1)) = a (1+ Serr). |Al <7.
(b) We have that
iAy)2
mikk yy ajell* Sap je tet = |A(e"")|

where c(0) = Ly a? and A(z) = Di ajzi . See Problem (9.7.2) also.

8. The spectral density function f is given by the inversion theorem (5.9.1) as
1 0° _;
fa)=— / e"'* p(t) dt
2m J—oo
under the condition JIo° |e(£)| dt < co; see Problem (5.12.20). Now

1 OO
If@I < =| lo(t)|dt
Te J—oo

and

1 fe,
If@e +h) — f@l < =f. je" 1 Ipayidt.

The integrand is dominated by the integrable function 2|o(t)|. Using the dominated convergence
theorem, we deduce that | f(x +) — f(x)| > 0as h — 0, uniformly in x.

9. By Exercise (9.5.2), var(n—! jal Xj) > o? if Ch = nv! ar, cov(X9, Xj) > o%. If
cov(Xo, Xn) — 0 then C, — 0, and the result follows.

10. Let X,, X2,... be independent identically distributed random variables with mean jz. The se-
quence X is stationary, and it is a consequence of the ergodic theorem that n=! vj=l Xj > Zas.
and in mean, where Z is a tail function of X;, X2,... with mean yz. Using the zero—one law, Z is a.s.
constant, and therefore P(Z = yt) = 1.

11. We have from the ergodic theorem that n~! rz 47 > E(Y | £) as. and in mean, where J is

the o-field of invariant events. The condition of the ‘question is therefore
(*) E(Y | £) = E(Y) a.s., for all appropriate Y.

Suppose (*) holds. Pick A € £, and set Y = J, to obtain 4 = Q(A) a.s. Now I, takes the values 0
and 1, so that Q(A) equals 0 or 1, implying that Q is ergodic. Conversely, suppose Q is ergodic. Then
ECY | £) is measurable on a trivial o-field, and therefore equals E(Y) a.s.

12. Suppose Q is strongly mixing. If A is an invariant event then A = t~" A. Therefore Q(A) =
QIAN tA) > Q(A)? as n —> oo, implying that Q(A) equals 0 or 1, and therefore Q is ergodic.

13. The vector X = (X 1, X2,...} induces a probability measure Q on (RT , B’). Since T is measure-
preserving, Q is stationary. Let Y : R’ — Rbe given by Y(x) = x; forx = (x1, x2,...), and define
Y; (x) = Y(ri7} (x)) where r is the usual shift operator on R?. The vector Y = (%, Yo, ...) has the
same distributions as the vector X. By the ergodic theorem for Y, n~! ri Yj > E(Y | J) as. and
in mean, where { is the invariant o-field of r. It follows that the limit

(*) Z= lim oe

n>0on

364

Problems Solutions [9.7.14]-[9.7.15]

exists a.s. and in mean. Now U = lim supy_, 99 (n7} > 7 X;) is invariant, since

fg 1
{oe - x Ta) | = 7X () -X(T"w)}>0 as.

i=1

implying that U(w) = U(Tw) as. It follows that U is £-measurable, and it is the case that Z = U
a.s. Take conditional expectations of («), given £, to obtain U = E(X | £) as.

If T is ergodic, then £ is trivial, so that E(X | £) is a.s. constant; therefore E(X | £) = E(X) as.
14. If (a, b) < [0, 1), then T~!(a, b) = (5a, 5b)U(4 + 5a, 5 +40), and therefore T is measurable.
Secondly,

P(T~\(a,b)) = 2(4b — $a) = b —a = P((a, b)),

so that T~! preserves the measure of intervals. The intervals generate B, and it is then standard that
T7! preserves the measures of all events.

Let A be invariant, in that A= T~!A. Let0 < @ < 53 it is easily seen that T(w) = T(w + 3):
Therefore co € A if and only if @ + 5 € A, implying that AN[4, 1) = 4 + {AN[O, 5)}; hence

P(ANE)= 4P(A)=P(A)P(E) for E = [0, 5), [9, D).

This proves that A is independent of both [0, 5) and [5 , 1). Asimilar proof gives that A is independent

of any set E which is, for some n, the union of intervals of the form [k2™ , (k+1)2~") forO < k < 2".
It is a fundamental result of measure theory that there exists a sequence FE), E2, ... of events such that

(a) Ey, is of the above form, for each n,
(b) P(A A E,) > Oasn > o0.
Choosing the £, accordingly, it follows that

P(A /N En) = P(A)P(En) > P(A)* by independence,
|P(A N En) — P(A) < P(A A En) > 0.

Therefore P(A) = P(A)? so that P(A) equals 0 or 1.

For w € , expand w in base 2, m = 0.012 --- , and define Y(w) = w. It is easily seen that

Y(T"~1) = wn, whence the ergodic theorem (Problem (9.7.13)) yields that nl wr ei > 5 as
n —> oo for all w in some event of probability 1.
15. We may as well assume that 0 < a < 1. Let T : [0,1) — [0, 1) be given by T(x) =x +a
(mod 1). It is easily seen that T is invertible and measure-preserving. Furthermore T(X) is uniform
on [0, 1], and it follows that the sequence Z;, Z2,... has the same fdds as Z2, Z3, ..., which is to
say that Z is stationary. It therefore suffices to prove that T is an ergodic shift, since this will imply
by the ergodic theorem that

n 1
~ 4a) = f g(u) du.
art 0

We use Fourier analysis. Let A be an invariant subset of [0, 1). The indicator function of A has
a Fourier series:

io, ¢)

(*) Ia(x)~ SS anen(x)

n=—OO

27 ine

where e,(x) = e and

1 f 1
an = =| Ta (x)e~n(x) dx = = / e—n(x) dx.
2x Jo 2m JA

365
[9.7.16]-[9.7.18] Solutions Stationary processes

Similarly the indicator function of T—!A has a Fourier series,

Tp-1 4) ~ So bnen(x)
n
where, using the substitution y = T(x),
1 1 1 I -1 —2nina
bn = =~ | Tp-1,(ee-n(x)dx = = J Ta(ye-n(T(y)) dy = ane '
2x Jo 27 JO
since é,(y — a) = e ATime pg (y). Therefore I-_1 , has Fourier series
Tp—1 (2) ~ Se an en (x).
n

Now [4 = I,-1, since A is invariant. We compare the previous formula with that of (+), and deduce

that an = e—27""%q, for all n. Since o is irrational, it follows that an = 0 if n 4 O, and therefore I,
has Fourier series ap, a constant. Therefore J, is a.s. constant, which is to say that either P(A) = 0
or P(A) = 1.

16. Let G;(z) = E(z*™), the probability generating function of X(t). Since X has stationary
independent increments, for any n (> 1), X(t) may be expressed as the sum

X(t) = So {X(it/n) — X@ - Vt/n)}

i=1

of independent identically distributed variables. Hence X(t) is infinitely divisible. By Problem
(5.12.13), we may write

(*) Gr(z) = e MOO AQ)

for some probability generating function A, and some A(t).

Similarly, X(s +t) = X(s)+{X(s +14) — X(s)}, whence Gs+;(z) = Gs(z)G+(z), implying that
G;(z) = e#@)t for some i(z); we have used a little monotonicity here. Combining this with («), we
obtain that G;(z) = e440 -A@) for some A.

Finally, X(¢) has jumps of unit magnitude only, whence the probability generating function A is
given by A(z) = z.

17. (a) We have that
(«) X(t) — XO) = {X) -XO}+{XO-X}, Osssz,
whence, by stationarity,

{m(t) — m(0)} = {m(s) — mO)} + {m(t — 5) — mO)}.

Now m is continuous, so that m(t) — m(O) = Bt, t = 0, for some f; see Problem (4.14.5).

(b) Take variances of (*) to obtain v(t) = v(s) + u(t — s), 0 < 5 <1, whence v(t) = o2¢ for some
2

oa”.

18. In the context of this chapter, a process Z is a standard Wiener process if it is Gaussian with
Z(Q) = 0, with zero means, and autocovariance function c(s, t) = min{s, t}.

366

Problems Solutions [9.7.19]-[9.7.20]
(a) Z(t) = aW (t/a) satisfies Z(0) = 0, E(Z(t)) = 0, and

cov(Z(s), Z(t)) = a? min{s/a”, t/a7} = min{s, t}.
(b) The only calculation of any interest here is

cov(W(s +a)—W(a), Wit+a)— W(a))
=c(st+a,t+a)—c(a,t+a)—c(s+a,a)+c(a, a)
=(sta)—a-a+a=s, S<t.

(c) V(0) = 0, and E(V(t)) = 0. Finally, ifs, ¢ > 0,
cov(V(s), V(t)) = stcov(W(1/s), W(1/t)) = st min{1/s, 1/t} = min{t, 5}.
(d) Z(t) = W(1) — WC — 4) satisfies Z(0) = 0, E(Z(t)) = 0. Also Z is Gaussian, and
cov(Z(s), Z(t)) =1-(1—s)-(1—t)+ min{l —s,1-1}

= min{s, t}, O<s,t<1.

19. The process W has stationary independent increments, and G(t) = EC W(t?) satisfies G(t) =
t > Oast — 0; hence for $(u) dW (u) is well defined for any ¢ satisfying

fo) oO
i owe dG@) = [ dw)? du < oo.

1

It is obvious that @(u) = Ifo,4](4) and d(u) = ef 4) Io, 1](4) are such functions.
Now X(f) is the limit (in mean-square) of the sequence

n-1

Sn(t) = SO{W(G + Dt/n)— Wit/n)}, ne 1.
j=0

However S,,(t) = W(t) for all n, and therefore S,,(t) as W(t) asn — oo.

Finally, Y(s) is the limit (in mean-square) of a sequence of normal random variables with mean
0, and therefore is Gaussian with mean 0. If s < ft,

OO
cov(Y(s), ¥(t)) = [ (eS Tig 5) (€ © ™ Tro.) dG)

s
=| e2t—s—t du= 5(es* _ e874),
0

Y is an Ornstein—Uhlenbeck process.
20. (a) W(t) is N(O, t), so that

oO 1,,,2
EWwol= | ee ht !D du = \/2t/n,

—oo V2nt

2 2
var((WO))) = EW) — = = 1 (1 _ =)

367
[(9.7.21]-[9.7.21] Solutions Stationary processes

The process X is never negative, and therefore it is not Gaussian. It is Markov since, if s < ¢ and
B is an event defined in terms of {X(u) : u < 5s}, then the conditional distribution function of X(t)
satisfies

P(X(t) < y| X(s) =x, B) = P(X) < y| Ws) =x, B)P(W(s) = x | X(s) = x, B)
+ P(X(t) < y| W(s) = —x, B)P(W(s) = —x | X(s) =x, B)

~ 2{P(x@) <y|W) =x) +P(XO sy|WO)= -x)},

which does not depend on B.
(b) Certainly,

00 1,2 1
ym) = [ e301) gy = 3.

—00 It
Secondly, W(s) + W(t) = 2W(s) + {W@ — W(s)} is NO, 3s +1) ifs < t, implying that

E(¥(s)¥(@)) =E (eVO+We) _ er Ost!)

and therefore ; ;
cov(Y(s), Y(t)) = e2Gstt) _ er Sth) s<f.
W 1) is N(0, 1), and therefore Y (1) has the log-normal distribution. Therefore Y is not Gaussian.
It is Markov since W is Markov, and Y(t) is a one—one function of W(t).

(c) We shall assume that the random function W is a.s. continuous, a point to which we return in
Chapter 13. Certainly,

t
E(Z(1)) = I E(W(u)) du = 0,

E(Z(s)Z(t)) = If E(W(u)W(v)) dudv
O<u<s

O<u<t

AY ue t
-[ | vdv+ | udv} du = }s?(3t —s), s<t,
u=0 v=0 v=u

since E(W (u)W(v)) = min{u, v}.
Z is Gaussian, as the following argument indicates. The single random variable Z(t) may be
expressed as a limit of the form

n
. t .
(*) Jim, 2 (5) Wit/n),
i=
each such summation being normal. The limit of normal variables is normal (see Problem (7.11.19)),
and therefore Z(t) is normal. The limit in («) exists a.s., and hence in probability. By an appeal to
(7.11.19b), pairs (Z(s), Z(t)) are bivariate normal, and a similar argument is valid for all n-tuples of
the Z(u).
The process Z is not Markov. An increment Z(t) — Z(s) depends very much on W(s) = Z’(s),
and the collection {Z(u) : u < s} contains much information about Z’(s) in excess of the information
contained in the single value Z(s).

21. Let U; = X(t;). The random variables A = U,, B = U2 — U1, C = U3 — U2, D= U4 — UG
are independent and normal with zero means and respective variances ty, t2 — t1, t3 — fo, t4 — t3. The
Jacobian of the transformation is 1, and it follows that U;, U2, U3, U4 have joint density function

1
e 22

(27)*./t1 (ft) — tH) (3 — #2) (4 — 83)

fu@ =

368
Problems Solutions [9.7.22]-[9.7.22]

where 5

“Ty (uz — 1)? | 3 — up)? | (ug — 3)?
ty ta —t] tz — to t4 — t3 ,
Likewise U, and U4 have joint density function

Q=

1
e 2k ua (ug — u1)”
= where R= — + ————~.
2m /t) (t4 — t1) th t4— ty
Hence the joint density function of U2 and U3, given U; = U4, = 0, is
1
e 2 4 — t
g(u2, 43) = 5)
mV (t2 — t1)(t3 — t2) (4 — 83)
where 3 3
_ Ww (43 — uy)” U3
Ig —t] tz -—t ty — 3

Now g is the density function of a bivariate normal distribution with zero means, marginal variances

2_ (2 —t)(3 — by) 2 __ (t4 — 3) (3 — fy)
oy =, OF =
t3-—t tq —t2
and correlation
p= o102, sf (t4 — 3) (tn — ty)
3-1 (t4 — t2)(& — tH)
See also Exercise (8.5.2).

22. (a) The random variables {I;(x) : 1 < j <n} are independent, so that

1 1-
E(Fn(x)) =x, var(Fn(x)) = = var(I (x) = se

By the central limit theorem, ./n{F,(x) — x} 2% Y(x), where Y (x) is N(O, x(1 — x)).

(b) The limit distribution is multivariate normal. There are general methods for showing this, and
here is a sketch. If 0 < x] < x2 < 1, then the number M2 (= nF (x2)) of the J; not greater than
x2 is approximately N(nx2, nx2(1 — x2)). Conditional on {Mz = m}, the number M,; = nF (x1)
is approximately N(mu, mu(1 — u)) where u = x,/x2. It is now a small exercise to see that the
pair (M,, M2) is approximately bivariate normal with means nx,, nx2, with variances nx,(1 — x1),
nx2(1 — x2), and such that

E(M1 Mp) = E{ M)E(M, | M2)} = E(M3x1/x2) ~ nx,(1 — x2) +n? x 1x9,

whence cov(M,, Mz) ~ nx (1 — x2). It follows similarly that the limit of the general collection is
multivariate normal with mean 0, variances x; (1 — x;), and covariances cjj = x;(1 — x;).

(c) The autocovariance function of the limit distribution is c(s,#) = min{s,t} — st, whereas, for
O<s <t <1, we have that cov(Z(s), Z()) = s —ts — st +st = min{s, t} — st. It may be shown
that the limit of the process {./n(Fn(x) — x) : 2 = 1} exists as n — oo, in a certain sense, the limit
being a Brownian bridge; such a limit theorem for processes is called a ‘functional limit theorem’.

369
10

Renewals

10.1 Solutions. The renewal equation

1. Since E(X,) > 0, there exists « (> 0) such that P(X; > €) > e. Let Xi = €l{x,>e}, and denote

by N’ the related renewal process. Now N(t) < N’(t), so that Ee°N®) < Be?" "Oy, for 6 > 0.
Let Z», be the number of renewals (in N’) between the times at which N’ reaches the values (m — 1)e

and me. The Z’s are independent with
ee®

whence E(e9N’"®) < (ce? {1 — (1 —e)e? yy for sufficiently small positive 6.

2. Let X, be the time of the first arrival. If X; > s, then W = 5. On the other hand if X; < s, then
the process starts off afresh at the new starting time X;. Therefore, by conditioning on the value of
Xi,

Fw) = [rw sx| Xi =warw = [pw sx-wdrwt [1-aFw
= [ Pov sx-warw+t- Fo)

ifx > s. Itis clear that Fy (x) = Oif x < s. This integral equation for Fy may be written in the
standard form

Fw(x) = H(x) +f Fw(x —u)dF(u)

where H and F are given by

may ={) ifx <s, Fay ={ oe ifx <s,
1-F(s) ifx>s, F(s) ifx>s.

This renewal-type equation may be solved in the usual way by the method of Laplace—Stieltjes trans-
forms. We have that Fy, = H* + FRF*, whence Fy, = H*/(1 — F*). If N is a Poisson process
then F(x) = 1 — e~*. In this case
OO
H*(@) = [ 2 dH (x) = eT At O8,

since H is constant apart from a jump at x = s. Similarly

A s Xr
F* (0) = ~8% g F(x) = —~(1 — e195),
(0) I e (x) L408 (le )

370
Limit theorems Solutions [10.1.3]-[10.2.2]

so that
at Ae O+8)s

o+ Re At+8)s .
Finally, replace 6 with —0, and differentiate to find the mean.

Fy @) =

3. We have as usual that P(N (t) = n) = P(Sp < t) — P(S,41 < t). In the respective cases,

It]
(a) P(N() =n) = )> fe many —e"*™ Ona + DI,
r=0
t qnb,nb-1 g@tlb,@+l)b-1 ax
(b) P(N() =n) = [ Tub) _ Tun + 1)b) \. dx.

4. By conditioning on X1, m(t) = E(N(¢)) satisfies
t t
mo = [ (Lt me—a)dxaet [ mexdx, O<t<il.
0 0
Hence m’ = 1+ m, with solution m(t) = e' — 1, for0 < t < 1. (For larger values of t, m(t) =
1+ fo m(t — x) dx, and a tiresome iteration is in principle possible.)
With v(t) = E(N()*),
t t
v(t) -| [v(t — x) +2m¢t —x) +1] dx =t+2(e' —t— +f v(x)dx, O<t<l.
0 0

Hence v’ = v + 2e' — 1, with solution v(t) = 1 — e' + 2te’ forO <t <1.

10.2 Solutions. Limit theorems

1. Let Z; be the number of passengers in the ith plane, and assume that the Z; are independent
of each other and of the arrival process. The number of passengers who have arrived by time ¢ is

SQ) = NO Z;. Now
N@) S@ | E(Z1)
= . aad
t N(t) a
by the law of the large numbers, since N(t)/t > 1/ua.s., and N(t) > ooa.s.
2. We have that

OO 2 CoO
E(Tjq) =E (> Ziliuzi) =SCE(Z?Iuey) +2 D> E(ZZjIu>j))
i=] i=1

1<i<j<oco

az
7 (t)

since Ijy>iyl{m>j} = Ipu>ivj}, where i V j = max{i, j}. Now
B(Z; Imai) = E(Z? Tw ci-1ye) = E(Z7)PM = i),
since {M < i — 1} is defined in terms of Z), Z2,..., Z;_1, and is therefore independent of Z;.
Similarly E(Z;Z;Ipy>j)) = E(Z))E(Ziljm>j}) = Oifi < j. It follows that
[o@) [o@)
E(T¥) = 5_E(Z?)P(M > i) = 0? S~ P(M > i) = 0° E(M).
i=] i=]

371

[(10.2.3]-[10.2.5] Solutions Renewals

3. (i) The shortest way is to observe that N(t) + & is a stopping time if k > 1. Alternatively, we
have by Wald’s equation that E(Ty()41) = w(m(t) + 1). Also

E(Xw@+k) = E{E(Xvwie|NO)}=u, kz=2,

and therefore, for k > 1,

k
E(Ty ask) = Ey) + 9 E(Xneyes) = w(t) +).
j=2
(ii) Suppose p 4 1 and
p ifa =1,
P(X, =a) = .
1-p ifa=2.

Then p = 2— p #1. Also
E(Tyqay) = 1 — p)EMo | NG) = 0) + pE(T | NG) = 1) =p,
whereas m(1) = p. Therefore E(Ty1)) # um(1).

4, Let V(t) = M(t) + 1, and let W), Wo, ... be defined inductively as follows. W; = V(1), Wo
is obtained similarly to W, but relative to the renewal process starting at the V(1)th renewal, i.e., at
time Tyv(1)41, and Wy is obtained similarly:
Wa =N(Tx,_,+D—-N(Tx,_))+1, 2 >2,

where Xj, = W1+W2+---+Wm. Foreachn, Wy is independent of the sequence W,, W2,..., Wn_1,
and therefore the W,, are independent copies of V (1). It is easily seen, by measuring the time-intervals
covered, that V(t) < yi W,, and hence

1 1

-~V(t) <- S- W; > E(V())) a.s. and in mean, as t —-> 00.

t t i=]
It follows that the family { m7} er, Wim = 1} is uniformly integrable (see Theorem (7.10.3)).
Now M(t) < V(¢), and so {N(t)/t : t > 0} is uniformly integrable also.

Since N(t)/t 3s ua!

5. (a) Using the fact that P(N(t) = k) = P(S; < t) — P(Sy41 < 1), we find that

fore) oo
E(sN@) = | (% s*P(N(t) = ‘) ve" dt
Oo \e=0

oo lore)
= yo s* { | [P(Sk St) — P(Seq1 <d)] ve ar} .
k=0 0

By integration by parts, or P(S, < tve "dt = M(—v)* for k > 0. Therefore,

1— M(-v)
1—sM(-v)

, it follows by uniform integrability that there is also convergence in mean.

[o.@)
E(s)) = S° s*{M(—v)* — M(-v)**1} =
k=0

(b) In this case, E(sN)) = Ee’ T6-D) = Mp(a(s—1)). When T has the given gamma distribution,

Mr (6) = {v/(v — 0), and
BEN) = ( v y (i As )
v+A v+iAz

The coefficient of s* may be found by use of the binomial theorem.

372

Excess life Solutions [10.3.1]-[10.3.3]

10.3 Solutions. Excess life

1. Let g(y) = P(E) > y), assumed not to depend ont. By the integral equation for the distribution
of E(1),

t
g(y) =1- Fe+y) +80 [ dF (x).

Write h(x) = 1 — F(x) to obtain g(y)A(t) = A(t + y), for y,t > 0. With t = 0, we have that
g(y)h(O) = A(y), whence g(y) = h(y)/h(O) satisfies g(t + y) = g(t)g(y), for y,t = 0. Now g is
left-continuous, and we deduce as usual that g(t) = e~*" for some A. Hence F @“#)=1- et , and
the renewal process is a Poisson process.

2. (a) Examine a sample path of £. If E(t) = x, then the sample path decreases (with slope —1)
until it reaches the value 0, at which point it jumps to a height X, where X is the next interarrival time.
Since X is independent of all previous interarrival times, the process is Markovian.

(b) In contrast, C has sample paths which increase (with slope 1) until a renewal occurs, at which they
drop to 0. If C(s) = x and, in addition, we know the entire history of the process up to time s, the
time of the next renewal depends only on the length of the spent period (i.e., x) of the interarrival time
in process. Hence C is Markovian.

3. (a) We have that

t
(*) PEW) <y)=Fe+y)~ [ GO +y—x)dm(x)

where G(u) = 1 — F(u). Check the conditions of the key renewal theorem (10.2.7): g(t) = G(t+ y)
satisfies:

(i) g@) = 0,

(i) 5° e@)dt < fo°[1 — Flu) du = E(X}) < &,
iii) g is non-increasing.
“We conclude, by that theorem, that

1° Yd]
lim P(E() < y) =1-— ~ | g(x) dx =| —[1l — F(x)] dx.
i000 H JO ou

(b) Integrating by parts,

io @) xt 1 CO xt tl E(Xxit1)
~1-F =— Fa) =— 1”,
[ bb U (1 ax a r+1 dF) Bir +1)

See Exercise (4.3.3).
(c) As in Exercise (4.3.3), we have that E(E(t)") = for ry’ P(E) > y)dy, implying by («) that

oo rt
BEM) = EK Ot) + ff ry tea > tty — x) dmx) dy,
y=0 /x=0
whence the given integral equation is valid with
[o.<)
h(u) - | ry’! P(X, >ut+y)dy= E({(X1 _ u)t}").
Now A satisfies the conditions of the key renewal theorem, whence
s r 1 °° 1 r
lim E(E(@)”) = — h(u) du = — y'dF(ut+ y)du
t>00 LK JO KM JI0<u,y<00

~f- "P(X )d
= > => ——,.
andy PRX > DaY= TD

373
[(10.3.4]-[10.3.5] Solutions Renewals

4, We have that

1-F
P(E(t) > y| C(t) = x) =P(X,; >ytx|[Xp>x= eee
whence
C1-FQt+x) E(x; —x)t}

E(E(t)|C@) =x) =| 1— F(x) y= 1— F(x)

5. (a) Apply Exercise (10.2.2) to the sequence X; — 4, 1 <i < 00, to obtain var(Tyq) — uM (t)) =
o°E(M(t)).

(b) Clearly Ty) = ¢ + E(z), where E is excess lifetime, and hence pM (ft) = (¢ + E(t)) — Iuq@ —
4M (t)), implying in turn that

(x) y? var(M(t)) = var(E(t)) + var(Siqy) — 2cov(E(t), Suey)

where Sy) = Tut) — UM (t). Now

2, _. E(X})
var(E(t)) < E(E(t)*) > va ast > oO
je
if E(X?) < 00 (see Exercise (10.3.3c)), implying that -
1
(x) ; var(E(t)) > 0 ast —> Oo.

This is valid under the weaker assumption that E(X *) < oo, as the following argument shows. By
Exercise (10.3.3),

t
E(E(t)2) = a(t) + | a(t — u)dm(u),

where a(u) = E({(X, — u)t}2). Now use the key renewal theorem together with the fact that
a(t) < E(X21¢x,>1) > Oast > oo.

Using the Cauchy—Schwarz inequality,

1 1
_|cov(EO, Suay)| S var E(O) var(Siy(q)) —> 0

as t —> ov, by part (a) and (*«). Returning to («), we have that

2 2 2
Ub _ jo o
= var(M (0) > tin, {= (men + 0} =F

374

Renewal-reward processes Solutions [10.4.1]-[10.5.3]

10.4 Solution. Applications

1. Visualize a renewal as arriving after two stages, type 1 stages being exponential parameter 2 and
type 2 stages being exponential parameter 2. The ‘stage’ process is the flip-flop two-state Markov
process of Exercise (6.9.1). With an obvious notation,

Ag Otu)t 4, 4

j= .
putt) hap hin

Hence the excess lifetime distribution is a mixture of the exponential distribution with parameter y,
and the distribution of the sum of two exponential random variables, thus,

fea) = pir(t)g(x) + 1 — prt) we,

where g(x) is the density function of a typical interarrival time. By Wald’s equation,

1 1
E¢t + E(@)) = ESy)+1) = EX DEW®) +) = € + =) (m(t) + 1).

We substitute ; 1 ; ; o
E(E(t)) = pitt) (5 + <) +(1-pu@)y—- =—4 PH
A ho pb a

to obtain the required expression.

10.5 Solutions. Renewal—reward processes

_ 1. Suppose, at time s, you are paid a reward at rate u(X(s)). By Theorem (10.5.10), equation
(10.5.7), and Exercise (6.9.11b),

1 ft as, 1
(*) aft =j)ds —> — =7j.
t Jo {X(s)=j} 1} 8} J

Suppose |u(i)| < K < oo for alli € S, and let F be a finite subset of the state space. Then
1 t . 1 t
+f u(X(s)) ds — So mui) = iS u(i) <f L(x (s)=i} 48 — ni)
i i .
t— )
<K K (| ———— K is
< 2 + ( ; +K Son;

igF

1 t
a I(x (s)=i} 45 — Hi

where 7;(/*) is the total time spent in F up to time t. Take the limit as tf > oo using (*), and then as
F ¢ S, to obtain the required result.

2. Suppose you are paid a reward at unit rate during every interarrival time of type X, i.e., at all
times ¢ at which M(t) is even. By the renewal-reward theorem (10.5.1),

1 [ t I as * E(reward during interarrival time) EX,
= : s—_ = .
0 {M(s) is even} Edlength of interarrival time) EX, + EY;

t

3. Suppose, at time ¢, you are paid a reward at rate C(t). The expected reward during an interval
(cycle) of length X is Se sds = 5X 2 | since the age C is the same at the time s into the interval. The

375

[10.5.4]-[10.6.3] Solutions Renewals

result follows by the renewal—reward theorem (10.5.1) and equation (10.5.7). The same conclusion is
valid for the excess lifetime E(s), the integral in this case being Se (X —s)ds = 5X 2.

4. Suppose Xp = j. Let Vj = min{n > 1: Xy, = j, Xm =k for some 1 < m < n}, the first visit
to j subsequent to a visit to k, and let V,4; = min{n > V, : Xn = j, Xm = k for some V; +1 <
m <n}. The V; are the times of a renewal process. Suppose a reward of one ecu is paid at every visit
to k. By the renewal—reward theorem and equation (10.5.7),

1 y,-1
"= BETH SED Mmm)

m=1
By considering the time of the first visit to k,
EV, | Xo = J) =ECH | Xo = J) + EGG | Xo = 4).

The latter expectation in (*) is the mean of a random variable N having the geometric distribution
P(N =n) = p(1— p)""! forn > 1, where p = P(T; < Ty | Xo = &). Since E(N) = p', we
deduce as required that

_ 1/P(T; < T, | Xo =)

~ E(Te | Xo = f) + EGG | Xo =k)

Tk

10.6 Solutions to problems

1. (a) For any 2, P(N(@) <2) <P, >t) > Oast > Ew.
(b) Either use Exercise (10.1.1), or argue as follows. Since x > 0, there exists € (> 0) such that
P(X, > €) > 0. For all xn,

P(T, < ne) =1—P(% > ne) < 1-—P(X, > ©)" <1,

so that, if t > 0, there exists n = n(t) such that P(7, < t) < 1.

Fix t and let n be chosen accordingly. Any positive integer k may be expressed in the form
k =an-+ B where 0 < B <n. Now P(T, <t) < P(T, < t)% foran <k < (@ + 1)n, and hence

co co
m(t)= D> P(T <t) < So nP(T, < 1% < ov.
k=] a=0

(c) It is easiest to use Exercise (10.1.1), which implies the stronger conclusion that the moment
generating function of N(¢) is finite in a neighbourhood of the origin.

2. (i) Condition on X, to obtain
t t
v(t) = | E{ (M(t —u) + 1)"} dF(u) = | {u(t — u) + 2m(t — u) + 1} dF (u).
0 0
Take Laplace-Stieltjes transforms to find that v* = (v* + 2m* + 1)F*, where m* = F* + m*F* as

usual. Therefore v* = m*(1 + 2m*), which may be inverted to obtain the required integral equation.
(ii) If N is a Poisson process with intensity A, then m(t) = At, and therefore v(t) = (At)? + At.

3. Fix x € R. Then

P (Since > *) =P (wo > (t/u) + xyta%/13) =PTyq) <1)

376

Problems Solutions [10.6.4]-[10.6.7]

where a(t) = | (t/) + xx/to2/u3|. Now,

P(Taq) <0) =P (2 ~ Ha) i a)

o/a(t) ~ o/a(t)
However a(t) ~ t/j as t —> oo, and therefore

t — pa(t)

x ast > oO,
o/a(t)

implying by the usual central limit theorem that

P N(t) — (¢/)
Vto2/p>
where ® is the N(O, 1) distribution function.
An alternative proof makes use of Anscombe’s theorem (7.11.28).

4, We have that, for y < f,

>*) — &(-x) ast > 0O

P(C(t) > y) =P(E(t —y)> y) > jim PEW > y) ast + 0O
=[- te F@ax
y HB

by Exercise (10.3.3a). The current and excess lifetimes have the same asymptotic distributions.

5. Using the lack-of-memory property of the Poisson process, the current lifetime C(t) is independent
of the excess lifetime E(t), the latter being exponentially distributed with parameter 4. To derive the
density function of C(t) either solve (without difficulty in this case) the relevant integral equation, or
argue as follows. Looking backwards in time from f, the arrival process looks like a Poisson process
up to distance f¢ (at the origin) where it stops. Therefore C(t) may be expressed as min{Z, t} where Z
is exponential with parameter 1; hence

nes ifs <1,

0) ifs >f,

fom) = {

and P(C(t) = t) = e7**. Now D(t) = C(t) + E(2), whose distribution is easily found (by the
convolution formula) to be as given.

6. The ith interarrival time may be expressed in the form T + Z; where Z; is exponential with
parameter A. In addition, Z,, Zz, ... are independent, by the lack-of-memory property. Now

1-F(@x) =P(T+Z,>x)=WMZ)>x—-T)=e%F-), x>T.

Taking into account the (conventional) dead period beginning at time 0, we have that

k
P(N(t) > k) = p(kr +5)°Z< ') =P(NG@—kT)>k),  t =kT,

i=]
where N is a Poisson process.

7. We have that X} = L + E(L) where L is the length of the dead period beginning at 0, and E(L)
is the excess lifetime at L. Therefore, conditioning on L,

P(X, <x) = [ P(E@) <x-1)dF,@.
0

377

[10.6.8]-[10.6.10] Solutions Renewals

We have that
t
P(E@) <y)=F(+y) -[ {1— Fa +y—-x)}dm(x).

By the renewal equation,

ty
m+) = Forty) | F(t + y —x)dm(x),

whence, by subtraction,

t+y
P(E(t)<y)= | {1- F(t +y—x)}dm(a).
t
It follows that

P(X; <x) = [| {I= FG —y) hdmi dFLO

= [rio [ (1-Fa-y}amyy] + f Fr(){1— Fe —D)}dm@

using integration by parts. The term in square brackets equals 0.

8. (a) Each interarrival time has the same distribution as the sum of two independent random variables
with the exponential distribution. Therefore N(t) has the same distribution as [5M (t)| where Misa
Poisson process with intensity 4. Therefore m(t) = 5E(M (t)) - 5P(M (t) is odd). Now E(M(#)) =

At, and
0° aryettle-at

P(M(t) is odd) = }>
< (n+)!

With more work, one may establish the probability generating function of N(t).
(b) Doing part (a) as above, one may see that m(t) = m(t).

et (et _ WAY

=5 é

9. Clearly C(t) and E(t) are independent if the process N is a Poisson process. Conversely, suppose
that C(t) and E(t) are independent, for each fixed choice of t. The event {C(t) > y}N{E@ = x}
occurs if and only if E(t — y) > x + y. Therefore

P(C(@) > y)P(E(@) => x) =P(E@-—y)>x+y).
Take the limit as t — oo, remembering Exercise (10.3.3) and Problem (10.6.4), to obtain that
G(y)G(«*) = Ga + y) if x, y > 0, where
4
G(u) = | —[1 — F(v)) dv.
u bb

Now 1 — G is a distribution function, and hence has the lack-of-memory property (Problem (4.14.5)),
implying that G(u) = e * for some A. This implies in turn that [1 — F(u)]/u = —G’(u) = new *@
whence 4 = 1/d and F(u) =1—-— ef,

10. Clearly N is a renewal process if Nz is Poisson. Suppose that N is a renewal process, and write
A for the intensity of N,, and F> for the interarrival time distribution of N2. By considering the time
X, to the first arrival of N,

(x) 1 — F(x) = P(N1 (x) = No(x) = 0) = e** (1 — Fo(x)).

378

Problems Solutions [10.6.11]-[10.6.12]
Writing Z, E; for the excess lifetimes of N, N;, we have that
P(E(t) > x) = P(E, (t) > x, Eo(t) > x) =e P(Eo(t) > x).

Take the limit as t > 00, using Exercise (10.3.3), to find that

oO | oO 4
| —[1 — F(u)]du =e" / —[1 — Fou) ] du,
x x M2

where {42 is the mean of F>. Differentiate, and use (x), to obtain

ex

1 hx hx °°
a [1 — Fy(x)] = de | [1 — Fo(x)],

x

1
—[l — Fy()) du t+
M2 M2
which simplifies to give 1 — Fy(x) = e fer — Fo(u)] du where c = Ap /(u2 — w); this integral
equation has solution Fy(x) = 1 — e~™.

11. (i) Taking transforms of the renewal equation in the usual way, we find that

F*@) 1

m'O) = 7 Fe) ~ To FO)”

1

where
F*(0) = E(e°*1) = 1-0 + 507(u? +07) + 0(67)

as 9 —> 0. Substitute this into the above expression to obtain

1

*@= -1
en outl — F0(u + o2/n) + 008))

and expand to obtain the given expression. A formal inversion yields the expression for m.
(ii) The transform of the right-hand side of the integral equation is

1
(*) mo Fy (0) + m* (0) — FE@)m*(6).
bb
By Exercise (10.3.3), F£ (0) = [1 — F*(@)]/(u), and (*) simplifies to m*(0) — (m* — m* F* —
F*)/(w0), which equals m*(@) since the quotient is 0 (by the renewal equation).
Using the key renewal theorem, as t + oo,

E(X{) _ 07 +p?
2p 22

t 1 [o.]
[ fi -— Fett — x)]dm(@x) > ~ | [1 — Fg()]du =
0 u JO

by Exercise (10.3.3b). Therefore,

ot+ py? 2p?

2p2 2p

t
m(t)-— > -1+
Bb

12. (i) Conditioning on X1, we obtain
t
m4(t) = F(t) + [ m(t — x)dF4(x).
0

379

[10.6.13]-[10.6.16] Solutions Renewals

Therefore m?* = F% 4 m*F%, Alsom* = F* + m* F*, so that

1-— F*

whence m** = F4* + m4* F*, the transform of the given integral equation.
(ii) Arguing as in Problem (10.6.2), v'* = F%*+2m* Fo +0* F% where v* = F*(1+2m*)/(1- F*)

is the corresponding object in the ordinary renewal process. We eliminate v* to find that

F%% (1 4+ 2m*
yt — aoe —— ) = m®*(1 4 2m*)

by («). Now invert.

13. Taking into account the structure of the process, it suffices to deal with the case J = 1. Refer
to Example (10.4.22) for the basic notation and analysis. It is easily seen that B = (v — 1)A. Now
F(t) =1—e—!. Solve the renewal equation (10.4.24) to obtain

t
g(t) = h(t) + | h(t — x) dii(x)

where m(x) = vAx is the renewal function associated with the interarrival time distribution F.
Therefore g(t) = 1, and m(t) = ef,

14, We have from Lemma (10.4.5) that p* = 1— F> + p*F*, where F* = Fy F3. Solve to obtain

pre 2
1 FRFS

15. The first locked period begins at the time of arrival of the first particle. Since all future events
may be timed relative to this arrival time, we may take this time to be 0. We shall therefore assume
that a particle arrives at 0; call this the Oth particle, with locking time Yo.

We shall condition on the time X, of the arrival of the next particle. Now

P(¥o > t) ifu >t,

PL > tix = ={ ; .
P(Yo > wWP(L'>t—u) ifu<t,

where L’ has the same distribution as L; the second part is a consequence of the fact that the process
‘restarts’ at each arrival. Therefore

t
PIL >t) = (1-G@) P(X, > +f P(L >t —u)(1— GW)) fx, @) du,
0

the required integral equation.

If G(x) = 1 — e~#*, the solution is P(L > t) = e~“, so that L has the same distribution as
the locking times of individual particles. This striking fact may be attributed to the lack-of-memory
property of the exponential distribution.

16. (a) It is clear that M(tp) is a renewal process whose interarrival times are distributed as X; +
Xo+---+XpR where P(R =r) = pq! forr > 1. It follows that M(t) is a renewal process whose
first interarrival time

X(p) =inf{t : M(t) = 1} = p inf{t : M(tp) = 1}

380

Problems Solutions [10.6.17]-[10.6.19]

has distribution function

oe)
P(X(p) <x) = 5) P(R =r)F;(x/p).

r=1

(b) The characteristic function @p of Fp is given by

oo oo ea)
= r—I ixt _ r~1 ro po (pt)
dp(t) = 2,P4 [oe dF,(t/p) = dora b(P0" = Ton

where ¢ is the characteristic function of F. Now @(pt) = 1+ipupt +o0(p) as p J 0, so that

L+tippt+o(p) 1+4+0(1)
dp(t) = Toi = -
—ipt +o) 1—ipt

as p | 0. The limit is the characteristic function of the exponential distribution with mean jz, and the
continuity theorem tells us that the process M converges in distribution as p | 0 to a Poisson process
with intensity 1/j (in the sense that the interarrival time distribution converges to the appropriate
limit).
(c) If M and N have the same fdds, then ¢p(t) = (t), which implies that #(pt) = o(t)/(p + g@()).
Hence y(t) = g(t)! satisfies w(pt) = q+ pw(t) fort € R. Now yw is continuous, and it follows as
in the solution to Problem (5.12.15) that y has the form y(t) = 1+ Bt, implying that d(t) = (1 +Bt)7!
for some 8 € C. The only characteristic function of this form is that of an exponential distribution,
and the claim follows.

17. (a) Let N(t) be the number of times the sequence has been typed up to the th keystroke. Then N is
arenewal process whose interarrival times have the required mean 4; we have that E(N(t))/t > po
as t — oo. Now each epoch of time marks the completion of such a sequence with probability

(ao) so that

1 1 t 1 14 1 14

implying that 4 = 1028.

The problem with ‘omo’ is ‘omomo’ (i.e., appearances may overlap). Let us call an epoch
of time a ‘renewal point’ if it marks the completion of the word ‘omo’, disjoint from the words
completed at previous renewal points. In each appearance of ‘omo’, either the first ‘o’ or the second
‘o’ (but not both) is a renewal point. Therefore the probability u,, that n is a renewal point, satisfies
(0) =Unt+ un—2 (79g): Average this over n to obtain

1 \3 im 2 : + 1 \? 113 2

——} = lim - Un +Un—2 | — =—+—-[—]},

100 noo p La Yom 2 \ 100 uw pw \100
and therefore jz = 10° + 102.
(b) (i) Arguing as for ‘omo’, we obtain p? =Unt+ pun_1 + p-Un—2; whence p? =(l+p+ p*)/w.
Gi) Similarly, p?q = un + pqun—p, so that w = (1 + pq)/(p?4).
18. The fdds of {N(u) — N(t) : u > t} depend on the distributions of E(t) and of the interarrival
times. In a stationary renewal process, the distribution of E(t) does not depend on the value of t,
whence {N(u) — N(t) : u > t} has the same fdds as {N(u) : u > O}, implying that X is strongly
stationary.
19, We use the renewal-reward theorem. The mean time between expeditions is Bu, and this is
the mean length of a cycle of the process. The mean cost of keeping the bears during a cycle is
5 B(B — 1ecp, whence the long-run average cost is {d + B(B — 1)cu/2}/(By).

381

11

Queues

11.2 Solutions. M/M/1

1. The stationary distribution satisfies 2 =P when it exists, where P is the transition matrix. The
equations

al 2 _ PMy-1 n+l

= ; 1) =t+ —, ty = for n > 2,
l¢4p) Oo Tp’ "14 p  14p

70
with 7°25 2; = 1, have the given solution. If p > 1, no such solution exists. It is slightly shorter to
use the fact that such a walk is reversible in equilibrium, from which it follows that mr satisfies

_ 7 PTn _ Mn+
1+)’ i+p 1+ p

forn > 1.

(*) mH

2. (i) This continuous-time walk is a Markov chain with generator given by go1 = 9, 8n.n+1 =
On0/(1 + p) and gy, n-1 = On/(1 + p) for n > 1, other off-diagonal terms being 0. Such a process
is reversible in equilibrium (see Problem (6.15.16)), and its stationary distribution v must satisfy
Vn8n,nt+1 = Yn+18n+1,n- These equations may be written as

vy 64 VnOnp _ Yn41 Ont
l+p l+p

forn > 1.

These are identical to the equations labelled (+) in the previous solution, with z, replaced by v,4,. It
follows that vy, = C/O, for some positive constant C.

Gi) If O99 =A, On, =A+ yu forn > 1, we have that

m . 1-2 Cc
i= Dm ac{ 24 \_e.
7 Xr w+nr 2r

whence C = 2) and the result follows.

3. Let Q be the number of people ahead of the arriving customer at the time of his arrival. Using the
lack-of-memory property of the exponential distribution, the customer in service has residual service-
time with the exponential distribution, parameter 44, whence W may be expressed as S] ++S7+---+SQ,
the sum of independent exponential variables, parameter 4». The characteristic function of W is

Q
_ itw _ bad
ov = Efe" | Q)} -={ (5)

1- p ( i, )
1 — pu/(u — it) (1) +e uw-i-it

382

M/M/1 Solutions [11.2.4]-[11.2.7]

This is the characteristic function of the given distribution. The atom at 0 corresponds to the possibility
that Q = 0.

4. We prove this by induction on the value of i + j. Ifi + 7 =O theni = j = 0, and it is easy to
check that 7(0; 0,0) = 1 and A(O; 0,0) = 1, A(@m; 0,0) = O for n > 1. Suppose then that K > 1,
and that the claim is valid for all pairs (i, j) satisfying i+ j = K. Leti and j satisfyi+ j = K +1.
The last ball picked has probability i/(i + j) of being red; conditioning on the colour of the last ball,
we have that . .

u(n3i, J) = ——n(n — 151 -—1, 7) + ——-2z@4+1;i,7 -1).

(m3 4, j) i4q7! Dd) i+ J-Y
Now @ —1)+ j= K =i+(j — 1). Applying the induction hypothesis, we find that
i
j= An —1si—1, jf) -A@si—1,j
nisi, J) = {AG Li = 1) AIL}

j _, -
——i A 1,i,j-l—-A 2;i, 7 —1)>.
+g {A@ + bi j-D-A@H+2EI-D}

Substitute to obtain the required answer, after a little cancellation and collection of terms. Can you
see a more natural way?

5. Let Aand B be independent Poisson process with intensities A and 1 respectively. These processes
generate a queue-process as follows. At each arrival time of A, a customer arrives in the shop. At
each arrival-time of B, the customer being served completes his service and leaves; if the queue
is empty at this moment, then nothing happens. It is not difficult to see that this queue-process is
M(A)/M(y)/1. Suppose that A(t) = i and B(t) = j. During the time-interval [0, ¢], the order of
arrivals and departures follows the schedule of Exercise (11.2.4), arrivals being marked as red balls
and departures as lemon balls. The imbedded chain has the same distributions as the random walk
of that exercise, and it follows that P(Q(t) = n | A(t) = i, B(t) = j) = x(n; i, j). Therefore
Pn(t) = 0, ; Hi, PPAO =D)PBO = J).
6. With p =A/w, the stationary distribution of the imbedded chain is, as in Exercise (11.2.1),

. {8 ifn =0,

m=) 1 2 nn—1

31 — p*)p ifn > 1.

In the usual notation of continuous-time Markov chains, g9 = A and g, = 1+ yw forn > 1, whence,

by Exercise (6.10.11), there exists a constant c such that
Cc

~ 20+)
Now 5°; 7; = 1, and therefore c = 2A and mp = (1 — p)e” as required. The working is reversible.

c —
my = ran —p), di — p*)p" 1 forn > 1.

7. (a) Let Q;(t) be the number of people in the ith queue at time f, including any currently in service.
The process Q, is reversible in equilibrium, and departures in the original process correspond to arrivals
in the reversed process. It follows that the departure process of the first queue is a Poisson process
with intensity 2, and that the departure process of Q is independent of the current value of Q .

(b) We have from part (a) that, for any given ft, the random variables Q;(t), Q2(t) are independent.
Consider an arriving customer when the queues are in equilibrium, and let W; be his waiting time
(before service) in the ith queue. With T the time of arrival, and recalling Exercise (11.2.3),

P(W, = 0, Wz = 0) > P(Q;(T) = O fori = 1,2) = P(Qi(T) = 0)P(Q2(T) = 0)
= (1 — p1)(1 — 92) = PCW; = 0)P(W2 = 0).
Therefore W; and W2 are not independent. There is a slight complication arising from the fact that T

is arandom variable. However, T is independent of everybody who has gone before, and in particular
of the earlier values of the queue processes Q;.

383

[11.3.1]-[11.4.1] Solutions Queues

11.3 Solutions. M/G/1

1. In equilibrium, the queue-length Q,, just after the nth departure satisfies

(*) On+1 = An + On — h(Qn)

where A, is the number of arrivals during the (7 + 1)th service period, and h(@m) = 1 — 3y9.-Now
Qn and Qy+) have the same distribution. Take expectations to obtain

(**) 0 = E(An) — P(Qn > 9),

where E(A,,) = Ad, the mean number of arrivals in an interval of length d. Next, square () and take
expectations:

0 = E(Az) + E((Qn)*) + 2{E(An On) — E(Ank(Qn)) —E(Qnk(Qn)) }.
Use the facts that Ay is independent of Q,, and that Q,4(Qn) = Qn, to find that

0 = {(ad)* + Ad} + P(Qn > 0) + 2{ (Ad — 1)E(Qn) — AdP(Qn > 0)}

and therefore, by («*«),

2p — p?

2(1 — p)”

2. From the standard theory, Mg satisfies Mg(s) = Ms(s — A +AMp(s)), where Ms(0) =

L/(u — 9). Substitute to find that x = Mg(s) is a root of the quadratic Ax? — xAtpu—s)+pu=0.
For some small positive s, Mz(s) is smooth and non-decreasing. Therefore Mp (s) is the root given.

E(Qn) =

3. Let T, be the instant of time at which the server is freed for the nth time. By the lack-of-memory
property of the exponential distribution, the time of the first arrival after 7, is independent of all
arrivals prior to T,, whence Ty is a ‘regeneration point’ of the queue (so to say). It follows that the
times which elapse between such regeneration points are independent, and it is easily seen that they
have the same distribution.

11.4 Solutions. G/M/1

1. The transition matrix of the imbedded chain obtained by observing queue-lengths just before
arrivals is

1-—apg a oO 0
l-—ap—ay a a OO

Pa= l-—ag-—aj—a2 a ay a
The equation x = 2P,4 may be written as
0° i foe)
m= om(1- oa). Tn = > QT n4i-1 forn > 1.

i=0 j=0 i=0

It is easily seen, by adding, that the first equation is a consequence of the remaining equations, taken
in conjunction with i nm; = 1. Therefore m is specified by the equation for z,,n > 1.

384

G/G/1 Solutions [11.4.2]-[11.5.2]

The indicated substitution gives

oo .
ni gn-l S- a;6'
i=0
which is satisfied whenever @ satisfies
foe) —px
Xoyjie"#
6=)_a;6' = be {were M a oe = E(e#* eX) — My (u(6 — 1).

It is easily seen that A(O) = My(u(0 — 1)) is convex and non-decreasing on [0, 1], and satisfies
A(0) > 0, AC) = 1. Now A’(1) = wE(X) = pal > 1, implying that there is a unique 7 € (0, 1)
such that A(y) = 7. With this value of 7, the vector m given by aj = (1 — nani, j = 0, isa
stationary distribution of the imbedded chain. This x is the unique such distribution because the chain
is irreducible.

2. (i) The equilibrium distribution is x, = (1 —)n” for n > 0, with mean reo nt =n/(1—7n).
(ii) Using the lack-of-memory property of the service time in progress at the time of the arrival, we

see that the waiting time may be expressed as W = Sj + Sz +--- + Sg where @ has distribution zr,
given above, and the S;, are service times independent of Q. Therefore

n/ we
(1-7)

E(W) = E(S))E(Q) =

3. Wehave that Q(n+) = 1+ Q(n—) as. for each integer n, whence lim;-+99 P(Q(t) = m) cannot
exist.

Since the traffic intensity is less than 1, the imbedded chain is ergodic with stationary distribution
as in Exercise (11.4.1).

11.5 Solutions. G/G/1

1. Let 7, be the starting time of the nth busy period. Then 7; is an arrival time, and also the
beginning of a service period. Conditional on the value of T,,, the future evolution of the queue is
independent of the past, whence the random variables {T,4, — Ty : n > 1} are independent. It is
easily seen that they are identically distributed.

2. If the server is freed at time T, the time J until the next arrival has the exponential distribution
with parameter yz (since arrivals form a Poisson process).

By the duality theory of queues, the waiting time in question has moment generating function
My (s) = (1 — 6)/(1 — ¢M7(s)) where M7 (s) = w/(u — 5) and = P(W > 0). Therefore,

fu —)
M = oO 4-8),
w() =F FO -8)

the moment generating function of a mixture of an atom at 0 and an exponential distribution with
parameter jz(1 — Z).

If G is the probability generating function of the equilibrium queue-length, then, using the lack-
of-memory property of the exponential distribution, we have that My (s) = G(u/(u—s)), since W is
the sum of the (residual) service times of the customers already present. Set u = /(j4—s) to find that
G(u) = (1 —¢)/(1 — fy), the generating function of the mass function f(k) = (1 — eck fork > 0.

385

(11.5.3]-[11.7.2] Solutions Queues

It may of course be shown that ¢ is the smallest positive root of the equation x = My (ux - 1),
where X is a typical interarrival time.

3. We have that
OO
1- Gy) =PS-x> y= | PS >uty)dFy(u), yeR,
0
where S and X are typical (independent) service and interarrival times. Hence, formally,

[o-e) OO
dG(y) = -| dP(S > u+y)dFx(u) = ay [ pe #UtY) ary (u),
0 —y
since fg(u + y) =e HU+Y) if u > —y, and is 0 otherwise.
With F as given,

x
/ F(x — y)dG(y) = | // {1 — ne HAM} pe“ BUTY) d Fy (u) dy.
oo —oo<y<x

—y<u<0o

First integrate over y, then over u (noting that Fy (u) = O for u < 0), and the double integral collapses
to F(x), when x > 0.

11.6 Solution. Heavy traffic

1. Qp has characteristic function

1—p
1— pet

O° .
bot) = pep" — p) =

n=0
Therefore the characteristic function of (1 — )Qp satisfies

l1-p 1
b0( et) = — imp ToT

The limit characteristic function is that of the exponential distribution, and the result follows by the
continuity theorem.

11.7 Solutions. Networks of queues

1. The first observation follows as in Example (11.7.4). The equilibrium distribution is given as in
Theorem (11.7.14) by

Nima:
c OL j

ate
x(n) = [| ao ; form = (n1,n2,...,Mc) € Z,
i!

i=]
the product of Poisson distributions. This is related to Bartlett’s theorem (see Problem (8.7.6)) by
defining the state A as ‘being in station i at some given time’.

2. The number of customers in the queue is a birth-death process, and is therefore reversible in
equilibrium. The claims follow in the same manner as was argued in the solution to Exercise (11.2.7).

386

Problems Solutions [11.7.3]-[11.8.1]

3. (a) We may take as state space the set {0, 1’, 1”,2,3,...}, where i € {0,2,3,...} is the state
of having i people in the system including any currently in service, and 1’ (respectively 1’) is the
state of having exactly one person in the system, this person being served by the first (respectively
second) server. It is straightforward to check that this process is reversible in equilibrium, whence the
departure process is as stated, by the argument used in Exercise (11.2.7).

(b) This time, we take as state space the set {0’, 0”, 1’, 1”, 2,3, ... } having the same states as in part
(a) with the difference that 0’ (respectively 0”) is the state in which there are no customers present and
the first (respectively second) server has been free for the shorter time. It is easily seen that transition
from 0’ to 1” has strictly positive probability whereas transition from 1” to 0’ has zero probability,
implying that the process is not reversible. By drawing a diagram of the state space, or otherwise, it
may be seen that the time-reversal of the process has the same structure as the original, with the unique
change that states 0’ are 0” are interchanged. Since departures in the original process correspond to
arrivals in the time-reversal, the required properties follow in the same manner as in Exercise (11.2.7).

4, The total time spent by a given customer in service may be expressed as the sum of geometrically
distributed number of exponential random variables, and this is easily shown to be exponential with
parameter 52. The queue is therefore in effect a M(A)/M(64)/1 system, and the stationary distribution
is the geometric distribution with parameter p = A/(52), provided p < 1. As in Exercise (11.2.7),
the process of departures is Poisson.

Assume that rejoining customers go to the end of the queue, and note that the number of customers

present constitutes a Markov chain. However, the composite process of arrivals is not Poisson, since
increments are no longer independent. This may be seen as follows. In equilibrium, the probability of
an arrival of either kind during the time interval (¢, f+) isAh+ pul -dh+o(h) = A/d)A+0(h).
If there were an arrival of either kind during (¢ — h, t), then (with conditional probability 1 — O(/))
the queue is non-empty at time t, whence the conditional probability of an arrival of either kind during
(t,t +h) is Ah + w(1 — 5)h + O(h); this is of a larger order of magnitude than the earlier probability
(A/d)h + o(h).
5. For stations r,s, we write r > s if an individual at r visits s at a later time with a strictly positive
probability. Let C comprise the station j together with all stations i such that i > j. The process
restricted to C is an open migration process in equilibrium. By Theorem (11.7.19), the restricted
process is reversible, whence the process of departures from C via j is a Poisson process with some
intensity ¢. Individuals departing C via j proceed directly to k with probability

AjRbj (Mj) _ Aj
Hjbj (Qj) + rec djrdi nj) Bj + Veeco Ajr’

independently of the number n; of individuals currently at j. Such a thinned Poisson process is a
Poisson process also (cf. Exercise (6.8.2)), and the claim follows.

11.8 Solutions to problems

1. Although the two cases may be done together, we choose to do them separately. When k = 1,
the equilibrium distribution 7 satisfies:

pm, — Ano = 0,
BTn41 — A+ )tn + Ann_1 = 9, l<n<N,
—pry +Ary—-1 =9,

a system of equations with solution 2, = m9(A/)" for 0 <n < N, where (if A # pe)

1—(/pXt!

N
-1_ n_
7 = LO/u) = Op

387
(11.8.2]-[11.8.3] Solutions Queues

Now let k = 2. The queue is a birth-death process with rates
dX ifi <N, Bw ifi=1,
= tps i= ups
0 ifi>N, Qu ifi > 2.

It is reversible in equilibrium, and its stationary distribution satisfies Aj;7; = 441741. We deduce
that 7; = 2p'zo for 1 <i < N, where p = A/(2y) and

N
m- =1 + 5° 2p!.
i=l

2. The answer is obtainable in either case by following the usual method. It is shorter to use the fact
that such processes are reversible in equilibrium.

(a) The stationary distribution satisfies z,Ap(n) = 7414 forn > 0, whence zn = 9p" /n! where
o =A/wp. Therefore my, = p"e7? /n!.

(b) Similarly,
n-1 1
Tn = Tp” Il p(m) = mop"27 2"), n> 0,
m=0
where

=1 Donde)
mm => P"(3)
=0

At the instant of arrival of a potential customer, the probability g that she joins the queue is
obtained by conditioning on its length:

[o-e) oo _,_1 l [o-e) 1 1 1 ;
q = Yi Pada = 19D pra BMD = DT DANY = m9 tg) =

n=0 n=0 n=0

3. First method. Let (Q1, Q2) be the queue-lengths, and suppose they are in equilibrium. Since
Q) is a birth-death process, it is reversible, and we write Oj; (t) = Q)(-t). The sample paths of
Q, have increasing jumps of size 1 at times of a Poisson process with intensity 4; these jumps mark
arrivals at the cash desk. By reversibility, 01 has the same property; such increasing jumps for 01
are decreasing jumps for Q1, and therefore the times of departures from the cash desk form a Poisson
process with intensity 4. Using the same argument, the quantity Q(t) together with the departures
prior to t have the same joint distribution as the quantity Q} (—t) together with all arrivals after —t.
However 01 (—t) is independent of its subsequent arrivals, and therefore Q(t) is independent of its
earlier departures.

It follows that arrivals at the second desk are in the manner of a Poisson process with intensity
A, and that Q(t) is independent of Q, (zt). Departures from the second desk form a Poisson process
also.

Hence, in equilibrium, Q; is M(A)/M(y1)/1 and Qo is M(A)/M(jz2)/1, and they are independent
at any given time. Therefore their joint stationary distribution is

mn = P(Q1(t) = m, Q2(t) =n) = (1 — pr) — pr) oF

where pj = A/L;.-
Second method. The pair (Q(t), Q2(t)) is a bivariate Markov chain. A stationary distribution
(mn : m,n > 0) satisfies

A+ uy + H2)tmn = Atm—1,n + M1 0m4+ijn—-1 + M2 n41> mn>l,

388

Problems Solutions  [11.8.4]-[11.8.5]

together with other equations when m = 0 or n = 0. It is easily checked that these equations have the
solution given above, when p; < 1 fori = 1,2.

4, Let Dy, bethe time of the nth departure, andlet Q, = Q(Dy,+) be the number of waiting customers
immediately after D,. We have in the usual way that Q,41 = An + Qn — h(Qn), where Ay is the
number of arrivals during the (n + 1)th service time, and A(x) = min{x, m}. Let G(s) = ro ast
be the equilibrium probability generating function of the Q,. Then, since Q» is independent of An,

G(s) = E(s4")E(s Qn—h(Qn))

where oo
E(s4") = [ e46—) Fo(u) du = Ms(A(s — 1)),
0

Ms being the moment generating function of a service time, and

oo m
3 sia, = m{oe) + So" - sym}.

m
E(s 2n-h(Qn)) = Soni +
i=-0 i=m+1 i=0

Combining these relations, we obtain that G satisfies
m .
5s G(s) = Ms(A(s — »){ G0) + S76" - sm},
i=Q

whenever it exists.
Finally suppose that m = 2 and Ms(@) = u/(u — 8). In this case,

_ eros +1) +715}
G(s) = p(s +1) —As2

Now G(1) = 1, whence (279 + 21) = 244 — A; this implies in particular that 2u — 7 > 0. Also
G(s) converges for |s| < 1. Therefore any zero of the denominator in the interval [—1, 1] is also a
zero of the numerator. There exists exactly one such zero, since the denominator is a quadratic which

takes the value —A at s = —1 and the value 24 — 4 at s = 1. The zero in question is at
wee Vu? +4hu
Oi
2a

and it follows that zg + (7% + 21)s9 = 0. Solving for 9 and 21, we obtain

G(s) = l-a

l—as’
where a = 2A/{u + Vu? + 4Au}.
5. Recalling standard M/G/1 theory, the moment generating function Mg satisfies

rv
pw —{s—-h+AMB(s)}

(*) Mp(s) = Ms(s —A +AMp(s)) =

whence M p(s) is one of

A+tu-s)tVA+m—s)? —4au
2a ,

389

[11.8.6]-[11.8.7] Solutions Queues

Now Mz(s) is non-decreasing in s, and therefore it is the value with the minus sign. The density
function of B may be found by inverting the moment generating function; see Feller (1971, p. 482),
who has also an alternative derivation of Mg.

As for the mean and variance, either differentiate Mz, or differentiate («). Following the latter
route, we obtain the following relations involving M (= Mp):

21MM’ +M+(s—A-—p)M’ =0,
24MM" +22(M’)? + 2M’! +(s —A —p)M” =0.

Set s = Oto obtain M’(0) = (u—A)7! and M” (0) = 2(4—A)~3, whence the claims are immediate.

6. (i) This question is closely related to Exercise (11.3.1). With the same notation as in that solution,
we have that

(*) On+1 = An + Qn — h(Qn)

where h(x) = min{1, x}. Taking expectations, we obtain P(Q, > 0) = E(Ay) where
oO
B(An) = |" B(An | $=5)dFs(6) = AB(S) = p,

and S is a typical service time. Square (*) and take expectations to obtain

p(1 — 2p) + E(A?, 1)
2(1 — p)

’

where E(A2) is found (as above) to equal p + 07E(S2).

Gi) If a customer waits for time W and is served for time S, he leaves behind him a queue-length
which is Poisson with parameter A(W + 5S). In equilibrium, its mean satisfies AE(W + S) = E(Qn),
whence E(W) is given as claimed.

(iii) E(W) is a minimum when E(S) is minimized, which occurs when S is concentrated at its mean.
Deterministic service times minimize mean waiting time.

7. Condition on arrivals in (¢, ¢+-A). If there are no arrivals, then W,+, < x ifandonly if W; < x-+h.
If there is an arrival, and his service time is S, then W;,, < x if andonly if W, < x+A—S. Therefore

x+h
F(aj;t+h) =(1—-AA)F+A;1) +an | F(x th—s;t)dFs(s) + o(h).
0

Subtract F (x; t), divide by A, and take the limit as h | 0, to obtain the differential equation.
We take Laplace-—Stieltjes transforms. Integrating by parts, for @ < 0,

| e°* dh(x) = —h(0) — 0{My (6) — H(0)},
(0,00)
| e** dH (x) = My (6) — H(0),
(0,00)
| e%* dP(U +S < x) = My(6)Ms(),
(0,00)

and therefore

0 = —h(0) — 0{My @) — H(0)} +AH (0) +AMy @){Ms(@) — 1}.

390

Problems Solutions [11.8.8]-[11.8.10]
Set 9 = 0 to obtain that h(0) = 1.H(0), and therefore
1
H(0) = — 5 Mu @){A(Ms@) —1)-6}.

Take the limit as 6 > 0, using L’H6pital’s rule, to obtain H(0) = 1 — AE(S) = 1 — p. The moment
generating function of U is given accordingly. Note that My is the same as the moment generating
function of the equilibrium distribution of actual waiting time. That is to say, virtual and actual
waiting times have the same equilibrium distributions in this case.

8. In this case U takes the values 1 and —2 each with probability 4 (as usual, U = S — X where S
and X are typical (independent) service and interarrival times). The integral equation for the limiting
waiting time distribution function F becomes

FQ) = 4FQ), Fox) =4{F@-D+F+2)} forx=1,2,....

The auxiliary equation is 63 — 26 +1 = 0, with roots 1 and — 5( 1+ /5). Only roots lying in [—1, 1]

can contribute, whence
x
—1 5
F(x) =A+B (+4)

for some constants A and B. Now F(x) > 1 as x — o, since the queue is stable, and therefore
A = 1. Using the equation for F (0), we find that B = (1 — J/5).

9. Q is a M(A)/M(2)/oo queue, otherwise known as an immigration—death process (see Exercise
(6.11.3) and Problem (6.15.18)). As found in (6.15.18), Q(t) has probability generating function

G(s, t) = {1+ (5 — Leet exp{p(s — 1)(1— ey}
where p = A/y. Hence

E(Q(t)) = le + p(l—e#),
P(Q(t) = 0) = (1 — ee")! exp{—p — e™)},
1
P(Q(t) =n) > — pre? ast —> 00,
ne
If E(/) and E(B) denote the mean lengths of an idle period and a busy period in equilibrium, we
have that the proportion of time spent idle is E(2)/{E(7) + E(B)}. This equals limy—.o0 P(Q(t) =

0) = e~. Now E(J) = A7!, by the lack-of-memory property of the arrival process, so that E(B) =
(e? —1)/a.

10. We have in the usual way that
(+) Qt +1) =A; + Q@) — min{1, OM}

where A; has the Poisson distribution with parameter 2. When the queue is in equilibrium, E(Q(t)) =
E(Q(t + 1)), and hence

P(Q(t) > 0) = E(min{1, Q(t)}) = E(A;) =A.

We have from («) that the probability generating function G(s) of the equilibrium distribution of
Qf) (= Q)is

G(s) = E(s41)B(s2~minllO}) = AE-D fR(s2-1 9.15) + P(Q = 0)}.

391
[11.8.11J-[11.8.13] Solutions Queues

Also,
G(s) = E(s2Ig>1;) + P(Q =),
and hence 1 1
G(s) =e 6—-D {560) + ¢ — *) a- anh
whence a yd —a)
—s\1—
G(s) = 1—se—@-1)°

The mean queue length is G’(1) = 5A(2 — 4)/(1 — A). Since service times are of unit length,
and arrivals form a Poisson process, the mean residual service time of the customer in service at an
arrival time is 5, so long as the queue is non-empty. Hence

Xr
E(W) = E(Q) — 3P(Q > 0) = ado

11. The length B of a typical busy period has moment generating function satisfying Mg(s) =
exp{s — 7 +AMa(s)}; this fact may be deduced from the standard theory of M/G/1, or alternatively
by a random-walk approach. Now T may be expressed as T = 7 + B where I is the length of the
first idle period, a random variable with the exponential distribution, parameter 4. It follows that
Mr(s) =AMp(s)/(A — 5). Therefore, as required,

(*) (A — s)My(s) =Aexp{s —A + (A —s)Mr(s)}.

If A > 1, the queue-length at moments of departure is either null persistent or transient, and it
follows that E(T) = oo. If 4 < 1, we differentiate («) and set s = 0 to obtain AE(T) — 1 = WE(T),
whence E(T) = {A(1 — a)}7!.

12. (a) Q is a birth-death process with parameters 4; = A, 4; = py, and is therefore reversible in
equilibrium; see Problems (6.15.16) and (11.8.3).

(b) The equilibrium distribution satisfies Ax; = uxj41 fori > 0, whence x; = (1 — p)p! where
p =A/w. A typical waiting time W is the sum of Q independent service times, so that

1—p _ U=p)u—s)
1—pu/(e—s) p—p)—s

Mwy (s) = Go(Ms(s)) =

(c) See the solution to Problem (11.8.3).

(d) Follow the solution to Problem (11.8.3) (either method) to find that, at any time ¢ in equilibrium,
the queue lengths are independent, the jth having the equilibrium distribution of M(A)/M(u;)/1. The
joint mass function is therefore

K
x;
F(x. ¥2,---4%K) = [] — 0))0;'
j=l
where pj = 1/p;.

13. The size of the queue is a birth—death process with rates A; = A, uj =  min{i, k}. Either solve
the equilibrium equations in order to find a stationary distribution 2, or argue as follows. The process
is reversible in equilibrium (see Problem (6.15.16)), and therefore A;2; = 4j412j+1 for alli. These
‘balance equations’ become

Ani = { wG+aj4, f0<i <k,

wkrj41 ifi >k.

392
Problems Solutions [11.8.14]-[11.8.14]

These are easily solved iteratively to obtain
= {ron if0<i <k,
| nola/kikk/k! ifi>k

where a = i/y. Therefore there exists a stationary distribution if and only if A < ky, and it is given

accordingly, with
k-1

m= a aa ye Ley

i =0 /

The cost of having k servers is

kyikk
Cy = Ak + Bao Sil _k+ jee
i=k
where 79 = 29(k). One finds, after a little computation, that
Ba 2Ba*
Ci = , C2 =2A .
1 tie 2 ane

Therefore
o3(A — B)+a2(2B — A) —4a(A+ B) + 4a

(1 —a)(4 - a)
Viewed as a function of a, the numerator is a cubic taking the value 4A at a = O and the value —3B
at a = 1. This cubic has a unique zero at some a* € (0, 1), and C, < C» if and only if 0 < a < a*.

C2-C, =

14. The state of the system is the number Q(t) of customers within it at time t. The state 1 may be
divided into two sub-states, being o, and 02, where og; is the state in which server i is occupied but
the other server is not. The state space is therefore § = {0, 01, 02, 2,3, ...}.

The usual way of finding the stationary distribution, when it exists, is to solve the equilibrium
equations. An alternative is to argue as follows. If there exists a stationary distribution, then the
process is reversible in equilibrium if and only if

(*) 811 ,i9 8i2, 13 °° * Sixty = Bi, ip Six,ig_y °° * Sig, iy

for ali sequences i;,i2,..., iz of states, where G = (guy)y,veg is the generator of the process (this
may be shown in very much the same way as was the corresponding claim for discrete-time chains
in Exercise (6.5.3); see also Problem (6.15.16)). It is clear that («) is satisfied by this process for all
sequences of states which do not include both o1 and 02; this holds since the terms gyy are exactly
those of a birth-death process in such a case. In order to see that («) holds for a sequence containing
both o1 and o», it suffices to perform the following calculation:

80,01 801 ,282,09 80,0 = (AA)AM2M = 80,07 807,282,061 801,0-
Since the process is reversible in equilibrium, the stationary distribution z satisfies my gyy =
Ty vu for allu, v € S,u ¥ v. Therefore
Hyd = Ty41(1 + M2), u>2,
570A = To, 1 = Mon h2, AoA =12K2, Noy = 72M,
and hence

d a 2 ( a \" foru >?
Noy ==—10, No = =—N), M= ——— no foru > 2.
1 Hy °2 Qua “ Quim2 \mi t+ M2

393

[11.8.15]-[11.8.17] Solutions Queues

This gives a stationary distribution if and only if 1 < 41 + 2, under which assumption 79 is easily
calculated.

A similar analysis is valid if there are s servers and an arriving customer is equally likely to go
to any free server, otherwise waiting in turn. This process also is reversible in equilibrium, and the
stationary distribution is similar to that given above.

15. We have from the standard theory that Q,, has as mass function m=Ud- mn ,j = 0, where
n is the smallest positive root of the equation x = e“@—))_ The moment generating function of
—H7)On is

1-7»

My(0) = Rexp{9 = w)Qu}) = papa’

Writing 4 = 1 + €, we have by expanding e#M-D as a Taylor series that 7 = n(€) = 1 — 2€ + ofe)
as € | 0. This gives

2€ + o(e) _ 2€ + o(€) 2
—(1—-2e)(1 +e) +0(-) (2-O)e+o0(e—) 2-0

M,() = 1

as € | 0, implying the result, by the continuity theorem.

16. The numbers P (of passengers) and T (of taxis) up to time t have the Poisson distribution with
respective parameters zt and tt. The required probabilities p, = P(P = T +n) have generating
function

oo Co oo
Spaz? = S\ SPP =m 4+n)P(T = m)z"

n=—-0O n=—00 m=0
foe)
= SP = m)z-"G plz)
m=0

in which the coefficient of z” is easily found to be that given.

17. Let N(t) be the number of machines which have arrived by time t. Given that V(t) =n, the times
T, Tn,..., Tn of their arrivals may be thought of as the order statistics of a family of independent
uniform variables on [0, t], say U,, Uz2,..., Un; see Theorem (6.12.7). The machine which arrived
at time U; is, at time t,

in the X-stage a(t)
in the Y-stage > with probability < 6(t)
repaired 1—a(t) — B®)

where a(t) = P(UU + X > t) and B(t) =P(U +X <t < U+X+/Y), where U is uniform on (0, £],
and (X, Y) is a typical repair pair, independent of U. Therefore

nat)! Bk — a(t) — B(t))"-*-d

P(U() = j, V(t) =k| N() =n) =

’

jiktm—j-bk!
implying that
. eM ALY"
PUG) =j,V@ =k) = » — PU = j,V() =k|N@ =n)
{ata(t) et" (arp(t)}ke MBO
~ i! , Kk!

394

Problems Solutions [11.8.18]-[11.8.19]

18. The maximum deficit M, seen up to and including the time of the nth claim satisfies
n
Mn = max{ My SOK - xp} = max{0, Uj, Uy + U2,...,U) + U2 +--+ Un},
j=l

where the X; are the inter-claim times, and U; = K; — X;. We have as in the analysis of G/G/1 that
My, has the same distribution as V, = max{0, Un, Un + Un_1,..-, Un + Un_1 +--+: + U1}, whence
M,, has the same distribution as the (x + 1)th waiting time in a M(A)/G/1 queue with service times
K; and interarrival times X;. The result follows by Theorem (11.3.16).

19. (a) Look for a solution to the detailed balance equations Am; = (i + 1)aj41, 0 <i <-s, to find
that the stationary distribution is given by 7; = (p' /i!)zo.
(b) Let pe be the required fraction. We have by Little’s theorem (10.5.18) that

_ A(%e-1 — Ke)

Po= a =P(%e_1—He), c=2,

and p; = 71, where zs is the probability that channels 1, 2,..., 5 are busy in a queue M/M/s having
the property that further calls are lost when all s servers are occupied.

395

12

Martingales

12.1 Solutions. Introduction

1. (i) We have that E(¥m) = E{E(%n41 | Fm)} = E(%n+1), and the result follows by induction.

(ii) For a submartingale, E(¥m) < E{E(¥n41 | Fm)} = E(%m4+1), and the result for supermartingales
follows similarly.

2. We have that
E(Qnim | Fn) = E{E(Yn+m | Fnim—1) | Fn} = E(Yn+m—1 | Fn)

ifm > 1, since Fp C Fy4m-1. Iterate to obtain E(Yntim | Fn) = E(Yn | Fn) = Yn.

3. (i) Znu—" has mean 1, and
E(Zn pth TY | Fy) = we OVE (Zags | Fn) = wns

where Fy, = 0(Z,, Z2,..., Zn).
(ii) Certainly nen < 1, and therefore it has finite mean. Also,

E(n2*+! | #,) =E (n= X;

Fn) = G(n)™

where the X; are independent family sizes with probability generating function G. Now G(n) = n,
and the claim follows.

4. (i) With X,, denoting the size of the nth jump,
E(Sn41 | Fn) = Sn + E(Xn41 | Fn) = Sn

where #y = 0 (X1, X2,..., Xn). Also E|Sy| <n, so that {S,} is a martingale.
(ii) Similarly E(S2) = var(Sn) =n, and

E(S2,, —(n +1) | Fn) = S2 + E(X2, 1) + 2SnE(Xn41) — (n +1) = S22.
(iii) Suppose the walk starts at k, and there are absorbing barriers at 0 and N (> k). Let T be the time
at which the walk is absorbed, and make the assumptions that E(S7) = So, E(S% -T)= Se. Then
the probability pz of ultimate ruin satisfies

O-pe+N-(—pe =k, O-pe+N?-(1- py) —E(T) =F,

396

Introduction Solutions [12.1.5]-[12.1.9]

and therefore py = 1 — (k/N) and E(T) = k(N — &).
5. (@ By Exercise (12.1.2), for r > i,

R(Y,¥;) = E{E(Y¥; | F)} = E{YEM | F))} = EP),
an answer which is independent of r. Therefore

E{(% —¥)¥i} =EM%Y)) -EQ¥)=0  ifi<j<k.
(ii) We have that

E{(% — Yj)" | Fi} = EE | Fi) — AEM; | F) + EW? | Fi).

Now E(Y¥; | #1) = E{E(%¥; | | Fi} =E(Y? | #)), and the claim follows.
(iti) Taking expectations of the last conclusion,

(*) 0 <E{(Y% — ¥;)?} =EQ) -E(Y?), =f Sh

Now {E(¥?) n> 1} is non-decreasing and bounded, and therefore converges. Therefore, by (*),

Yyin> 1} is Cauchy convergent in mean square, and therefore convergent in mean square, by
Problem (7.11.11).

6. (i) Using Jensen’s inequality (Exercise (7.9.4)),
E(u(Y¥n+1) | Fn) = u(EQng1 | Fn)) = u(Pn)-

(ii) It suffices to note that |x|, x”, and xt are convex functions of x; draw pictures if you are in doubt
about these functions.

7. (i) This follows just as in Exercise (12.1.6), using the fact that u{E(Y¥n41 | Fn)} = u(Yn) in this
case,

(ii) The function x* is convex and non-decreasing. Finally, let {S_ : 2 > 0} be a simple random walk
whose steps are +1 with probability p (= 1 — qg > 5) and —1 otherwise. If S, < 0, then

E (\Sn4il | Fn) = pUSnl — 1) + 40Snl + 1) = |Snl — (Pp — 9) < |Sal;
note that P(S, < 0) > Oifn > 1. The same example suffices in the remaining case.
8. Clearly E|A~" oy (Xn)| < 47" sup{|()| : 7 € S}. Also,
E(v(Xn41) | Fn) = 9° Px, j VG) SAW(Xn)
jeS
where Fy, = 0 (X1, X2,..., Xn). Divide by "+1 to obtain that the given sequence is a supermartin-
gale.

9. Since var(Z,) > 0, the function G, and hence also Gy, is a strictly increasing function on [0, 1].
Since 1 = Gn41(An41(s)) = Gn(G(An41(s))) and Gn(Hn(s)) = 1, we have that G(Hy41(s)) =
Ay, (s). With Fin = 0(Zy 1 0<k < m),

E(Hn41(s)“"+! | Fn) = G(An41(8))" = An(s)”".

397

[12.2.1]-[12.3.2] Solutions Martingales

12.2 Solutions. Martingale differences and Hoeffding’s inequality

1. Let #} =o({Vj, Wj: 1 < 7 <i}) and ¥; =E(Z | F;). With Z(j) the maximal worth attainable
without using the jth object, we have that

E(Z(j)| Fj) =E(ZG)|Fj-1), ZG) $< Z< ZG) +M.

Take conditional expectations of the second inequality, given F; and given F;_,, and deduce that
|¥; — ¥;-1| < M. Therefore Y is a martingale with bounded differences, and Hoeffding’s inequality
yields the result.

2. Let ¥; be the o-field generated by the (random) edges joining pairs (vq, vp) with 1 < a,b <i,
and let x; = E(x | ¥;). We write x (/) for the minimal number of colours required in order to colour
each vertex in the graph obtained by deleting v;. The argument now follows that of the last exercise,
using the fact that x(J) < x <= xQ)4+1.

12.3 Solutions. Crossings and convergence
1. Let T; = min{n : Y, > b}, To = min{n > T, : Yn < a}, and define 7; inductively by
To-1 = min{n > T2¢-2: Yn = b}, Tox = min{n > To¢_1 : Yn < a}.

The number of downcrossings by time n is Dn (a, b; Y) = max{k : Toy <n}.

(a) Between each pair of upcrossings of [a, b], there must be a downcrossing, and vice versa. Hence
|Dn(a, b; Y) — Un(a, bs Y)| <1.

(b) Let J; be the indicator function of the event that i € (Tox1, To] for some k, and let

n
Zn= DG -¥i-1), 2 20.
i=]

It is easily seen that
Zn < —(b— a)Dn(a, b; Y) + (Yn — 5)*,

whence
(x) (b — a)EDn (a, b; ¥) < E{ (Yn — byt} — E(Zn).

Now J; is #;_1-measurable, since

i = 1) =| ((De-1 Si - 1) \ (Toe <i - 1).
k

Therefore,
E(Zn — Zp_-1) = E{E(In(Yn — ¥n-1) | Fn—1)} = E{In(E(Pn | Fn—1) - Yn—1)} >0

since I, > 0 and Y is a submartingale. It follows that E(Zn) > E(Zy,—1) > --- = E(Zo) = 0, and
the final inequality follows from (+).

2. If Y is a supermartingale, then —Y is a submartingale. Upcrossings of [a, b] by Y correspond to
downcrossings of [—b, —a] by —Y, so that

E{(—Yn +a)t}  E{(¥n — a)7}
b-a ~ b-—a ,

EUn (a, b; Y) = EDn(—b, -a; -Y) <

398

Stopping times Solutions [12.3.3]-[12.4.5]

by Exercise (12.3.1). If a, Yn, => O then (Yn —a)™ <a.

3. The random sequence {y(X») : n > 1} is a bounded supermartingale, which converges a.s. to
some limit Y. The chain is irreducible and persistent, so that each state is visited infinitely often a.s.;
it follows that limy-.o0 w(Xn) cannot exist (a.s.) unless y is a constant function.

4. Yisamartingale since Y, is the sum of independent variables with zero means. Also )\7° P(Zn #
0= xP n2 < @, implying by the Borel-Cantelli lemma that Z, = 0 except for finitely many
values of n (a.s.); therefore the partial sum Y, converges a.s. as n —> 00 to some finite limit.

It is easily seen that an = Sa,_1 and therefore ay, = 8-5"-2 ifn > 3. It follows that |Yn| > 54n
if and only if |Z,| = a,. Therefore

a
El¥n| > 3anP(Yal 2 34n) = 34nP(\Znl = an) = 5

which tends to infinity as n — oo.

12.4 Solutions. Stopping times

1. We have that

{y+ =n}= VU IN =m =2-&)),
k=0
{ max{71, 72} <n} ={T, <n} N{T2 <n},
{ min{7,, 72} <n} ={T, <n} U {Tz <n}.

Each event on the right-hand side lies in Fy.
2. Let Fn = 0(X1, X2,..., Xn) and Sy = X1 + X2+-+-- + Xn. Now

{N(t) +1 =n} ={Sn-1 <t} {Sn > t} © Fn.

3. (Y+,F) is asubmartingale, and T = min{k : ¥, > x} is a stopping time. Now 0 < T An <n,
so that E(¥°) < E(¥7,,,) < E(¥,'), whence

E(¥t) = E(¥¢.,lr<n}) = xP(T <n).

4. We may suppose that E(¥) < oo. With the notation of the previous solution, we have that

E(Yo) = E(¥ran) = E(¥ranliren}) = xP(T <n).

§. It suffices to prove that EYs < EY7y, since the other inequalities are of the same form but with
different choices of pairs of stopping times. Let Im be the indicator function of the event {S < m < T},
and define

n
Zn = >) Imm —Ym-1), OS SN.

m=1

Note that Im is Fj, 1-measurable, so that
(Zn — Zn—1) = E{InE(¥n — Yn—1 | Fn—1)} = 0,

399

[12.4.6]-[12.5.2] Solutions Martingales
since Y is a submartingale. Therefore E(Zy)) => E(Zy—1) = --- => E(Zo) = 0. On the other hand,
Zn = Yr — Ys, and therefore E(Yr) > E(Ys).

6. De Moivre’s martingale is Y, = (q/p)*", where g = 1 — p. Now Y, > 0, and E(Yo) = 1, and
the maximal inequality gives that

P( max Sm > x) =P (,max Ym = c/o) < (p/q)’.

O<m<n
Take the limit as n —> oo to find that Soo = sup, Sm satisfies

Pp
q—P

lo.@)
(*) E(Soo) = )_ P(Soo = x) S

x=!

We can calculate E(So¢) exactly as follows. It is the case that Soo > x if and only if the walk
ever visits the point x, an event with probability f* for x > 0, where f = p/q (see Exercise (5.3.1).
The inequality of (*) may be replaced by equality.

7. (a) First, ON{T <n} =@ € Fy. Secondly, if AN{T <n} € F, then
ASO {T <n} ={T <n}\ (AN{T <n}) € Fa.

Thirdly, if Ay, Az, ... satisfy A; N{T <n} € F, for each i, then

(Ua) OT <ny= (Ai A{T <n}) € Fh.

i

Therefore ¥7 is a o-field.
For each integer m, it is the case that

{T <n} ifm>n,

(emir enj={ a ifm <n,

an event lying in ¥,,. Therefore {T < m)} € ff for all m.
(b) Let A € Fs. Then, for any n,
nt
(ANS <TH) NT <n}= LJ (ANS <m})N{T =m},
m=0
the union of events in ¥,, which therefore lies in ¥,. Hence AN {S < T} € Fr.
(c) We have {S < T} = Q, and (b) implies that A €¢ #7 whenever A € Fs.

12.5 Solutions. Optional stopping

1. Under the conditions of (a) or (b), the family {Y¥7,, : 2 > O} is uniformly integrable. Now
T An —> T asn —> oo, so that ¥r~, — Yr as. Using uniform integrability, E(¥7~n) > E(Yp),
and the claim follows by the fact that E(Y¥r,n) = E(Yo).

2. It suffices to prove that {Yr,, : 1 > 0} is uniformly integrable. Recall that {X, : n > 0} is
uniformly integrable if
(im, {sup (Xnliuxatzai)} +0 asa—>oo.

400

Optional stopping Solutions [12.5.3]-[12.5.5]

(a) Now,

E (l¥rangyeaptza)) = E (I¥rlr<niypizay) + E (Yall an,\¥n|>a})
<E (|¥rll(ypi>a}) + E (l¥nld(r>n}) = g(a) +h),

say. We have that g(a) > 0 asa —> o, since E/Y7| < oo. Also h(n) > Oasn > ~w,
so that sup,,>, A(x) may be made arbitrarily small by suitable choice of N. On the other hand,

E (|¥nlZ{¥,|>a}) —> 0 as a > 00 uniformly inn € {0, 1,..., N}, and the claim follows.

(b) Since Y,* defines a submartingale, we have that sup, E(Yz, an) < SUPp E(¥q') < 00, the second
inequality following by the uniform integrability of {Y,,}. Using the martingale convergence theorem,
Yrtan — Yr a.s. where E|Y7| < 00. Now

El¥ran — Yr| = E(I¥n _ Yr |I(r>n}) < E(|¥nllr>n}) +E(I¥r|r>n})-

Also P(T > n) — 0.as n — on, so that the final two terms tend to 0 (by the uniform integrability of

the Y; and the finiteness of E|Y7| respectively). Therefore Y7 ,n 4 Yr, and the claim follows by the
standard theorem (7.10.3).

3. By uniform integrability, Yoo = limp—+soo Yn exists a.s. and in mean, and Yn = E(Yoo | Fn).

(a) On the event {7 = n} it is the case that Yr = Y, and E(Yoo | Fr) = E(Yoo | Fn); for the
latter statement, use the definition of conditional expectation. It follows that Yr = E(Yoo | Fr),
irrespective of the value of T.

(b) We have from Exercise (12.4.7) that Fs C Fr. Now Ys = E(Yoo | Fg) = E{E(Yoo | Fr) |
Fs) = E(¥7 | Fs).

4. Let T be the time until absorption, and note that {S,} is a bounded, and therefore uniformly
integrable, martingale. Also P(T < oo) = 1 since T is no larger than the waiting time for N
consecutive steps in the same direction. It follows that E(Sg) = E(S7) = NP(Sr = N), so that
P(Sr = N) = E(So)/N. Secondly, {s? —n:n > 0} is a martingale (see Exercise (12.1.4)), and the
optional stopping theorem (if it may be applied) gives that

E(S3) = E(S% — T) = N*P(Sp = N) —E(T),

and hence E(T) = NE(So) — E(SQ) as required.

It remains to check the conditions of the optional stopping theorem. Certainly P(T < oo) = 1,
and in addition E(T2) < oo by the argument above. We have that E|S% —T| <N24+KE(T) < ©.
Finally,

E{(S2 —n)Irsny} < (N? +n)P(T > n) > 0

as n —> 00, since E(T2) < 0.

5. Let Fy = o(S1, Sz,.-.., Sn). It is immediate from the identity cos(A + A) + cos(A — A) =
2cos A cos A that

cos[A(Sn + 1 — 5(b — a))] + cosfA(Sn — 1 — 5(6 — a))]

E(Yn+41 | Fn) = (cos A+

= Yn,

and therefore Y is a martingale (it is easy to see that E] Y,| < oo for all 7).

Suppose that 0 <A < 2/(a +b), and note that 0 < |A{S, — 5(b —a)}| < 5Xa +b) < 50 for
n<T.Now Yr, constitutes a martingale which satisfies

cos{5A(a + b)} -_!
(cosayr™  — Tan = (cos A)T

(x)

401
[12.5.6]-[12.5.8] Solutions Martingales

If we can prove that E{(cos NT} < 00, it will follow that {Y7,,,} is uniformly integrable. This will
imply in turn that E(Yr) = limp—oo E(¥7~n) = E(%), and therefore

cos{5A(a + b)JE{(cos an7?y = cos{5A(b —a)}
as required. We have from (*) that
E(Y) = E(¥ran) = cos{Za(a + b)}E{(cos a) 7*"}.
Now T An > T asn — on, implying by Fatou’s lemma that

E(Y%) __ cos{zA(a — b)}

E{(cos a)~7 7 )
{(cosa)"* } < cos{4a(a +b)} cos{4ZA(a + b)}

6. (a) The occurrence of the event {U = n} depends on S1, S2,..., S, only, and therefore U is a
stopping time. Think of U as the time until the first sequence of five consecutive heads in a sequence
of coins tosses. Using the renewal-theory argument of Problem (10.5.17), we find that E(U) = 62.
(b) Knowledge of $1, So, ..., Sn is insufficient to determine whether or not V = n, and therefore V
is not a stopping time. Now E(V) = E(U) — 5 = 57.

(c) W is a stopping time, since it is a first-passage time. Also E(W) = oo since the walk is null
persistent.

7. With the usual notation,

m m+n
E(Mm+n | Fm) = B(d> Spt S> Sp — 5 (Smtn — Sm + Sm)? | Fn)
=0 r=m+1

= Mm + Sm — SmE{(Smn ~ Sm)?}
= Mn + Sm — nSmE(X?) = Mm.
Thus {M, : n > 0} is a martingale, and evidently T is a stopping time. The conditions of the optional

stopping theorem (12.5.1) hold, and therefore, by a result of Example (3.9.6),

T
a
a — 4a? = My = E(M7) -2(5 s,) —4K?. ra
=0

8. We partition the sequence into consecutive batches of a + b flips. If any such batch contains only
1’s, then the game is over. Hence P(T > n(a + b)) < {1-— (gato y" — 0asn — oo. Therefore,
E|Sp — T| < E(S7) + E(T) < @ +6)" +E(T) < 0,

and
E[(S# —T) ron] < @+b)P(T > n)+E(TIrsny) > 0  asn > oo.

402
Problems Solutions [12.7.1]-[12.9.2]

12.7 Solutions. Backward martingales and continuous-time martingales

1. Lets < ft. We have that E(n(X(t)) | Fs, Xs =i) = Di pij (t — s)n(j). Hence
<B(n(X(O) | Fs, Xs =1) = (Pr-sGn/); = 0,
so that E(n(X (8) | Fs, Xs =i) = ni), which is to say that E(7(X(t)) | Fs) = 4 (X(s)).
2. Let W(t) = exp{-ON(t) +410 — e*)} where 6 > 0. It may be seen that W(t A T,), t = 9,
constitutes a martingale. Furthermore
[W(t A Ta)| < exp{At A Ta) (1 — e*)} + exp{ATa(1 - ey} as t > 00,

where, by assumption, the limit has finite expectation for sufficiently small positive @ (this fact may be
checked easily). In this case, {W(t A Tz) : t => 0} is uniformly integrable. Now W(t A Tz) > W(Ta)
a.s. aS f > 00, and it follows by the optional stopping theorem that

1 = E(WO) = E(W(t A Ta) > E(W(Tq)) = € Ot Bfe*72l-8)),
Write s = e~? to obtain s~? = Ef{e*42—-s))}_ Differentiate at s = 1 to find that @ = AE(T,) and
afat+l)= WE(T2), whence the claim is immediate.

3. Let G,, be the o-field generated by the two sequences of random variables Sm, Sm41..-, Sn and
Um+1+ Um42)+-++ Un. It is a straightforward exercise in conditional density functions to see that

Um+2 (m + 1)x™71 m+1

-1
Smt» EU na, | $m40 =| (Um42)"t! dx = mUmi2°

m
E(Sm | $m41) = mal
whence E(Rin | $41) = R41 a8 required. [The integrability condition is elementary.]

Let T = max{m : Rm > 1} with the convention that T = 1 if Rm < 1 for all m. As in the closely
related Example (12.7.6), T is a stopping time. We apply the optional stopping theorem (12.7.5) to
the backward martingale R to obtain that E(Rr | $,) = Rn = Sn/t. Now, Rr = 1 on the event
{Rm = 1 for some m < n}, whence

~ =E(Rr | Sn = y) = P(Rm 2 1 for some m < n| Sp = y).

[Equality may be shown to hold. See Karlin and Taylor 1981, pages 110-113, and Example (12.7.6).]

12.9 Solutions to problems

1. Clearly E(Z,) < (u +m)", and hence E[¥,| < oo. Secondly, Z,41 may be expressed as

yaa X; +A, where X;, X2,... are the family sizes of the members of the nth generation, and A is
the number of immigrants to the (x + 1)th generation. Therefore E(Zn+41 | Zn) = uZn +m, whence

1 1— prt!
E(n+1 | Zn) = aay UZn+m nr ara = Vy.

2. Each birth in the (n + 1)th generation is to an individual, say the sth, in the nth generation. Hence,
for each r, Biy+1),r may be expressed in the form Bin41),r = Bn,s + B; (s), where B; (s) is the age
of the parent when its jth child is born. Therefore

E{> e Baty Fn = ED oO Bn st Bis)
-

SJ

Fa = Sie Fs M1),
s§

403

(12.9.3]-[12.9.5] Solutions Martingales

which gives that E(Y,41 | Fn) = Yn. Finally, E(¥,(0)) = 1, and hence E(¥,(@)) = 1.
3. Ifx,c > 0, then

(*) P ( max Y; > 3) <P ( max (Ye + c)* > at <?) :

1<k<n

Now (¥% + c)? is a convex function of Y;, and therefore defines a submartingale (Exercise (12.1.7)).
Applying the maximal inequality to this submartingale, we obtain an upper bound of E{(Y¥n +c)7}/(x+
c)? for the right-hand side of («). We set c = R(Y2) /x to obtain the result.

4, (a) Note that Z, = Z,_1 + ¢en{Xn — E(Xn | Fp_1)}, so that (Z, F) is a martingale. Let T be
the stopping time T = min{k : cy ¥, > x}. Then E(Zr,~,) = E(Zg) = 0, so that

TAn
0= B{ cran¥r an — So ceE(Xx | nS
k=1

since the final term in the definition of Z, is non-negative. Therefore

n
xP(T <n) <Eferan¥ran} < >, chE{E(X¢ | Fe-1)},
k=1

where we have used the facts that ¥, > 0 and E(X, | ¥%_1) = 0. The claim follows.
(b) Let x 1, X2,... be independent random variables, with zero means and finite variances, and let
Yj = vi X;. Then y? defines a non-negative submartingale, whence

1 < i<
P{ max |%|>x)=P( max y2>x7)<  Y E?-¥2,)=—.Y E(X?).
(zs 2s) (1 ee )end Cee 2 oP

1<k<n

5. The function h(u) = |u|’ is convex, and therefore Y;(m) = |S; — Sm|", i > m, defines a
submartingale with respect to the filtration ¥; = a ({X jiilsj< i}). Apply the HRC inequality of
Problem (12.9.4), with cz = 1, to obtain the required inequality.

If r = 1, we have that

m+n

(*) E(|Sm+n—Sml)< >. EIZgl
=m+1

by the triangle inequality. Let m,n — oc to find, in the usual way, that the sequence {S,} converges
a.s.; Kronecker’s lemma (see Exercise (7.8.2)) then yields the final claim.

Suppose 1 <r < 2, in which case a little more work is required. The function h is differentiable,
and therefore

v-H
h(v) — h(u) = wv — uh’) + {h’(u +x) —h'(u)} dx.
0
Now h’(y) = rly|" —lsien(y) has a derivative decreasing in |y|. It follows (draw a picture) that
h’(ut+x)—h') < 2n'(5x) if x > 0, and therefore the above integral is no larger than ang (v—x)).
Apply this with v = Sy,4%41 — Sm and u = S44 — Sm, to obtain

E(lSmn+k+1 — Sml”) — E(\Smak — Sml”) < E(Zm4e+ih! (Sm+k — Sm)) + 2E(15Zm+k+11")-

404

Problems Solutions [12.9.6]-[12.9.10]

Sum over k and use the fact that
E(Zm+k+14/(Sm+k — Sm)) = E{h’(Sm4k — SmJE(Zm+k+1 | Fm+k)} = 9,

to deduce that
m-+n

E(|Smin — Sml") <2?" S> E(IZxl’).
k=m+1

The argument is completed as after («).

6 With ik = Tty;,=0}; we have that

En | Fn—1) =E(Xndn—1 + 2¥n—11Xnl(l = Int) | Fu—1)
= In1E(Xn) + 2¥q—11 = In-EIXnl = Yoo

since E(Xn) = 0, E|Xn| = n—!. Also El¥n| < E{|Xal(1 + |¥n—1|)} and E|¥;| < 00, whence
E|¥,| < oo. Therefore (Y, F) is a martingale.

Now Y, = 0 if and only if X, = 0. Therefore P(¥, = 0) = P(X, = 0) =1-n7! > 1
as n —> oo, implying that Y¥, + 0. On the other hand, 3°, P(Xn # 0) = oo, and therefore
Pn # 01.0.) = 1 by the second Borel—-Cantelli lemma. However, Y, takes only integer values, and
therefore Y, does not converge to 0 a.s. The martingale convergence theorem is inapplicable since
sup, E|Yn| = 00.

7, Assume that t > 0 and M(t) = 1. Then ¥, = e’ Sn defines a positive martingale (with mean 1)
with respect to Fy = 0(X1, X2,..., Xn). By the maximal inequality,

P( max S; > x) = P( max ¥; > e) <e *E(Yn),

1<k<n 1l<k<n

and the result follows by taking the limit as n — oo.

8. The sequence ¥, = &%" defines a martingale; this may be seen easily, as in the solution to
Exercise (12.1.15). Now {¥,} is uniformly bounded, and therefore Yoo = limy-+oo Yn exists a.s. and
satisfies E(Yoo) = E(Yo) = &.

Suppose 0 < & < 1. In this case Zj is not a.s. zero, so that Z, cannot converge a.s. to a constant
c unless c € {0, 00}. Therefore the a.s. convergence of Y;, entails the a.s. convergence of Zy, to a limit
tandom variable taking values 0 and oo. In this case, E(Yo9) = 1-P(Zn — 0) +0-P(Zn > o&),
implying that P(Z, — 0) = é, and therefore P(Z, > co) = 1-&.

9. It is a consequence of the maximal inequality that P(¥* > x) < x E(Yn Lyyt>xy) for x > 0.
Therefore

[o.<) oe)
BaD = f Pg =H) dx <1 f P(Y* > x) dx

OO
<1 +E{% | Marg 4x
=1+E(¥q logt ¥*) < 1+E(Yn log? ¥,) + E(Y;)/e.

10. (a) We have, as in Exercise (12.7.1), that

(x) E(h(X(t)) |B, X(s) =i) => py @h(/) fors <2,
j

405

(12.9.11]-[12.9.13] Solutions Martingales

for any event B defined in terms of {X (u) : u < s}. The derivative of this expression, with respect to
t, is (P;Gh’);, where P, is the transition semigroup, G is the generator, andh = (A(j) : j = 0). In
this case,

Gh’); = > gjah@® = Aj {hG +1) — h/)} - wy {hG) —hG —D} =0
k

for all 7. Therefore the left side of (*) is constant for t > s, and is equal to its value at time s, ie.
X(s). Hence h(X(t)) defines a martingale.

(b) We apply the optional stopping theorem with T = min{t : X(t) € {0, n}} to obtain E(A(X(T))) =
E(A(X (0))), and therefore (1 — 2(m))h(n) = h(m) as required. It is necessary but not difficult to
check the conditions of the optional stopping theorem.

11. (a) Since Y is a submartingale, so is Y+ (see Exercise (12.1.6)). Now
EV mat | Fn) = ELEY mat | Fnt1) | Fn} > EM | Fn):

Therefore {E(¥g4m | Fn) : m > O} is (a.s.) non-decreasing, and therefore converges (a.s.) to a limit
M,,. Also, by monotone convergence of conditional expectation,

E(Mn+1 | Fn) = lim ELEC mgt | Fatt) | Fa} = him ECs ng1 | Fn) = Mn,

and furthermore E(Mn) = limy—oo E(Y yt in) < M. It is the case that My is F,-measurable, and
therefore it is a martingale.

(b) We have that Zn = Mn — Yp is the difference of a martingale and a submartingale, and is therefore
a supermartingale. Also M, > yt > 0, and the decomposition for Y, follows.
(c) In this case Zn is a martingale, being the difference of two martingales. Also My, > E(Y,' | Fn) =
Y," > Yn as., and the claim follows.
12. We may as well assume that ~ < P since the inequality is trivial otherwise. The moment
12,2

generating function of P — C, is M(t) = e! ? —H)+70°t" and we choose t such that M(t) = 1,
ie. t = —2(P — p)/o?. Now define Zn = min{e'™, 1} and Fy = o(C}, Co,..., Cn). Certainly
E|Zn| < co; also

E(Zn41 | Fn) SECC "+ | Fn) = ef MQ) = etl
and E(Z,41 | Fn) < 1, implying that E(Z,41 | Fn) < Zn. Therefore (Zy, Fn) is a positive
supermartingale. Let T = inf{n : Y, < 0} = inf{n : Z, = 1}. Then T A m is a bounded stopping
time, whence E(Zg) > E(Zram) = P(T < m). Letm — oo to obtain the result.
13. Let F, =o (R1, Ro,..., Rn).
(a) 0 < Y, < 1, and Y, is ¥,-measurable. Also

Rn

K(X, Rn) = R, _—-—;
(Rn+1 | n) n+ oad

whence Yy satisfies E(¥;+41 | Fn) = Yn. Therefore {Y, : n > 0} is a uniformly integrable martingale,
and therefore converges a.s. and in mean.

(b) In order to apply the optional stopping theorem, it suffices that P(T < oo) = 1 (since Y is
uniformly integrable). However P(T > n) = 4 . s vee aH =(nt1)7! +0. Using that theorem,
E(Y’r) = E(¥o), which is to say that E{7/(T + 2)} = 3, and the result follows.

(c) Apply the maximal inequality.

406

Problems Solutions [12.9.14]-[12.9.17]

14. As in the previous solution, with $n the o-field generated by A,, Ao, ... and Fn,

Ry + An Rn Rn Bn
E(Y, =
(n+1 | Gn) Cees) (acm) + (asec) (ate)

— Rn —
~ Rit Bn

Yn,

sothatE(Yn41 | Fn) = E{E(¥n+41 | $n) | Fn} = Yn. Also|¥n| < 1, and therefore Y, is a martingale.
We need to show that P(T < 00) = 1. Let I, be the indicator function of the event {T > n}. We
have by conditioning on the A, that

E(In | A) = 1- a 1-
ns 2+ Sj je 245;

j=

asn —> 00, where Sj = al Aj. The infinite product equals 0 a.s. if and only if Li (2+ sj)! = 00
a.s. By monotone convergence, P(T < 00) = 1 under this condition. If this holds, we may apply the
optional stopping theorem to obtain that E(Y7) = E(Yo), which is to say that

1
e(1- wat) a5.
2+ Sy 2

15. At each stage k, let Lx be the length of the sequence ‘in play’, and let Y; be the sum of its
entries, so that Lp = n, Yo = ye x;. If you lose the (k + 1)th gamble, then Ly4; = Lx + 1 and
Ypiy = Ye + Z, where Zx is the stake on that play, whereas if you win, then Lyi, = Lx — 2 and
Yei1 = Ye — Zp; we have assumed that Ly > 2, similar relations being valid if Lz; = 1. Note that
Lz is arandom walk with mean step size — 1, implying that the first-passage time T to 0 is a.s. finite,
and has all moments finite. Your profits at time k amount to Yo — Yx, whence your profit at time T is
Yo, since Yr = 0.

Since the games are fair, Y; constitutes a martingale. Therefore E(Y¥7~jm) = E(Y¥) # 0 for
all m. However TA m — T as. asm — oo, so that Yr~m — Yr as. Now E(¥7) = 0 4
limm—+oo E(¥ram), and it follows that {¥r,jm : m > 1} is not uniformly integrable. Therefore
E(sup,;, YT Am) = 00; see Exercise (7.10.6).

16. Since the game is fair, E(Sp41 | Sn) = Sn. Also |S,| < 1+2+4---+n < oo. Therefore S, is
a martingale. The occurrence of the event {N = n} depends only on the outcomes of the coin-tosses
up to and including the nth; therefore N is a stopping time.

A tail appeared at time N — 3, followed by three heads. Therefore the gamblers G,, G2,...,
Gy_3 have forfeited their initial capital by time N, while Gy_j; has had i + 1 successful rounds for
0 <i <2. Therefore Sy =N—(p—!+ p e+ p?), after a little calculation. It is easy to check that
N satisfies the conditions of the optional stopping theorem, and it follows that E(Sy) = E(So) = 9,
which is to say that E(N) = p—! + p72 + po.

In order to deal with HTH, the gamblers are re-programmed to act as follows. If they win
on their first bet, they bet their current fortune on fails, returning to heads thereafter. In this case,
Sy = N- (p7! + p?q7}) where g = 1 — p (remember that the game is fair), and therefore
E(N) = p+ pq".

17. Let Fy = o({Xi, ¥; : 1 < i < n}), and note that T is a stopping time with respect to this
filtration. Furthermore P(T < co) = | since T is no larger than the first-passage time to 0 of either
of the two single-coordinate random walks, each of which has mean 0 and is therefore persistent.

Let of = var(X,) and af = var(Y,). We have that U, — Up and V, — Vo are sums of
independent summands with means 0 and variances a? and of respectively. It follows by considering

407
[12.9.18]-[12.9.19] Solutions Martingales

the martingales (Un — Uo)? - na? and (Vn — Vo)? - nos (see equation (12.5.14) and Exercise (10.2.2))
that
E{(Ur — Up)"}=o7E(T), E{(Vr — Vo)”} = of E(Z).

Applying the same argument to (U, + Vi) — (Up + Vo), we obtain

E{(Up + Vr — Up — Vo)*} = ECTYE((X1 + Yy)7} = E(T)(0? 4+ 2c +09).
Subtract the two earlier equations to obtain
(*) E{(Ur — Up)(Vr — Vo)} = cE(T)

if E(T) < co. Now Ur Vy = 0, and in addition E(Ur) = Up, E(Vr) = Vo, by Wald’s equation and
the fact that E(X;) = E(Y;) = 0. It follows that -E(Up Vo) = cE(T) if E(T) < oe, in which case
c <0.

Suppose conversely that c < 0. Then (+) is valid with T replaced throughout by the bounded
stopping time T A m, and hence

0 < EUramVram) = E(UpVo) + cE(T Am).

Therefore E(T Am) < E(UgVo)/(2|c|) for all m, implying that E(T) = limm—oo E(T Am) < 00,
and so E(T) = —E(Ug Vo)/c as before.

18. Certainly 0 < X, < 1, and in addition X, is measurable with respect to the o-field Fy, =
o(R1, R,..., Rn). Also E(Ry41 | Rn) = Rn — Rn/(S2 — 1), whence E(Xn41 | Fn) = Xn.
Therefore X» is a martingale. ;

A strategy corresponds to a stopping time. If the player decides to call at the stopping time T, he
wins with (conditional) probability X7, and therefore P(wins) = E(X7), which equals E(X9) (= 5)
by the optional stopping theorem.

Here is a trivial solution to the problem. It may be seen that the chance of winning is the same
for a player who, after calling “Red Now”, picks the card placed at the bottom of the pack rather than
that at the top. The bottom card is red with probability 5 irrespective of the strategy of the player.

19, (a) Asums of money in week t is equivalent toasum s/(1+a)! in week 0, since the latter sum may
be invested now to yield s in week t. If he sells in week tf, his discounted costs are sv =j¢e/(i+a)”"
and his discounted profit is X;/(1 + a)‘. He wishes to find a stopping time for which his mean
discounted gain is a maximum.

Now

T
- Sr taye = “fa +a)? - 1},
n=l o

so that u(T) = E{(1 +a)~7 Zr} — (c/a).
(b) The function A(y) = ay — SS P(Zn > y)dy is continuous and strictly increasing on [0, 00),
with h(0) = —E(Z,) < 0 and h(yv) > co as y > oo. Therefore there exists a unique y (> 0) such
that h(y) = 0, and we choose y accordingly.
(c) Let Fy = 0 (Z1, Z2,..., Zn). We have that

oO

E(max(Zn, y}) = y +/ [1 -GQ)ldy = +.a)y
Y

where G(y) = P(Zn < y). Therefore E(V,41 | Fn) = 1 +a)""y < Vn, 8o that (Vn, Fn) is a
non-negative supermartingale.

408

Problems Solutions [12.9.20]-[12.9.21]

Let j4(t) be the mean gain of following the strategy ‘accept the first offer exceeding t — (c/a)’.
The corresponding stopping time T satisfies P(T =n) = G(r)" (1 — G(t)), and therefore

oo
u(t) + (c/a) = SO E{( +a)? Zr lranj}
n=0

lo.@)
Sod + a)" G(2)" (1 ~ G(t))E(Z]_ | Z1 > 1)
n=0

l+a

= The gw {ra — G(t)) + a- Gun ay}.

Differentiate with care to find that the only value of t lying in the support of Z, such that u’(r) = 0
is the value t = y. Furthermore this value gives a maximum for z(t). Therefore, amongst strategies
of the above sort, the best is that with r = y. Note that u(y) = y(1 + @) — (c/a).

Consider now a general strategy with corresponding stopping time T, where P(T < 00) = 1. For
any positive integer m, T Am is abounded stopping time, whence E(Vr~m) < E(Vo) = y(14+a). Now
IVraml < "fo |Vil, and S772 ElV;| < co. Therefore {Vr,jm : m > 0} is uniformly integrable.
Also Vr~m — Vr as. aS m —> ov, and it follows that E(Vr,~m) — E(Vr). We conclude that
LT) = E(Vr) — (c/a) < y(1 +@) — (c/a) = w(y). Therefore the strategy given above is optimal.
(d) In the special case, P(Z,; > y) = (Gy — 1)~? for y > 2, whence y = 10. The target price is
therefore 9, and the mean number of weeks before selling is G(v)/(1 — G(yv)) = 80.

20. Since G is convex on [0, 00) wherever it is finite, and since G(1) = 1 and G’(1) < 1, there exists
a unique value of 7 (> 1) such that G(7) = 7». Furthermore, ¥, = 72" defines a martingale with
mean E(Y¥o) = 7. Using the maximal inequality (12.6.6),

P(sup Zn > k) = P(sup Yn > n*) < —
n n n

for positive integers k. Therefore

Co
E(sup Zn) <y>——

k= 7

21. Let M,, be the number present after the nth round, so My = K, and My41 = Mn — Xn41," = 1,
where X,» is the number of matches in the nth round. By the result of Problem (3.11.17), EX, = 1
for all n, whence

E(Myn41 +n +1] Fn) = Mn +n,

where ¥;, is the o-field generated by Mo, M,,..., Mn. Thus the sequence {M, + 7} is a martingale.
Now, N is clearly a stopping time, and therefore K = Mp) +0 = E(My + N) = EN
We have that
2
E{(Mn41 +24 1)° + Mnii| Fn}
= (Mn +n)? — 2(Mn + nJE(Xng1 — 1) + Mn +E{(Xn41 — 1)? — Xn41 | Fn}
< (My +n)? + Mn,

where we have used the fact that

1 if M, > 1,

var(X, Fn) =
(Xn+1 | Fn) Lo if M, =1.

409
[12.9.22]-[12.9.24] Solutions Martingales

Hence the sequence {(M, +n)? + Mn} is a supermartingale. By an optional stopping theorem for
supermartingales,

K? + K = Mj + Mo > E{(My +N)? + My} =E(N?),

and therefore var(N) < K.

22. In the usual notation,

Ss s+t
E(M(s +1) | Fs) =e( [ wan + | W(u) du — 4{ Ws +.) — Ws) + WOS)}?
0 s

‘)

as required. We apply the optional stopping theorem (12.7.12) with the stopping time T = inf {u :
Wu) € {a, b}}. The hypotheses of the theorem follow easily from the boundedness of the process
for t € [0, 7], and it follows that

T
e( | W(u) du — swore) =0.
0

= M(s) + tW(s) — WOS)E([W(s +1) — W(s)]? | Fs) = MOS)

‘ Hence the required area A has mean

Ba) =B( Wiw)du) = EW) J= 34 ()+3 (5).

[We have used the optional stopping theorem twice actually, in that E(W(T)) = 0 and therefore
P(W(T) =a) = —b/(a—b).]

23. With #; = o0(W(u) :0<u <s), we have fors < t that

E(R(t)? | Fs) = E(\W(s)|? + |W) — Wis)? + 2W(s) - (Wit) — Wis) | Fs) = RG)? + (t— 5),

and the first claim follows. We apply the optional stopping theorem (12.7.12) with T = inf{u :
|W (w)| = a}, as in Problem (12.9.22), to find that 0 = E(R(T)? — T) = a — E(T).

24. We apply the optional stopping theorem to the martingale W(t) with the stopping time T to find
that E(W(T)) = —a(1 — pp) + bpp = 0, where pp = P(W(T) = b). By Example (12.7.10),
W(t)? —t is a martingale, and therefore, by the optional stopping theorem again,

E((W(T)? — T) = a? — pp) +b? pp —E(T) =0,

whence E(7) = ab. For the final part, we take a = b and apply the optional stopping theorem to the
martingale exp[@ W(t) — 5071] to obtain

E(exp[0W(T) — 407T]) = {e-°°(1 — pp) + cP py \E(e7 27) =1,

on noting that the conditional distribution of T given W(T) = b is the same as that given W(T) = —D.
192
Therefore, E(e~2° T) = 1/cosh(b@), and the answer follows by substituting s = 507.

410

13

Diffusion processes

13.3 Solutions. Diffusion processes

1. Itis easily seen that
E{X@+h)— X()|XO} =A-wXOh + oh),
E({X@+h) —X()}" |X) =A+u)XOA + off),
which suggest a diffusion approximation with instantaneous mean a(t, x) = (A — y2)x and instanta-
neous variance b(t, x) = (A + a)x.

2. The following method is not entirely rigorous (it is an argument of the following well-known
type: it is valid when it works, and not otherwise). We have that

aM oo 8 oo
<= / oy of dy= / {da(t, y) + 462b«, y) te” f dy,
ot —oo ot oo

by using the forward equation and integrating by parts. Assume that a(t, y) = >°, @n(t)y”, b(t, y) =
Yn Bn(t)y”. The required expression follows from the ‘fact’ that

Oy n dy = — By dy- .
[oe y fay = aan oo. f dy 39n

3. Using Exercise (13.3.2) or otherwise, we obtain the equation

M
OM mM + 40°M
ot

with boundary condition M(0, 6) = 1. The solution is M(t) = exp{ 50 (2m + O)t}.
4. Using Exercise (13.3.2) or otherwise, we obtain the equation
sad = 0 + 46°M
with boundary condition M(0, 6) = 1. The characteristics of the equation are given by
at _do 2am
1° 6 62M

>

1 92 192
with solution M(t, 0) = e4° g(0e~*) where g is a function satisfying 1 = ea? g(@). Therefore
M = exp{qz07(1 — e~*)).

411

[13.3.5]-[13.3.10] Solutions Diffusion processes

5. Fixt > 0. Suppose we are given W1(s), Wo(s), W3(s), for0 < s < t. By Pythagoras’s theorem,
Rit+u)?=X : + x3 + x3 where the X; are independent N(W; (¢), «) variables. Using the result of
Exercise (5.7.7), the conditional distribution of R(t + u)* (and hence of R(t + u) also) depends only
on the value of the non-centrality parameter 9 = R(t) of the relevant non-central x” distribution.
It follows that R satisfies the Markov property. This argument is valid for the n-dimensional Bessel
process.

6. By the spherical symmetry of the process, the conditional distribution of R(s +a) given R(s) = x
is the same as that given W(s) = (x, 0, 0). Therefore, recalling the solution to Exercise (13.3.5),

P(R(s +a) < y| R(s) =x)
ux? +0? +0?

1
= . ——-sz5 CX dudvd
cee rays? *P 2a ws

y p?m pm 1 p* —2pxcosd+x*| 4
~ Qnay3/2 ~ ind dd dod
I o=0 - (2n.a)3/2 oo{ 2a p- sin dp

-[° pix J(_ @-2\ (e+e) |
— /2ta P 2a P 2a p>

and the result follows by differentiating with respect to y.

7. Continuous functions of continuous functions are continuous. The Markov property is preserved
because g(-) is single-valued with a unique inverse.

1,2
8. (a) Since E(e7 ¥) = ¢2°", this is not a martingale.
(b) This is a Wiener process (see Problem (13.12.1)), and is certainly a martingale.
(c) With ¥; = o (W(s):0<s <1t) andt,u > 0,

t+u

t+u
B{ e+ motu) - W(s)ds
0

t
wi =¢+wHW) - [ Wis)ds — Wt) ds
0 t

t
=tWi(t) - W(s)ds,
0

whence this is a martingale. [The integrability condition is easily verified.]

9. (a) Withs < ft, S(t) = S(s) exp{a(t — 5) ++ b(W(t) — W(s))}. Now W(t) — W(s) is independent
of {W(u) : 0 < u < s}, and the claim follows.
(b) S(t) is clearly integrable and adapted to the filtration ¥ = (¥;) so that, for s < f,

E(S(t) | Fs) = S(s)E(exp{a(t — s) + (Wt) — W(s))} | Fs) = Ss) expla(t — s) + 507 — s)},
which equals S(s) if and only if a + 5b? = 0. In this case, E(S(t)) = E(S(0)) = 1.

10. Either find the instantaneous mean and variance, and solve the forward equation, or argue directly
as follows. With s < ft,

P(S(t) < y| S(s) =x) = P(OW(t) < —at + log y| bW(s) = —as + logx).

Now b(W(t) — W(s)) is independent of W(s) and is distributed as N (0, b2(t —s)), and we obtain on
differentiating with respect to y that

_ _ 2
_ (log(y/x) — a(t - 5) ). ny > 0.

1
f@, y |s,x) = y an b2(t — 5) 5) exp ( 2b2(t —s)

412

Excursions and the Brownian bridge Solutions [13.4.1]-[13.6.1]

13.4 Solutions. First passage times

1. Certainly X has continuous sample paths, and in addition E|X (#)| < oo. Also, ifs < t,
192 . 1 p2 ivy
E(X(t) | Fs) — X(s)e2® (5) (elIWO-WO)) | Fs) = X(s)e2? (t-s) ,— 3 (t—s) _ X(s)

as required, where we have used the fact that W(t) — W(s) is N(0, t — s) and is independent of F;.

2. Apply the optional stopping theorem to the martingale X of Exercise (13.4.1), with the stopping
time 7, to obtain E(X(T)) = 1. Now W(T) = aT + b, and therefore E(e¥? +46) = | where
w = iad + 46%. Solve to find that

Ee?) = e719 = exp {-6(y — 2 + a}

is the solution which gives a moment generating function.

3. We have that T < wu if and only if there is no zero in (u, t], an event with probability 1 —
(2/1) cos™! {Ju / i}, and the claim follows on drawing a triangle.

13.5 Solution. Barriers

1. Solving the forward equation subject to the appropriate boundary conditions, we obtain as usual

that
—~d

Py) = ety |d)-+e4 g(t, y|—d)— [Ame g(t, y | dx
—C
1
where g(t, y | x) = (2mt)” 2 exp{—(y — x — mt)? /(2t)}. The first two terms tend to 0 as t > oo,
regardless of the sign of m. As for the integral, make the substitution u = (x — y — mt)/./t to obtain,
ast > 0d,

peer amet enn iu 2|\mle~2'"lY ifm <0,
00 Jon if m > 0.

13.6 Solutions. Excursions and the Brownian bridge
1, Let f@,xn= (mt) 2e7?/ 20, It may be seen that
P(W(t) > x |Z, WO) =0) = lim (We) >x|Z,W0)=w)
w

where Z = {no zeros in (0, t]}; the small missing step here may be filled by conditioning instead on
the event {W(e) = w, no zeros in (e, t]}, and taking the limit as e | 0. Now, if w > 0,

Co
P(W(t) > x,Z|W0) =w) = / {f(t,y—w)— fit, y+ w)} dy
x
by the reflection principle, and

OO w
P(Z| WO) = w) =1-2 f fee ydy= [ Ft, y)dy

w

413

[13.6.2]-[13.6.5] Solutions Diffusion processes

by a consideration of the minimum value of W on (0, t]. It follows that the density function of W(4),
conditional on ZN {W(O) = w}, where w > 0, is

f@,x—w)— f@,x+w)

hy) = x > 0.
. Lew FQ, y) dy
Divide top and bottom by 2w, and take the limit as w | 0:
1 fF, /00 x >0.

li =- el
lim hw x) f@t,0) ox t

2. Itis a standard exercise that, for a Wiener process W,

l-s

E{W(t) |W) =a, wy =0} =a(-—),

E{W(s)?| W(0) = W(1) = 0} =s(1—s),

if0 <s <+t <1. Therefore the Brownian bridge B satisfies, forO < 5s <t <1,
1-t
E(B(s)B(t)) = E{ B(s)E(B(t) | B(s))} = FTE”) =s(1—t)

as required. Certainly E(B(s)) = 0 for all s, by symmetry.

3. W isa zero-mean Gaussian process on [0, 1] with continuous sample paths, and also W(0) =
W(1) = 0. Therefore W is a Brownian bridge if it has the same autocovariance function as the
Brownian bridge, that is, c(s, t) = min{s, t} — st. Fors < f,

cov(W(s), W(t)) = cov(W(s) — sW(1), W(t) —fW(1)) =s —ts —st4+st=s—st
since cov(W(u), W(v)) = min{u, v}. The claim follows.

4. Either calculate the instantaneous mean and variance of Ww, or repeat the argument in the solution
to Exercise (13.6.3). The only complication in this case is the necessity to show that Wit) is as.
continuous at ¢ = 1, i.e., thatu—! W(u— 1) > 0a.s.asu —> oo. There are various ways to show this.
Certainly it is true in the limit as u — oo through the integers, since, for integral u, W(u — 1) may be
expressed as the sum of u — 1 independent N (0, 1) variables (use the strong law). It remains to fill in the
gaps. Let n be a positive integer, let x > 0, and write My, = max{|W(u) —W(n)|:n<u<n+ti}.
We have by the stationarity of the increments that

lee) Cc

SOP Mn > nx) = POM > nx) <1 + =D < oo,

n=0 n=0 *
implying by the Borel—Cantelli lemma that n~!M, < x for all but finitely many values of 7, a.s.
Therefore n~!M, — Oa.s.asn —> oo, implying that

1
lim
u>oou + 1

1
|Wiu)| < lim —{|W@|+Mn}>0 as.
noo n

5. In the notation of Exercise (13.6.4), we are asked to calculate the probability that W has no zeros
in the time interval between s/(1 — s) andt/(i — t). By Theorem (13.4.8), this equals

1 cos! sy) _ 2 ogg fas .
au t(dl—s) cra td —s)

414

Stochastic calculus Solutions [13.7.1]-[13.7.5]

13.7 Solutions. Stochastic calculus
1. Let F = o(W, :0< u <s). Fixn > 1 and define X,(k) = | Wee 727 | forO < k < 2",
By Jensen’s inequality, the sequence {X,(k) : 0 < k < 2”} is a non-negative submartingale with

respect to the filtration ¥%;/2n , with finite variance. Hence, by Exercise (4.3.3) and equation (12.6.2),
Xx = max{Xn(k) : 0 < k < 2"} satisfies

ore) 0° XR
E(X*) = 2 [ xP(X* > x)dx <2 [ E(W;* Iry#>.)) dx = 2e{ w/t [ ax}
= 2E(W,* X nS 2y E(W2)E(X#2) by the Cauchy—Schwarz inequality.

Hence E(X*?) < 4E(W?). Now xe? is monotone increasing in n, and W has continuous sample
paths. By monotone convergence,

2\_ 1: «2 2
(max | We ) = lim E(X}?) < 4E(W?).

2. See the solution to Exercise (8.5.4).
3. (a) We have that

n—l n—l
Nn) = dopa — VP) —- SOV - yy}.
j=0 j=0

The first summation equals w?, by successive concellation, and the mean-square limit of the second
summation is ¢, by Exercise (8.5.4). Hence limy+o0 11 (”) = 5W? - st in mean square.
Likewise, we obtain the mean-square limits:

: 1 wy2 1 . : 1 y2

lim, Inn) = 5 Wr t at, pam, h(n) = wim, iy) = 3W/.

4. Clearly E(U(t)) = 0. The process U is Gaussian with autocovariance function
E(U(s)U(s +1) =E(E(U()UGs +1) | Fs)) = Pe P+ BW EPA?) = “FF,

Thus U is a stationary Gaussian Markov process, namely the Ornstein—Uhlenbeck process. [See
Example (9.6.10).]

5. Clearly E(U;) = 0. Fors <t,

S t
E(U;Us44) = E(WsW;) + B2E ( | ; | oe ue PE Wy du av)
us i

Ss t
-e(wis [ e PS) yw, au) - E(w, [ e Pl) yw, av)
0 0

Ss t
=s + preP St) | / eP UF) min{u, v} du dv
u=0 /v=0

S t

-— B i e PS) min{u, t}du — [ e PE-Y) min{s, v} dv
0 0

e2Bs _ |

e bCtt)

415

[13.8.1]-[13.8.1] Solutions Diffusion processes

after prolonged integration. By the linearity of the definition of U, it is a Gaussian process. From the
calculation above, it has autocovariance function c(s, s +t) = (e~PC—S) _ eB ts my /@8). From this
we may calculate the instantaneous mean and variance, and thus we recognize an Ornstein—-Uhlenbeck
process. See also Exercise (13.3.4) and Problem (13.12.4).

13.8 Solutions. The It6 integral

1, (a) Fixt > Oand let n > 1 and dé = t/n. We write t; = jt/n and Vj = Wi;- By the absence of
correlation of Wiener increments, and the Cauchy—Schwarz inequality,
)

t n-1 2 n—1 tl
e(|/ Ws ds — S° Vjai(tj41 - 4) ) =E( > | (Vj41 — Ws) ds
° j=0 joo 74
n—l t+] 2
=S2(| [wa - woes] )
j=o NE
nol +d 5
<v {Gu | E(\Vj41 — Ws| pas}
j=0 qj

n—l 1 n—-1 1 /t 3
= SGu-m =D 5 (<) +0 asn—> oo.
j=0

j=0
Therefore,

t n—1 n-1

[saws = li Do (Wen — MD = ln, Daa Yins 9%) — Gar — 19 ¥)
n—1 t
= jim, (om, > VjaiG4i- “)) =1W; -[ W, ds.
(b) As n > oo,
n—-1

n-1
do VP Va — Vp) = 3 VA — VP — 3 41 — Yi)? — 41 — Yi}
j=0 j=0

a~l n—1
= 3h — Oi H+ YY — WO? Gar — DW} — FOG — VV
t
> swe — | W(s)ds +0 +0.
0

The fact that the last two terms tend to 0 in mean square may be verified in the usual way. For example,

n—l 2 n—-l
YVYja1- wi | ) = SE[(Vj41 - ¥)*]
=0 j=0

n

n—-1 -1 t 3
=6 t41—-t))? =6 ~)} -0 —> 00.
d(H i) > (<) as n

j

416
It6’s formula Solutions [13.8.2]-[13.9.1]

(c) It was shown in Exercise (13.7.3a) that fo W,; dW, = 3 W; _ dy. Hence,
t 2
e(| | Ws aw, | ) = i f{Ecw;) — 2E(W?) +17} _ 1,2,
0

and the result follows because E(W?) =f,

2. Fix t > Oandn > 1, and let 6 = t/n. We set Vi = Wit/n- It is the case that X; =
limy—oo >> 7; VjQj41 — tj). Each term in the sum is normally distributed, and all partial sums are
multivariate normal for all 6 > 0, and hence also in the limit as 6 > 0. Obviously E(X;) = 0. For
st,

t ps t ops
E(X; Xz) -| | E(W, Wy) du dv - | [ min{u, v}dudv
0 JO 0 JO

s 2 s 2 t AY
-[ 5u aut [ u(tt—u)du=s (5-3):

Hence var(X;) = Rt, and the autocovariance function is

s fl Ss
p(Xs, X7) =3/* €¢ _ =)

3. By the Cauchy—Schwarz inequality, as n —> oo,
E[{E(Xn | 9) — E(X | 9)}"] < E[E{(Xn — X)*|9}] =E[(Xn — X)71 > 0.

4. Wesquare the equation ||7 (v1 + ¥2)\l2 = Ilv1 + ¥2|| and use the fact that || 7 (4) |l2 = || ¥% || for
i = 1, 2, to deduce the result.

5. The question permits us to use the integrating factor eFt to give, formally,

t dW, t
Pt x, - | es ds = cP wy, -6 | eS W, ds
0 ds 0

on integrating by parts. This is the required result, and substitution verifies that it satisfies the given
equation.

6. Finda sequence @ = (6) of predictable step functions such that ||@ — y|| > 0asn > oo.
By the argument before equation (13.8.9), 1@™) as I(w) asn —> oo. By Lemma (13.8.4),
11@™)|l2 = ||@™ ||, and the claim follows.

13.9 Solutions. It6é’s formula

1. The process Z is continuous and adapted with Zg = 0. We have by Theorem (13.8.11) that
E(Z; — Zs | Fs) = 0, and by Exercise (13.8.6) that

t x2 y2
B((Z ~ Zs? |¥2) =B{ [ Met du) = 1s,
OR

The first claim follows by the Lévy characterization of a Wiener process (12.7.10).

417

[13.9.2]-[13.10.2] Solutions Diffusion processes

We have in n dimensions that R*? = X 2 + xs ++ X 2 and the same argument yields that
Z => fs (X;/R) dX; is a Wiener process. By Example (13.9.7) and the above,

n n
X;
d(R*) =2S- Xj dX; tndt=2R)) > dX; +ndt =2RdW +ndt.

i=] i=l]

2. Applying It6’s formula (13.9.4) to ¥; = W; we obtain dY; = 4W? dW; + 6W; dt. Hence,

t t t
Ba) = | aw? aWs) +2([ 6w? 4s) -6 | sds = 3t?.
0 0 0

3. Apply It6’s formula (13.9.4) to obtain dY; = W; dt +tdW;. Cf. Exercise (13.8.1).
4. Note that X; = cos W and X2 = sin W. By It6’s formula (13.9.4),

d¥ = d(X, +iX2) =dX) +idX2 = d(cos W) +i d(sin W)
= —sin WdW — 4 cos W dt +icos WdW — 5 sin Wdt.

5. We apply It6’s formula to obtain:
(a) 1 +t)dX =-Xdt+dw,

(b) dX =-}Xdt+V/1- X?dw,
xX _iil xX 0 —-a/b xX
oa(F)=-3(P)ae (na) (Paw.

13.10 Solutions. Option pricing

1. (a) We have that

E((ae” —K)t)= [ (ae — — exp _& mh dz
log(K/a) J In t2 2
1.2
oo ~7y _ 1 K —
-| (eV — K)o = dy where y= 22”, g = BRM -Y
a V20 T T
1 2
1.2 oo e72Z07-7)
=ae’tit [ —_- dy —- K ®(-a
A Vin y (—a@)

12
= ae’t2™ @(t — a) — K®(-a).

(b) We have that Sr = ae“ where a = S; and, under the relevant conditional Q-distribution, Z is
normal with mean y = (r — 50°)(T — t) and variance r= o2(T — t). The claim now follows by
the result of part (a).

2. (a) Set E(t, S) = E(t, S) and w(t, S) = w(C, S;), in the natural notation. By Theorem (13.10.15),
we have wy = Wr = 0, whence y(t, x) = c for all t, x, and some constant c.
(b) Recall that dS = nS dt +a0SdW. Now,

d(ES + wel") = d(S* + we) = (0 S)* dt +28 dS +e" dw + wre" dt,

418

Option pricing Solutions [13.10.3]-[13.10.5]

by Example (13.9.7). By equation (13.10.4), the portfolio is self-financing if this equals SdS +
wre't dt, and thus we arrive at the SDE e”! dy = —SdS — 2S? dt, whence

t t
v(t, S) --| eS, dSy -o? | e "$2 du.
0 0

(c) Note first that Z,; = fo S, du satisfies dZ,; = S,; dt. By Example (13.9.8), d(S;Z;) = Z; dS; +
Ss? dt, whence
d(ES + wel) = Z, dS, 4+ SP dt+e™dw+re™ dt.

Using equation (13.10.4), the portfolio is self-financing if this equals Z; dS; + wre” dt, and thus we
require that e”’ dy = —S? dt, which is to say that

t
wt, s=- e™ $2 du,

3. We need to check equation (13.10.4) remembering that dM; = 0. Each of these portfolios is
self-financing.

(a) This case is obvious.

(b) dES + w) =d(2S? — $* — 1) = 28 dS + dt —dt =EdS.
(c) dES+ hw) =—-S—tdS+S$=édS.

(d) Recalling Example (13.9.8), we have that

t t t
ags+y=a(s [ S.ds— [ Stas) = Spar+as; [ Ss ds — S? dt =€dS;.
0 0 0

4. The time of exercise of an American call option must be a stopping time for the filtration (¥;).
The value of the option, if exercised at the stopping time t, is V; = (S; — K)T, and it follows by
the usual argument that the value at time 0 of the option exercised at t is Eg(e~”* Vr). Thus the
value at time 0 of the American option is sup, {Eg(e~"" Vz)}, where the supremum is taken over all
stopping times t satisfying P(t < T) = 1. Under the probability measure Q, the process e—”! V; is
a martingale, whence, by the optional stopping theorem, Eg(e~’* Vr} = Vo for all stopping times T.
The claim follows.

5. We rewrite the value at time 0 of the European call option, possibly with the aid of Exercise
(13.10.1), as

eTE( (Soexp{rT - 50°T +aVTN} -K)*) = E((Soexp{—30?T +o-VTN} —Ke~!?)*),

where N is an N(O, 1) random variable. It is immediate that this is increasing in Sp and r and is
decreasing in K. To show monotonicity in T, we argue as follows. Let T; < T> and consider the
European option with exercise date 7. In the corresponding American option we are allowed to
exercise the option at the earlier time T,. By Exercise (13.10.4), it is never better to stop earlier than
Tp, and the claim follows.

Monotonicity in ao may be shown by differentiation.

419

[13.11.1]-[13.12.2] Solutions Diffusion processes

13.11 Solutions. Passage probabilities and potentials

1. Let H beaclosed sphere with radius R (> |w|), and define pr(r) = P(G before H ||W(0)| =r).
Then pp satisfies Laplace’s equation in IR¢, and hence

d
a -d-19PR -~0
dr dr

since pr is spherically symmetric. Solve subject to the boundary equations pr(¢) = 1, pr(R) = 0,

to obtain
72-4 _ R2-d

d-—
PRO) = 3-a_ pid (e/r) 2

as R > oo.

2. The electrical resistance R, between 0 and the set A, is no smaller than the resistance obtained
by, for every i = 1,2,..., ‘shorting out’ all vertices in the set A;. This new network amounts to a
linear chain of resistances in series, points labelled Aj and Aj, being joined by a resistance if size

N;"!. It follows that
a
RG) = lim, Rn 2 » Ni

By Theorem (13.11.18), the walk is persistent if 57; Ny} = 00.

3. Thinking of G as an electrical network, one may obtain the network H by replacing the resistance
of every edge e lying in G but not in H by oo. Let 0 be a vertex of H. By a well known fact in the
theory of electrical networks, R(H) > R(G), and the result follows by Theorem (13.11.19).

13.12 Solutions to problems

1 (@Ta)=aWe/ a2) has continuous sample paths with stationary independent increments, since
W has these properties. Also T (t)/a is N(0, t/a”), whence T(t) is N(0, t).

(b) As for part (a).

(c) Certainly V has continuous sample paths on (0, oo). For continuity at 0 it suffices to prove that
twa) — Oas. ast | 0; this was done in the solution to Exercise (13.6.4).

If (u,v), (s,f) are disjoint time-intervals, then so are (ol, uy, a}, sh; since W has
independent increments, so has V. Finally,

Vis +2) — V(s) =tW((s +2)7!) — s{ Ws!) — Ws + 1)7})}

is N(O, 8) ifs, t > 0, where

2
t 1 1
p= +8(2- jae
s+t RY s+t

2. Certainly W is Gaussian with continuous sample paths and zero means, and itis therefore sufficient
to prove that cov(W(s), W(t)) = min{s, t}. Now, ifs < t,

_ cov(Xr7'(s)), X71) — ue TO)
cov(W(s), WO) = Tee) ve) TO OS

as required.

420

Problems Solutions [13.12.3]-[13.12.4]

If u(s) = s, v(t) = 1—¢, thenr(t) =t/(1 — 8), andr7!(w) = w/(1 + w) for 0 < w < oo. In
this case X(t) = (1 —1)Wet/d —2)).

3. Certainly U is Gaussian with zero means, and U(0) = 0. Now, with s; = e2ht _ 1,

E{UG +h) |UQ) =u} =e FOR Ws 44) | WO) = we}
= ue PUT) Bt — y — Buh + o(h),

whence the instantaneous mean of U is a(t, u) = —Bu. Secondly, 5,44 = s¢ + 2Berht h + o(h), and
therefore

E{UG +h)? | Ut) =u} =e POR Ws, 44)* | Wor) = ue}
= e BUF) (4207! 4 2B 67Pth + o(h))
= u* — 2Bh(u? — 1) + o(h).

It follows that

E{\U@ +h) — U@/? | UG) =u} =u? — 2Bh(u? — 1) — 2u(u — Buh) + v? + o(h)
= 28h + o(h),

and the instantaneous variance is b(t, u) = 26.

4. Bartlett’s equation (see Exercise (13.3.4)) for M(t, 0) = E(e9") is

OM _

aM
—— = — $9 ——
ot B

1242
with boundary condition M(6, 0) = e, Solve this equation (as in the exercise given) to obtain

1 2
M(t, @) = exp {ee + 50° . x _ |

the moment generating function of the given normal distribution. Now M(t, @) > exp{ 50707 /(2B)}
as tf — oo, whence by the continuity theorem V(t) converges in distribution to the N(0, 507/ B)
distribution.

If V (0) has this limit distribution, then so does V(t) for all +. Therefore the sequence (V(f1), ...,
V (tn)) has the same joint distribution as (V(t, +A),...,W(tn +)) for all h, thy..., ta, whenever
V (0) has this normal distribution.

In the stationary case, E(V(#)) = 0 and, for s < f,

cov(V(s), V(t) =E{V(s)E(V(t) | V(s))} =E{V(s)2e7F E91 = c@e AIFS

where c(0) = var(V(s)); we have used the first part here. This is the autocovariance function
of a stationary Gaussian Markov process (see Example (9.6.10)). Since all such processes have
autocovariance functions of this form (1.e., for some choice of 8), all such processes are stationary
Ornstein—Uhlenbeck processes.

The autocorrelation function is p(s) = e ®lsl, which is the characteristic function of the Cauchy

density function
1

Ba{i + (@/B)?}"
421

f@)= xeER

(13.12.5]-[13.12.7] Solutions Diffusion processes

5. Bartlett’s equation (see Exercise (13.3.2)) for M is

aM _ 2am,
ai 0
“Ot + 2B

subject to M(0, 0) = e°4. The characteristics satisfy

dM dt 2d0

0 1 200+ pe2”

The solution is M = g(6e% /(a + 5 B@)) where g is a function satisfying g(@/(a + $B0)) = 4
The solution follows as given.
By elementary calculations,

aM
E(D(t)) = = de™,
a)
a2M d
E(D(t)?) = —— = Ba oat (gat — 1) 4+d%er",
362 go |

whence var(D(t)) = (Bd/a)e™ (e! — 1). Finally

at
P(D(@®) = 0) =, lim M(, 6) = exp eer dade \

BC — e**)

which converges to e—2#4/8 as t + 00.

6. The equilibrium density function g(y) satisfies the (reduced) forward equation

d 1 d?
(*) ~ dy 08? +> 2 dy = (bg) =
where a(y) = —fy and b(y) = o” are the instantaneous mean and variance. The boundary conditions
are 4
1,248
z0°— =0, =-c,d.
Byg + 50 dy y c
Integrate («) from —c to y, using the boundary conditions, to obtain
d
byg + 40? =0, -ce<y<d.
dy

Integrate again to obtain g(y) = Ae—F */ oF The constant A is given by the fact that fe 8) dy =1.

7. First we show that the series converges uniformly (along a subsequence), implying that the limit
exists and is a continuous function of t. Set

mal sin(kt)
Zmn(t) = > k Xk, Mmn = sup{|Zmn(t)| :O0<t<z}.

k=m
We have that
n—1 ike —1 y2 —m—1)n—Il— lyy id
© Why sap | Sy) ST aa SS
O<t<mlgom k=m i=l | jam IG+D

422

Problems Solutions [13.12.8]-[13.12.8]

The mean value of the final term is, by the Cauchy—Schwarz inequality, no larger than

Combine this with (*) to obtain

E(Myn,2m)> < E(M2. om) <

—
Te

It follows that
oo

oo
6
n=] n=1
implying that 77°, Mon-1 9n < 00 a.8. Therefore the series which defines W converges uniformly
with probability 1 (along a subsequence), and hence W has (a.s.) continuous sample paths.

Certainly W is a Gaussian process since W(t) is the sum of normal variables (see Problem
(7.11.19)). Furthermore E(W (t)) = 0, and

oo. .
cov(W(s), W(t) = 2 4+ 2 singh) sink
™ a k=1

since the X; are independent with zero means and unit variances. It is an exercise in Fourier analysis
to deduce that cov(W(s), W(t)) = min{s, f}.

8. We wish to find a solution g(t, y) to the equation
dg 17

naan 3? b,
(*) at 2 ay? lyl <

satisfying the boundary conditions
8O,y)=s0 flylsb, g@.y)=0 ifly|=d.

Let g(t, y | d) be the N(d, ft) density function, and note that g(-,- | d) satisfies («) for any
‘source’ d. Let

oe)
st. yy= S> Ck gee, y | 2kb),
k=—00

a series which converges absolutely and is differentiable term by term. Since each summand satisfies
(*), so does the sum. Now g(0, y) is a combination of Dirac delta functions, one at each multiple of 2b.
Only one such multiple lies in [—b, b], and hence g(y, 0) = dg. Also, setting y = b, the contributions
from the sources at —2(k — 1)b and 2kb cancel, so that g(t, b) = 0. Similarly g(t, —b) = 0, and
therefore g is the required solution.

Here is an alternative method. Look for the solution to («) of the form e~*"" sin{ snr (y + b)/b};

such a sine function vanishes when |y| = b. Substitute into («) to obtain Ay, = n272 / (8b2). A linear
combination of such functions has the form

~w
b
gt, y= Xu ane?" sin (=aS*) .

423

[13.12.9]-[13.12.11] Solutions Diffusion processes

We choose the constants a, such that g(0, y) = dy9 for |y| < b. With the aid of a little Fourier
analysis, one finds that ay, = bo} sin(5 nz).

Finally, the required probability equals the probability that W* has been absorbed by time ¢, a
probability expressible as 1 — f°, f?(t, y) dy. Using the second expression for f?, this yields

421 .
-5 —e7 nt! sin} (Sn).
Fa n

n=]

9. Recall that U(t) = e~2"?© isa martingale. Let T be the time of absorption, and assume that
the conditions of the optional stopping theorem are satisfied. Then E(U(0)) = E(U(T)), which is to
say that 1 = e2"4 py + e~2™(] — pa).

10. (a) We may assume that a, b > 0. With
pr(b) = P(W(t) > b, F(O,t)| WO) =a),
we have by the reflection principle that
pr(b) = P(W(t) > b| W(0) = a) — P(Wit) < —b| WO) =a)
= P(b-—a< W(t) <b+a|W(0) =0),

giving that

Opr(b

PO) = f(t,b +a) ~ f(t,b—a)

where f(t, x) is the N(0, t) density function. Now, using conditional probabilities,

1 Opr(b) _
fit,b-—a) 0b

P(FQ, 1) |W) =a, W(t) =b) =- 1—¢7 abit.

(b) We know that

P(F(s,t)) =1—- = cos!{ s/t} = = sin! { /s7F}

if 0 < s <t, The claim follows since F (fg, t2) © F(t, t).
(c) Remember that sinx = x + o(x) as x | 0. Take the limit in part (b) as fg | 0 to obtain /t; /to.

11. Let M(t) = sup{W(s) : 0 < s < t} and recall that M(t) has the same distribution as |W(¢)|. By
symmetry,

P( sup |W(s)| > w) < 2P(M(t) > w) = 2P(|W()| > w).

O<s<t

By Chebyshov’s inequality,

E(W(t)?
P(|W(t)| = w) < —S = “5.

Fix € > 0, and let
An(€) = {|W(s)|/s > € for some s satisfying Wles< an}.

Note that
Ano ¢ { sup wena} cf sup woo >a |

an-les<Qn O<s<2"

424
Problems Solutions [13.12.12]-[13.12.13]

for all large n, and also

= 2n/3 ont
SP sup |W(s)| > 22”/ <)) Gma < ©
nt

Therefore >, P(An(€)) < 00, implying by the Borel-Cantelli lemma that (a.s.) only finitely many of
the A, (€) occur. Therefore to} W(t) > 0a. ast — oo. Compare with the solution to the relevant
part of Exercise (13.6.4).

12. We require the solution to Laplace’s equation V2p =0, subject to the boundary condition

0 ifwedH,

pow) = {4 ifweG.

Look for a solution in polar coordinates of the form

io, ¢)

pr, 8) = S- r" {an sin(nO) + by cos(nd) }.

n=0

Certainly combinations having this form satisfy Laplace’s equation, and the boundary condition gives
that

[o.<)
(x) H(O) = by + 9— {an sin(nO) + by cos(nd)}, |0| < x,
n=l
where 0 if 6<0
Ho) ={ ve
1 ifO0<6<z.

The collection {sin(m6@), cos(m@) : m > 0} are orthogonal over (—z, 7). Multiply through («) by
sin(m@) and integrate over (—7z, 7) to obtain 7a, = {1 — cos(z7m)}/m, and similarly bo = 5 and

bm =O form > 1.
13. The joint density function of two independent N (0, t) random variables is (2nt)7! exp{—(x” +
y’) /(2t)}. Since this function is unchanged by rotations of the plane, it follows that the two coordi-

nates of the particle’s position are independent Wiener processes, regardless of the orientation of the
coordinate system. We may thus assume that / is the line x = d for some fixed positive d.

The particle is bound to visit the line / sooner or later, since P(W,(t) < d for allt) = 0. The
first-passage time T has density function

__ 4 #21)
Frit) = =e , t>0.

Conditional on {T = t}, D = W2(T) is N(O, t). Therefore the density function of D is

fou) = [ » fpyr(u | t)fr(t)dt = [ 8d -w+@ 2g, 4 yer
0 | 0. 2art2 n(u2 + d2)’ ,

giving that D/d has the Cauchy distribution.
The angle © = POR satisfies 9 = tan—!(D/d), whence

)
P(® <0) =P(D <dtand) =-+-, |O| < 42.
oie

1
2
425

[(13.12.14]-[13.12.18] Solutions Diffusion processes

14. By an extension of It6’s formula to functions of two Wiener processes, U = u(W ,, W2) and
V = v(W,, W2) satisfy

dU = uz dW, + uy dW + f(uxx + Wyy) dt,

dV =v, dW, + vy dW + Axx + vyy) dt,
where ux, Vyy, etc, denote partial derivatives of u and v. Since ¢ is analytic, u and v satisfy the

Cauchy—Riemann equations ux = vy, 4 = —vx, whence u and v are harmonic in that uy + Uyy =
Uxx + Vyy = 0. Therefore,

dU =uydW,+uydW2, dV =—uydW, +uxydWyo.

. u Uu
The matrix x Y
—u y uy

of the pair (W,, W2) is invariant under such rotations, the claim follows.

) is an orthogonal rotation of IR? when uz +u5 = 1. Since the joint distribution

15. One method of solution uses the fact that the reversed Wiener process {W(t—s)-W(t):O0<s<
t} has the same distribution as {W(s) : 0 < s <t}. Thus M(t)— W(t) = maxg<s<;{W(s)— W(e)} has
the same distribution as maxp<,<;{W(u) — W(0)} = M(t). Alternatively, by the reflection principle,

P(M(t) > x, W(t) < y) =P(W(t) = 2x —y) forx > max{O, y}.

By differentiation, the pair M(t), W(t) has joint density function —2¢’(2x — y) for y < x, x > 0,
where @ is the density function of the N(0, t) distribution. Hence M(t) and M(t) — W(t) have the
joint density function —2¢’(x + y). Since this function is symmetric in its arguments, M(t) and
M(t) — W(t) have the same marginal distribution.

16. The Lebesgue measure A(Z) is given by

io. ¢)
A(Z) = [ Tiwot)=u} 24,
whence by Fubini’s theorem (cf. equation (5.6.13)),

E(A(Z)) = [ P(W(t) =u) dt =0.
0

17. LetO0 <a <b <c <d, and let M(x, y) = maxy<s<y W(s). Then

M c,d) — M(a, b) = max { W(s) — Wec)} + {We) — Woe)} - max, { W(s) — Wob)}.

Since the three terms on the right are independent and continuous random variables, it follows that
P((M (c,d) = Ma, b)) = 0. Since there are only countably many rationals, we deduce that
P((M¢c, d) = Mca, b) for all rationals a < b < c < d) = 1, and the result follows.

18. The result is easily seen by exhaustion to be true when n = 1. Suppose it is true for allm <n—1
where n > 2.

(i) If sy < 0, then (whatever the final term of the permutation) the number of positive partial sums and
the position of the first maximum depend only on the remaining n — 1 terms. Equality follows by the
induction hypothesis.

Gi) If s, > 0, then
nt
A; = S- A,-1(k),
k=1

426

Problems Solutions [13.12.19]-[13.12.20]

where A; _1 (k) is the number of permutations with x; in the final place, for which exactly r — 1 of the
first n ~ 1 terms are strictly positive. Consider a permutation 7 = (Xj, , Xin, ---,Xig_1> Xk) With xy in
the final place, and move the position of x; to obtain the new permutation x’ = (xz, Kips Xins ++ +s Xin)

The first appearance of the maximum in 7’ is at its rth place if and only if the first maximum of the
reduced permutation (xj, , xj,,---,Xi,_1) is at its (r — 1)th place. [Note that r = 0 is impossible
since s, > 0.] It follows that

n
By =) By-1®)
k=1

where B,_1(k) is the number of permutations with x, in the final place, for which the first appearance
of the maximum is at the (r — 1)th place.

By the induction hypothesis, A,_;(k) = B,— (k), since these quantities depend on the n — 1
terms excluding x,;. The result follows.
19. Suppose that Sm = viel X;, 0 < m < ni, are the partial sums of n independent identically
distributed random variables X;. Let An be the number of strictly positive partial sums, and Ry, the
index of the first appearance of the value of the maximal partial sum. Each of the n! permutations
of (X,, X2,..., Xn) has the same joint distribution. Consider the kth permutation, and let J; be
the indicator function of the event that exactly 7 partial sums are positive, and let Jz be the indicator
function that the first appearance of the maximum is at the rth place. Then, using Problem (13.12.18),

1 ni 1 ni
P(An =r) = mi 2 Pele) = ni 2 Pe) =P(Rn =r).

We apply this with X; = W(jt/n) — W(j — 1t/n), so that Sy, = W(mt/n). Thus A, =
ay Tw (jt/n)>0} has the same distribution as

Rn = min{k > 0: W(kt/n) = max W(jt/n)}.
<j<n

By Problem (13.12.17), Rn #4; Rasn — oo. By Problem (13.12.16), the time spent by W at zero

is a.s. anull set, whence A, 25; A. Hence A and R have the same distribution. We argue as follows
to obtain that that L and R have the same distribution. Making repeated use of Theorem (13.4.6) and
the symmetry of W,

PIL <x)= P( sup Ws) < 0) +P(_ inf, Wis) > 0)
SSS a

= 2P( sup {W(s) — W(x} < -W()) = 2P(|W(t) — W(x)| < WG)

XS <t

= P(|W(t) — W)| < |WO@)l)
= P( sup (W(s) — WO)) < sup (WO) — WG)}) = F(R sv).

XSSSt O<s<x

Finally, by Problem (13.12.15) and the circular symmetry of the joint density distribution of two
independent N(0, 1) variables U, V,

P(IWa) — WO) < |W@)|) = P(x) V2 <xU*) =P (aon

20. Let

r= { inf{¢ <1: W(t) =x} if this set is non-empty,
Y=

1 otherwise,

427

[13.12.21]-[13.12.24] Solutions Diffusion processes

and similarly V; = sup{t < 1: W(t) = x}, with V, = 1 if Wt) # x for all t € [0, 1]. Recall that
Up and Vo have an arc sine distribution as in Problem (13.12.19). On the event {U, < 1}, we may
write (using the re-scaling property of W)

Uy =T, +(1—Tx)Uo, Vx =Tx +(1— T)Vo,

where Uo and Vo are independent of U, and V;, and have the above arc sine distribution. Hence U,
and V; have the same distribution. Now T, has the first passage distribution of Theorem (13.4.5),
whence

x x 2 1
Tr, fig 0 = (yaa a (-z)] (aca }
Therefore,

u—t 1
fT,,U, (t, 4) = Fr. (« 3) ‘Top

and

uu 1 x2
fue = [ STU, u) du = aay OP On : O<x <1.

21. Note that V is a martingale, by Theorem (13.8.11). Fix ¢ and let ws = sign(Ws), O< 5s <t.
We have that ||yv|| = ./t, implying by Exercise (13.8.6) that E(V7) = || (W)|I3 = ¢. By a similar
calculation, E(VZ | Fs) = v2 +t—sfor0 <s <t. Thatis to say, v? — t defines a martingale, and
the result follows by the Lévy characterization theorem of Example (12.7.10).

22. The mean cost per unit time is

T T
u(T) = 7{R + c | P(IW(H)| = a) ar} = z{R +2¢ | (1 — &(a/¥/2)) ar}.
0 0

Differentiate to obtain that ’(T) = 0 if
T T
R=2C { ®(a/Jt) dt — rowiv| = ac | t | 6(a/V0) dt,
0 0

where we have integrated by parts.

23. Consider the portfolio with &(¢, S;) units of stock and y(t, $+) units of bond, having total value
w(t, St) = x&(t, x) + e”* w(t, St). By assumption,

(x) (1— y)x&@, x) = ye W(t, x).

Differentiate this equation with respect to x and substitute from equation (13.10.16) to obtain the
differential equation (1 — y)& + x&, =, with solution &(¢, x) = h(t)x”—!, for some function A(t).
We substitute this, together with («), into equation (13.10.17) to obtain that

h' —hQ—y)(4yo? +r) =0.

It follows that h(t) = A exp{(i — yy yo2 +r)t}, where A is an absolute constant to be determined
according to the size of the initial investment. Finally, w(t, x) = yl xé(t, xy= y Ax’.
24. Using Ité’s formula (13.9.4), the drift term in the SDE for U; is

(—w1(T —t, W) + 3u22(T — 1, W)) dt,

where wu and w22 denote partial derivatives of u. The drift function is identically zero if and only if
uy = 549.

428

Bibliography

A man will turn over half a library to make one book. Samuel Johnson

Abramowitz, M. and Stegun, L. A. (1965). Handbook of mathematical functions with formulas,
graphs and mathematical tables. Dover, New York.

Billingsley, P. (1995). Probability and measure (3rd edn). Wiley, New York.

Breiman, L. (1968). Probability. Addison-Wesley, Reading, MA, reprinted by SIAM, 1992,

Chung, K. L. (1974). A course in probability theory (2nd edn). Academic Press, New York.

Cox, D. R. and Miller, H. D. (1965). The theory of stochastic processes. Chapman and Hall,
London.

Doob, J. L. (1953). Stochastic processes. Wiley, New York.

Feller, W. (1968). An introduction to probability theory and its applications, Vol. 1 (3rd edn).
Wiley, New York.

Feller, W. (1971). An introduction to probability theory and its applications, Vol. 2 (2nd edn).
Wiley, New York.

Grimmett, G. R. and Stirzaker, D. R. 2001). Probability and random processes, 3rd edn).
Oxford University Press, Oxford.

Grimmett, G. R. and Welsh, D. J. A. (1986). Probability, an introduction. Clarendon Press,
Oxford.

Hall, M. (1983). Combinatorial theory (2nd edn). Wiley, New York.

Harris, T. E. (1963). The theory of branching processes. Springer, Berlin.

Karlin, S. and Taylor, H. M. (1975). A first course in stochastic processes (2nd edn). Academic
Press, New York.

Karlin, S. and Taylor, H. M. (1981). A second course in stochastic processes. Academic
Press, New York.

Laha, R. G. and Rohatgi, V. K. (1979). Probability theory. Wiley, New York.

Loéve, M. (1977). Probability theory, Vol. 1 (4th edn). Springer, Berlin.

Loéve, M. (1978). Probability theory, Vol. 2 (4th edn). Springer, Berlin.

Moran, P. A. P. (1968). An introduction to probability theory. Clarendon Press, Oxford.

Stirzaker, D. R. (1994). Elementary probability. Cambridge University Press, Cambridge.

Stirzaker, D. R. (1999). Probability and random variables. Cambridge University Press,
Cambridge.

Williams, D. (1991). Probability with martingales. Cambridge University Press, Cambridge.

429

Index

Abbreviations used in this index: c.f. characteristic function; distn distribution; eqn equation;
fn function; m.g.f. moment generating function; p.g.f. probability generating function; pr.
process; r.v. random variable; r.w. random walk; s.r.w. simple random walk; thm theorem.

A
absolute value of s.r.w. 6.1.3

absorbing barriers: s.r.w. 1.7.3,
3.9.1, 5, 3.11.23, 25-26,
12.5.4-5, 7; Wiener pr.
12.9.22-3, 13.12.8-9

absorbing state 6.2.1
adapted process 13.8.6

affine transformation 4.13.11;
4.14.60

age-dependent branching pr.
5.5.1+2; conditional 5.1.2;
honest martingale 12.9.2; mean
10.6.13

age, see current life

airlines 1.8.39, 2.7.7

alarm clock 6.15.21

algorithm 3.11.33, 4.14.63, 6.14.2
aliasing method 4.11.6

alternating renewal pr. 10.5.2,
10.6.14

American call option 13.10.4

analytic fn 13.12.14

ancestors in common 5.4.2

anomalous numbers 3.6.7

Anscombe’s theorem 7.11.28

antithetic variable 4.11.11

ants 6.15.41

arbitrage 3.3.7, 6.6.3

Arbuthnot, J. 3.11.22

arc sine distn 4.11.13; sample
from 4.11.13

arc sine law density 4.1.1

arc sine laws for r.w.: maxima
3.11.28; sojourns 5.3.5; visits
3.10.3

arc sine laws for Wiener pr.
13.4.3, 13.12.10, 13.12.19

Archimedes’s theorem 4.11.32
arithmetic rv. 5.9.4
attraction 1.8.29

autocorrelation function 9.3.3,
9.7.5, 8

autocovariance function 9.3.3,
9.5.2, 9.7.6, 19-20, 22

autoregressive sequence 8.7.2,
9.1.1, 9.2.1, 9.7.3

average, see moving average

B
babies 5.10.2
backward martingale 12.7.3
bagged balls 7.11.27, 12.9.13~14
balance equations 11.7.13
Bandrika 1.8.35-36, 4.2.3
bankruptcy, see gambler’s ruin
Barker’s algorithm 6.14.2

barriers: absorbing/retaining in
rw. 1.7.3, 3.9.1-2, 3.11.23,
25-26; hitting by Wiener pr.
13.4.2

Bartlett: equation 13.3.2—4;
theorem 8.7.6, 11.7.1

batch service 11.8.4

baulking 8.4.4, 11.8.2, 19
Bayes’s formula 1.8.14, 1.8.36
bears 6.13.1, 10.6.19
Benford’s distn 3.6.7
Berkson’s fallacy 3.11.37

Bernoulli: Daniel 3.3.4, 3.4.3—4;
Nicholas 3.3.4

430

Bernoulli: model 6.15.36;
renewal 8.7.3; shift 9.17.14;
sum of rv.s 3.11.14, 35

Bertrand’s paradox 4.14.8

Bessel: function 5.8.5, 11.8.5,
11.8.16; B. pr. 12.9.23,
13.3.5-6, 13.9.1

best predictor 7.9.1; linear 7.9.3,
9.2.1-2, 9.7.1, 3

beta fn 4.4.2, 4.10.6

beta distn: b.—binomial 4.6.5;
sample from 4.11.4—-5

betting scheme 6.6.3

binary: expansion 9.1.2; fission
5.5.1

binary tree 5.12.38; rw. on 6.4.7

binomial rv. 2.1.3; sum of 3.11.8,
11

birth process 6.8.6; dishonest
6.8.7; forward eqns
6.8.4; divergent 6.8.7;
with immigration 6.8.5;
non-homogeneous 6.15.24;
see also simple birth

birth—death process: coupled
6.15.46; extinction 6.11.3,

5, 6.15.25, 12.9.10; honest
6.15.26; immigration—death
6.11.3, 6.13.18, 28; jump chain
6.11.1; martingale 12.9.10;
queue 8.7.4; reversible 6.5.1,
6.15.16; symmetric 6.15.27;
see also simple birth-death

birthdays 1.8.30

bivariate: Markov chain 6.15.4;
negative binomial distn
5.12.16; p.g-f. 5.1.3

bivariate normal distn 4.7.5—6,
12, 4.9.4.5, 4.14.13, 16, 7.9.2,

Index

7.11.19; cf. 5.8.11; positive
part 4.7.5, 4.8.8, 5.9.8

Black-Scholes: model 13.12.23;
value 13.10.5

Bonferroni’s inequality 1.8.37

books 2.7.15

Boole’s inequalities 1.8.11

Borel: normal number theorem
9.7.14; paradox 4.6.1

Borel-Cantelli lemmas 7.6.1,
13.12.11

bounded convergence 12.1.5

bow tie 6.4.11

Box-Muller normals 4.11.7

branching process: age-dependent
5.5.1+2, 10.6.13; ancestors
5.4.2; conditioned 5.12.21,
6.7.1-4; convergence, 12.9.8;
correlation 5.4.1; critical
7.10.1; extinction 5.4.3;
geometric 5.4.3, 5.4.6;
imbedded in queue 11.3.2,
11.7.5, 11; with immigration
5.4.5, 7.7.2; inequality 5.12.12;
martingale 12.1.3, 9, 12.9.1-2,
8; maximum of 12.9.20;
moments 5.4.1; p.g.f. 5.4.4;
supercritical 6.7.2; total
population 5.12.11; variance
5.12.9; visits 5.4.6

bridge 1.8.32

Brownian bridge 9.7.22, 13.6.2—5;
autocovariance 9.7.22, 13.6.2;
zeros of 13.6.5

Brownian motion; geometric
13.3.9; tied-down, see
Brownian bridge

Buffon: cross 4.5.3; needle 4.5.2,
4.14.31~32; noodle 4.14.31

busy period 6.12.1; in G/G/1
11.5.1; in M/G/1 11.3.3; in
M/M/1 11.3.2, 11.8.5; in
M/M/oo 11.8.9

Cc

cake, hot 3.11.32

call option: American 13.10.4;
European 13.10.4—5

Campbell—-Hardy theorem 6.13.2

capture-recapture 3.5.4

car, parking 4.14.30

cards 1.7.2, 5, 1.8.33

Carroll, Lewis 1.4.4

casino 3.9.6, 7.7.4, 12.9.16

Cauchy convergence 7.3.1; in
m.s. 7.11.11

Cauchy distn 4.4.4; maximum
7.11.14; moments 4.4.4;

sample from 4.11.9; sum 4.8.2,
5.11.4, 5.12.24—25

Cauchy—Schwarz inequality
4.14.27

central limit theorem 5.10.1, 3, 9,
5.12.33, 40, 7.11.26, 10.6.3

characteristic function
5.12.26—31; bivariate normal
5.8.11; continuity theorem
5.12.39; exponential distn
5.8.8; extreme-value distn
5.12.27; first passage distn
§.10.7—8; joint 5.12.30; law
of large numbers 7.11.15;
multinormal distn 5.8.6; tails
5.7.6; weak law 7.11.15

Chebyshov’s inequality, one-sided
TALS

cherries 1.8.22

chess 6.6.6—7

chimeras 3.11.36

chi-squared distn: non-central
5.7.7; sum 4.10.1, 4.14.12

Cholesky decomposition 4.14.62

chromatic number 12.2.2

coins: double 1.4.3; fair 1.3.2;
first head 1.3.2, 1.8.2, 2.7.1;
patterns 1.3.2, 5.2.6, 5.12.2,

10.6.17, 12.9.16; transitive
2.7.16; see Poisson flips
colouring: graph 12.2.2; sphere
1.8.28; theorem 6.15.39
competition lemma 6.13.8
complete convergence 7.3.7
complex-valued process 9.7.8
compound: Poisson pr. 6.15.21;
Poisson distn 3.8.6, 5.12.13
compounding 5.2.3, 5.2.8
computer queue 6.9.3
concave fn 6.15.37
conditional: birth—death pr.
6.11.4—5; branching pr.
5.12.21, 6.7.1-4; convergence
13.8.3; correlation 9.7.21;
entropy 6.15.45; expectation
3.7.2-3, 4.6.2, 4.14.13, 7.9.4;
independence 1.5.5; probability
1.8.9; s..w. 3.9.2+3; variance
3.7.4, 4.6.7; Wiener pr.
8.5.2, 9.7.21, 13.6.1; see also
regression
continuity of: distn fns 2.7.10;
marginals 4.5.1; probability
measures 1.8.16, 1.8.18;
transition probabilities 6.15.14

continuity theorem 5.12.35, 39

431

continuous r.v.: independence
4.5.5, 4.14.6; limits of discrete
rv.s 2.3.1

convergence: bounded 12.1.5;
Cauchy 7.3.1, 7.11.11;
complete 7.3.7; conditional
13.8.3; in distn 7.2.4, 7.11.8,
16, 24; dominated 5.6.3, 7.2.2;
event of 7.2.6; martingale
7.8.3, 12.1.5, 12.9.6; Poisson
pr. 7.11.5; in probability 7.2.8,
7.11.15; subsequence 7.11.25;
in total variation 7.2.9

convex: fn 5.6.1, 12.1.6—7; rock
4.14.47; shape 4.13.2-3,
4.14.61

corkscrew 8.4.5

Corn Flakes 1.3.4, 1.8.13

countable additivity 1.8.18

counters 10.6.6—8, 15

coupling: birth—-death pr. 6.15.46;
maximal 4.12.4—6, 7.11.16

coupons 3.3.2, 5.2.9, 5.12.34

covariance: matrix 3.11.15, 7.9.3;
of Poisson pr. 7.11.5

Cox process 6.15.22

Cp inequality

Cramér—Wold device 7.11.19,
5.8.11

criterion: irreducibility 6.15.15;
Kolmogorov’s 6.5.2; for
persistence 6.4.10

Crofton’s method 4.13.9

crudely stationary 8.2.3

cube: point in 7.11.22; rw. on
6.3.4

cumulants 5.7.3-4

cups and saucers 1.3.3

current life 10.5.4; and excess
10.6.9; limit 10.6.4; Markov
10.3.2; Poisson 10.5.4

D
dam 6.4.3
dead period 10.6.6—7
death-immigration pr. 6.11.3
decay 5.12.48, 6.4.8
decimal expansion 3.1.4, 7.11.4
decomposition: Cholesky 4.14.62;
Krickeberg 12.9.1]
degrees of freedom 5.7.7-8
delayed renewal pr. 10.6.12
de Moivre: martingale 12.1.4,
12.4.6; trial 3.5.1
De Morgan laws 1.2.1

density: arc sine 4.11.13; arc
sine law 4.1.1; betsa 4.11.4,

4.14.11, 19, 5.8.3; bivariate
normal 4.7.5-6, 12, 4.9.4—-5,
4.14.13, 16, 7.9.2, 7.11.19;
Cauchy 4.4.4, 4.8.2, 4.10.3,
4.14.4, 16, 5.7.1, 5.11.4,
5.12.19, 24-25, 7.11.14;
chi-squared 4.10.1, 4.14.12,
5.7.7; Dirichlet 4.14.58;
exponential 4.4.3, 4.5.5, 4.7.2,
4.8.1, 4.10.4, 4.14.4-5, 17-19,
24, 33, 5.12.32, 39, 6.7.1;
extreme-value 4.1.1, 4.14.46,
7.11.13; F(r,s) 4.10.2, 4,
5.7.8; first passage 5.10.78,
5.12.18-19; Fisher’s spherical
4.14.36; gamma 4.14.10-12,
5.8.3, 5.9.3, 5.10.3, 5.12.14,
33; hypoexponential 4.8.4;
log-normal 4.4.5, 5.12.43;
multinormal 4.9.2, 5.8.6;
normal 4.9.3, 5, 4.14.1,
5.8.46, 5.12.23, 42, 7.11.19;
spectral 9.3.3; standard normal
4.7.5; Student’s ¢ 4.10.2-~3,
5.7.8; uniform 4.4.3, 4.5.4,
4.6.6, 4.7.1, 3, 4, 4.8.5, 4.11.1,
8, 4.14.4, 15, 19, 20, 23-26,
5.12.32, 7.11.4, 9.1.2, 9.7.5;
Weibull 4.4.7, 7.11.13

departure pr. 11.2.7, 11.7.2-4,
11.8.12

derangement 3.4.9

diagonal selection 6.4.5

dice 1.5.2, 3.2.4, 3.3.3, 6.1.2;
weighted/loaded 2.7.12,
5.12.36

difference eqns 1.8.20, 3.4.9,
5.2.5

difficult customers 11.7.4

diffusion: absorbing barrier
13.12.8—9; Bessel pr. 12.9.23,
13.3.5—6, 13.9.1; Ehrenfest
model 6.5.5, 36; first passage
13.4.2; It6 pr. 13.9.3; models
3.4.4, 6.5.5, 6.15.12, 36;
Omstein—Uhlenbeck pr.
13.3.4, 13.7.4-5, 13.12.34,
6; osmosis 6.15.36; reflecting
barrier 13.5.1, 13.12.6; Wiener
pr. 12.7.22—23; zeros 13.4.1,
13.12.10; Chapter 13 passim

diffusion approximation to
birth-death pr. 13.3.1

dimer problem 3.11.34

Dirichlet: density 4.14.58; distn
3.11.31

disasters 6.12.2—3, 6.15.28
discontinuous marginal 4.5.1
dishonest birth pr. 6.8.7

distribution: see also density;
arc sine 4.11.13; arithmetic
5.9.4; Benford 3.6.7; Bernoulli
3.11.14, 35; beta 4.11.4;
beta—binomial 4.6.5; binomial
2.1.3, 3.11.8, 11, 5.12.39;
bivariate normal 4.7.5—6, 12;
Cauchy 4.4.4; chi-squared
4.10.1; compound 5.2.3;
compound Poisson 5.12.13;
convergence 7.2.4; Dirichlet
3.11.31, 4.14.58; empirical
9.7.22; exponential 4.4.3,
5.12.39; F(r, s) 4.10.2-4;
extreme-value 4.1.1, 4.14.46,
7.11.13; first passage
§.10.7-8. 5.12.18-19; gamma
4.14.10-12; Gaussian, see
normal; geometric 3.1.1,
3.2.2, 3.7.5, 3.11.7, 5.12.34,
39, 6.11.4; hypergeometric
3.11.10-11; hypoexponential
4.8.4; infinitely divisible
5.12.13~14; inverse square
3.1.1; joint 2.5.4; lattice 5.7.5;
logarithmic 3.1.1, 5.2.3;
log-normal 4.4.5, 5.12.43;
maximum 4.2.2, 4.14.17;
median 2.7.11, 4.3.4, 7.3.11;
mixed 2.1.4, 2.3.4, 4.1.3;
modified Poisson 3.1.1;
moments 5.11.3; multinomial
3.5.1, 3.6.2; multinormal
4.9.2; negative binomial
3.8.4, 5.2.3, 5.12.4, 16;
negative hypergeometric 3.5.4;
non-central 5.7.7—8; normal
4.4.6, 8, 4.9.3-5; Poisson
3.1.1, 3.5.2~3, 3.11.6, 4.14.11,
§.2.3, 5.10.3, 5.12.8, 14, 17,
33, 37, 39, 7.11.18; spectral
9.3.2, 4; standard normal 4.7.5;
stationary 6.9.11; Student’s
t 4.10.2—3, 5.7.8; symmetric
3.2.5; tails 5.1.2, 5.6.4, 5.11.3;
tilted 5.7.11; trapezoidal 3.8.1;
trinormal 4.9.8-9; uniform
2.7.20, 3.7.5, 3.8.1, 5.1.6,
9.7.5; Weibull 4.4.7, 7.11.13;
zeta or Zipf 3.11.5

divergent birth pr. 6.8.7
divine providence 3.11.22

Dobrushin’s bound and ergodic
coefficient 6.14.4

dog-—flea model 6.5.5, 6.15.36

dominated convergence 5.6.3,
7.2.2

Doob’s Lz inequality 13.7.1

Doob-Kolmogorov inequality
7.8.1-2

432

Index

doubly stochastic: matrix 6.1.12,
6.15.2; Poisson pr. 6.15.22—23

downcrossings inequality 12.3.1

drift 13.3.3, 13.5.1, 13.8.9,
13.12.9

dual queue 11.5.2

duration of play 12.1.4

E

Eddington’s controversy 1.8.27

editors 6.4.1

eggs 5.12.13

Ehrenfest model 6.5.5, 6.15.36

eigenvector 6.6.1—2, 6.15.7

embarrassment 2.2.1

empires 6.15.10

empirical distn 9.7.22

entrance fee 3.3.4

entropy 7.5.1; conditional
6.15.45; mutual 3.6.5

epidemic 6.15.32

equilibrium, see stationary

equivalence class 7.1.1

ergodic: coefficient 6.14.4;
measure 9.7.11; stationary
measure 9.7.11

ergodic theorem: Markov chain
6.15.44, 7.11.32; Markov pr.
7.11.33, 10.5.1; stationary pr.
9.7.10-11, 13

Erlang’s loss formula 11.8.19

error 3.7.9; of prediction 9.2.2

estimation 2.2.3, 4.5.3, 4.14.9,
7TAL31

Euler: constant 5.12.27, 6.15.32;
product 5.12.34

European call option 13.10.4-5

event: of convergence 7.2.6;
exchangeable 7.3.4—5; invariant
9.5.1; sequence 1.8.16; tail
73.3

excess life 10.5.4; conditional
10.3.4; and current 10.6.9;
limit 10.3.3; Markov 8.3.2,
10.3.2; moments 10.3.3;
Poisson 6.8.3, 10.3.1, 10.6.9;
reversed 8.3.2; stationary
10.3.3

exchangeability 7.3.45

expectation: conditional
3.7,.2-3, 4.6.2, 4.14.12,
7.9.4; independent rv.s 7.2.3;
linearity 5.6.2; tail integral
4.3.3, 5; tail sum 3.11.13,
4.14.3

exponential distn: c.f. 5.8.8;
holding time 11.2.2; in Poisson
pr. 6.8.3; lack-of-memory
Index

property 4.14.5; limit in
branching pr. 5.6.2, 5.12.21,
6.7.1; limit of geometric distn
5.12.39; heavy traffic limit
11.6.1; distn of maximum
4.14.18; in Markov pr. 6.8.3,
6.9.9; order statistics 4.14.33;
sample from 4.14.48; sum
4.8.1, 4, 4.14.10, 5.12.50,
6.15.42
exponential martingale 13.3.9
exponential smoothing 9.7.2
extinction: of birth-death pr.
6.11.3, 6.15.25, 27, 12.9.10;
of branching pr. 6.7.2-3
extreme-value distn 4.1.1,
4.14.46, 5.12.34, 7.11.13; c.f.
and mean 5.12.27

F

Fi(r,s) distn 4.10.2, 4;
non-central 5.7.8

fair fee 3.3.4

families 1.5.7, 3.7.8

family, planning 3.11.30

Farkas’s theorem 6.6.2

filter 9.7.2

filtration 12.4.1-2, 7

fingerprinting 3.11.21

finite: Markov chain 6.5.8, 6.6.5,
6.15.43-44; stopping time
12.4.5; waiting room 11.8.1

first passage: c.f. 5.10.7-8;
diffusion pr. 13.4.2; distn
5.10.7-8, 5.12.18-19; Markov
chain 6.2.1, 6.3.6; Markov pr.
6.9.56; mean 6.3.7; m.g.f.
5.12.18; s.r.w. 5.3.8; Wiener
pr. 13.4.2

first visit by s.r.w. 3.10.1, 3

Fisher: spherical distn 4.14.36;
F-—Tippett~Gumbel distn
4.14.46

FKG inequality 3.11.18, 4.11.11

flip-flop 8.2.1

forest 6.15.30

Fourier: inversion thm 5.9.5;
series 9.7.15, 13.12.7

fourth moment strong law 7.11.6

fractional moments 3.3.5, 4.3.1

function, upper-class 7.6.1

functional eqn 4.14.5, 19

G
Galton’s paradox 1.5.8

gambler’s ruin 3.11.25—26, 12.1.4,
12.5.8

gambling: advice 3.9.4; systems
1.74

gamma distn 4.14.10-12, 5.8.3,
5.9.3, 5.10.3, 5.12.14, 33; g.
and Poisson 4.14.11; sample
from 4.11.3; sum 4.14.11

gamma fn 4.4.1, 5.12.34

gaps: Poisson 8.4.3, 10.1.2;
recurrent events 5.12.45;
renewal 10.1.2

Gaussian distn, see normal distn

Gaussian pr. 9.6.2-4, 13.12.2;
Markov 9.6.2; stationary 9.4.3;
white noise 13.8.5

generator 6.9.1

geometric Brownian motion,
Wiener pr. 13.3.9

geometric distn 3.1.1, 3.2.2,
3.7.5, 3.11.7, 5.12.34, 39;
lack-of-memory property
3.11.7; as limit 6.11.4; sample
from 4.11.8; sum 3.8.34

goat 1.4.5

graph: colouring 12.2.2; rw.
6.4.6, 9, 13.11.2-3

H
H4jek—Rényi—Chow inequality
12.9.4-5
Hall, Monty 1.4.5
Hastings algorithm 6.14.2
Hawaii 2.7.17
hazard rate 4.1.4, 4.4.7; technique
4.11.10
heat eqn 13.12.24
Heathrow 10.2.1
heavy traffic 11.6.1, 11.7.16
hen, see eggs
Hewitt~Savage zero—one law
7.3.4-5
hitting time 6.9.5—6; theorem
3.10.1, 5.3.8
Hoeffding’s inequality 12.2.1-2
Holder’s inequality 4.14.27
holding time 11.2.2
homogeneous Markov chain 6.1.1
honest birth—-death pr. 6.15.26
Hotelling’s theorem 4.14.59
house 4.2.1, 6.15.20, 51
hypergeometric distn 3.11.10-11
hypoexponential distn 4.8.4

I
idle period 11.5.2, 11.8.9
imbedding: jump chain 6.9.11,
Markov chain 6.9.12, 6.15.17;
queues: D/M/1 11.7.16; G/M/1

433

11.4.1, 3; M/G/1 11.3.1,
11.7.4; unsuccessful 6.15.17

immigration: birth-i. 6.8.5;
branching 5.4.5, 7.7.2; i—death
6.11.2, 6.15.18; with disasters
6.12.2-3, 6.15.28

immoral stimulus 1.2.1

importance sampling 4.11.12

inclusion-exclusion principle
1.3.4, 1.8.12

increasing sequence: of events
1.8.16; of rv.s 2.7.2

increments: independent 9.7.6,
16-17; orthogonal 7.7.1;
spectral 9.4.1, 3; stationary
9.7.17; of Wiener pr. 9.7.6

independence and symmetry 1.5.3

independent: conditionally 1.5.5;
continuous r.v.s 4.5.5, 4.14.6;
current and excess life 10.6.9;
customers 11.7.1; discrete
rv.s 3.11.1, 3; events 1.5.1;
increments 9.7.17; mean,
variance of normal sample
4.10.5, 5.12.42; normal distn
4.7.5; pairwise 1.5.2, 3.2.1,
5.1.7; set 3.11.40; triplewise
§.1.7

indicators and matching 3.11.17

inequality: bivariate normal
4.7.12; Bonferroni 1.8.37;
Boole 1.8.11; Cauchy—Schwarz
4.14.27; Chebyshov
7.11.9; Dobrushin 6.14.4;
Doob—Kolmogorov
7.8.1; Doob Lz 13.7.1;
downcrossings 12.3.1; FKG
3.11.18; Hajek—Rényi-Chow
12.9.4—5; Hoeffding 12.2.1-2;
Holder 4.14.27; Jensen 5.6.1,
7.9.4; Kolmogorov 7.8.12,
7.11.29-30; Kounias 1.8.38;
Lyapunov 4.14.28; maximal
12.4.3-4, 12.9.3, 5, 9; m.g.f.
5.8.2, 12.9.7; Minkowski
4.14.27; triangle 7.1.1, 3;
upcrossings 12.3.2

infinite divisibility 5.12.13-14

inner product 6.14.1, 7.1.2

inspection paradox 10.6.5

insurance 11.8.18, 12.9.12

integral: Monte Carlo 4.14.9;
normal 4.14.1; stochastic
9.7.19, 13.8.1-2

invariant event 9.5.1

inverse square distn 3.1.1

inverse transform technique 2.3.3

inversion theorem 5.9.5; c.f.
5.12.20
irreducible Markov pr. 6.15.15
iterated logarithm 7.6.1

Jté: formula 13.9.2; process
13.9.3

J
Jaguar 3.11.25
Jensen’s inequality 5.6.1, 7.9.4
joint: c.f. 5.12.30; density 2.7.20;
distn 2.5.4; mass fn 2.5.5;
moments 5.12.30; p.g.f. 5.1.3-5
jump chain: of M/M/1 11.2.6

K
key renewal theorem 10.3.3, 5,
10.6.11
Keynes, J. M. 3.9.6
knapsack problem 12.2.1
Kolmogorov: criterion 6.5.2;
inequality 7.8.1-2, 7.11.29-30
Korolyuk—Khinchin theorem 8.2.3
Kounias’s inequality 1.8.38
Krickeberg decomposition 12.9.11
Kronecker’s lemma 7.8.2,
7.11.30, 12.9.5
kurtosis 4.14.45

L

Ly inequality 13.7.1

Labouchere system 12.9.15

lack of anticipation 6.9.4

lack-of-memory property:
exponential distn 4.14.5;
geometric distn 3.11.7

ladders, see records

Lancaster’s theorem 4.14.38

large deviations 5.11.1-3, 12.9.7

last exits 6.2.1, 6.15.7

lattice distn 5.7.5

law: anomalous numbers 3.6.7;
arc sine 3.10.3, 3.11.28, 5.3.5;
De Morgan 1.2.1; iterated
logarithm 7.6.1; large numbers
2.2.2; strong 7.4.1, 7.8.2,
7.11.6, 9.7.10; unconscious
statistician 3.11.3; weak 7.4.1,
7.11.15, 20-21; zero—one
7345

Lebesgue measure 6.15.29,
13.12.16

left-continuous rw. 5.3.7, 5.12.7

level sets of Wiener pr. 13.12.16

Lévy metric 2.7.13, 7.1.4, 7.2.4

limit: binomial 3.11.10;
binomial—Poisson 5.12.39;
branching 12.9.8; central
limit theorem 5.10.1, 3, 9,

5.12.33, 40, 7.11.26, 10.6.3;
diffusion 13.3.1; distns 2.3.1;
events 1.8.16; gamma 5.9.3;
geometric—exponential 5.12.39;
lim inf 1.8.16; lim sup 1.8.16,
5.6.3, 7.3.2, 9~10, 12; local
5.9.2, 5.10.5—6; martingale
7.8.3; normal 5.12.41, 7.11.19;
Poisson 3.11.17; probability
1.8.16—18; rv. 2.7.2; uniform

linear dependence 3.11.15

linear fn of normal rv. 4.9.34
linear prediction 9.7.1, 3

local central limit theorem 5.9.2,

logarithmic distn 3.1.1, 5.2.3
log-convex 3.1.5
log-likelihood 7.11.31
log-normal r.v. 4.4.5, 5.12.43

Lyapunov’s inequality 4.14.28

machine 11.8.17

marginal: discontinuous 4.5.1;
multinomial 3.6.2; order
statistics 4.14.22

Markov chain in continuous
time: ergodic theorem 7.11.33,
10.5.1; first passage 6.9.5-6;
irreducible 6.15.15; jump chain
6.9.11; martingale 12.7.1;
mean first passage 6.9.6;
mean recurrence time 6.9.11;
renewal pr. 8.3.5; reversible
6.15.16, 38; stationary distn
6.9.11; two-state 6.9.1~2,
6.15.17; visits 6.9.9

Markov chain in discrete time:
absorbing state 6.2.2; bivariate
6.15.4; convergence 6.15.43;
dice 6.1.2; ergodic theorem
7.11.32; finite 6.5.8, 6.15.43;
first passages 6.2.1, 6.3.6;
homogeneous 6.1.1; imbedded
6.9.11, 6.15.17, 11.4.1; last
exits 6.2.1, 6.15.7; martingale
12.1.8, 12.3.3; mean first
passage 6.3.7; mean recurrence
time 6.9.11; persistent 6.4.10;
renewal 10.3.2; reversible
6.14.1; sampled 6.1.4, 6.3.8;
simulation of 6.14.3; stationary
distn 6.9.11; sum 6.1.8;
two-state 6.15.11, 17, 8.2.1;
visits 6.2.3~—5, 6.3.5, 6.15.5, 44

Markov—Kakutani theorem 6.6.1

Index

Markov process: Gaussian 9.6.2;
reversible 6.15.16

Markov property 6.1.5, 10; strong
6.1.6

Markov renewal, see Poisson pr.

Markov time, see stopping time

Markovian queue, see M/M/1

marriage problem 3.4.3, 4.14.35

martingale: backward 12.7.3;
birth-death 12.9.10;
branching pr. 12.1.3, 9,
12.9.1-2, 8, 20; casino
7.7.4; continuous parameter
12.7.1-2; convergence 7.8.3,
12.1.5, 12.9.6; de Moivre
12.1.4, 12.4.6; exponential
13.3.9; finite stopping time
12.4.5; gambling 12.1.4,
12.5.8; Markov chain 12.1.8,
12.3.3, 12.7.1; optional
stopping 12.5.1-8; orthogonal
increments 7.7.1; partial sum
12.7.3; patterns 12.9.16;
Poisson pr. 12.7.2; reversed
12.7.3; simple rw. 12.1.4,
12.4.6, 12.5.4-7; stopping
time 12.4.1, 5, 7; urn 7.11.27,
12.9.13-14; Wiener pr.
12.9.22-23

mass function, joint 2.5.5

matching 3.4.9, 3.11.17, 5.2.7,
12.9.21

matrix: covariance 3.11.15;
definite 4.9.1; doubly
stochastic 6.1.12, 6.15.2;
multiplication 4.14.63; square
root 4.9.1; stochastic 6.1.12,
6.14.1; sub-stochastic 6.1.12;
transition 7.11.31; tridiagonal
6.5.1, 6.15.16

maximal: coupling 4.12.4-6,
7.11.16; inequality 12.4.3—4,
12.9.3, 5, 9

maximum of: branching pr.
12.9.20; multinormal 5.9.7;
rw. 3.10.2, 3.11.28, 5.3.1,
6.1.3, 12.4.6; uniforms
5.12.32; Wiener pr. 13.12.8,
11, 15, 17

maximum ry. 4.2.2, 4.5.4,
4.14.17-18, 5.12.32, 7.11.14

mean: extreme-value 5.12.27;
first passage 6.3.7, 6.9.6;
negative binomial 5.12.4;
normal 4.4.6; recurrence time
6.9.11; waiting time 11.4.2,
11.8.6, 10

measure: ergodic 9.7.11-12;
Lebesgue 6.15.29, 13.12.16;

Index

stationary 9.7.11-12; strongly

mixing 9.7.12
median 2.7.11, 4.3.4, 7.3.11
ménages 1.8.23
Mercator projection 6.13.5
meteorites 9.7.4

metric 2.7.13, 7.1.4; Lévy 2.7.13,

7.1.4, 7.2.4; total variation
2.7.13
m.g.f. inequality 5.8.2, 12.9.7
migration pr., open 11.7.1, 5
millionaires 3.9.4
Mills’s ratio 4.4.8, 4.14.1
minimal, solution 6.3.6—7, 6.9.6
Minkowski’s inequality 4.14.27
misprints 6.4.1
mixing, strong 9.7.12
mixture 2.1.4, 2.3.4, 4.1.3, 5.1.9
modified renewal 10.6.12

moments: branching pr. 5.4.1;
fractional 3.3.5, 4.3.1;
generating fn 5.1.8, 5.8.2,

5.11.3; joint 5.12.30; problem
5.12.43; renewal pr. 10.1.1; tail

integral 4.3.3, 5.11.3
Monte Carlo 4.14.9
Monty Hall 1.4.5
Moscow 11.8.3

moving average 8.7.1, 9.1.3,
9.4.2, 9.5.3, 9.7.1-2, 7;
spectral density 9.7.7

multinomial distn 3.5.1; marginals

3.6.2; p.g.f. 5.1.5

multinormal distn 4.9.2; c.f.
5.8.6; conditioned 4.9.6-7;
covariance matrix 4.9.2;
maximum 5.9.7; sampling
from 4.14.62; standard 4.9.2;
transformed 4.9.3, 4.14.62

Murphy’s law 1.3.2

mutual information 3.6.5

N

needle, Buffon’s 4.5.2,
4.14.31-32

negative binomial distn 3.8.4;
bivariate 5.12.16; moments
5.12.4; p.gf. 5.1.1, 5.12.4

negative hypergeometric distn
3.5.4

Newton, I. 3.8.5

non-central distn 5.7.7-8

non-homogeneous: birth pr.
6.15.24; Poisson pr. 6.13.7,
6.15.19-20

noodle, Buffon’s 4.14.31

norm 7.1.1, 7.9.6; equivalence

class 7.1.1; mean square 7.9.6;
rth mean 7.1.1

normal distn 4.4.6; bivariate

4.7.5—6, 12; central limit
theory 5.10.1, 3, 9, 5.12.23,
40; characterization of 5.12.23;
cumulants 5.7.4; limit 7.11.19;
linear transfomations 4.9.3--4;
Mills’s ratio 4.4.8, 4.14.1;
moments 4.4.6, 4.14.1;
multivariate 4.9.2, 5.8.6;
regression 4.8.7, 4.9.6, 4.14.13,
7.9.2; sample 4.10.5, 5.12.42;
simulation of 4.11.7, 4.14.49;
square 4.14.12; standard 4.7.5;

sum 4.9.3; sum of squares
4.14.12; trivariate 4.9.8-9;
uncorrelated 4.8.6

normal integral 4.14.1

normal number theorem 9.7.14

now 6.15.50

O

occupation time for Wiener pr.
13.12.20

open migration 11.7.1, 5

optimal: packing 12.2.1; price
12.9.19; reset time 13.12.22;
serving 11.8.13

optimal stopping: dice 3.3.8—9;
marriage 4.14.35

optional stopping 12.5.1-8,
12.9.19; diffusion 13.4.2;
Poisson 12.7.2

order statistics 4.14.21;
exponential 4.14.33; general
4.14.21; marginals 4.14.22;
uniform 4,14.23~24, 39,
6.15.42, 12.7.3

Ornstein—Uhlenbeck pr. 9.7.19,

13.3.4, 13.7.4-5, 13.12.3-4, 6;

reflected 13.12.6

orthogonal: increments 7.7.1;
polynomials 4.14.37

osmosis 6.15.36

Pp
pairwise independent: events
1.5.2; nv.s 3.2.1, 3.3.3, 5.1.7
paradox: Bertrand 4.14.8;
Borel 4.6.1; Carroll 1.4.4;
Galton 1.5.8; inspection
10.6.5; Parrando 6.15.48; St
Petersburg 3.3.4; voter 3.6.6
parallel lines 4.14.52
parallelogram 4.14.60; property
7.1.2;

435

parking 4.14.30

Parrando’s paradox 6.15.48

particle 6.15.33

partition of sample space 1.8.10

Pasta property 6.9.4

patterns 1.3.2, 5.2.6, 5.12.2,
10.6.17, 12.9.16

pawns 2.7.18

Pepys’s problem 3.8.5

periodic state 6.5.4, 6.15.3

persistent: chain 6.4.10, 6.15.6;
rw. 5.12.5-6, 6.3.2, 6.9.8,
7.3.3, 13.11.2; state 6.2.3-4,
6.9.7

Petersburg, see St Petersburg

pig 1.8.22

points, problem of 3.9.4, 3.11.24

Poisson: approximation 3.11.35;
coupling 4.12.2; flips 3.5.2,
5.12.37; sampling 6.9.4

Poisson distn 3.5.3;
characterization of 5.12.8, 15;
compound 3.8.6, 5.12.13; and
gamma distn 4.14.11; limit of
binomial 5.12.39; modified
3.1.1; sum 3.11.6, 7.2.10

Poisson pr. 6.15.29; age 10.5.4;
arrivals 8.7.4, 10.6.8;
autocovariance 7.11.5,
9.6.1; characterization
6.15.29, 9.7.16; colouring
theorem 6.15.39; compound
6.15.21; conditional property
6.13.6; continuity in m.s.
7.11.5; covariance 7.11.5;
differentiability 7.11.5;
doubly stochastic 6.15.22—23;
excess life 6.8.3, 10.3.1;
forest 6.15.30; gaps 10.1.2;
Markov renewal pr. 8.3.5,
10.6.9; martingales 12.7.2;
non-homogeneous 6.13.7,
6.15.19-20; optional stopping
12.7.2; perturbed 6.15.39-40;
renewal 8.3.5, 10.6.9-10;
Rényi’s theorem 6.15.39;
repairs 11.7.18; sampling
6.9.4; spatial 6.15.30-31,
7.4.3; spectral density
9.7.6; sphere 6.13.3-4;
stationary increments 9.7.6,
16; superposed 6.8.1; thinned
6.8.2; total life 10.6.5; traffic
6.15.40, 49, 8.4.3

poker 1.8.33; dice 1.8.34

Pélya’s urn 12.9.13-14

portfolio 13.12.23; self-financing
13.10.2~3

positive definite 9.6.1, 4.9.1

positive state, see non-null

postage stamp lemma 6.3.9

potential theory 13.11.1-3

power series approximation
71117

Pratt’s lemma 7.10.5

predictable step fn 13.8.4

predictor: best 7.9.1; linear 7.9.3,

9.2.1-2, 9.7.1, 3
probabilistic method 1.8.28, 3.4.7
probability: continuity 1.8.16,

1.8.18; p.g.f. 5.12.4, 13; vector
4.11.6

problem: matching 3.4.9, 3.11.17,

5.2.7, 12.9.21; ménages 1.8.23;
Pepys 3.8.5; of points 3.9.4,

exchange 11.8.9; two servers
8.4.1, 5, 11.7.3, 11.8.14;
virtual waiting 11.8.7; waiting
time 11.2.3, 11.5.2-3, 11.8.6,
8, 10

quotient 3.3.1, 4.7.2, 10, 13-14,
4.10.4, 4.11.10, 4.14.11, 14,
16, 40, 5.2.4, 5.12.49, 6.15.42

R
radioactivity 10.6.6-8
random: bias 5.10.9; binomial

coefficient 5.2.1; chord
4.13.1; dead period 10.6.7;
harmonic series 7.11.37;

Index

orthogonal 7.7.1; p.g.f. 5.12.4,
13; Poisson 3.5.3; standard
normal 4.7.5; Student’s ¢
4.10.2-3, 5.7.8; symmetric
3.2.5, 4.1.2, 5.12.22; tails
3.11.13, 4.3.3, 5, 4.14.3, 5.1.2,
5.6.4, 5.11.3; tilted 5.1.9,
5.7.11; trivial 3.11.2; truncated
2.4.2; uncorrelated 3.11.12, 16,
4.5.7-8, 4.8.6; uniform 3.8.1,
4.8.4, 4.11.1, 9.7.5; waiting
time, see queue; Weibull 4.4.7;

zeta or Zipf 3.11.5

random walk: absorbed 3.11.39,
12.5.4—5; arc sine laws
3.10.3, 3.11.28, 5.3.5; on

integers 6.15.34; line 4,13.1-3,

3.11.24; Waldegrave 5.12.10
program, dual, linear, and primal
6.6.3
projected rw. 5.12.6
projection theorem 9.2.10
proof-reading 6.4.1
proportion, see empirical ratio
proportional investor 13.12.23
prosecutor’s fallacy 1.4.6
protocol 1.4.5, 1.8.26
pull-through property 3.7.1

Q

quadratic variation 8.5.4, 13.7.2
queue: batch 11.8.4; baulking

8.4.4, 11.8.2, 19; busy period
6.12.1, 11.3.2-3, 11.5.1,
11.8.5, 9; costs 11.8.13;
departure pr. 11.2.7, 11.7.2-4,
11.8.12; difficult customer
11.7.4; D/M/1 11.4.3, 11.8.15;
dual 11.5.2; Erlang’s loss fn
11.8.19; finite waiting room
11.8.1; G/G/1 11.5.1, 11.8.8;
G/M/1 11.4.1-2, 11.5.2-3;
heavy traffic 11.6.1, 11.8.15;
idle period 11.5.2, 11.8.9;
imbedded branching 11.3.2,
11.8.5, 11; imbedded Markov
pr. 11.2.6, 11.4.1, 3, 11.4.1;
imbedded renewal 11.3.3,
11.5.1; imbedded rw. 11.2.2,
5; Markov, see M/M/1; M/D/1
11.3.1, 11.8.10-11; M/G/1
11.3.3, 11.8.6-7; M/G/oo
6.12.4, 11.8.9; migration
system 11.7.1, 5; M/M/1 6.9.3,
6.12.1, 11.2.2-3, 5-6, 11.3.2,
11.6.1, 11.8.5, 12; M/M/k
11.7.2, 11.8.13; M/M/oo 8.7.4;
series 11.8.3, 12; supermarket
11.8.3; tandem 11.2.7, 11.8.3;
taxicabs 11.8.16; telephone

4.14.52; paper 4.14.56;
parameter 4.6.5, 5.1.6,

5.2.3, 5.2.8; particles 6.4.8;
pebbles 4.14.51; permutation
4.11.2; perpendicular 4.14.50;
polygons 4.13.10, 6.4.9; rock
4.14.57; rods 4.14.25-26,
53-54; sample 4.14.21;
subsequence 7.11.25; sum
3.7.6, 3.8.6, 5.2.3, 5.12.50,

10.2.2; telegraph 8.2.2; triangle

4.5.6, 4.13.6-8, 11, 13;
velocity 6.15.33, 40

random sample: normal 4.10.5;
ordered 4.12.21

random variable: see also density

and distribution; arc sine
4.11.13; arithmetic 5.9.4;
Bernoulli 3.11.14, 35; beta
4.11.4; beta—binomial 4.6.5;
binomial 2.1.3, 3.11.8, 11,
5.12.39; bivariate normal
4.7.5-6, 12; Cauchy 4.4.4;
c.f. 5.12.26-31; chi-squared
4.10.1; compounding

5.2.3; continuous 2.3.1;
Dirichlet 3.11.31, 4.14.58;
expectation 5.6.2, 7.2.3;
exponential 4.4.3, 5.12.39;
extreme-value 4.1.1, 4.14.46;
F(r,s) 4.10.2, 4, 5.7.8;
gamma 4.11.3, 4.14.10-12;
geometric 3.1.1, 3.11.7;
hypergeometric 3.11.10-11;
independent 3.11.1, 3, 4.5.5,
7.2.3; indicator 3.11.17;
infinitely divisible 5.12.13-14;
logarithmic 3.1.1, 5.2.3;
log-normal 4.4.5; median
2.7.11, 4.3.4; m.g.f. 5.1.8,
5.8.2, 5.11.3; multinomial
3.5.1; multinormal 4.9.2;
negative binomial 3.8.4;
normal 4.4.6, 4.7.5—6, 12;

436

binary tree 6.4.7; conditional
3.9,2-3; on cube 6.3.4;

first passage 5.3.8; first

visit 3.10.3; on graph 6.4.6,

9, 13.11.2-3; on hexagon
6.15.35; imbedded in queue
11.2.2, 5; left-continuous 5.3.7,
5.12.7; martingale 12.1.4,
12.4.6, 12.5.4-5; maximum
3.10.2, 3.11.28, 5.3.1;
persistent 5.12.5—6, 6.3.2;
potentials 13.11.2~3; projected
5.12.6; range of 3.11.27;

reflected 11.2.1-2; retaining
barrier 11.2.4; returns to origin
3.10.1, 5.3.2; reversible 6.5.1;
simple 3.9.1~3, 5, 3.10.1-3; on
square 5.3.3; symmetric 1.7.3,
3.11.23; three dimensional
6.15.9-10; transient 5.12.44,
6.15.9, 7.5.3; truncated 6.5.7;
two dimensional 5.3.4, 5.12.6,
12.9.17; visits 3.11.23, 6.9.8,
10; zero mean 7.5.3; zeros of
3.10.1, 5.3.2, 5.12.56

range of r.w. 3.11.27

rate of convergence 6.15.43

ratios 4.3.2; Mills’s 4.4.8; sex
3.11.22

record: times 4.2.1, 4, 4.6.6, 10;
values 6.15.20, 7.11.36

recurrence, see difference

recurrence time 6.9.11

recurrent: event 5.12.45, 7.5.2,
9.7.4; see persistent

red now 12.9.18

reflecting barrier: s.r.w. 11.2.1-2;
drifting Wiener pr. 13.5.1;
Ornstein—Uhlenbeck pr.
13.12.6

regeneration 11.3.3

regression 4.8.7, 4.9.6, 4.14.13,
79.2

rejection method 4.11.3-4, 13
- Index

reliability 3.4.5-6, 3.11.18-20,

renewal: age, see current life;
alternating 10.5.2, 10.6.14;
asymptotics 10.6.11; Bernoulli
8.7.3; central limit theorem
10.6.3; counters 10.6.6-8,
15; current life 10.3.2,
10.5.4, 10.6.4; delayed
10.6.12; excess life 8.3.2,
10.3.1-4, 10.5.4; r. function
10.6.11; gaps 10.1.2; key r.
theorem 10.3.3, 5, 10.6.11;
Markov 8.3.5; m.g.f. 10.1.1;
moments 10.1.1; Poisson
8.3.5, 10.6.9-10; r. process
8.3.4; r—reward 10.5.1-4;
r. sequence 6.15.8, 8.3.1, 3;
stationary 10.6.18; stopping
time 12.4.2; sum/superposed
10.6.10; thinning 10.6.16

Rényi’s theorem 6.15.39

repairman 11.7.18

repulsion 1.8.29

reservoir 6.4.3

resources 6.15.47

retaining barrier 11.2.4

reversible: birth—death pr.
6.15.16; chain 6.14.1; Markov
pr. 6.15.16, 38; queue
11.7.2-3, 11.8.12, 14; nw.
6.5.1

Riemann—Lebesgue lemma 5.7.6

robots 3.7.7

rods 4,.14.25~26,

ruin 11.8.18, 12.9.12; see also
gambler’s ruin

runs 1.8.21, 3.4.1, 3.7.10, 5.12.3,
46-47

S

o-field 1.2.2, 4, 1.8.3, 9.5.1,
9.7.13; increasing sequence
of 12.4.7

St John’s College 4.14.51

St Petersburg paradox 3.3.4

sample: normal 4.10.5, 5.12.42;
ordered 4.12.21

sampling 3.11.36; Poisson 6.9.4;
with and without replacement
3.11.10

sampling from distn: arc sine
4.11.13; beta 4.11.4-5; Cauchy
4.11.9; exponential 4.14.48;
gamma 4.11.3; geometric
4.11.8; Markov chain 6.14.3;
multinormal 4.14.62; normal
4.11.7, 4.14.49; s.rw. 4.11.6;
uniform 4.11.1

secretary problem 3.11.17,
4.14.35

self-financing portfolio 13.10.2~3

semi-invariant 5.7.34

sequence: of c.f.s 5.12.35; of
distns 2.3.1; of events 1.8.16;
of heads and tails, see pattern;
renewal 6.15.8, 8.3.1, 3; of
Lvs 2.7.2

series of queues 11.8.3, 12

shift operator 9.7.11-12

shocks 6.13.6

shorting 13.11.2

simple birth pr. 6.8.4-5, 6.15.23

simple birth-death pr.:
conditioned 6.11.4—5; diffusion
approximation 13.3.1;
extinction 6.11.3, 6.15.27;
visits 6.11.6—7

simple immigration—death pr.
6.11.2, 6.15.18

simple: process 8.2.3; rw.
3.9.1-3, 5, 3.10.1-3, 3.11.23,
27-29, 11.2.1, 12.1.4, 12.4.6,
12.5.4-7

simplex 6.15.42; algorithm
3.11.33

simulation, see sampling

sixes 3.2.4

skewness 4.14.44

sleuth 3.11.21

Slutsky’s theorem 7.2.5

smoothing 9.7.2

snow 1.7.1

space, vector 2.7.3, 3.6.1

span of rv. 5.7.5, 5.9.4

Sparre Andersen theorem
13.12.18

spectral: density 9.3.3;
distribution 9.3.2, 4, 9.7.2-7;
increments 9.4.1, 3

spectrum 9.3.1

sphere 1.8.28, 4.6.1, 6.13.3-4,
12.9.23, 13.11.1; empty
6.15.31

squeezing 4.14.47

standard: bivariate normal
4.7.5; multinormal 4.9.2;
normal 4.7.5; Wiener pr. 9.6.1,
9.7.18~21, 13.12.1-3

state: absorbing 6.2.2; persistent
6.2.3-4; symmetric 6.2.5;
transient 6.2.4

stationary distn 6.9.1, 3-4, 11-12;
6.11.2; birth-death pr. 6.11.4;
current life 10.6.4; excess
life 10.3.3; Markov chain
9.1.4; open migration 11.7.1,

437

5; queue length 8.4.4, 8.7.4,
11.2.1-2, 6, 11.4.1, 11.5.2,
and Section 11.8 passim; r.w.
11.2.1-2; waiting time 11.2.3,
11.5.2-3, 11.8.8

stationary excess life 10.3.3

stationary increments 9.7.17

stationary measure 9.7.11-12

stationary renewal pr. 10.6.18,

Stirling’s formula 3.10.1, 3.11.22,
5.9.6, 5.12.5, 6.15.9, 7.11.26

stochastic: integral 9.7.19,
13.8.1-2; matrix 6.1.12,
6.14.1, 6.15.2; ordering
4.12.1-2

stopping time 6.1.6, 10.2.2,
12.4.1-2, 5, 7; for renewal pr.
12.4.2

strategy 3.3.8-9, 3.11.25, 4.14.35,
6.15.50, 12.9.19, 13.12.22

strong law of large numbers
TAA, 7.5.1-3, 7.8.2, 7.11.6,
9.7.10

strong Markov property 6.1.6

strong mixing 9.7.12

Student’s ¢ distn 4.10.23;
non-central 5.7.8

subadditive fn 6.15.14, 8.3.3

subgraph 13.11.3

sum of independent r.v.s:
Bernoulli 3.11.14, 35; binomial
3.11.8, 11; Cauchy 4.8.2,
5.11.4, 5.12.24-25; chi-squared
4.10.1, 4.14.12; exponential
4.8.1, 4, 4.14.10, 5.12.50,
6.15.42; gamma 4.14.11;
geometric 3.8.34; normal
4.9.3; p.gf. 5.12.1; Poisson
3.11.6, 7.2.10; random 3.7.6,
3.8.6, 5.2.3, 5.12.50, 10.2.2;
renewals 10.6.10; uniform
3.8.1, 4.8.5

sum of Markov chains 6.1.8

supercritical branching 6.7.2

supermartingale 12.1.8

superposed: Poisson pr. 6.8.1;
renewal pr. 10.6.10

sure thing principle 1.7.4

survival 3.4.3, 4.1.4

Sylvester’s problem 4.13.12,
4.14.60

symmetric: rv. 3.2.5, 4.1.2,
5.12.22; rw. 1.7.3; state 6.2.5

symmetry and independence 1.5.3

system 7.7.4; Labouchere 12.9.15

T

t, Student’s 4.10.2~3; non-central
5.7.8

tail: c.f. 5.7.6; equivalent 7.11.34;
event 7.3.3, 5; function 9.5.3;
integral 4.3.3, 5; sum 3.11.13,
4.143

tail of distn: and moments 5.6.4,
5.11.3; p.g.f. 5.1.2

tandem queue 11.2.7, 11.8.3
taxis 11.8.16
telekinesis 2.7.8

telephone: exchange 11.8.9; sales
3.11.38

testimony 1.8.27
thinning 6.8.2; renewal 10.6.16

three-dimensional r.w. 6.15.10;
transience of 6.15.9,

three-dimensional Wiener pr.
13.11.1

three series theorem 7.11.35

tied-down Wiener pr. 9.7.21-22,
13.6.2-5

tilted distn 5.1.9, 5.7.11
time-reversibility 6.5.1-3, 6.15.16
total life 10.6.5

total variation distance 4.12.34,
7.2.9, 7.11.16

tower property 3.7.1, 4.14.29

traffic: gaps 5.12.45, 8.4.3;
heavy 11.6.1, 11.8.15; Poisson
6.15.40, 49, 8.4.3

transform, inverse 2.3.3

transient: rw. 5.12.44, 7.5.3;
Wiener pr. 13.11.1

transition matrix 7.11.31
transitive coins 2.7.16
trapezoidal distn 3.8.1

trial, de Moivre 3.5.1

triangle inequality 7.1.1, 3
Trinity College 12.9.15
triplewise independent 5.1.7
trivariate normal distn 4.9.8~9
trivial r.v. 3.11.2

truncated: rv. 2.4.2; rw. 6.5.7
Turdan’s theorem 3.11.40
turning time 4.6.10

two-dimensional: rw. 5.3.4,
5.12.6, 12.9.17; Wiener pr.
13.12.1214

two server queue 8.4.1, 5, 11.7.3,
11.8.14

two-state Markov chain 6.15.11,
17, 8.2.1; Markov pr. 6.9.1-2,
6.15.16~17

Type: T. one counter 10.6.6~7; T.
two counter 10.6.15

U

U My8aka 8.7.7

unconscious statistician 3.11.3

uncorrelated rv. 3.11.12, 16,
4.5.7-8, 4.8.6

uniform integrability: Section
7.10 passim, 10.2.4, 12.5.1-2

uniform distn 3.8.1, 4.8.4, 4.11.1,
9.7.5; maximum 5,12.32;
order statistics 4.14.23-24, 39,
6.15.42; sample from 4.11.1;
sum 3.8.1, 4.8.4

uniqueness of conditional
expectation 3.7.2

upcrossings inequality 12.3.2

upper class fn 7.6.1

ums 1.4.4, 1.8.24-25, 3.4.2,
4, 6.3.10, 6.15.12; Pélya’s
12.9.13-14

Vv

value function 13.10.5

variance: branching pr. 5.12.9;
conditional 3.7.4, 4.6.7; normal
4.4.6

vector space 2.7.3, 3.6.1

Vice-Chancellor 1.3.4, 1.8.13

virtual waiting 11.8.7

visits: birth—death pr. 6.11.6-7;
Markov chain 6.2.3—5, 6.3.5,
6.9.9, 6.15.5, 44; nw. 3.11.23,
29, 6.9.8, 10

voter paradox 3.6.6

Ww

waiting room 11.8.1, 15
waiting time: dependent 11.2.7;
for a gap 10.1.2; in G/G/1

11.5.2, 11.8.8; in G/M/1

Index

11.4.2; in M/D/1 11.8.10;

in M/G/1 11.8.6; in M/M/1
11.2.3; stationary distn 11.2.3,
11.5.2-3; virtual 11.8.7

Wald’s eqn 10.2.2~3
Waldegrave’s problem 5.12.10
Waring’s theorem 1.8.13, 5.2.1

weak law of large numbers 7.4.1,
7.11.15, 20-21

Weibull distn 4.4.7, 7.11.13
Weierstrass’s theorem 7.3.6
white noise, Gaussian 13.8.5

Wiener process: absorbing
barriers 13.13.8, 9; arc
sine laws 13.4.3, 13.12.10,
13.12.19; area 12.9.22;
Bartlett eqn 13.3.3; Bessel
pr. 13.3.5; on circle 13.9.4;
conditional 8.5.2, 9.7.21,
13.6.1; constructed 13.12.7;
d-dimensional 13.7.1; drift
13.3.3, 13.5.1; on ellipse
13.9.5; expansion 13.12.7;
first passage 13.4.2; geometric
13.3.9, 13.4.1; hits sphere
13.11.1; hitting barrier
13.4.2; integrated 9.7.20,
12.9.22, 13.3.8, 13.8.1-2; level
sets 13.12.16; martingales
12.9,.22~23, 13.3.8-9;
maximum 13.12.8, 11, 15,
17; occupation time 13.12.20;
quadratic variation 8.5.4,
13.7.2; reflected 13.5.1; sign
13.12.21; standard 9.7.18~21;
three-dimensional 13.11.1;
tied-down, see Brownian
bridge; transformed 9.7.18,
13.12.1, 3; two-dimensional
13.12.12-14; zeros of 13.4.3,
13.12.10

Wiener—Hopf eqn 11.5.3, 11.8.8

xX

X-ray 4.14.32

Z

zero—one law, Hewitt-Savage
7.3.45

438

This is a revised, updated and greatly expanded version of the authors’ Probability
and Random Processes: Problems and Solutions, tirst published in 1992. The 1000+
kercises contained within are not merely drill problems but have been chosen to
illustrate the concepts, illuminate the subject and both inform and entertain the
student. A broad range of subjects is covered, including: elementary aspects of
probability and random variables; sampling; Markov chains; convergence; Station
ary processes; renewals; queues; martingales; diffusion; mathematical finance and
the Black-Scholes model. A bibliography and extensive index conclude the volume

This text is intended to serye students as a companion for elementary, intermedi-
ate or advanced courses in probability and random processes. It will also be useful
for anyone needing a source for large numbers of problems in these areas, In par
ticular this text acts as a companion to the authors’ Probability and Random Processe:
Third Edition (OUP, 2001).

A wealth of intcresting teaching material at all levels
PA. L. Embrpwhts Shorr Book Ret itwy

ALSO PUBLISHED BY OXFORD UNIVERSITY PRESS

Probability: An Introduction
Geoffrey Grimmett and Dominic Welsh
Codes and Cryptography

Dominic Welsh

Introduction to Algebra

Peter J, Cameron

OXFORD

UNIVERSITY PRESS

www.oup.com

